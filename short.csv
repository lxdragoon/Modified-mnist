,repo,path,url,code,code_tokens,docstring,docstring_tokens,language,partition,lower_docstring_tokens,doc,lower_code_tokens,code1
0,soimort/you-get,src/you_get/extractors/youtube.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/youtube.py#L135-L143,"def get_vid_from_url(url):
        """"""Extracts video ID from URL.
        """"""
        return match1(url, r'youtu\.be/([^?/]+)') or \
          match1(url, r'youtube\.com/embed/([^/?]+)') or \
          match1(url, r'youtube\.com/v/([^/?]+)') or \
          match1(url, r'youtube\.com/watch/([^/?]+)') or \
          parse_query_param(url, 'v') or \
          parse_query_param(parse_query_param(url, 'u'), 'v')","['def', 'get_vid_from_url', '(', 'url', ')', ':', 'return', 'match1', '(', 'url', ',', ""r'youtu\\.be/([^?/]+)'"", ')', 'or', 'match1', '(', 'url', ',', ""r'youtube\\.com/embed/([^/?]+)'"", ')', 'or', 'match1', '(', 'url', ',', ""r'youtube\\.com/v/([^/?]+)'"", ')', 'or', 'match1', '(', 'url', ',', ""r'youtube\\.com/watch/([^/?]+)'"", ')', 'or', 'parse_query_param', '(', 'url', ',', ""'v'"", ')', 'or', 'parse_query_param', '(', 'parse_query_param', '(', 'url', ',', ""'u'"", ')', ',', ""'v'"", ')']",Extracts video ID from URL.,"['Extracts', 'video', 'ID', 'from', 'URL', '.']",python,test,"['extracts', 'video', 'id', 'from', 'url', '.']",extracts video id from url .,"['def', 'get_vid_from_url', '(', 'url', ')', ':', 'return', 'match1', '(', 'url', ',', ""r'youtu\\.be/([^?/]+)'"", ')', 'or', 'match1', '(', 'url', ',', ""r'youtube\\.com/embed/([^/?]+)'"", ')', 'or', 'match1', '(', 'url', ',', ""r'youtube\\.com/v/([^/?]+)'"", ')', 'or', 'match1', '(', 'url', ',', ""r'youtube\\.com/watch/([^/?]+)'"", ')', 'or', 'parse_query_param', '(', 'url', ',', ""'v'"", ')', 'or', 'parse_query_param', '(', 'parse_query_param', '(', 'url', ',', ""'u'"", ')', ',', ""'v'"", ')']","def get_vid_from_url ( url ) : return match1 ( url , r'youtu\.be/([^?/]+)' ) or match1 ( url , r'youtube\.com/embed/([^/?]+)' ) or match1 ( url , r'youtube\.com/v/([^/?]+)' ) or match1 ( url , r'youtube\.com/watch/([^/?]+)' ) or parse_query_param ( url , 'v' ) or parse_query_param ( parse_query_param ( url , 'u' ) , 'v' )"
1,soimort/you-get,src/you_get/extractors/miomio.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/miomio.py#L41-L51,"def sina_xml_to_url_list(xml_data):
    """"""str->list
    Convert XML to URL List.
    From Biligrab.
    """"""
    rawurl = []
    dom = parseString(xml_data)
    for node in dom.getElementsByTagName('durl'):
        url = node.getElementsByTagName('url')[0]
        rawurl.append(url.childNodes[0].data)
    return rawurl","['def', 'sina_xml_to_url_list', '(', 'xml_data', ')', ':', 'rawurl', '=', '[', ']', 'dom', '=', 'parseString', '(', 'xml_data', ')', 'for', 'node', 'in', 'dom', '.', 'getElementsByTagName', '(', ""'durl'"", ')', ':', 'url', '=', 'node', '.', 'getElementsByTagName', '(', ""'url'"", ')', '[', '0', ']', 'rawurl', '.', 'append', '(', 'url', '.', 'childNodes', '[', '0', ']', '.', 'data', ')', 'return', 'rawurl']","str->list
    Convert XML to URL List.
    From Biligrab.","['str', '-', '>', 'list', 'Convert', 'XML', 'to', 'URL', 'List', '.', 'From', 'Biligrab', '.']",python,test,"['str', '-', '>', 'list', 'convert', 'xml', 'to', 'url', 'list', '.', 'from', 'biligrab', '.']",str - > list convert xml to url list . from biligrab .,"['def', 'sina_xml_to_url_list', '(', 'xml_data', ')', ':', 'rawurl', '=', '[', ']', 'dom', '=', 'parsestring', '(', 'xml_data', ')', 'for', 'node', 'in', 'dom', '.', 'getelementsbytagname', '(', ""'durl'"", ')', ':', 'url', '=', 'node', '.', 'getelementsbytagname', '(', ""'url'"", ')', '[', '0', ']', 'rawurl', '.', 'append', '(', 'url', '.', 'childnodes', '[', '0', ']', '.', 'data', ')', 'return', 'rawurl']",def sina_xml_to_url_list ( xml_data ) : rawurl = [ ] dom = parsestring ( xml_data ) for node in dom . getelementsbytagname ( 'durl' ) : url = node . getelementsbytagname ( 'url' ) [ 0 ] rawurl . append ( url . childnodes [ 0 ] . data ) return rawurl
2,soimort/you-get,src/you_get/extractors/fc2video.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/fc2video.py#L11-L17,"def makeMimi(upid):
    """"""From http://cdn37.atwikiimg.com/sitescript/pub/dksitescript/FC2.site.js
    Also com.hps.util.fc2.FC2EncrptUtil.makeMimiLocal
    L110""""""
    strSeed = ""gGddgPfeaf_gzyr""
    prehash = upid + ""_"" + strSeed
    return md5(prehash.encode('utf-8')).hexdigest()","['def', 'makeMimi', '(', 'upid', ')', ':', 'strSeed', '=', '""gGddgPfeaf_gzyr""', 'prehash', '=', 'upid', '+', '""_""', '+', 'strSeed', 'return', 'md5', '(', 'prehash', '.', 'encode', '(', ""'utf-8'"", ')', ')', '.', 'hexdigest', '(', ')']","From http://cdn37.atwikiimg.com/sitescript/pub/dksitescript/FC2.site.js
    Also com.hps.util.fc2.FC2EncrptUtil.makeMimiLocal
    L110","['From', 'http', ':', '//', 'cdn37', '.', 'atwikiimg', '.', 'com', '/', 'sitescript', '/', 'pub', '/', 'dksitescript', '/', 'FC2', '.', 'site', '.', 'js', 'Also', 'com', '.', 'hps', '.', 'util', '.', 'fc2', '.', 'FC2EncrptUtil', '.', 'makeMimiLocal', 'L110']",python,test,"['from', 'http', ':', '//', 'cdn37', '.', 'atwikiimg', '.', 'com', '/', 'sitescript', '/', 'pub', '/', 'dksitescript', '/', 'fc2', '.', 'site', '.', 'js', 'also', 'com', '.', 'hps', '.', 'util', '.', 'fc2', '.', 'fc2encrptutil', '.', 'makemimilocal', 'l110']",from http : // cdn37 . atwikiimg . com / sitescript / pub / dksitescript / fc2 . site . js also com . hps . util . fc2 . fc2encrptutil . makemimilocal l110,"['def', 'makemimi', '(', 'upid', ')', ':', 'strseed', '=', '""ggddgpfeaf_gzyr""', 'prehash', '=', 'upid', '+', '""_""', '+', 'strseed', 'return', 'md5', '(', 'prehash', '.', 'encode', '(', ""'utf-8'"", ')', ')', '.', 'hexdigest', '(', ')']","def makemimi ( upid ) : strseed = ""ggddgpfeaf_gzyr"" prehash = upid + ""_"" + strseed return md5 ( prehash . encode ( 'utf-8' ) ) . hexdigest ( )"
3,soimort/you-get,src/you_get/extractors/fc2video.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/fc2video.py#L46-L57,"def fc2video_download(url, output_dir = '.', merge = True, info_only = False, **kwargs):
    """"""wrapper""""""
    #'http://video.fc2.com/en/content/20151021bTVKnbEw'
    #'http://xiaojiadianvideo.asia/content/20151021bTVKnbEw'
    #'http://video.fc2.com/ja/content/20151021bTVKnbEw'
    #'http://video.fc2.com/tw/content/20151021bTVKnbEw'
    hostname = urlparse(url).hostname
    if not ('fc2.com' in hostname or 'xiaojiadianvideo.asia' in hostname):
        return False
    upid = match1(url, r'.+/content/(\w+)')

    fc2video_download_by_upid(upid, output_dir, merge, info_only)","['def', 'fc2video_download', '(', 'url', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'True', ',', 'info_only', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', ""#'http://video.fc2.com/en/content/20151021bTVKnbEw'"", ""#'http://xiaojiadianvideo.asia/content/20151021bTVKnbEw'"", ""#'http://video.fc2.com/ja/content/20151021bTVKnbEw'"", ""#'http://video.fc2.com/tw/content/20151021bTVKnbEw'"", 'hostname', '=', 'urlparse', '(', 'url', ')', '.', 'hostname', 'if', 'not', '(', ""'fc2.com'"", 'in', 'hostname', 'or', ""'xiaojiadianvideo.asia'"", 'in', 'hostname', ')', ':', 'return', 'False', 'upid', '=', 'match1', '(', 'url', ',', ""r'.+/content/(\\w+)'"", ')', 'fc2video_download_by_upid', '(', 'upid', ',', 'output_dir', ',', 'merge', ',', 'info_only', ')']",wrapper,['wrapper'],python,test,['wrapper'],wrapper,"['def', 'fc2video_download', '(', 'url', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'true', ',', 'info_only', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', ""#'http://video.fc2.com/en/content/20151021btvknbew'"", ""#'http://xiaojiadianvideo.asia/content/20151021btvknbew'"", ""#'http://video.fc2.com/ja/content/20151021btvknbew'"", ""#'http://video.fc2.com/tw/content/20151021btvknbew'"", 'hostname', '=', 'urlparse', '(', 'url', ')', '.', 'hostname', 'if', 'not', '(', ""'fc2.com'"", 'in', 'hostname', 'or', ""'xiaojiadianvideo.asia'"", 'in', 'hostname', ')', ':', 'return', 'false', 'upid', '=', 'match1', '(', 'url', ',', ""r'.+/content/(\\w+)'"", ')', 'fc2video_download_by_upid', '(', 'upid', ',', 'output_dir', ',', 'merge', ',', 'info_only', ')']","def fc2video_download ( url , output_dir = '.' , merge = true , info_only = false , * * kwargs ) : #'http://video.fc2.com/en/content/20151021btvknbew' #'http://xiaojiadianvideo.asia/content/20151021btvknbew' #'http://video.fc2.com/ja/content/20151021btvknbew' #'http://video.fc2.com/tw/content/20151021btvknbew' hostname = urlparse ( url ) . hostname if not ( 'fc2.com' in hostname or 'xiaojiadianvideo.asia' in hostname ) : return false upid = match1 ( url , r'.+/content/(\w+)' ) fc2video_download_by_upid ( upid , output_dir , merge , info_only )"
4,soimort/you-get,src/you_get/extractors/dailymotion.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/dailymotion.py#L13-L35,"def dailymotion_download(url, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""Downloads Dailymotion videos by URL.
    """"""

    html = get_content(rebuilt_url(url))
    info = json.loads(match1(html, r'qualities"":({.+?}),""'))
    title = match1(html, r'""video_title""\s*:\s*""([^""]+)""') or \
            match1(html, r'""title""\s*:\s*""([^""]+)""')
    title = unicodize(title)

    for quality in ['1080','720','480','380','240','144','auto']:
        try:
            real_url = info[quality][1][""url""]
            if real_url:
                break
        except KeyError:
            pass

    mime, ext, size = url_info(real_url)

    print_info(site_info, title, mime, size)
    if not info_only:
        download_urls([real_url], title, ext, size, output_dir=output_dir, merge=merge)","['def', 'dailymotion_download', '(', 'url', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'True', ',', 'info_only', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', 'html', '=', 'get_content', '(', 'rebuilt_url', '(', 'url', ')', ')', 'info', '=', 'json', '.', 'loads', '(', 'match1', '(', 'html', ',', 'r\'qualities"":({.+?}),""\'', ')', ')', 'title', '=', 'match1', '(', 'html', ',', 'r\'""video_title""\\s*:\\s*""([^""]+)""\'', ')', 'or', 'match1', '(', 'html', ',', 'r\'""title""\\s*:\\s*""([^""]+)""\'', ')', 'title', '=', 'unicodize', '(', 'title', ')', 'for', 'quality', 'in', '[', ""'1080'"", ',', ""'720'"", ',', ""'480'"", ',', ""'380'"", ',', ""'240'"", ',', ""'144'"", ',', ""'auto'"", ']', ':', 'try', ':', 'real_url', '=', 'info', '[', 'quality', ']', '[', '1', ']', '[', '""url""', ']', 'if', 'real_url', ':', 'break', 'except', 'KeyError', ':', 'pass', 'mime', ',', 'ext', ',', 'size', '=', 'url_info', '(', 'real_url', ')', 'print_info', '(', 'site_info', ',', 'title', ',', 'mime', ',', 'size', ')', 'if', 'not', 'info_only', ':', 'download_urls', '(', '[', 'real_url', ']', ',', 'title', ',', 'ext', ',', 'size', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ')']",Downloads Dailymotion videos by URL.,"['Downloads', 'Dailymotion', 'videos', 'by', 'URL', '.']",python,test,"['downloads', 'dailymotion', 'videos', 'by', 'url', '.']",downloads dailymotion videos by url .,"['def', 'dailymotion_download', '(', 'url', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'true', ',', 'info_only', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', 'html', '=', 'get_content', '(', 'rebuilt_url', '(', 'url', ')', ')', 'info', '=', 'json', '.', 'loads', '(', 'match1', '(', 'html', ',', 'r\'qualities"":({.+?}),""\'', ')', ')', 'title', '=', 'match1', '(', 'html', ',', 'r\'""video_title""\\s*:\\s*""([^""]+)""\'', ')', 'or', 'match1', '(', 'html', ',', 'r\'""title""\\s*:\\s*""([^""]+)""\'', ')', 'title', '=', 'unicodize', '(', 'title', ')', 'for', 'quality', 'in', '[', ""'1080'"", ',', ""'720'"", ',', ""'480'"", ',', ""'380'"", ',', ""'240'"", ',', ""'144'"", ',', ""'auto'"", ']', ':', 'try', ':', 'real_url', '=', 'info', '[', 'quality', ']', '[', '1', ']', '[', '""url""', ']', 'if', 'real_url', ':', 'break', 'except', 'keyerror', ':', 'pass', 'mime', ',', 'ext', ',', 'size', '=', 'url_info', '(', 'real_url', ')', 'print_info', '(', 'site_info', ',', 'title', ',', 'mime', ',', 'size', ')', 'if', 'not', 'info_only', ':', 'download_urls', '(', '[', 'real_url', ']', ',', 'title', ',', 'ext', ',', 'size', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ')']","def dailymotion_download ( url , output_dir = '.' , merge = true , info_only = false , * * kwargs ) : html = get_content ( rebuilt_url ( url ) ) info = json . loads ( match1 ( html , r'qualities"":({.+?}),""' ) ) title = match1 ( html , r'""video_title""\s*:\s*""([^""]+)""' ) or match1 ( html , r'""title""\s*:\s*""([^""]+)""' ) title = unicodize ( title ) for quality in [ '1080' , '720' , '480' , '380' , '240' , '144' , 'auto' ] : try : real_url = info [ quality ] [ 1 ] [ ""url"" ] if real_url : break except keyerror : pass mime , ext , size = url_info ( real_url ) print_info ( site_info , title , mime , size ) if not info_only : download_urls ( [ real_url ] , title , ext , size , output_dir = output_dir , merge = merge )"
5,soimort/you-get,src/you_get/extractors/ucas.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/ucas.py#L18-L29,"def dictify(r,root=True):
    """"""http://stackoverflow.com/a/30923963/2946714""""""
    if root:
        return {r.tag : dictify(r, False)}
    d=copy(r.attrib)
    if r.text:
        d[""_text""]=r.text
    for x in r.findall(""./*""):
        if x.tag not in d:
            d[x.tag]=[]
        d[x.tag].append(dictify(x,False))
    return d","['def', 'dictify', '(', 'r', ',', 'root', '=', 'True', ')', ':', 'if', 'root', ':', 'return', '{', 'r', '.', 'tag', ':', 'dictify', '(', 'r', ',', 'False', ')', '}', 'd', '=', 'copy', '(', 'r', '.', 'attrib', ')', 'if', 'r', '.', 'text', ':', 'd', '[', '""_text""', ']', '=', 'r', '.', 'text', 'for', 'x', 'in', 'r', '.', 'findall', '(', '""./*""', ')', ':', 'if', 'x', '.', 'tag', 'not', 'in', 'd', ':', 'd', '[', 'x', '.', 'tag', ']', '=', '[', ']', 'd', '[', 'x', '.', 'tag', ']', '.', 'append', '(', 'dictify', '(', 'x', ',', 'False', ')', ')', 'return', 'd']",http://stackoverflow.com/a/30923963/2946714,"['http', ':', '//', 'stackoverflow', '.', 'com', '/', 'a', '/', '30923963', '/', '2946714']",python,test,"['http', ':', '//', 'stackoverflow', '.', 'com', '/', 'a', '/', '30923963', '/', '2946714']",http : // stackoverflow . com / a / 30923963 / 2946714,"['def', 'dictify', '(', 'r', ',', 'root', '=', 'true', ')', ':', 'if', 'root', ':', 'return', '{', 'r', '.', 'tag', ':', 'dictify', '(', 'r', ',', 'false', ')', '}', 'd', '=', 'copy', '(', 'r', '.', 'attrib', ')', 'if', 'r', '.', 'text', ':', 'd', '[', '""_text""', ']', '=', 'r', '.', 'text', 'for', 'x', 'in', 'r', '.', 'findall', '(', '""./*""', ')', ':', 'if', 'x', '.', 'tag', 'not', 'in', 'd', ':', 'd', '[', 'x', '.', 'tag', ']', '=', '[', ']', 'd', '[', 'x', '.', 'tag', ']', '.', 'append', '(', 'dictify', '(', 'x', ',', 'false', ')', ')', 'return', 'd']","def dictify ( r , root = true ) : if root : return { r . tag : dictify ( r , false ) } d = copy ( r . attrib ) if r . text : d [ ""_text"" ] = r . text for x in r . findall ( ""./*"" ) : if x . tag not in d : d [ x . tag ] = [ ] d [ x . tag ] . append ( dictify ( x , false ) ) return d"
6,soimort/you-get,src/you_get/extractors/ucas.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/ucas.py#L102-L117,"def ucas_download_single(url, output_dir = '.', merge = False, info_only = False, **kwargs):
    '''video page'''
    html = get_content(url)
    # resourceID is UUID
    resourceID = re.findall( r'resourceID"":""([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})', html)[0]
    assert resourceID != '', 'Cannot find resourceID!'

    title = match1(html, r'<div class=""bc-h"">(.+)</div>')
    url_lists = _ucas_get_url_lists_by_resourceID(resourceID)
    assert url_lists, 'Cannot find any URL of such class!'
    
    for k, part in enumerate(url_lists):
        part_title = title + '_' + str(k)
        print_info(site_info, part_title, 'flv', 0)
        if not info_only:
            download_urls(part, part_title, 'flv', total_size=None, output_dir=output_dir, merge=merge)","['def', 'ucas_download_single', '(', 'url', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'False', ',', 'info_only', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', 'html', '=', 'get_content', '(', 'url', ')', '# resourceID is UUID', 'resourceID', '=', 're', '.', 'findall', '(', 'r\'resourceID"":""([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})\'', ',', 'html', ')', '[', '0', ']', 'assert', 'resourceID', '!=', ""''"", ',', ""'Cannot find resourceID!'"", 'title', '=', 'match1', '(', 'html', ',', 'r\'<div class=""bc-h"">(.+)</div>\'', ')', 'url_lists', '=', '_ucas_get_url_lists_by_resourceID', '(', 'resourceID', ')', 'assert', 'url_lists', ',', ""'Cannot find any URL of such class!'"", 'for', 'k', ',', 'part', 'in', 'enumerate', '(', 'url_lists', ')', ':', 'part_title', '=', 'title', '+', ""'_'"", '+', 'str', '(', 'k', ')', 'print_info', '(', 'site_info', ',', 'part_title', ',', ""'flv'"", ',', '0', ')', 'if', 'not', 'info_only', ':', 'download_urls', '(', 'part', ',', 'part_title', ',', ""'flv'"", ',', 'total_size', '=', 'None', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ')']",video page,"['video', 'page']",python,test,"['video', 'page']",video page,"['def', 'ucas_download_single', '(', 'url', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'false', ',', 'info_only', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', 'html', '=', 'get_content', '(', 'url', ')', '# resourceid is uuid', 'resourceid', '=', 're', '.', 'findall', '(', 'r\'resourceid"":""([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})\'', ',', 'html', ')', '[', '0', ']', 'assert', 'resourceid', '!=', ""''"", ',', ""'cannot find resourceid!'"", 'title', '=', 'match1', '(', 'html', ',', 'r\'<div class=""bc-h"">(.+)</div>\'', ')', 'url_lists', '=', '_ucas_get_url_lists_by_resourceid', '(', 'resourceid', ')', 'assert', 'url_lists', ',', ""'cannot find any url of such class!'"", 'for', 'k', ',', 'part', 'in', 'enumerate', '(', 'url_lists', ')', ':', 'part_title', '=', 'title', '+', ""'_'"", '+', 'str', '(', 'k', ')', 'print_info', '(', 'site_info', ',', 'part_title', ',', ""'flv'"", ',', '0', ')', 'if', 'not', 'info_only', ':', 'download_urls', '(', 'part', ',', 'part_title', ',', ""'flv'"", ',', 'total_size', '=', 'none', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ')']","def ucas_download_single ( url , output_dir = '.' , merge = false , info_only = false , * * kwargs ) : html = get_content ( url ) # resourceid is uuid resourceid = re . findall ( r'resourceid"":""([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})' , html ) [ 0 ] assert resourceid != '' , 'cannot find resourceid!' title = match1 ( html , r'<div class=""bc-h"">(.+)</div>' ) url_lists = _ucas_get_url_lists_by_resourceid ( resourceid ) assert url_lists , 'cannot find any url of such class!' for k , part in enumerate ( url_lists ) : part_title = title + '_' + str ( k ) print_info ( site_info , part_title , 'flv' , 0 ) if not info_only : download_urls ( part , part_title , 'flv' , total_size = none , output_dir = output_dir , merge = merge )"
7,soimort/you-get,src/you_get/extractors/ucas.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/ucas.py#L119-L127,"def ucas_download_playlist(url, output_dir = '.', merge = False, info_only = False, **kwargs):
    '''course page'''
    html = get_content(url)

    parts = re.findall( r'(getplaytitle.do\?.+)""', html)
    assert parts, 'No part found!'

    for part_path in parts:
        ucas_download('http://v.ucas.ac.cn/course/' + part_path, output_dir=output_dir, merge=merge, info_only=info_only)","['def', 'ucas_download_playlist', '(', 'url', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'False', ',', 'info_only', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', 'html', '=', 'get_content', '(', 'url', ')', 'parts', '=', 're', '.', 'findall', '(', 'r\'(getplaytitle.do\\?.+)""\'', ',', 'html', ')', 'assert', 'parts', ',', ""'No part found!'"", 'for', 'part_path', 'in', 'parts', ':', 'ucas_download', '(', ""'http://v.ucas.ac.cn/course/'"", '+', 'part_path', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ')']",course page,"['course', 'page']",python,test,"['course', 'page']",course page,"['def', 'ucas_download_playlist', '(', 'url', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'false', ',', 'info_only', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', 'html', '=', 'get_content', '(', 'url', ')', 'parts', '=', 're', '.', 'findall', '(', 'r\'(getplaytitle.do\\?.+)""\'', ',', 'html', ')', 'assert', 'parts', ',', ""'no part found!'"", 'for', 'part_path', 'in', 'parts', ':', 'ucas_download', '(', ""'http://v.ucas.ac.cn/course/'"", '+', 'part_path', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ')']","def ucas_download_playlist ( url , output_dir = '.' , merge = false , info_only = false , * * kwargs ) : html = get_content ( url ) parts = re . findall ( r'(getplaytitle.do\?.+)""' , html ) assert parts , 'no part found!' for part_path in parts : ucas_download ( 'http://v.ucas.ac.cn/course/' + part_path , output_dir = output_dir , merge = merge , info_only = info_only )"
8,soimort/you-get,src/you_get/extractors/sina.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/sina.py#L41-L52,"def sina_download_by_vid(vid, title=None, output_dir='.', merge=True, info_only=False):
    """"""Downloads a Sina video by its unique vid.
    http://video.sina.com.cn/
    """"""
    xml = api_req(vid)
    urls, name, size = video_info(xml)
    if urls is None:
        log.wtf(name)
    title = name
    print_info(site_info, title, 'flv', size)
    if not info_only:
        download_urls(urls, title, 'flv', size, output_dir = output_dir, merge = merge)","['def', 'sina_download_by_vid', '(', 'vid', ',', 'title', '=', 'None', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'True', ',', 'info_only', '=', 'False', ')', ':', 'xml', '=', 'api_req', '(', 'vid', ')', 'urls', ',', 'name', ',', 'size', '=', 'video_info', '(', 'xml', ')', 'if', 'urls', 'is', 'None', ':', 'log', '.', 'wtf', '(', 'name', ')', 'title', '=', 'name', 'print_info', '(', 'site_info', ',', 'title', ',', ""'flv'"", ',', 'size', ')', 'if', 'not', 'info_only', ':', 'download_urls', '(', 'urls', ',', 'title', ',', ""'flv'"", ',', 'size', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ')']","Downloads a Sina video by its unique vid.
    http://video.sina.com.cn/","['Downloads', 'a', 'Sina', 'video', 'by', 'its', 'unique', 'vid', '.', 'http', ':', '//', 'video', '.', 'sina', '.', 'com', '.', 'cn', '/']",python,test,"['downloads', 'a', 'sina', 'video', 'by', 'its', 'unique', 'vid', '.', 'http', ':', '//', 'video', '.', 'sina', '.', 'com', '.', 'cn', '/']",downloads a sina video by its unique vid . http : // video . sina . com . cn /,"['def', 'sina_download_by_vid', '(', 'vid', ',', 'title', '=', 'none', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'true', ',', 'info_only', '=', 'false', ')', ':', 'xml', '=', 'api_req', '(', 'vid', ')', 'urls', ',', 'name', ',', 'size', '=', 'video_info', '(', 'xml', ')', 'if', 'urls', 'is', 'none', ':', 'log', '.', 'wtf', '(', 'name', ')', 'title', '=', 'name', 'print_info', '(', 'site_info', ',', 'title', ',', ""'flv'"", ',', 'size', ')', 'if', 'not', 'info_only', ':', 'download_urls', '(', 'urls', ',', 'title', ',', ""'flv'"", ',', 'size', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ')']","def sina_download_by_vid ( vid , title = none , output_dir = '.' , merge = true , info_only = false ) : xml = api_req ( vid ) urls , name , size = video_info ( xml ) if urls is none : log . wtf ( name ) title = name print_info ( site_info , title , 'flv' , size ) if not info_only : download_urls ( urls , title , 'flv' , size , output_dir = output_dir , merge = merge )"
9,soimort/you-get,src/you_get/extractors/sina.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/sina.py#L54-L64,"def sina_download_by_vkey(vkey, title=None, output_dir='.', merge=True, info_only=False):
    """"""Downloads a Sina video by its unique vkey.
    http://video.sina.com/
    """"""

    url = 'http://video.sina.com/v/flvideo/%s_0.flv' % vkey
    type, ext, size = url_info(url)

    print_info(site_info, title, 'flv', size)
    if not info_only:
        download_urls([url], title, 'flv', size, output_dir = output_dir, merge = merge)","['def', 'sina_download_by_vkey', '(', 'vkey', ',', 'title', '=', 'None', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'True', ',', 'info_only', '=', 'False', ')', ':', 'url', '=', ""'http://video.sina.com/v/flvideo/%s_0.flv'"", '%', 'vkey', 'type', ',', 'ext', ',', 'size', '=', 'url_info', '(', 'url', ')', 'print_info', '(', 'site_info', ',', 'title', ',', ""'flv'"", ',', 'size', ')', 'if', 'not', 'info_only', ':', 'download_urls', '(', '[', 'url', ']', ',', 'title', ',', ""'flv'"", ',', 'size', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ')']","Downloads a Sina video by its unique vkey.
    http://video.sina.com/","['Downloads', 'a', 'Sina', 'video', 'by', 'its', 'unique', 'vkey', '.', 'http', ':', '//', 'video', '.', 'sina', '.', 'com', '/']",python,test,"['downloads', 'a', 'sina', 'video', 'by', 'its', 'unique', 'vkey', '.', 'http', ':', '//', 'video', '.', 'sina', '.', 'com', '/']",downloads a sina video by its unique vkey . http : // video . sina . com /,"['def', 'sina_download_by_vkey', '(', 'vkey', ',', 'title', '=', 'none', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'true', ',', 'info_only', '=', 'false', ')', ':', 'url', '=', ""'http://video.sina.com/v/flvideo/%s_0.flv'"", '%', 'vkey', 'type', ',', 'ext', ',', 'size', '=', 'url_info', '(', 'url', ')', 'print_info', '(', 'site_info', ',', 'title', ',', ""'flv'"", ',', 'size', ')', 'if', 'not', 'info_only', ':', 'download_urls', '(', '[', 'url', ']', ',', 'title', ',', ""'flv'"", ',', 'size', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ')']","def sina_download_by_vkey ( vkey , title = none , output_dir = '.' , merge = true , info_only = false ) : url = 'http://video.sina.com/v/flvideo/%s_0.flv' % vkey type , ext , size = url_info ( url ) print_info ( site_info , title , 'flv' , size ) if not info_only : download_urls ( [ url ] , title , 'flv' , size , output_dir = output_dir , merge = merge )"
10,soimort/you-get,src/you_get/extractors/sina.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/sina.py#L94-L121,"def sina_download(url, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""Downloads Sina videos by URL.
    """"""
    if 'news.sina.com.cn/zxt' in url:
        sina_zxt(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)
        return

    vid = match1(url, r'vid=(\d+)')
    if vid is None:
        video_page = get_content(url)
        vid = hd_vid = match1(video_page, r'hd_vid\s*:\s*\'([^\']+)\'')
        if hd_vid == '0':
            vids = match1(video_page, r'[^\w]vid\s*:\s*\'([^\']+)\'').split('|')
            vid = vids[-1]

    if vid is None:
        vid = match1(video_page, r'vid:""?(\d+)""?')
    if vid:
        #title = match1(video_page, r'title\s*:\s*\'([^\']+)\'')
        sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)
    else:
        vkey = match1(video_page, r'vkey\s*:\s*""([^""]+)""')
        if vkey is None:
            vid = match1(url, r'#(\d+)')
            sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)
            return
        title = match1(video_page, r'title\s*:\s*""([^""]+)""')
        sina_download_by_vkey(vkey, title=title, output_dir=output_dir, merge=merge, info_only=info_only)","['def', 'sina_download', '(', 'url', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'True', ',', 'info_only', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', 'if', ""'news.sina.com.cn/zxt'"", 'in', 'url', ':', 'sina_zxt', '(', 'url', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ',', '*', '*', 'kwargs', ')', 'return', 'vid', '=', 'match1', '(', 'url', ',', ""r'vid=(\\d+)'"", ')', 'if', 'vid', 'is', 'None', ':', 'video_page', '=', 'get_content', '(', 'url', ')', 'vid', '=', 'hd_vid', '=', 'match1', '(', 'video_page', ',', ""r'hd_vid\\s*:\\s*\\'([^\\']+)\\''"", ')', 'if', 'hd_vid', '==', ""'0'"", ':', 'vids', '=', 'match1', '(', 'video_page', ',', ""r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\''"", ')', '.', 'split', '(', ""'|'"", ')', 'vid', '=', 'vids', '[', '-', '1', ']', 'if', 'vid', 'is', 'None', ':', 'vid', '=', 'match1', '(', 'video_page', ',', 'r\'vid:""?(\\d+)""?\'', ')', 'if', 'vid', ':', ""#title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'')"", 'sina_download_by_vid', '(', 'vid', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ')', 'else', ':', 'vkey', '=', 'match1', '(', 'video_page', ',', 'r\'vkey\\s*:\\s*""([^""]+)""\'', ')', 'if', 'vkey', 'is', 'None', ':', 'vid', '=', 'match1', '(', 'url', ',', ""r'#(\\d+)'"", ')', 'sina_download_by_vid', '(', 'vid', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ')', 'return', 'title', '=', 'match1', '(', 'video_page', ',', 'r\'title\\s*:\\s*""([^""]+)""\'', ')', 'sina_download_by_vkey', '(', 'vkey', ',', 'title', '=', 'title', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ')']",Downloads Sina videos by URL.,"['Downloads', 'Sina', 'videos', 'by', 'URL', '.']",python,test,"['downloads', 'sina', 'videos', 'by', 'url', '.']",downloads sina videos by url .,"['def', 'sina_download', '(', 'url', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'true', ',', 'info_only', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', 'if', ""'news.sina.com.cn/zxt'"", 'in', 'url', ':', 'sina_zxt', '(', 'url', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ',', '*', '*', 'kwargs', ')', 'return', 'vid', '=', 'match1', '(', 'url', ',', ""r'vid=(\\d+)'"", ')', 'if', 'vid', 'is', 'none', ':', 'video_page', '=', 'get_content', '(', 'url', ')', 'vid', '=', 'hd_vid', '=', 'match1', '(', 'video_page', ',', ""r'hd_vid\\s*:\\s*\\'([^\\']+)\\''"", ')', 'if', 'hd_vid', '==', ""'0'"", ':', 'vids', '=', 'match1', '(', 'video_page', ',', ""r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\''"", ')', '.', 'split', '(', ""'|'"", ')', 'vid', '=', 'vids', '[', '-', '1', ']', 'if', 'vid', 'is', 'none', ':', 'vid', '=', 'match1', '(', 'video_page', ',', 'r\'vid:""?(\\d+)""?\'', ')', 'if', 'vid', ':', ""#title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'')"", 'sina_download_by_vid', '(', 'vid', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ')', 'else', ':', 'vkey', '=', 'match1', '(', 'video_page', ',', 'r\'vkey\\s*:\\s*""([^""]+)""\'', ')', 'if', 'vkey', 'is', 'none', ':', 'vid', '=', 'match1', '(', 'url', ',', ""r'#(\\d+)'"", ')', 'sina_download_by_vid', '(', 'vid', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ')', 'return', 'title', '=', 'match1', '(', 'video_page', ',', 'r\'title\\s*:\\s*""([^""]+)""\'', ')', 'sina_download_by_vkey', '(', 'vkey', ',', 'title', '=', 'title', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ')']","def sina_download ( url , output_dir = '.' , merge = true , info_only = false , * * kwargs ) : if 'news.sina.com.cn/zxt' in url : sina_zxt ( url , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs ) return vid = match1 ( url , r'vid=(\d+)' ) if vid is none : video_page = get_content ( url ) vid = hd_vid = match1 ( video_page , r'hd_vid\s*:\s*\'([^\']+)\'' ) if hd_vid == '0' : vids = match1 ( video_page , r'[^\w]vid\s*:\s*\'([^\']+)\'' ) . split ( '|' ) vid = vids [ - 1 ] if vid is none : vid = match1 ( video_page , r'vid:""?(\d+)""?' ) if vid : #title = match1(video_page, r'title\s*:\s*\'([^\']+)\'') sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) else : vkey = match1 ( video_page , r'vkey\s*:\s*""([^""]+)""' ) if vkey is none : vid = match1 ( url , r'#(\d+)' ) sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) return title = match1 ( video_page , r'title\s*:\s*""([^""]+)""' ) sina_download_by_vkey ( vkey , title = title , output_dir = output_dir , merge = merge , info_only = info_only )"
11,soimort/you-get,src/you_get/extractors/yixia.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/yixia.py#L65-L93,"def yixia_download(url, output_dir = '.', merge = True, info_only = False, **kwargs):
    """"""wrapper""""""
    hostname = urlparse(url).hostname
    if 'n.miaopai.com' == hostname: 
        smid = match1(url, r'n\.miaopai\.com/media/([^.]+)') 
        miaopai_download_by_smid(smid, output_dir, merge, info_only)
        return
    elif 'miaopai.com' in hostname:  #Miaopai
        yixia_download_by_scid = yixia_miaopai_download_by_scid
        site_info = ""Yixia Miaopai""

        scid = match1(url, r'miaopai\.com/show/channel/([^.]+)\.htm') or \
               match1(url, r'miaopai\.com/show/([^.]+)\.htm') or \
               match1(url, r'm\.miaopai\.com/show/channel/([^.]+)\.htm') or \
               match1(url, r'm\.miaopai\.com/show/channel/([^.]+)')

    elif 'xiaokaxiu.com' in hostname:  #Xiaokaxiu
        yixia_download_by_scid = yixia_xiaokaxiu_download_by_scid
        site_info = ""Yixia Xiaokaxiu""

        if re.match(r'http://v.xiaokaxiu.com/v/.+\.html', url):  #PC
            scid = match1(url, r'http://v.xiaokaxiu.com/v/(.+)\.html')
        elif re.match(r'http://m.xiaokaxiu.com/m/.+\.html', url):  #Mobile
            scid = match1(url, r'http://m.xiaokaxiu.com/m/(.+)\.html')

    else:
        pass

    yixia_download_by_scid(scid, output_dir, merge, info_only)","['def', 'yixia_download', '(', 'url', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'True', ',', 'info_only', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', 'hostname', '=', 'urlparse', '(', 'url', ')', '.', 'hostname', 'if', ""'n.miaopai.com'"", '==', 'hostname', ':', 'smid', '=', 'match1', '(', 'url', ',', ""r'n\\.miaopai\\.com/media/([^.]+)'"", ')', 'miaopai_download_by_smid', '(', 'smid', ',', 'output_dir', ',', 'merge', ',', 'info_only', ')', 'return', 'elif', ""'miaopai.com'"", 'in', 'hostname', ':', '#Miaopai', 'yixia_download_by_scid', '=', 'yixia_miaopai_download_by_scid', 'site_info', '=', '""Yixia Miaopai""', 'scid', '=', 'match1', '(', 'url', ',', ""r'miaopai\\.com/show/channel/([^.]+)\\.htm'"", ')', 'or', 'match1', '(', 'url', ',', ""r'miaopai\\.com/show/([^.]+)\\.htm'"", ')', 'or', 'match1', '(', 'url', ',', ""r'm\\.miaopai\\.com/show/channel/([^.]+)\\.htm'"", ')', 'or', 'match1', '(', 'url', ',', ""r'm\\.miaopai\\.com/show/channel/([^.]+)'"", ')', 'elif', ""'xiaokaxiu.com'"", 'in', 'hostname', ':', '#Xiaokaxiu', 'yixia_download_by_scid', '=', 'yixia_xiaokaxiu_download_by_scid', 'site_info', '=', '""Yixia Xiaokaxiu""', 'if', 're', '.', 'match', '(', ""r'http://v.xiaokaxiu.com/v/.+\\.html'"", ',', 'url', ')', ':', '#PC', 'scid', '=', 'match1', '(', 'url', ',', ""r'http://v.xiaokaxiu.com/v/(.+)\\.html'"", ')', 'elif', 're', '.', 'match', '(', ""r'http://m.xiaokaxiu.com/m/.+\\.html'"", ',', 'url', ')', ':', '#Mobile', 'scid', '=', 'match1', '(', 'url', ',', ""r'http://m.xiaokaxiu.com/m/(.+)\\.html'"", ')', 'else', ':', 'pass', 'yixia_download_by_scid', '(', 'scid', ',', 'output_dir', ',', 'merge', ',', 'info_only', ')']",wrapper,['wrapper'],python,test,['wrapper'],wrapper,"['def', 'yixia_download', '(', 'url', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'true', ',', 'info_only', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', 'hostname', '=', 'urlparse', '(', 'url', ')', '.', 'hostname', 'if', ""'n.miaopai.com'"", '==', 'hostname', ':', 'smid', '=', 'match1', '(', 'url', ',', ""r'n\\.miaopai\\.com/media/([^.]+)'"", ')', 'miaopai_download_by_smid', '(', 'smid', ',', 'output_dir', ',', 'merge', ',', 'info_only', ')', 'return', 'elif', ""'miaopai.com'"", 'in', 'hostname', ':', '#miaopai', 'yixia_download_by_scid', '=', 'yixia_miaopai_download_by_scid', 'site_info', '=', '""yixia miaopai""', 'scid', '=', 'match1', '(', 'url', ',', ""r'miaopai\\.com/show/channel/([^.]+)\\.htm'"", ')', 'or', 'match1', '(', 'url', ',', ""r'miaopai\\.com/show/([^.]+)\\.htm'"", ')', 'or', 'match1', '(', 'url', ',', ""r'm\\.miaopai\\.com/show/channel/([^.]+)\\.htm'"", ')', 'or', 'match1', '(', 'url', ',', ""r'm\\.miaopai\\.com/show/channel/([^.]+)'"", ')', 'elif', ""'xiaokaxiu.com'"", 'in', 'hostname', ':', '#xiaokaxiu', 'yixia_download_by_scid', '=', 'yixia_xiaokaxiu_download_by_scid', 'site_info', '=', '""yixia xiaokaxiu""', 'if', 're', '.', 'match', '(', ""r'http://v.xiaokaxiu.com/v/.+\\.html'"", ',', 'url', ')', ':', '#pc', 'scid', '=', 'match1', '(', 'url', ',', ""r'http://v.xiaokaxiu.com/v/(.+)\\.html'"", ')', 'elif', 're', '.', 'match', '(', ""r'http://m.xiaokaxiu.com/m/.+\\.html'"", ',', 'url', ')', ':', '#mobile', 'scid', '=', 'match1', '(', 'url', ',', ""r'http://m.xiaokaxiu.com/m/(.+)\\.html'"", ')', 'else', ':', 'pass', 'yixia_download_by_scid', '(', 'scid', ',', 'output_dir', ',', 'merge', ',', 'info_only', ')']","def yixia_download ( url , output_dir = '.' , merge = true , info_only = false , * * kwargs ) : hostname = urlparse ( url ) . hostname if 'n.miaopai.com' == hostname : smid = match1 ( url , r'n\.miaopai\.com/media/([^.]+)' ) miaopai_download_by_smid ( smid , output_dir , merge , info_only ) return elif 'miaopai.com' in hostname : #miaopai yixia_download_by_scid = yixia_miaopai_download_by_scid site_info = ""yixia miaopai"" scid = match1 ( url , r'miaopai\.com/show/channel/([^.]+)\.htm' ) or match1 ( url , r'miaopai\.com/show/([^.]+)\.htm' ) or match1 ( url , r'm\.miaopai\.com/show/channel/([^.]+)\.htm' ) or match1 ( url , r'm\.miaopai\.com/show/channel/([^.]+)' ) elif 'xiaokaxiu.com' in hostname : #xiaokaxiu yixia_download_by_scid = yixia_xiaokaxiu_download_by_scid site_info = ""yixia xiaokaxiu"" if re . match ( r'http://v.xiaokaxiu.com/v/.+\.html' , url ) : #pc scid = match1 ( url , r'http://v.xiaokaxiu.com/v/(.+)\.html' ) elif re . match ( r'http://m.xiaokaxiu.com/m/.+\.html' , url ) : #mobile scid = match1 ( url , r'http://m.xiaokaxiu.com/m/(.+)\.html' ) else : pass yixia_download_by_scid ( scid , output_dir , merge , info_only )"
12,soimort/you-get,src/you_get/extractors/veoh.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/veoh.py#L8-L16,"def veoh_download(url, output_dir = '.', merge = False, info_only = False, **kwargs):
    '''Get item_id'''
    if re.match(r'http://www.veoh.com/watch/\w+', url):
        item_id = match1(url, r'http://www.veoh.com/watch/(\w+)')
    elif re.match(r'http://www.veoh.com/m/watch.php\?v=\.*', url):
        item_id = match1(url, r'http://www.veoh.com/m/watch.php\?v=(\w+)')
    else:
        raise NotImplementedError('Cannot find item ID')
    veoh_download_by_id(item_id, output_dir = '.', merge = False, info_only = info_only, **kwargs)","['def', 'veoh_download', '(', 'url', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'False', ',', 'info_only', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', 'if', 're', '.', 'match', '(', ""r'http://www.veoh.com/watch/\\w+'"", ',', 'url', ')', ':', 'item_id', '=', 'match1', '(', 'url', ',', ""r'http://www.veoh.com/watch/(\\w+)'"", ')', 'elif', 're', '.', 'match', '(', ""r'http://www.veoh.com/m/watch.php\\?v=\\.*'"", ',', 'url', ')', ':', 'item_id', '=', 'match1', '(', 'url', ',', ""r'http://www.veoh.com/m/watch.php\\?v=(\\w+)'"", ')', 'else', ':', 'raise', 'NotImplementedError', '(', ""'Cannot find item ID'"", ')', 'veoh_download_by_id', '(', 'item_id', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'False', ',', 'info_only', '=', 'info_only', ',', '*', '*', 'kwargs', ')']",Get item_id,"['Get', 'item_id']",python,test,"['get', 'item_id']",get item_id,"['def', 'veoh_download', '(', 'url', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'false', ',', 'info_only', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', 'if', 're', '.', 'match', '(', ""r'http://www.veoh.com/watch/\\w+'"", ',', 'url', ')', ':', 'item_id', '=', 'match1', '(', 'url', ',', ""r'http://www.veoh.com/watch/(\\w+)'"", ')', 'elif', 're', '.', 'match', '(', ""r'http://www.veoh.com/m/watch.php\\?v=\\.*'"", ',', 'url', ')', ':', 'item_id', '=', 'match1', '(', 'url', ',', ""r'http://www.veoh.com/m/watch.php\\?v=(\\w+)'"", ')', 'else', ':', 'raise', 'notimplementederror', '(', ""'cannot find item id'"", ')', 'veoh_download_by_id', '(', 'item_id', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'false', ',', 'info_only', '=', 'info_only', ',', '*', '*', 'kwargs', ')']","def veoh_download ( url , output_dir = '.' , merge = false , info_only = false , * * kwargs ) : if re . match ( r'http://www.veoh.com/watch/\w+' , url ) : item_id = match1 ( url , r'http://www.veoh.com/watch/(\w+)' ) elif re . match ( r'http://www.veoh.com/m/watch.php\?v=\.*' , url ) : item_id = match1 ( url , r'http://www.veoh.com/m/watch.php\?v=(\w+)' ) else : raise notimplementederror ( 'cannot find item id' ) veoh_download_by_id ( item_id , output_dir = '.' , merge = false , info_only = info_only , * * kwargs )"
13,soimort/you-get,src/you_get/extractors/veoh.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/veoh.py#L19-L33,"def veoh_download_by_id(item_id, output_dir = '.', merge = False, info_only = False, **kwargs):
    """"""Source: Android mobile""""""
    webpage_url = 'http://www.veoh.com/m/watch.php?v={item_id}&quality=1'.format(item_id = item_id)

    #grab download URL
    a = get_content(webpage_url, decoded=True)
    url = match1(a, r'<source src=""(.*?)\""\W')

    #grab title
    title = match1(a, r'<meta property=""og:title"" content=""([^""]*)""')

    type_, ext, size = url_info(url)
    print_info(site_info, title, type_, size)
    if not info_only:
        download_urls([url], title, ext, total_size=None, output_dir=output_dir, merge=merge)","['def', 'veoh_download_by_id', '(', 'item_id', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'False', ',', 'info_only', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', 'webpage_url', '=', ""'http://www.veoh.com/m/watch.php?v={item_id}&quality=1'"", '.', 'format', '(', 'item_id', '=', 'item_id', ')', '#grab download URL', 'a', '=', 'get_content', '(', 'webpage_url', ',', 'decoded', '=', 'True', ')', 'url', '=', 'match1', '(', 'a', ',', 'r\'<source src=""(.*?)\\""\\W\'', ')', '#grab title', 'title', '=', 'match1', '(', 'a', ',', 'r\'<meta property=""og:title"" content=""([^""]*)""\'', ')', 'type_', ',', 'ext', ',', 'size', '=', 'url_info', '(', 'url', ')', 'print_info', '(', 'site_info', ',', 'title', ',', 'type_', ',', 'size', ')', 'if', 'not', 'info_only', ':', 'download_urls', '(', '[', 'url', ']', ',', 'title', ',', 'ext', ',', 'total_size', '=', 'None', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ')']",Source: Android mobile,"['Source', ':', 'Android', 'mobile']",python,test,"['source', ':', 'android', 'mobile']",source : android mobile,"['def', 'veoh_download_by_id', '(', 'item_id', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'false', ',', 'info_only', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', 'webpage_url', '=', ""'http://www.veoh.com/m/watch.php?v={item_id}&quality=1'"", '.', 'format', '(', 'item_id', '=', 'item_id', ')', '#grab download url', 'a', '=', 'get_content', '(', 'webpage_url', ',', 'decoded', '=', 'true', ')', 'url', '=', 'match1', '(', 'a', ',', 'r\'<source src=""(.*?)\\""\\w\'', ')', '#grab title', 'title', '=', 'match1', '(', 'a', ',', 'r\'<meta property=""og:title"" content=""([^""]*)""\'', ')', 'type_', ',', 'ext', ',', 'size', '=', 'url_info', '(', 'url', ')', 'print_info', '(', 'site_info', ',', 'title', ',', 'type_', ',', 'size', ')', 'if', 'not', 'info_only', ':', 'download_urls', '(', '[', 'url', ']', ',', 'title', ',', 'ext', ',', 'total_size', '=', 'none', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ')']","def veoh_download_by_id ( item_id , output_dir = '.' , merge = false , info_only = false , * * kwargs ) : webpage_url = 'http://www.veoh.com/m/watch.php?v={item_id}&quality=1' . format ( item_id = item_id ) #grab download url a = get_content ( webpage_url , decoded = true ) url = match1 ( a , r'<source src=""(.*?)\""\w' ) #grab title title = match1 ( a , r'<meta property=""og:title"" content=""([^""]*)""' ) type_ , ext , size = url_info ( url ) print_info ( site_info , title , type_ , size ) if not info_only : download_urls ( [ url ] , title , ext , total_size = none , output_dir = output_dir , merge = merge )"
14,soimort/you-get,src/you_get/extractors/bokecc.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/bokecc.py#L17-L39,"def download_by_id(self, vid = '', title = None, output_dir='.', merge=True, info_only=False,**kwargs):
        """"""self, str->None
        
        Keyword arguments:
        self: self
        vid: The video ID for BokeCC cloud, something like
        FE3BB999594978049C33DC5901307461
        
        Calls the prepare() to download the video.
        
        If no title is provided, this method shall try to find a proper title
        with the information providin within the
        returned content of the API.""""""

        assert vid

        self.prepare(vid = vid, title = title, **kwargs)

        self.extract(**kwargs)

        self.download(output_dir = output_dir, 
                    merge = merge, 
                    info_only = info_only, **kwargs)","['def', 'download_by_id', '(', 'self', ',', 'vid', '=', ""''"", ',', 'title', '=', 'None', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'True', ',', 'info_only', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', 'assert', 'vid', 'self', '.', 'prepare', '(', 'vid', '=', 'vid', ',', 'title', '=', 'title', ',', '*', '*', 'kwargs', ')', 'self', '.', 'extract', '(', '*', '*', 'kwargs', ')', 'self', '.', 'download', '(', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ',', '*', '*', 'kwargs', ')']","self, str->None
        
        Keyword arguments:
        self: self
        vid: The video ID for BokeCC cloud, something like
        FE3BB999594978049C33DC5901307461
        
        Calls the prepare() to download the video.
        
        If no title is provided, this method shall try to find a proper title
        with the information providin within the
        returned content of the API.","['self', 'str', '-', '>', 'None', 'Keyword', 'arguments', ':', 'self', ':', 'self', 'vid', ':', 'The', 'video', 'ID', 'for', 'BokeCC', 'cloud', 'something', 'like', 'FE3BB999594978049C33DC5901307461', 'Calls', 'the', 'prepare', '()', 'to', 'download', 'the', 'video', '.', 'If', 'no', 'title', 'is', 'provided', 'this', 'method', 'shall', 'try', 'to', 'find', 'a', 'proper', 'title', 'with', 'the', 'information', 'providin', 'within', 'the', 'returned', 'content', 'of', 'the', 'API', '.']",python,test,"['self', 'str', '-', '>', 'none', 'keyword', 'arguments', ':', 'self', ':', 'self', 'vid', ':', 'the', 'video', 'id', 'for', 'bokecc', 'cloud', 'something', 'like', 'fe3bb999594978049c33dc5901307461', 'calls', 'the', 'prepare', '()', 'to', 'download', 'the', 'video', '.', 'if', 'no', 'title', 'is', 'provided', 'this', 'method', 'shall', 'try', 'to', 'find', 'a', 'proper', 'title', 'with', 'the', 'information', 'providin', 'within', 'the', 'returned', 'content', 'of', 'the', 'api', '.']",self str - > none keyword arguments : self : self vid : the video id for bokecc cloud something like fe3bb999594978049c33dc5901307461 calls the prepare () to download the video . if no title is provided this method shall try to find a proper title with the information providin within the returned content of the api .,"['def', 'download_by_id', '(', 'self', ',', 'vid', '=', ""''"", ',', 'title', '=', 'none', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'true', ',', 'info_only', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', 'assert', 'vid', 'self', '.', 'prepare', '(', 'vid', '=', 'vid', ',', 'title', '=', 'title', ',', '*', '*', 'kwargs', ')', 'self', '.', 'extract', '(', '*', '*', 'kwargs', ')', 'self', '.', 'download', '(', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ',', '*', '*', 'kwargs', ')']","def download_by_id ( self , vid = '' , title = none , output_dir = '.' , merge = true , info_only = false , * * kwargs ) : assert vid self . prepare ( vid = vid , title = title , * * kwargs ) self . extract ( * * kwargs ) self . download ( output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs )"
15,soimort/you-get,src/you_get/extractors/qie.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/qie.py#L35-L48,"def get_vid_from_url(self, url):
        """"""Extracts video ID from live.qq.com.
        """"""
        hit = re.search(r'live.qq.com/(\d+)', url)
        if hit is not None:
            return hit.group(1)
        hit = re.search(r'live.qq.com/directory/match/(\d+)', url)
        if hit is not None:
            return self.get_room_id_from_url(hit.group(1))
        html = get_content(url)
        room_id = match1(html, r'room_id\"":(\d+)')
        if room_id is None:
            log.wtf('Unknown page {}'.format(url))
        return room_id","['def', 'get_vid_from_url', '(', 'self', ',', 'url', ')', ':', 'hit', '=', 're', '.', 'search', '(', ""r'live.qq.com/(\\d+)'"", ',', 'url', ')', 'if', 'hit', 'is', 'not', 'None', ':', 'return', 'hit', '.', 'group', '(', '1', ')', 'hit', '=', 're', '.', 'search', '(', ""r'live.qq.com/directory/match/(\\d+)'"", ',', 'url', ')', 'if', 'hit', 'is', 'not', 'None', ':', 'return', 'self', '.', 'get_room_id_from_url', '(', 'hit', '.', 'group', '(', '1', ')', ')', 'html', '=', 'get_content', '(', 'url', ')', 'room_id', '=', 'match1', '(', 'html', ',', 'r\'room_id\\"":(\\d+)\'', ')', 'if', 'room_id', 'is', 'None', ':', 'log', '.', 'wtf', '(', ""'Unknown page {}'"", '.', 'format', '(', 'url', ')', ')', 'return', 'room_id']",Extracts video ID from live.qq.com.,"['Extracts', 'video', 'ID', 'from', 'live', '.', 'qq', '.', 'com', '.']",python,test,"['extracts', 'video', 'id', 'from', 'live', '.', 'qq', '.', 'com', '.']",extracts video id from live . qq . com .,"['def', 'get_vid_from_url', '(', 'self', ',', 'url', ')', ':', 'hit', '=', 're', '.', 'search', '(', ""r'live.qq.com/(\\d+)'"", ',', 'url', ')', 'if', 'hit', 'is', 'not', 'none', ':', 'return', 'hit', '.', 'group', '(', '1', ')', 'hit', '=', 're', '.', 'search', '(', ""r'live.qq.com/directory/match/(\\d+)'"", ',', 'url', ')', 'if', 'hit', 'is', 'not', 'none', ':', 'return', 'self', '.', 'get_room_id_from_url', '(', 'hit', '.', 'group', '(', '1', ')', ')', 'html', '=', 'get_content', '(', 'url', ')', 'room_id', '=', 'match1', '(', 'html', ',', 'r\'room_id\\"":(\\d+)\'', ')', 'if', 'room_id', 'is', 'none', ':', 'log', '.', 'wtf', '(', ""'unknown page {}'"", '.', 'format', '(', 'url', ')', ')', 'return', 'room_id']","def get_vid_from_url ( self , url ) : hit = re . search ( r'live.qq.com/(\d+)' , url ) if hit is not none : return hit . group ( 1 ) hit = re . search ( r'live.qq.com/directory/match/(\d+)' , url ) if hit is not none : return self . get_room_id_from_url ( hit . group ( 1 ) ) html = get_content ( url ) room_id = match1 ( html , r'room_id\"":(\d+)' ) if room_id is none : log . wtf ( 'unknown page {}' . format ( url ) ) return room_id"
16,soimort/you-get,src/you_get/util/log.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L60-L62,"def sprint(text, *colors):
    """"""Format text with color or other effects into ANSI escaped string.""""""
    return ""\33[{}m{content}\33[{}m"".format("";"".join([str(color) for color in colors]), RESET, content=text) if IS_ANSI_TERMINAL and colors else text","['def', 'sprint', '(', 'text', ',', '*', 'colors', ')', ':', 'return', '""\\33[{}m{content}\\33[{}m""', '.', 'format', '(', '"";""', '.', 'join', '(', '[', 'str', '(', 'color', ')', 'for', 'color', 'in', 'colors', ']', ')', ',', 'RESET', ',', 'content', '=', 'text', ')', 'if', 'IS_ANSI_TERMINAL', 'and', 'colors', 'else', 'text']",Format text with color or other effects into ANSI escaped string.,"['Format', 'text', 'with', 'color', 'or', 'other', 'effects', 'into', 'ANSI', 'escaped', 'string', '.']",python,test,"['format', 'text', 'with', 'color', 'or', 'other', 'effects', 'into', 'ansi', 'escaped', 'string', '.']",format text with color or other effects into ansi escaped string .,"['def', 'sprint', '(', 'text', ',', '*', 'colors', ')', ':', 'return', '""\\33[{}m{content}\\33[{}m""', '.', 'format', '(', '"";""', '.', 'join', '(', '[', 'str', '(', 'color', ')', 'for', 'color', 'in', 'colors', ']', ')', ',', 'reset', ',', 'content', '=', 'text', ')', 'if', 'is_ansi_terminal', 'and', 'colors', 'else', 'text']","def sprint ( text , * colors ) : return ""\33[{}m{content}\33[{}m"" . format ( "";"" . join ( [ str ( color ) for color in colors ] ) , reset , content = text ) if is_ansi_terminal and colors else text"
17,soimort/you-get,src/you_get/util/log.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L72-L74,"def print_log(text, *colors):
    """"""Print a log message to standard error.""""""
    sys.stderr.write(sprint(""{}: {}"".format(script_name, text), *colors) + ""\n"")","['def', 'print_log', '(', 'text', ',', '*', 'colors', ')', ':', 'sys', '.', 'stderr', '.', 'write', '(', 'sprint', '(', '""{}: {}""', '.', 'format', '(', 'script_name', ',', 'text', ')', ',', '*', 'colors', ')', '+', '""\\n""', ')']",Print a log message to standard error.,"['Print', 'a', 'log', 'message', 'to', 'standard', 'error', '.']",python,test,"['print', 'a', 'log', 'message', 'to', 'standard', 'error', '.']",print a log message to standard error .,"['def', 'print_log', '(', 'text', ',', '*', 'colors', ')', ':', 'sys', '.', 'stderr', '.', 'write', '(', 'sprint', '(', '""{}: {}""', '.', 'format', '(', 'script_name', ',', 'text', ')', ',', '*', 'colors', ')', '+', '""\\n""', ')']","def print_log ( text , * colors ) : sys . stderr . write ( sprint ( ""{}: {}"" . format ( script_name , text ) , * colors ) + ""\n"" )"
18,soimort/you-get,src/you_get/util/log.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L88-L92,"def e(message, exit_code=None):
    """"""Print an error log message.""""""
    print_log(message, YELLOW, BOLD)
    if exit_code is not None:
        sys.exit(exit_code)","['def', 'e', '(', 'message', ',', 'exit_code', '=', 'None', ')', ':', 'print_log', '(', 'message', ',', 'YELLOW', ',', 'BOLD', ')', 'if', 'exit_code', 'is', 'not', 'None', ':', 'sys', '.', 'exit', '(', 'exit_code', ')']",Print an error log message.,"['Print', 'an', 'error', 'log', 'message', '.']",python,test,"['print', 'an', 'error', 'log', 'message', '.']",print an error log message .,"['def', 'e', '(', 'message', ',', 'exit_code', '=', 'none', ')', ':', 'print_log', '(', 'message', ',', 'yellow', ',', 'bold', ')', 'if', 'exit_code', 'is', 'not', 'none', ':', 'sys', '.', 'exit', '(', 'exit_code', ')']","def e ( message , exit_code = none ) : print_log ( message , yellow , bold ) if exit_code is not none : sys . exit ( exit_code )"
19,soimort/you-get,src/you_get/util/log.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L94-L98,"def wtf(message, exit_code=1):
    """"""What a Terrible Failure!""""""
    print_log(message, RED, BOLD)
    if exit_code is not None:
        sys.exit(exit_code)","['def', 'wtf', '(', 'message', ',', 'exit_code', '=', '1', ')', ':', 'print_log', '(', 'message', ',', 'RED', ',', 'BOLD', ')', 'if', 'exit_code', 'is', 'not', 'None', ':', 'sys', '.', 'exit', '(', 'exit_code', ')']",What a Terrible Failure!,"['What', 'a', 'Terrible', 'Failure!']",python,test,"['what', 'a', 'terrible', 'failure!']",what a terrible failure!,"['def', 'wtf', '(', 'message', ',', 'exit_code', '=', '1', ')', ':', 'print_log', '(', 'message', ',', 'red', ',', 'bold', ')', 'if', 'exit_code', 'is', 'not', 'none', ':', 'sys', '.', 'exit', '(', 'exit_code', ')']","def wtf ( message , exit_code = 1 ) : print_log ( message , red , bold ) if exit_code is not none : sys . exit ( exit_code )"
20,soimort/you-get,src/you_get/util/os.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/os.py#L5-L32,"def detect_os():
    """"""Detect operating system.
    """"""

    # Inspired by:
    # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py

    syst = system().lower()
    os = 'unknown'

    if 'cygwin' in syst:
        os = 'cygwin'
    elif 'darwin' in syst:
        os = 'mac'
    elif 'linux' in syst:
        os = 'linux'
        # detect WSL https://github.com/Microsoft/BashOnWindows/issues/423
        try:
            with open('/proc/version', 'r') as f:
                if 'microsoft' in f.read().lower():
                    os = 'wsl'
        except: pass
    elif 'windows' in syst:
        os = 'windows'
    elif 'bsd' in syst:
        os = 'bsd'

    return os","['def', 'detect_os', '(', ')', ':', '# Inspired by:', '# https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py', 'syst', '=', 'system', '(', ')', '.', 'lower', '(', ')', 'os', '=', ""'unknown'"", 'if', ""'cygwin'"", 'in', 'syst', ':', 'os', '=', ""'cygwin'"", 'elif', ""'darwin'"", 'in', 'syst', ':', 'os', '=', ""'mac'"", 'elif', ""'linux'"", 'in', 'syst', ':', 'os', '=', ""'linux'"", '# detect WSL https://github.com/Microsoft/BashOnWindows/issues/423', 'try', ':', 'with', 'open', '(', ""'/proc/version'"", ',', ""'r'"", ')', 'as', 'f', ':', 'if', ""'microsoft'"", 'in', 'f', '.', 'read', '(', ')', '.', 'lower', '(', ')', ':', 'os', '=', ""'wsl'"", 'except', ':', 'pass', 'elif', ""'windows'"", 'in', 'syst', ':', 'os', '=', ""'windows'"", 'elif', ""'bsd'"", 'in', 'syst', ':', 'os', '=', ""'bsd'"", 'return', 'os']",Detect operating system.,"['Detect', 'operating', 'system', '.']",python,test,"['detect', 'operating', 'system', '.']",detect operating system .,"['def', 'detect_os', '(', ')', ':', '# inspired by:', '# https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py', 'syst', '=', 'system', '(', ')', '.', 'lower', '(', ')', 'os', '=', ""'unknown'"", 'if', ""'cygwin'"", 'in', 'syst', ':', 'os', '=', ""'cygwin'"", 'elif', ""'darwin'"", 'in', 'syst', ':', 'os', '=', ""'mac'"", 'elif', ""'linux'"", 'in', 'syst', ':', 'os', '=', ""'linux'"", '# detect wsl https://github.com/microsoft/bashonwindows/issues/423', 'try', ':', 'with', 'open', '(', ""'/proc/version'"", ',', ""'r'"", ')', 'as', 'f', ':', 'if', ""'microsoft'"", 'in', 'f', '.', 'read', '(', ')', '.', 'lower', '(', ')', ':', 'os', '=', ""'wsl'"", 'except', ':', 'pass', 'elif', ""'windows'"", 'in', 'syst', ':', 'os', '=', ""'windows'"", 'elif', ""'bsd'"", 'in', 'syst', ':', 'os', '=', ""'bsd'"", 'return', 'os']","def detect_os ( ) : # inspired by: # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py syst = system ( ) . lower ( ) os = 'unknown' if 'cygwin' in syst : os = 'cygwin' elif 'darwin' in syst : os = 'mac' elif 'linux' in syst : os = 'linux' # detect wsl https://github.com/microsoft/bashonwindows/issues/423 try : with open ( '/proc/version' , 'r' ) as f : if 'microsoft' in f . read ( ) . lower ( ) : os = 'wsl' except : pass elif 'windows' in syst : os = 'windows' elif 'bsd' in syst : os = 'bsd' return os"
21,soimort/you-get,src/you_get/extractors/miaopai.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/miaopai.py#L20-L37,"def miaopai_download_by_fid(fid, output_dir = '.', merge = False, info_only = False, **kwargs):
    '''Source: Android mobile'''
    page_url = 'http://video.weibo.com/show?fid=' + fid + '&type=mp4'

    mobile_page = get_content(page_url, headers=fake_headers_mobile)
    url = match1(mobile_page, r'<video id=.*?src=[\'""](.*?)[\'""]\W')
    if url is None:
        wb_mp = re.search(r'<script src=([\'""])(.+?wb_mp\.js)\1>', mobile_page).group(2)
        return miaopai_download_by_wbmp(wb_mp, fid, output_dir=output_dir, merge=merge,
                                        info_only=info_only, total_size=None, **kwargs)
    title = match1(mobile_page, r'<title>((.|\n)+?)</title>')
    if not title:
        title = fid
    title = title.replace('\n', '_')
    ext, size = 'mp4', url_info(url)[2]
    print_info(site_info, title, ext, size)
    if not info_only:
        download_urls([url], title, ext, total_size=None, output_dir=output_dir, merge=merge)","['def', 'miaopai_download_by_fid', '(', 'fid', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'False', ',', 'info_only', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', 'page_url', '=', ""'http://video.weibo.com/show?fid='"", '+', 'fid', '+', ""'&type=mp4'"", 'mobile_page', '=', 'get_content', '(', 'page_url', ',', 'headers', '=', 'fake_headers_mobile', ')', 'url', '=', 'match1', '(', 'mobile_page', ',', 'r\'<video id=.*?src=[\\\'""](.*?)[\\\'""]\\W\'', ')', 'if', 'url', 'is', 'None', ':', 'wb_mp', '=', 're', '.', 'search', '(', 'r\'<script src=([\\\'""])(.+?wb_mp\\.js)\\1>\'', ',', 'mobile_page', ')', '.', 'group', '(', '2', ')', 'return', 'miaopai_download_by_wbmp', '(', 'wb_mp', ',', 'fid', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ',', 'total_size', '=', 'None', ',', '*', '*', 'kwargs', ')', 'title', '=', 'match1', '(', 'mobile_page', ',', ""r'<title>((.|\\n)+?)</title>'"", ')', 'if', 'not', 'title', ':', 'title', '=', 'fid', 'title', '=', 'title', '.', 'replace', '(', ""'\\n'"", ',', ""'_'"", ')', 'ext', ',', 'size', '=', ""'mp4'"", ',', 'url_info', '(', 'url', ')', '[', '2', ']', 'print_info', '(', 'site_info', ',', 'title', ',', 'ext', ',', 'size', ')', 'if', 'not', 'info_only', ':', 'download_urls', '(', '[', 'url', ']', ',', 'title', ',', 'ext', ',', 'total_size', '=', 'None', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ')']",Source: Android mobile,"['Source', ':', 'Android', 'mobile']",python,test,"['source', ':', 'android', 'mobile']",source : android mobile,"['def', 'miaopai_download_by_fid', '(', 'fid', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'false', ',', 'info_only', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', 'page_url', '=', ""'http://video.weibo.com/show?fid='"", '+', 'fid', '+', ""'&type=mp4'"", 'mobile_page', '=', 'get_content', '(', 'page_url', ',', 'headers', '=', 'fake_headers_mobile', ')', 'url', '=', 'match1', '(', 'mobile_page', ',', 'r\'<video id=.*?src=[\\\'""](.*?)[\\\'""]\\w\'', ')', 'if', 'url', 'is', 'none', ':', 'wb_mp', '=', 're', '.', 'search', '(', 'r\'<script src=([\\\'""])(.+?wb_mp\\.js)\\1>\'', ',', 'mobile_page', ')', '.', 'group', '(', '2', ')', 'return', 'miaopai_download_by_wbmp', '(', 'wb_mp', ',', 'fid', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ',', 'total_size', '=', 'none', ',', '*', '*', 'kwargs', ')', 'title', '=', 'match1', '(', 'mobile_page', ',', ""r'<title>((.|\\n)+?)</title>'"", ')', 'if', 'not', 'title', ':', 'title', '=', 'fid', 'title', '=', 'title', '.', 'replace', '(', ""'\\n'"", ',', ""'_'"", ')', 'ext', ',', 'size', '=', ""'mp4'"", ',', 'url_info', '(', 'url', ')', '[', '2', ']', 'print_info', '(', 'site_info', ',', 'title', ',', 'ext', ',', 'size', ')', 'if', 'not', 'info_only', ':', 'download_urls', '(', '[', 'url', ']', ',', 'title', ',', 'ext', ',', 'total_size', '=', 'none', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ')']","def miaopai_download_by_fid ( fid , output_dir = '.' , merge = false , info_only = false , * * kwargs ) : page_url = 'http://video.weibo.com/show?fid=' + fid + '&type=mp4' mobile_page = get_content ( page_url , headers = fake_headers_mobile ) url = match1 ( mobile_page , r'<video id=.*?src=[\'""](.*?)[\'""]\w' ) if url is none : wb_mp = re . search ( r'<script src=([\'""])(.+?wb_mp\.js)\1>' , mobile_page ) . group ( 2 ) return miaopai_download_by_wbmp ( wb_mp , fid , output_dir = output_dir , merge = merge , info_only = info_only , total_size = none , * * kwargs ) title = match1 ( mobile_page , r'<title>((.|\n)+?)</title>' ) if not title : title = fid title = title . replace ( '\n' , '_' ) ext , size = 'mp4' , url_info ( url ) [ 2 ] print_info ( site_info , title , ext , size ) if not info_only : download_urls ( [ url ] , title , ext , total_size = none , output_dir = output_dir , merge = merge )"
22,soimort/you-get,src/you_get/extractors/vimeo.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/vimeo.py#L15-L19,"def vimeo_download_by_channel(url, output_dir='.', merge=False, info_only=False, **kwargs):
    """"""str->None""""""
    # https://vimeo.com/channels/464686
    channel_id = match1(url, r'http://vimeo.com/channels/(\w+)')
    vimeo_download_by_channel_id(channel_id, output_dir, merge, info_only, **kwargs)","['def', 'vimeo_download_by_channel', '(', 'url', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'False', ',', 'info_only', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', '# https://vimeo.com/channels/464686', 'channel_id', '=', 'match1', '(', 'url', ',', ""r'http://vimeo.com/channels/(\\w+)'"", ')', 'vimeo_download_by_channel_id', '(', 'channel_id', ',', 'output_dir', ',', 'merge', ',', 'info_only', ',', '*', '*', 'kwargs', ')']",str->None,"['str', '-', '>', 'None']",python,test,"['str', '-', '>', 'none']",str - > none,"['def', 'vimeo_download_by_channel', '(', 'url', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'false', ',', 'info_only', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', '# https://vimeo.com/channels/464686', 'channel_id', '=', 'match1', '(', 'url', ',', ""r'http://vimeo.com/channels/(\\w+)'"", ')', 'vimeo_download_by_channel_id', '(', 'channel_id', ',', 'output_dir', ',', 'merge', ',', 'info_only', ',', '*', '*', 'kwargs', ')']","def vimeo_download_by_channel ( url , output_dir = '.' , merge = false , info_only = false , * * kwargs ) : # https://vimeo.com/channels/464686 channel_id = match1 ( url , r'http://vimeo.com/channels/(\w+)' ) vimeo_download_by_channel_id ( channel_id , output_dir , merge , info_only , * * kwargs )"
23,soimort/you-get,src/you_get/extractors/vimeo.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/vimeo.py#L22-L36,"def vimeo_download_by_channel_id(channel_id, output_dir='.', merge=False, info_only=False, **kwargs):
    """"""str/int->None""""""
    html = get_content('https://api.vimeo.com/channels/{channel_id}/videos?access_token={access_token}'.format(channel_id=channel_id, access_token=access_token))
    data = loads(html)
    id_list = []

    #print(data)
    for i in data['data']:
        id_list.append(match1(i['uri'], r'/videos/(\w+)'))

    for id in id_list:
        try:
            vimeo_download_by_id(id, None, output_dir, merge, info_only, **kwargs)
        except urllib.error.URLError as e:
            log.w('{} failed with {}'.format(id, e))","['def', 'vimeo_download_by_channel_id', '(', 'channel_id', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'False', ',', 'info_only', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', 'html', '=', 'get_content', '(', ""'https://api.vimeo.com/channels/{channel_id}/videos?access_token={access_token}'"", '.', 'format', '(', 'channel_id', '=', 'channel_id', ',', 'access_token', '=', 'access_token', ')', ')', 'data', '=', 'loads', '(', 'html', ')', 'id_list', '=', '[', ']', '#print(data)', 'for', 'i', 'in', 'data', '[', ""'data'"", ']', ':', 'id_list', '.', 'append', '(', 'match1', '(', 'i', '[', ""'uri'"", ']', ',', ""r'/videos/(\\w+)'"", ')', ')', 'for', 'id', 'in', 'id_list', ':', 'try', ':', 'vimeo_download_by_id', '(', 'id', ',', 'None', ',', 'output_dir', ',', 'merge', ',', 'info_only', ',', '*', '*', 'kwargs', ')', 'except', 'urllib', '.', 'error', '.', 'URLError', 'as', 'e', ':', 'log', '.', 'w', '(', ""'{} failed with {}'"", '.', 'format', '(', 'id', ',', 'e', ')', ')']",str/int->None,"['str', '/', 'int', '-', '>', 'None']",python,test,"['str', '/', 'int', '-', '>', 'none']",str / int - > none,"['def', 'vimeo_download_by_channel_id', '(', 'channel_id', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'false', ',', 'info_only', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', 'html', '=', 'get_content', '(', ""'https://api.vimeo.com/channels/{channel_id}/videos?access_token={access_token}'"", '.', 'format', '(', 'channel_id', '=', 'channel_id', ',', 'access_token', '=', 'access_token', ')', ')', 'data', '=', 'loads', '(', 'html', ')', 'id_list', '=', '[', ']', '#print(data)', 'for', 'i', 'in', 'data', '[', ""'data'"", ']', ':', 'id_list', '.', 'append', '(', 'match1', '(', 'i', '[', ""'uri'"", ']', ',', ""r'/videos/(\\w+)'"", ')', ')', 'for', 'id', 'in', 'id_list', ':', 'try', ':', 'vimeo_download_by_id', '(', 'id', ',', 'none', ',', 'output_dir', ',', 'merge', ',', 'info_only', ',', '*', '*', 'kwargs', ')', 'except', 'urllib', '.', 'error', '.', 'urlerror', 'as', 'e', ':', 'log', '.', 'w', '(', ""'{} failed with {}'"", '.', 'format', '(', 'id', ',', 'e', ')', ')']","def vimeo_download_by_channel_id ( channel_id , output_dir = '.' , merge = false , info_only = false , * * kwargs ) : html = get_content ( 'https://api.vimeo.com/channels/{channel_id}/videos?access_token={access_token}' . format ( channel_id = channel_id , access_token = access_token ) ) data = loads ( html ) id_list = [ ] #print(data) for i in data [ 'data' ] : id_list . append ( match1 ( i [ 'uri' ] , r'/videos/(\w+)' ) ) for id in id_list : try : vimeo_download_by_id ( id , none , output_dir , merge , info_only , * * kwargs ) except urllib . error . urlerror as e : log . w ( '{} failed with {}' . format ( id , e ) )"
24,soimort/you-get,src/you_get/extractors/vimeo.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/vimeo.py#L134-L164,"def vimeo_download_by_id(id, title=None, output_dir='.', merge=True, info_only=False, **kwargs):
    '''
    try:
        # normal Vimeo video
        html = get_content('https://vimeo.com/' + id)
        cfg_patt = r'clip_page_config\s*=\s*(\{.+?\});'
        cfg = json.loads(match1(html, cfg_patt))
        video_page = get_content(cfg['player']['config_url'], headers=fake_headers)
        title = cfg['clip']['title']
        info = loads(video_page)
    except:
        # embedded player - referer may be required
        if 'referer' in kwargs:
            fake_headers['Referer'] = kwargs['referer']

        video_page = get_content('http://player.vimeo.com/video/%s' % id, headers=fake_headers)
        title = r1(r'<title>([^<]+)</title>', video_page)
        info = loads(match1(video_page, r'var t=(\{.+?\});'))

    streams = info['request']['files']['progressive']
    streams = sorted(streams, key=lambda i: i['height'])
    url = streams[-1]['url']

    type, ext, size = url_info(url, faker=True)

    print_info(site_info, title, type, size)
    if not info_only:
        download_urls([url], title, ext, size, output_dir, merge=merge, faker=True)
    '''
    site = VimeoExtractor()
    site.download_by_vid(id, info_only=info_only, output_dir=output_dir, merge=merge, **kwargs)","['def', 'vimeo_download_by_id', '(', 'id', ',', 'title', '=', 'None', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'True', ',', 'info_only', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', 'site', '=', 'VimeoExtractor', '(', ')', 'site', '.', 'download_by_vid', '(', 'id', ',', 'info_only', '=', 'info_only', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', '*', '*', 'kwargs', ')']","try:
        # normal Vimeo video
        html = get_content('https://vimeo.com/' + id)
        cfg_patt = r'clip_page_config\s*=\s*(\{.+?\});'
        cfg = json.loads(match1(html, cfg_patt))
        video_page = get_content(cfg['player']['config_url'], headers=fake_headers)
        title = cfg['clip']['title']
        info = loads(video_page)
    except:
        # embedded player - referer may be required
        if 'referer' in kwargs:
            fake_headers['Referer'] = kwargs['referer']

        video_page = get_content('http://player.vimeo.com/video/%s' % id, headers=fake_headers)
        title = r1(r'<title>([^<]+)</title>', video_page)
        info = loads(match1(video_page, r'var t=(\{.+?\});'))

    streams = info['request']['files']['progressive']
    streams = sorted(streams, key=lambda i: i['height'])
    url = streams[-1]['url']

    type, ext, size = url_info(url, faker=True)

    print_info(site_info, title, type, size)
    if not info_only:
        download_urls([url], title, ext, size, output_dir, merge=merge, faker=True)","['try', ':', '#', 'normal', 'Vimeo', 'video', 'html', '=', 'get_content', '(', 'https', ':', '//', 'vimeo', '.', 'com', '/', '+', 'id', ')', 'cfg_patt', '=', 'r', 'clip_page_config', '\\', 's', '*', '=', '\\', 's', '*', '(', '\\', '{', '.', '+', '?', '\\', '}', ')', ';', 'cfg', '=', 'json', '.', 'loads', '(', 'match1', '(', 'html', 'cfg_patt', '))', 'video_page', '=', 'get_content', '(', 'cfg', '[', 'player', ']', '[', 'config_url', ']', 'headers', '=', 'fake_headers', ')', 'title', '=', 'cfg', '[', 'clip', ']', '[', 'title', ']', 'info', '=', 'loads', '(', 'video_page', ')', 'except', ':', '#', 'embedded', 'player', '-', 'referer', 'may', 'be', 'required', 'if', 'referer', 'in', 'kwargs', ':', 'fake_headers', '[', 'Referer', ']', '=', 'kwargs', '[', 'referer', ']']",python,test,"['try', ':', '#', 'normal', 'vimeo', 'video', 'html', '=', 'get_content', '(', 'https', ':', '//', 'vimeo', '.', 'com', '/', '+', 'id', ')', 'cfg_patt', '=', 'r', 'clip_page_config', '\\', 's', '*', '=', '\\', 's', '*', '(', '\\', '{', '.', '+', '?', '\\', '}', ')', ';', 'cfg', '=', 'json', '.', 'loads', '(', 'match1', '(', 'html', 'cfg_patt', '))', 'video_page', '=', 'get_content', '(', 'cfg', '[', 'player', ']', '[', 'config_url', ']', 'headers', '=', 'fake_headers', ')', 'title', '=', 'cfg', '[', 'clip', ']', '[', 'title', ']', 'info', '=', 'loads', '(', 'video_page', ')', 'except', ':', '#', 'embedded', 'player', '-', 'referer', 'may', 'be', 'required', 'if', 'referer', 'in', 'kwargs', ':', 'fake_headers', '[', 'referer', ']', '=', 'kwargs', '[', 'referer', ']']",try : # normal vimeo video html = get_content ( https : // vimeo . com / + id ) cfg_patt = r clip_page_config \ s * = \ s * ( \ { . + ? \ } ) ; cfg = json . loads ( match1 ( html cfg_patt )) video_page = get_content ( cfg [ player ] [ config_url ] headers = fake_headers ) title = cfg [ clip ] [ title ] info = loads ( video_page ) except : # embedded player - referer may be required if referer in kwargs : fake_headers [ referer ] = kwargs [ referer ],"['def', 'vimeo_download_by_id', '(', 'id', ',', 'title', '=', 'none', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'true', ',', 'info_only', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', 'site', '=', 'vimeoextractor', '(', ')', 'site', '.', 'download_by_vid', '(', 'id', ',', 'info_only', '=', 'info_only', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', '*', '*', 'kwargs', ')']","def vimeo_download_by_id ( id , title = none , output_dir = '.' , merge = true , info_only = false , * * kwargs ) : site = vimeoextractor ( ) site . download_by_vid ( id , info_only = info_only , output_dir = output_dir , merge = merge , * * kwargs )"
25,soimort/you-get,src/you_get/extractors/ckplayer.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/ckplayer.py#L13-L39,"def ckplayer_get_info_by_xml(ckinfo):
    """"""str->dict
    Information for CKPlayer API content.""""""
    e = ET.XML(ckinfo)
    video_dict = {'title': '',
                  #'duration': 0,
                  'links': [],
                  'size': 0,
                  'flashvars': '',}
    dictified = dictify(e)['ckplayer']
    if 'info' in dictified:
        if '_text' in dictified['info'][0]['title'][0]:  #title
            video_dict['title'] = dictified['info'][0]['title'][0]['_text'].strip()

    #if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration
        #video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip()

    if '_text' in dictified['video'][0]['size'][0]:  #size exists for 1 piece
        video_dict['size'] = sum([int(i['size'][0]['_text']) for i in dictified['video']])

    if '_text' in dictified['video'][0]['file'][0]:  #link exist
        video_dict['links'] = [i['file'][0]['_text'].strip() for i in dictified['video']]

    if '_text' in dictified['flashvars'][0]:
        video_dict['flashvars'] = dictified['flashvars'][0]['_text'].strip()

    return video_dict","['def', 'ckplayer_get_info_by_xml', '(', 'ckinfo', ')', ':', 'e', '=', 'ET', '.', 'XML', '(', 'ckinfo', ')', 'video_dict', '=', '{', ""'title'"", ':', ""''"", ',', ""#'duration': 0,"", ""'links'"", ':', '[', ']', ',', ""'size'"", ':', '0', ',', ""'flashvars'"", ':', ""''"", ',', '}', 'dictified', '=', 'dictify', '(', 'e', ')', '[', ""'ckplayer'"", ']', 'if', ""'info'"", 'in', 'dictified', ':', 'if', ""'_text'"", 'in', 'dictified', '[', ""'info'"", ']', '[', '0', ']', '[', ""'title'"", ']', '[', '0', ']', ':', '#title', 'video_dict', '[', ""'title'"", ']', '=', 'dictified', '[', ""'info'"", ']', '[', '0', ']', '[', ""'title'"", ']', '[', '0', ']', '[', ""'_text'"", ']', '.', 'strip', '(', ')', ""#if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration"", ""#video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip()"", 'if', ""'_text'"", 'in', 'dictified', '[', ""'video'"", ']', '[', '0', ']', '[', ""'size'"", ']', '[', '0', ']', ':', '#size exists for 1 piece', 'video_dict', '[', ""'size'"", ']', '=', 'sum', '(', '[', 'int', '(', 'i', '[', ""'size'"", ']', '[', '0', ']', '[', ""'_text'"", ']', ')', 'for', 'i', 'in', 'dictified', '[', ""'video'"", ']', ']', ')', 'if', ""'_text'"", 'in', 'dictified', '[', ""'video'"", ']', '[', '0', ']', '[', ""'file'"", ']', '[', '0', ']', ':', '#link exist', 'video_dict', '[', ""'links'"", ']', '=', '[', 'i', '[', ""'file'"", ']', '[', '0', ']', '[', ""'_text'"", ']', '.', 'strip', '(', ')', 'for', 'i', 'in', 'dictified', '[', ""'video'"", ']', ']', 'if', ""'_text'"", 'in', 'dictified', '[', ""'flashvars'"", ']', '[', '0', ']', ':', 'video_dict', '[', ""'flashvars'"", ']', '=', 'dictified', '[', ""'flashvars'"", ']', '[', '0', ']', '[', ""'_text'"", ']', '.', 'strip', '(', ')', 'return', 'video_dict']","str->dict
    Information for CKPlayer API content.","['str', '-', '>', 'dict', 'Information', 'for', 'CKPlayer', 'API', 'content', '.']",python,test,"['str', '-', '>', 'dict', 'information', 'for', 'ckplayer', 'api', 'content', '.']",str - > dict information for ckplayer api content .,"['def', 'ckplayer_get_info_by_xml', '(', 'ckinfo', ')', ':', 'e', '=', 'et', '.', 'xml', '(', 'ckinfo', ')', 'video_dict', '=', '{', ""'title'"", ':', ""''"", ',', ""#'duration': 0,"", ""'links'"", ':', '[', ']', ',', ""'size'"", ':', '0', ',', ""'flashvars'"", ':', ""''"", ',', '}', 'dictified', '=', 'dictify', '(', 'e', ')', '[', ""'ckplayer'"", ']', 'if', ""'info'"", 'in', 'dictified', ':', 'if', ""'_text'"", 'in', 'dictified', '[', ""'info'"", ']', '[', '0', ']', '[', ""'title'"", ']', '[', '0', ']', ':', '#title', 'video_dict', '[', ""'title'"", ']', '=', 'dictified', '[', ""'info'"", ']', '[', '0', ']', '[', ""'title'"", ']', '[', '0', ']', '[', ""'_text'"", ']', '.', 'strip', '(', ')', ""#if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration"", ""#video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip()"", 'if', ""'_text'"", 'in', 'dictified', '[', ""'video'"", ']', '[', '0', ']', '[', ""'size'"", ']', '[', '0', ']', ':', '#size exists for 1 piece', 'video_dict', '[', ""'size'"", ']', '=', 'sum', '(', '[', 'int', '(', 'i', '[', ""'size'"", ']', '[', '0', ']', '[', ""'_text'"", ']', ')', 'for', 'i', 'in', 'dictified', '[', ""'video'"", ']', ']', ')', 'if', ""'_text'"", 'in', 'dictified', '[', ""'video'"", ']', '[', '0', ']', '[', ""'file'"", ']', '[', '0', ']', ':', '#link exist', 'video_dict', '[', ""'links'"", ']', '=', '[', 'i', '[', ""'file'"", ']', '[', '0', ']', '[', ""'_text'"", ']', '.', 'strip', '(', ')', 'for', 'i', 'in', 'dictified', '[', ""'video'"", ']', ']', 'if', ""'_text'"", 'in', 'dictified', '[', ""'flashvars'"", ']', '[', '0', ']', ':', 'video_dict', '[', ""'flashvars'"", ']', '=', 'dictified', '[', ""'flashvars'"", ']', '[', '0', ']', '[', ""'_text'"", ']', '.', 'strip', '(', ')', 'return', 'video_dict']","def ckplayer_get_info_by_xml ( ckinfo ) : e = et . xml ( ckinfo ) video_dict = { 'title' : '' , #'duration': 0, 'links' : [ ] , 'size' : 0 , 'flashvars' : '' , } dictified = dictify ( e ) [ 'ckplayer' ] if 'info' in dictified : if '_text' in dictified [ 'info' ] [ 0 ] [ 'title' ] [ 0 ] : #title video_dict [ 'title' ] = dictified [ 'info' ] [ 0 ] [ 'title' ] [ 0 ] [ '_text' ] . strip ( ) #if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration #video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip() if '_text' in dictified [ 'video' ] [ 0 ] [ 'size' ] [ 0 ] : #size exists for 1 piece video_dict [ 'size' ] = sum ( [ int ( i [ 'size' ] [ 0 ] [ '_text' ] ) for i in dictified [ 'video' ] ] ) if '_text' in dictified [ 'video' ] [ 0 ] [ 'file' ] [ 0 ] : #link exist video_dict [ 'links' ] = [ i [ 'file' ] [ 0 ] [ '_text' ] . strip ( ) for i in dictified [ 'video' ] ] if '_text' in dictified [ 'flashvars' ] [ 0 ] : video_dict [ 'flashvars' ] = dictified [ 'flashvars' ] [ 0 ] [ '_text' ] . strip ( ) return video_dict"
26,soimort/you-get,src/you_get/extractors/ixigua.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/ixigua.py#L34-L78,"def get_video_url_from_video_id(video_id):
    """"""Splicing URLs according to video ID to get video details""""""
    # from js
    data = [""""] * 256
    for index, _ in enumerate(data):
        t = index
        for i in range(8):
            t = -306674912 ^ unsigned_right_shitf(t, 1) if 1 & t else unsigned_right_shitf(t, 1)
        data[index] = t

    def tmp():
        rand_num = random.random()
        path = ""/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}"".format(video_id=video_id,
                                                                              random_num=str(rand_num)[2:])
        e = o = r = -1
        i, a = 0, len(path)
        while i < a:
            e = ord(path[i])
            i += 1
            if e < 128:
                r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ e)]
            else:
                if e < 2048:
                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (192 | e >> 6 & 31))]
                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]
                else:
                    if 55296 <= e < 57344:
                        e = (1023 & e) + 64
                        i += 1
                        o = 1023 & t.url(i)
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (240 | e >> 8 & 7))]
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 2 & 63))]
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | o >> 6 & 15 | (3 & e) << 4))]
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & o))]
                    else:
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (224 | e >> 12 & 15))]
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 6 & 63))]
                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]

        return ""https://ib.365yg.com{path}&s={param}"".format(path=path, param=unsigned_right_shitf(r ^ -1, 0))

    while 1:
        url = tmp()
        if url.split(""="")[-1][0] != ""-"":  # 参数s不能为负数
            return url","['def', 'get_video_url_from_video_id', '(', 'video_id', ')', ':', '# from js', 'data', '=', '[', '""""', ']', '*', '256', 'for', 'index', ',', '_', 'in', 'enumerate', '(', 'data', ')', ':', 't', '=', 'index', 'for', 'i', 'in', 'range', '(', '8', ')', ':', 't', '=', '-', '306674912', '^', 'unsigned_right_shitf', '(', 't', ',', '1', ')', 'if', '1', '&', 't', 'else', 'unsigned_right_shitf', '(', 't', ',', '1', ')', 'data', '[', 'index', ']', '=', 't', 'def', 'tmp', '(', ')', ':', 'rand_num', '=', 'random', '.', 'random', '(', ')', 'path', '=', '""/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}""', '.', 'format', '(', 'video_id', '=', 'video_id', ',', 'random_num', '=', 'str', '(', 'rand_num', ')', '[', '2', ':', ']', ')', 'e', '=', 'o', '=', 'r', '=', '-', '1', 'i', ',', 'a', '=', '0', ',', 'len', '(', 'path', ')', 'while', 'i', '<', 'a', ':', 'e', '=', 'ord', '(', 'path', '[', 'i', ']', ')', 'i', '+=', '1', 'if', 'e', '<', '128', ':', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', 'e', ')', ']', 'else', ':', 'if', 'e', '<', '2048', ':', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', '(', '192', '|', 'e', '>>', '6', '&', '31', ')', ')', ']', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', '(', '128', '|', '63', '&', 'e', ')', ')', ']', 'else', ':', 'if', '55296', '<=', 'e', '<', '57344', ':', 'e', '=', '(', '1023', '&', 'e', ')', '+', '64', 'i', '+=', '1', 'o', '=', '1023', '&', 't', '.', 'url', '(', 'i', ')', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', '(', '240', '|', 'e', '>>', '8', '&', '7', ')', ')', ']', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', '(', '128', '|', 'e', '>>', '2', '&', '63', ')', ')', ']', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', '(', '128', '|', 'o', '>>', '6', '&', '15', '|', '(', '3', '&', 'e', ')', '<<', '4', ')', ')', ']', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', '(', '128', '|', '63', '&', 'o', ')', ')', ']', 'else', ':', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', '(', '224', '|', 'e', '>>', '12', '&', '15', ')', ')', ']', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', '(', '128', '|', 'e', '>>', '6', '&', '63', ')', ')', ']', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', '(', '128', '|', '63', '&', 'e', ')', ')', ']', 'return', '""https://ib.365yg.com{path}&s={param}""', '.', 'format', '(', 'path', '=', 'path', ',', 'param', '=', 'unsigned_right_shitf', '(', 'r', '^', '-', '1', ',', '0', ')', ')', 'while', '1', ':', 'url', '=', 'tmp', '(', ')', 'if', 'url', '.', 'split', '(', '""=""', ')', '[', '-', '1', ']', '[', '0', ']', '!=', '""-""', ':', '# 参数s不能为负数', 'return', 'url']",Splicing URLs according to video ID to get video details,"['Splicing', 'URLs', 'according', 'to', 'video', 'ID', 'to', 'get', 'video', 'details']",python,test,"['splicing', 'urls', 'according', 'to', 'video', 'id', 'to', 'get', 'video', 'details']",splicing urls according to video id to get video details,"['def', 'get_video_url_from_video_id', '(', 'video_id', ')', ':', '# from js', 'data', '=', '[', '""""', ']', '*', '256', 'for', 'index', ',', '_', 'in', 'enumerate', '(', 'data', ')', ':', 't', '=', 'index', 'for', 'i', 'in', 'range', '(', '8', ')', ':', 't', '=', '-', '306674912', '^', 'unsigned_right_shitf', '(', 't', ',', '1', ')', 'if', '1', '&', 't', 'else', 'unsigned_right_shitf', '(', 't', ',', '1', ')', 'data', '[', 'index', ']', '=', 't', 'def', 'tmp', '(', ')', ':', 'rand_num', '=', 'random', '.', 'random', '(', ')', 'path', '=', '""/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}""', '.', 'format', '(', 'video_id', '=', 'video_id', ',', 'random_num', '=', 'str', '(', 'rand_num', ')', '[', '2', ':', ']', ')', 'e', '=', 'o', '=', 'r', '=', '-', '1', 'i', ',', 'a', '=', '0', ',', 'len', '(', 'path', ')', 'while', 'i', '<', 'a', ':', 'e', '=', 'ord', '(', 'path', '[', 'i', ']', ')', 'i', '+=', '1', 'if', 'e', '<', '128', ':', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', 'e', ')', ']', 'else', ':', 'if', 'e', '<', '2048', ':', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', '(', '192', '|', 'e', '>>', '6', '&', '31', ')', ')', ']', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', '(', '128', '|', '63', '&', 'e', ')', ')', ']', 'else', ':', 'if', '55296', '<=', 'e', '<', '57344', ':', 'e', '=', '(', '1023', '&', 'e', ')', '+', '64', 'i', '+=', '1', 'o', '=', '1023', '&', 't', '.', 'url', '(', 'i', ')', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', '(', '240', '|', 'e', '>>', '8', '&', '7', ')', ')', ']', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', '(', '128', '|', 'e', '>>', '2', '&', '63', ')', ')', ']', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', '(', '128', '|', 'o', '>>', '6', '&', '15', '|', '(', '3', '&', 'e', ')', '<<', '4', ')', ')', ']', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', '(', '128', '|', '63', '&', 'o', ')', ')', ']', 'else', ':', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', '(', '224', '|', 'e', '>>', '12', '&', '15', ')', ')', ']', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', '(', '128', '|', 'e', '>>', '6', '&', '63', ')', ')', ']', 'r', '=', 'unsigned_right_shitf', '(', 'r', ',', '8', ')', '^', 'data', '[', '255', '&', '(', 'r', '^', '(', '128', '|', '63', '&', 'e', ')', ')', ']', 'return', '""https://ib.365yg.com{path}&s={param}""', '.', 'format', '(', 'path', '=', 'path', ',', 'param', '=', 'unsigned_right_shitf', '(', 'r', '^', '-', '1', ',', '0', ')', ')', 'while', '1', ':', 'url', '=', 'tmp', '(', ')', 'if', 'url', '.', 'split', '(', '""=""', ')', '[', '-', '1', ']', '[', '0', ']', '!=', '""-""', ':', '# 参数s不能为负数', 'return', 'url']","def get_video_url_from_video_id ( video_id ) : # from js data = [ """" ] * 256 for index , _ in enumerate ( data ) : t = index for i in range ( 8 ) : t = - 306674912 ^ unsigned_right_shitf ( t , 1 ) if 1 & t else unsigned_right_shitf ( t , 1 ) data [ index ] = t def tmp ( ) : rand_num = random . random ( ) path = ""/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}"" . format ( video_id = video_id , random_num = str ( rand_num ) [ 2 : ] ) e = o = r = - 1 i , a = 0 , len ( path ) while i < a : e = ord ( path [ i ] ) i += 1 if e < 128 : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ e ) ] else : if e < 2048 : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 192 | e >> 6 & 31 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] else : if 55296 <= e < 57344 : e = ( 1023 & e ) + 64 i += 1 o = 1023 & t . url ( i ) r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 240 | e >> 8 & 7 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 2 & 63 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | o >> 6 & 15 | ( 3 & e ) << 4 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & o ) ) ] else : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 224 | e >> 12 & 15 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 6 & 63 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] return ""https://ib.365yg.com{path}&s={param}"" . format ( path = path , param = unsigned_right_shitf ( r ^ - 1 , 0 ) ) while 1 : url = tmp ( ) if url . split ( ""="" ) [ - 1 ] [ 0 ] != ""-"" : # 参数s不能为负数 return url"
27,soimort/you-get,src/you_get/extractors/mgtv.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/mgtv.py#L27-L33,"def get_vid_from_url(url):
        """"""Extracts video ID from URL.
        """"""
        vid = match1(url, 'https?://www.mgtv.com/(?:b|l)/\d+/(\d+).html')
        if not vid:
            vid = match1(url, 'https?://www.mgtv.com/hz/bdpz/\d+/(\d+).html')
        return vid","['def', 'get_vid_from_url', '(', 'url', ')', ':', 'vid', '=', 'match1', '(', 'url', ',', ""'https?://www.mgtv.com/(?:b|l)/\\d+/(\\d+).html'"", ')', 'if', 'not', 'vid', ':', 'vid', '=', 'match1', '(', 'url', ',', ""'https?://www.mgtv.com/hz/bdpz/\\d+/(\\d+).html'"", ')', 'return', 'vid']",Extracts video ID from URL.,"['Extracts', 'video', 'ID', 'from', 'URL', '.']",python,test,"['extracts', 'video', 'id', 'from', 'url', '.']",extracts video id from url .,"['def', 'get_vid_from_url', '(', 'url', ')', ':', 'vid', '=', 'match1', '(', 'url', ',', ""'https?://www.mgtv.com/(?:b|l)/\\d+/(\\d+).html'"", ')', 'if', 'not', 'vid', ':', 'vid', '=', 'match1', '(', 'url', ',', ""'https?://www.mgtv.com/hz/bdpz/\\d+/(\\d+).html'"", ')', 'return', 'vid']","def get_vid_from_url ( url ) : vid = match1 ( url , 'https?://www.mgtv.com/(?:b|l)/\d+/(\d+).html' ) if not vid : vid = match1 ( url , 'https?://www.mgtv.com/hz/bdpz/\d+/(\d+).html' ) return vid"
28,soimort/you-get,src/you_get/extractors/mgtv.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/mgtv.py#L37-L58,"def get_mgtv_real_url(url):
        """"""str->list of str
        Give you the real URLs.""""""
        content = loads(get_content(url))
        m3u_url = content['info']
        split = urlsplit(m3u_url)
        
        base_url = ""{scheme}://{netloc}{path}/"".format(scheme = split[0],
                                                      netloc = split[1],
                                                      path = dirname(split[2]))

        content = get_content(content['info'])  #get the REAL M3U url, maybe to be changed later?
        segment_list = []
        segments_size = 0
        for i in content.split():
            if not i.startswith('#'):  #not the best way, better we use the m3u8 package
                segment_list.append(base_url + i)
            # use ext-info for fast size calculate
            elif i.startswith('#EXT-MGTV-File-SIZE:'):
                segments_size += int(i[i.rfind(':')+1:])

        return m3u_url, segments_size, segment_list","['def', 'get_mgtv_real_url', '(', 'url', ')', ':', 'content', '=', 'loads', '(', 'get_content', '(', 'url', ')', ')', 'm3u_url', '=', 'content', '[', ""'info'"", ']', 'split', '=', 'urlsplit', '(', 'm3u_url', ')', 'base_url', '=', '""{scheme}://{netloc}{path}/""', '.', 'format', '(', 'scheme', '=', 'split', '[', '0', ']', ',', 'netloc', '=', 'split', '[', '1', ']', ',', 'path', '=', 'dirname', '(', 'split', '[', '2', ']', ')', ')', 'content', '=', 'get_content', '(', 'content', '[', ""'info'"", ']', ')', '#get the REAL M3U url, maybe to be changed later?', 'segment_list', '=', '[', ']', 'segments_size', '=', '0', 'for', 'i', 'in', 'content', '.', 'split', '(', ')', ':', 'if', 'not', 'i', '.', 'startswith', '(', ""'#'"", ')', ':', '#not the best way, better we use the m3u8 package', 'segment_list', '.', 'append', '(', 'base_url', '+', 'i', ')', '# use ext-info for fast size calculate', 'elif', 'i', '.', 'startswith', '(', ""'#EXT-MGTV-File-SIZE:'"", ')', ':', 'segments_size', '+=', 'int', '(', 'i', '[', 'i', '.', 'rfind', '(', ""':'"", ')', '+', '1', ':', ']', ')', 'return', 'm3u_url', ',', 'segments_size', ',', 'segment_list']","str->list of str
        Give you the real URLs.","['str', '-', '>', 'list', 'of', 'str', 'Give', 'you', 'the', 'real', 'URLs', '.']",python,test,"['str', '-', '>', 'list', 'of', 'str', 'give', 'you', 'the', 'real', 'urls', '.']",str - > list of str give you the real urls .,"['def', 'get_mgtv_real_url', '(', 'url', ')', ':', 'content', '=', 'loads', '(', 'get_content', '(', 'url', ')', ')', 'm3u_url', '=', 'content', '[', ""'info'"", ']', 'split', '=', 'urlsplit', '(', 'm3u_url', ')', 'base_url', '=', '""{scheme}://{netloc}{path}/""', '.', 'format', '(', 'scheme', '=', 'split', '[', '0', ']', ',', 'netloc', '=', 'split', '[', '1', ']', ',', 'path', '=', 'dirname', '(', 'split', '[', '2', ']', ')', ')', 'content', '=', 'get_content', '(', 'content', '[', ""'info'"", ']', ')', '#get the real m3u url, maybe to be changed later?', 'segment_list', '=', '[', ']', 'segments_size', '=', '0', 'for', 'i', 'in', 'content', '.', 'split', '(', ')', ':', 'if', 'not', 'i', '.', 'startswith', '(', ""'#'"", ')', ':', '#not the best way, better we use the m3u8 package', 'segment_list', '.', 'append', '(', 'base_url', '+', 'i', ')', '# use ext-info for fast size calculate', 'elif', 'i', '.', 'startswith', '(', ""'#ext-mgtv-file-size:'"", ')', ':', 'segments_size', '+=', 'int', '(', 'i', '[', 'i', '.', 'rfind', '(', ""':'"", ')', '+', '1', ':', ']', ')', 'return', 'm3u_url', ',', 'segments_size', ',', 'segment_list']","def get_mgtv_real_url ( url ) : content = loads ( get_content ( url ) ) m3u_url = content [ 'info' ] split = urlsplit ( m3u_url ) base_url = ""{scheme}://{netloc}{path}/"" . format ( scheme = split [ 0 ] , netloc = split [ 1 ] , path = dirname ( split [ 2 ] ) ) content = get_content ( content [ 'info' ] ) #get the real m3u url, maybe to be changed later? segment_list = [ ] segments_size = 0 for i in content . split ( ) : if not i . startswith ( '#' ) : #not the best way, better we use the m3u8 package segment_list . append ( base_url + i ) # use ext-info for fast size calculate elif i . startswith ( '#ext-mgtv-file-size:' ) : segments_size += int ( i [ i . rfind ( ':' ) + 1 : ] ) return m3u_url , segments_size , segment_list"
29,soimort/you-get,src/you_get/util/git.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/git.py#L7-L15,"def get_head(repo_path):
    """"""Get (branch, commit) from HEAD of a git repo.""""""
    try:
        ref = open(os.path.join(repo_path, '.git', 'HEAD'), 'r').read().strip()[5:].split('/')
        branch = ref[-1]
        commit = open(os.path.join(repo_path, '.git', *ref), 'r').read().strip()[:7]
        return branch, commit
    except:
        return None","['def', 'get_head', '(', 'repo_path', ')', ':', 'try', ':', 'ref', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'repo_path', ',', ""'.git'"", ',', ""'HEAD'"", ')', ',', ""'r'"", ')', '.', 'read', '(', ')', '.', 'strip', '(', ')', '[', '5', ':', ']', '.', 'split', '(', ""'/'"", ')', 'branch', '=', 'ref', '[', '-', '1', ']', 'commit', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'repo_path', ',', ""'.git'"", ',', '*', 'ref', ')', ',', ""'r'"", ')', '.', 'read', '(', ')', '.', 'strip', '(', ')', '[', ':', '7', ']', 'return', 'branch', ',', 'commit', 'except', ':', 'return', 'None']","Get (branch, commit) from HEAD of a git repo.","['Get', '(', 'branch', 'commit', ')', 'from', 'HEAD', 'of', 'a', 'git', 'repo', '.']",python,test,"['get', '(', 'branch', 'commit', ')', 'from', 'head', 'of', 'a', 'git', 'repo', '.']",get ( branch commit ) from head of a git repo .,"['def', 'get_head', '(', 'repo_path', ')', ':', 'try', ':', 'ref', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'repo_path', ',', ""'.git'"", ',', ""'head'"", ')', ',', ""'r'"", ')', '.', 'read', '(', ')', '.', 'strip', '(', ')', '[', '5', ':', ']', '.', 'split', '(', ""'/'"", ')', 'branch', '=', 'ref', '[', '-', '1', ']', 'commit', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'repo_path', ',', ""'.git'"", ',', '*', 'ref', ')', ',', ""'r'"", ')', '.', 'read', '(', ')', '.', 'strip', '(', ')', '[', ':', '7', ']', 'return', 'branch', ',', 'commit', 'except', ':', 'return', 'none']","def get_head ( repo_path ) : try : ref = open ( os . path . join ( repo_path , '.git' , 'head' ) , 'r' ) . read ( ) . strip ( ) [ 5 : ] . split ( '/' ) branch = ref [ - 1 ] commit = open ( os . path . join ( repo_path , '.git' , * ref ) , 'r' ) . read ( ) . strip ( ) [ : 7 ] return branch , commit except : return none"
30,soimort/you-get,src/you_get/util/fs.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/fs.py#L5-L47,"def legitimize(text, os=detect_os()):
    """"""Converts a string to a valid filename.
    """"""

    # POSIX systems
    text = text.translate({
        0: None,
        ord('/'): '-',
        ord('|'): '-',
    })

    # FIXME: do some filesystem detection
    if os == 'windows' or os == 'cygwin' or os == 'wsl':
        # Windows (non-POSIX namespace)
        text = text.translate({
            # Reserved in Windows VFAT and NTFS
            ord(':'): '-',
            ord('*'): '-',
            ord('?'): '-',
            ord('\\'): '-',
            ord('\""'): '\'',
            # Reserved in Windows VFAT
            ord('+'): '-',
            ord('<'): '-',
            ord('>'): '-',
            ord('['): '(',
            ord(']'): ')',
            ord('\t'): ' ',
        })
    else:
        # *nix
        if os == 'mac':
            # Mac OS HFS+
            text = text.translate({
                ord(':'): '-',
            })

        # Remove leading .
        if text.startswith("".""):
            text = text[1:]

    text = text[:80] # Trim to 82 Unicode characters long
    return text","['def', 'legitimize', '(', 'text', ',', 'os', '=', 'detect_os', '(', ')', ')', ':', '# POSIX systems', 'text', '=', 'text', '.', 'translate', '(', '{', '0', ':', 'None', ',', 'ord', '(', ""'/'"", ')', ':', ""'-'"", ',', 'ord', '(', ""'|'"", ')', ':', ""'-'"", ',', '}', ')', '# FIXME: do some filesystem detection', 'if', 'os', '==', ""'windows'"", 'or', 'os', '==', ""'cygwin'"", 'or', 'os', '==', ""'wsl'"", ':', '# Windows (non-POSIX namespace)', 'text', '=', 'text', '.', 'translate', '(', '{', '# Reserved in Windows VFAT and NTFS', 'ord', '(', ""':'"", ')', ':', ""'-'"", ',', 'ord', '(', ""'*'"", ')', ':', ""'-'"", ',', 'ord', '(', ""'?'"", ')', ':', ""'-'"", ',', 'ord', '(', ""'\\\\'"", ')', ':', ""'-'"", ',', 'ord', '(', '\'\\""\'', ')', ':', ""'\\''"", ',', '# Reserved in Windows VFAT', 'ord', '(', ""'+'"", ')', ':', ""'-'"", ',', 'ord', '(', ""'<'"", ')', ':', ""'-'"", ',', 'ord', '(', ""'>'"", ')', ':', ""'-'"", ',', 'ord', '(', ""'['"", ')', ':', ""'('"", ',', 'ord', '(', ""']'"", ')', ':', ""')'"", ',', 'ord', '(', ""'\\t'"", ')', ':', ""' '"", ',', '}', ')', 'else', ':', '# *nix', 'if', 'os', '==', ""'mac'"", ':', '# Mac OS HFS+', 'text', '=', 'text', '.', 'translate', '(', '{', 'ord', '(', ""':'"", ')', ':', ""'-'"", ',', '}', ')', '# Remove leading .', 'if', 'text', '.', 'startswith', '(', '"".""', ')', ':', 'text', '=', 'text', '[', '1', ':', ']', 'text', '=', 'text', '[', ':', '80', ']', '# Trim to 82 Unicode characters long', 'return', 'text']",Converts a string to a valid filename.,"['Converts', 'a', 'string', 'to', 'a', 'valid', 'filename', '.']",python,test,"['converts', 'a', 'string', 'to', 'a', 'valid', 'filename', '.']",converts a string to a valid filename .,"['def', 'legitimize', '(', 'text', ',', 'os', '=', 'detect_os', '(', ')', ')', ':', '# posix systems', 'text', '=', 'text', '.', 'translate', '(', '{', '0', ':', 'none', ',', 'ord', '(', ""'/'"", ')', ':', ""'-'"", ',', 'ord', '(', ""'|'"", ')', ':', ""'-'"", ',', '}', ')', '# fixme: do some filesystem detection', 'if', 'os', '==', ""'windows'"", 'or', 'os', '==', ""'cygwin'"", 'or', 'os', '==', ""'wsl'"", ':', '# windows (non-posix namespace)', 'text', '=', 'text', '.', 'translate', '(', '{', '# reserved in windows vfat and ntfs', 'ord', '(', ""':'"", ')', ':', ""'-'"", ',', 'ord', '(', ""'*'"", ')', ':', ""'-'"", ',', 'ord', '(', ""'?'"", ')', ':', ""'-'"", ',', 'ord', '(', ""'\\\\'"", ')', ':', ""'-'"", ',', 'ord', '(', '\'\\""\'', ')', ':', ""'\\''"", ',', '# reserved in windows vfat', 'ord', '(', ""'+'"", ')', ':', ""'-'"", ',', 'ord', '(', ""'<'"", ')', ':', ""'-'"", ',', 'ord', '(', ""'>'"", ')', ':', ""'-'"", ',', 'ord', '(', ""'['"", ')', ':', ""'('"", ',', 'ord', '(', ""']'"", ')', ':', ""')'"", ',', 'ord', '(', ""'\\t'"", ')', ':', ""' '"", ',', '}', ')', 'else', ':', '# *nix', 'if', 'os', '==', ""'mac'"", ':', '# mac os hfs+', 'text', '=', 'text', '.', 'translate', '(', '{', 'ord', '(', ""':'"", ')', ':', ""'-'"", ',', '}', ')', '# remove leading .', 'if', 'text', '.', 'startswith', '(', '"".""', ')', ':', 'text', '=', 'text', '[', '1', ':', ']', 'text', '=', 'text', '[', ':', '80', ']', '# trim to 82 unicode characters long', 'return', 'text']","def legitimize ( text , os = detect_os ( ) ) : # posix systems text = text . translate ( { 0 : none , ord ( '/' ) : '-' , ord ( '|' ) : '-' , } ) # fixme: do some filesystem detection if os == 'windows' or os == 'cygwin' or os == 'wsl' : # windows (non-posix namespace) text = text . translate ( { # reserved in windows vfat and ntfs ord ( ':' ) : '-' , ord ( '*' ) : '-' , ord ( '?' ) : '-' , ord ( '\\' ) : '-' , ord ( '\""' ) : '\'' , # reserved in windows vfat ord ( '+' ) : '-' , ord ( '<' ) : '-' , ord ( '>' ) : '-' , ord ( '[' ) : '(' , ord ( ']' ) : ')' , ord ( '\t' ) : ' ' , } ) else : # *nix if os == 'mac' : # mac os hfs+ text = text . translate ( { ord ( ':' ) : '-' , } ) # remove leading . if text . startswith ( ""."" ) : text = text [ 1 : ] text = text [ : 80 ] # trim to 82 unicode characters long return text"
31,soimort/you-get,src/you_get/util/term.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/term.py#L3-L9,"def get_terminal_size():
    """"""Get (width, height) of the current terminal.""""""
    try:
        import fcntl, termios, struct # fcntl module only available on Unix
        return struct.unpack('hh', fcntl.ioctl(1, termios.TIOCGWINSZ, '1234'))
    except:
        return (40, 80)","['def', 'get_terminal_size', '(', ')', ':', 'try', ':', 'import', 'fcntl', ',', 'termios', ',', 'struct', '# fcntl module only available on Unix', 'return', 'struct', '.', 'unpack', '(', ""'hh'"", ',', 'fcntl', '.', 'ioctl', '(', '1', ',', 'termios', '.', 'TIOCGWINSZ', ',', ""'1234'"", ')', ')', 'except', ':', 'return', '(', '40', ',', '80', ')']","Get (width, height) of the current terminal.","['Get', '(', 'width', 'height', ')', 'of', 'the', 'current', 'terminal', '.']",python,test,"['get', '(', 'width', 'height', ')', 'of', 'the', 'current', 'terminal', '.']",get ( width height ) of the current terminal .,"['def', 'get_terminal_size', '(', ')', ':', 'try', ':', 'import', 'fcntl', ',', 'termios', ',', 'struct', '# fcntl module only available on unix', 'return', 'struct', '.', 'unpack', '(', ""'hh'"", ',', 'fcntl', '.', 'ioctl', '(', '1', ',', 'termios', '.', 'tiocgwinsz', ',', ""'1234'"", ')', ')', 'except', ':', 'return', '(', '40', ',', '80', ')']","def get_terminal_size ( ) : try : import fcntl , termios , struct # fcntl module only available on unix return struct . unpack ( 'hh' , fcntl . ioctl ( 1 , termios . tiocgwinsz , '1234' ) ) except : return ( 40 , 80 )"
32,soimort/you-get,src/you_get/extractors/cbs.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/cbs.py#L9-L17,"def cbs_download(url, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""Downloads CBS videos by URL.
    """"""

    html = get_content(url)
    pid = match1(html, r'video\.settings\.pid\s*=\s*\'([^\']+)\'')
    title = match1(html, r'video\.settings\.title\s*=\s*\""([^\""]+)\""')

    theplatform_download_by_pid(pid, title, output_dir=output_dir, merge=merge, info_only=info_only)","['def', 'cbs_download', '(', 'url', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'True', ',', 'info_only', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', 'html', '=', 'get_content', '(', 'url', ')', 'pid', '=', 'match1', '(', 'html', ',', ""r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\''"", ')', 'title', '=', 'match1', '(', 'html', ',', 'r\'video\\.settings\\.title\\s*=\\s*\\""([^\\""]+)\\""\'', ')', 'theplatform_download_by_pid', '(', 'pid', ',', 'title', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ')']",Downloads CBS videos by URL.,"['Downloads', 'CBS', 'videos', 'by', 'URL', '.']",python,test,"['downloads', 'cbs', 'videos', 'by', 'url', '.']",downloads cbs videos by url .,"['def', 'cbs_download', '(', 'url', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'true', ',', 'info_only', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', 'html', '=', 'get_content', '(', 'url', ')', 'pid', '=', 'match1', '(', 'html', ',', ""r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\''"", ')', 'title', '=', 'match1', '(', 'html', ',', 'r\'video\\.settings\\.title\\s*=\\s*\\""([^\\""]+)\\""\'', ')', 'theplatform_download_by_pid', '(', 'pid', ',', 'title', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ')']","def cbs_download ( url , output_dir = '.' , merge = true , info_only = false , * * kwargs ) : html = get_content ( url ) pid = match1 ( html , r'video\.settings\.pid\s*=\s*\'([^\']+)\'' ) title = match1 ( html , r'video\.settings\.title\s*=\s*\""([^\""]+)\""' ) theplatform_download_by_pid ( pid , title , output_dir = output_dir , merge = merge , info_only = info_only )"
33,soimort/you-get,src/you_get/extractors/iqiyi.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/iqiyi.py#L158-L218,"def download(self, **kwargs):
        """"""Override the original one
        Ugly ugly dirty hack""""""
        if 'json_output' in kwargs and kwargs['json_output']:
            json_output.output(self)
        elif 'info_only' in kwargs and kwargs['info_only']:
            if 'stream_id' in kwargs and kwargs['stream_id']:
                # Display the stream
                stream_id = kwargs['stream_id']
                if 'index' not in kwargs:
                    self.p(stream_id)
                else:
                    self.p_i(stream_id)
            else:
                # Display all available streams
                if 'index' not in kwargs:
                    self.p([])
                else:
                    stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']
                    self.p_i(stream_id)

        else:
            if 'stream_id' in kwargs and kwargs['stream_id']:
                # Download the stream
                stream_id = kwargs['stream_id']
            else:
                # Download stream with the best quality
                stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']

            if 'index' not in kwargs:
                self.p(stream_id)
            else:
                self.p_i(stream_id)

            if stream_id in self.streams:
                urls = self.streams[stream_id]['src']
                ext = self.streams[stream_id]['container']
                total_size = self.streams[stream_id]['size']
            else:
                urls = self.dash_streams[stream_id]['src']
                ext = self.dash_streams[stream_id]['container']
                total_size = self.dash_streams[stream_id]['size']

            if not urls:
                log.wtf('[Failed] Cannot extract video source.')
            # For legacy main()
            
            #Here's the change!!
            download_url_ffmpeg(urls[0], self.title, 'mp4', output_dir=kwargs['output_dir'], merge=kwargs['merge'], stream=False)

            if not kwargs['caption']:
                print('Skipping captions.')
                return
            for lang in self.caption_tracks:
                filename = '%s.%s.srt' % (get_filename(self.title), lang)
                print('Saving %s ... ' % filename, end="""", flush=True)
                srt = self.caption_tracks[lang]
                with open(os.path.join(kwargs['output_dir'], filename),
                          'w', encoding='utf-8') as x:
                    x.write(srt)
                print('Done.')","['def', 'download', '(', 'self', ',', '*', '*', 'kwargs', ')', ':', 'if', ""'json_output'"", 'in', 'kwargs', 'and', 'kwargs', '[', ""'json_output'"", ']', ':', 'json_output', '.', 'output', '(', 'self', ')', 'elif', ""'info_only'"", 'in', 'kwargs', 'and', 'kwargs', '[', ""'info_only'"", ']', ':', 'if', ""'stream_id'"", 'in', 'kwargs', 'and', 'kwargs', '[', ""'stream_id'"", ']', ':', '# Display the stream', 'stream_id', '=', 'kwargs', '[', ""'stream_id'"", ']', 'if', ""'index'"", 'not', 'in', 'kwargs', ':', 'self', '.', 'p', '(', 'stream_id', ')', 'else', ':', 'self', '.', 'p_i', '(', 'stream_id', ')', 'else', ':', '# Display all available streams', 'if', ""'index'"", 'not', 'in', 'kwargs', ':', 'self', '.', 'p', '(', '[', ']', ')', 'else', ':', 'stream_id', '=', 'self', '.', 'streams_sorted', '[', '0', ']', '[', ""'id'"", ']', 'if', ""'id'"", 'in', 'self', '.', 'streams_sorted', '[', '0', ']', 'else', 'self', '.', 'streams_sorted', '[', '0', ']', '[', ""'itag'"", ']', 'self', '.', 'p_i', '(', 'stream_id', ')', 'else', ':', 'if', ""'stream_id'"", 'in', 'kwargs', 'and', 'kwargs', '[', ""'stream_id'"", ']', ':', '# Download the stream', 'stream_id', '=', 'kwargs', '[', ""'stream_id'"", ']', 'else', ':', '# Download stream with the best quality', 'stream_id', '=', 'self', '.', 'streams_sorted', '[', '0', ']', '[', ""'id'"", ']', 'if', ""'id'"", 'in', 'self', '.', 'streams_sorted', '[', '0', ']', 'else', 'self', '.', 'streams_sorted', '[', '0', ']', '[', ""'itag'"", ']', 'if', ""'index'"", 'not', 'in', 'kwargs', ':', 'self', '.', 'p', '(', 'stream_id', ')', 'else', ':', 'self', '.', 'p_i', '(', 'stream_id', ')', 'if', 'stream_id', 'in', 'self', '.', 'streams', ':', 'urls', '=', 'self', '.', 'streams', '[', 'stream_id', ']', '[', ""'src'"", ']', 'ext', '=', 'self', '.', 'streams', '[', 'stream_id', ']', '[', ""'container'"", ']', 'total_size', '=', 'self', '.', 'streams', '[', 'stream_id', ']', '[', ""'size'"", ']', 'else', ':', 'urls', '=', 'self', '.', 'dash_streams', '[', 'stream_id', ']', '[', ""'src'"", ']', 'ext', '=', 'self', '.', 'dash_streams', '[', 'stream_id', ']', '[', ""'container'"", ']', 'total_size', '=', 'self', '.', 'dash_streams', '[', 'stream_id', ']', '[', ""'size'"", ']', 'if', 'not', 'urls', ':', 'log', '.', 'wtf', '(', ""'[Failed] Cannot extract video source.'"", ')', '# For legacy main()', ""#Here's the change!!"", 'download_url_ffmpeg', '(', 'urls', '[', '0', ']', ',', 'self', '.', 'title', ',', ""'mp4'"", ',', 'output_dir', '=', 'kwargs', '[', ""'output_dir'"", ']', ',', 'merge', '=', 'kwargs', '[', ""'merge'"", ']', ',', 'stream', '=', 'False', ')', 'if', 'not', 'kwargs', '[', ""'caption'"", ']', ':', 'print', '(', ""'Skipping captions.'"", ')', 'return', 'for', 'lang', 'in', 'self', '.', 'caption_tracks', ':', 'filename', '=', ""'%s.%s.srt'"", '%', '(', 'get_filename', '(', 'self', '.', 'title', ')', ',', 'lang', ')', 'print', '(', ""'Saving %s ... '"", '%', 'filename', ',', 'end', '=', '""""', ',', 'flush', '=', 'True', ')', 'srt', '=', 'self', '.', 'caption_tracks', '[', 'lang', ']', 'with', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'kwargs', '[', ""'output_dir'"", ']', ',', 'filename', ')', ',', ""'w'"", ',', 'encoding', '=', ""'utf-8'"", ')', 'as', 'x', ':', 'x', '.', 'write', '(', 'srt', ')', 'print', '(', ""'Done.'"", ')']","Override the original one
        Ugly ugly dirty hack","['Override', 'the', 'original', 'one', 'Ugly', 'ugly', 'dirty', 'hack']",python,test,"['override', 'the', 'original', 'one', 'ugly', 'ugly', 'dirty', 'hack']",override the original one ugly ugly dirty hack,"['def', 'download', '(', 'self', ',', '*', '*', 'kwargs', ')', ':', 'if', ""'json_output'"", 'in', 'kwargs', 'and', 'kwargs', '[', ""'json_output'"", ']', ':', 'json_output', '.', 'output', '(', 'self', ')', 'elif', ""'info_only'"", 'in', 'kwargs', 'and', 'kwargs', '[', ""'info_only'"", ']', ':', 'if', ""'stream_id'"", 'in', 'kwargs', 'and', 'kwargs', '[', ""'stream_id'"", ']', ':', '# display the stream', 'stream_id', '=', 'kwargs', '[', ""'stream_id'"", ']', 'if', ""'index'"", 'not', 'in', 'kwargs', ':', 'self', '.', 'p', '(', 'stream_id', ')', 'else', ':', 'self', '.', 'p_i', '(', 'stream_id', ')', 'else', ':', '# display all available streams', 'if', ""'index'"", 'not', 'in', 'kwargs', ':', 'self', '.', 'p', '(', '[', ']', ')', 'else', ':', 'stream_id', '=', 'self', '.', 'streams_sorted', '[', '0', ']', '[', ""'id'"", ']', 'if', ""'id'"", 'in', 'self', '.', 'streams_sorted', '[', '0', ']', 'else', 'self', '.', 'streams_sorted', '[', '0', ']', '[', ""'itag'"", ']', 'self', '.', 'p_i', '(', 'stream_id', ')', 'else', ':', 'if', ""'stream_id'"", 'in', 'kwargs', 'and', 'kwargs', '[', ""'stream_id'"", ']', ':', '# download the stream', 'stream_id', '=', 'kwargs', '[', ""'stream_id'"", ']', 'else', ':', '# download stream with the best quality', 'stream_id', '=', 'self', '.', 'streams_sorted', '[', '0', ']', '[', ""'id'"", ']', 'if', ""'id'"", 'in', 'self', '.', 'streams_sorted', '[', '0', ']', 'else', 'self', '.', 'streams_sorted', '[', '0', ']', '[', ""'itag'"", ']', 'if', ""'index'"", 'not', 'in', 'kwargs', ':', 'self', '.', 'p', '(', 'stream_id', ')', 'else', ':', 'self', '.', 'p_i', '(', 'stream_id', ')', 'if', 'stream_id', 'in', 'self', '.', 'streams', ':', 'urls', '=', 'self', '.', 'streams', '[', 'stream_id', ']', '[', ""'src'"", ']', 'ext', '=', 'self', '.', 'streams', '[', 'stream_id', ']', '[', ""'container'"", ']', 'total_size', '=', 'self', '.', 'streams', '[', 'stream_id', ']', '[', ""'size'"", ']', 'else', ':', 'urls', '=', 'self', '.', 'dash_streams', '[', 'stream_id', ']', '[', ""'src'"", ']', 'ext', '=', 'self', '.', 'dash_streams', '[', 'stream_id', ']', '[', ""'container'"", ']', 'total_size', '=', 'self', '.', 'dash_streams', '[', 'stream_id', ']', '[', ""'size'"", ']', 'if', 'not', 'urls', ':', 'log', '.', 'wtf', '(', ""'[failed] cannot extract video source.'"", ')', '# for legacy main()', ""#here's the change!!"", 'download_url_ffmpeg', '(', 'urls', '[', '0', ']', ',', 'self', '.', 'title', ',', ""'mp4'"", ',', 'output_dir', '=', 'kwargs', '[', ""'output_dir'"", ']', ',', 'merge', '=', 'kwargs', '[', ""'merge'"", ']', ',', 'stream', '=', 'false', ')', 'if', 'not', 'kwargs', '[', ""'caption'"", ']', ':', 'print', '(', ""'skipping captions.'"", ')', 'return', 'for', 'lang', 'in', 'self', '.', 'caption_tracks', ':', 'filename', '=', ""'%s.%s.srt'"", '%', '(', 'get_filename', '(', 'self', '.', 'title', ')', ',', 'lang', ')', 'print', '(', ""'saving %s ... '"", '%', 'filename', ',', 'end', '=', '""""', ',', 'flush', '=', 'true', ')', 'srt', '=', 'self', '.', 'caption_tracks', '[', 'lang', ']', 'with', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'kwargs', '[', ""'output_dir'"", ']', ',', 'filename', ')', ',', ""'w'"", ',', 'encoding', '=', ""'utf-8'"", ')', 'as', 'x', ':', 'x', '.', 'write', '(', 'srt', ')', 'print', '(', ""'done.'"", ')']","def download ( self , * * kwargs ) : if 'json_output' in kwargs and kwargs [ 'json_output' ] : json_output . output ( self ) elif 'info_only' in kwargs and kwargs [ 'info_only' ] : if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : # display the stream stream_id = kwargs [ 'stream_id' ] if 'index' not in kwargs : self . p ( stream_id ) else : self . p_i ( stream_id ) else : # display all available streams if 'index' not in kwargs : self . p ( [ ] ) else : stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] self . p_i ( stream_id ) else : if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : # download the stream stream_id = kwargs [ 'stream_id' ] else : # download stream with the best quality stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] if 'index' not in kwargs : self . p ( stream_id ) else : self . p_i ( stream_id ) if stream_id in self . streams : urls = self . streams [ stream_id ] [ 'src' ] ext = self . streams [ stream_id ] [ 'container' ] total_size = self . streams [ stream_id ] [ 'size' ] else : urls = self . dash_streams [ stream_id ] [ 'src' ] ext = self . dash_streams [ stream_id ] [ 'container' ] total_size = self . dash_streams [ stream_id ] [ 'size' ] if not urls : log . wtf ( '[failed] cannot extract video source.' ) # for legacy main() #here's the change!! download_url_ffmpeg ( urls [ 0 ] , self . title , 'mp4' , output_dir = kwargs [ 'output_dir' ] , merge = kwargs [ 'merge' ] , stream = false ) if not kwargs [ 'caption' ] : print ( 'skipping captions.' ) return for lang in self . caption_tracks : filename = '%s.%s.srt' % ( get_filename ( self . title ) , lang ) print ( 'saving %s ... ' % filename , end = """" , flush = true ) srt = self . caption_tracks [ lang ] with open ( os . path . join ( kwargs [ 'output_dir' ] , filename ) , 'w' , encoding = 'utf-8' ) as x : x . write ( srt ) print ( 'done.' )"
34,soimort/you-get,src/you_get/extractors/acfun.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/acfun.py#L42-L109,"def acfun_download_by_vid(vid, title, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""str, str, str, bool, bool ->None

    Download Acfun video by vid.

    Call Acfun API, decide which site to use, and pass the job to its
    extractor.
    """"""

    #first call the main parasing API
    info = json.loads(get_content('http://www.acfun.cn/video/getVideo.aspx?id=' + vid))

    sourceType = info['sourceType']

    #decide sourceId to know which extractor to use
    if 'sourceId' in info: sourceId = info['sourceId']
    # danmakuId = info['danmakuId']

    #call extractor decided by sourceId
    if sourceType == 'sina':
        sina_download_by_vid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)
    elif sourceType == 'youku':
        youku_download_by_vid(sourceId, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)
    elif sourceType == 'tudou':
        tudou_download_by_iid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)
    elif sourceType == 'qq':
        qq_download_by_vid(sourceId, title, True, output_dir=output_dir, merge=merge, info_only=info_only)
    elif sourceType == 'letv':
        letvcloud_download_by_vu(sourceId, '2d8c027396', title, output_dir=output_dir, merge=merge, info_only=info_only)
    elif sourceType == 'zhuzhan':
        #As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this
#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player
#old code removed
        url = 'http://www.acfun.cn/v/ac' + vid
        yk_streams = youku_acfun_proxy(info['sourceId'], info['encode'], url)
        seq = ['mp4hd3', 'mp4hd2', 'mp4hd', 'flvhd']
        for t in seq:
            if yk_streams.get(t):
                preferred = yk_streams[t]
                break
#total_size in the json could be incorrect(F.I. 0)
        size = 0
        for url in preferred[0]:
            _, _, seg_size = url_info(url)
            size += seg_size
#fallback to flvhd is not quite possible
        if re.search(r'fid=[0-9A-Z\-]*.flv', preferred[0][0]):
            ext = 'flv'
        else:
            ext = 'mp4'
        print_info(site_info, title, ext, size)
        if not info_only:
            download_urls(preferred[0], title, ext, size, output_dir=output_dir, merge=merge)
    else:
        raise NotImplementedError(sourceType)

    if not info_only and not dry_run:
        if not kwargs['caption']:
            print('Skipping danmaku.')
            return
        try:
            title = get_filename(title)
            print('Downloading %s ...\n' % (title + '.cmt.json'))
            cmt = get_srt_json(vid)
            with open(os.path.join(output_dir, title + '.cmt.json'), 'w', encoding='utf-8') as x:
                x.write(cmt)
        except:
            pass","['def', 'acfun_download_by_vid', '(', 'vid', ',', 'title', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'True', ',', 'info_only', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', '#first call the main parasing API', 'info', '=', 'json', '.', 'loads', '(', 'get_content', '(', ""'http://www.acfun.cn/video/getVideo.aspx?id='"", '+', 'vid', ')', ')', 'sourceType', '=', 'info', '[', ""'sourceType'"", ']', '#decide sourceId to know which extractor to use', 'if', ""'sourceId'"", 'in', 'info', ':', 'sourceId', '=', 'info', '[', ""'sourceId'"", ']', ""# danmakuId = info['danmakuId']"", '#call extractor decided by sourceId', 'if', 'sourceType', '==', ""'sina'"", ':', 'sina_download_by_vid', '(', 'sourceId', ',', 'title', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ')', 'elif', 'sourceType', '==', ""'youku'"", ':', 'youku_download_by_vid', '(', 'sourceId', ',', 'title', '=', 'title', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ',', '*', '*', 'kwargs', ')', 'elif', 'sourceType', '==', ""'tudou'"", ':', 'tudou_download_by_iid', '(', 'sourceId', ',', 'title', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ')', 'elif', 'sourceType', '==', ""'qq'"", ':', 'qq_download_by_vid', '(', 'sourceId', ',', 'title', ',', 'True', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ')', 'elif', 'sourceType', '==', ""'letv'"", ':', 'letvcloud_download_by_vu', '(', 'sourceId', ',', ""'2d8c027396'"", ',', 'title', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ')', 'elif', 'sourceType', '==', ""'zhuzhan'"", ':', '#As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this', ""#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player"", '#old code removed', 'url', '=', ""'http://www.acfun.cn/v/ac'"", '+', 'vid', 'yk_streams', '=', 'youku_acfun_proxy', '(', 'info', '[', ""'sourceId'"", ']', ',', 'info', '[', ""'encode'"", ']', ',', 'url', ')', 'seq', '=', '[', ""'mp4hd3'"", ',', ""'mp4hd2'"", ',', ""'mp4hd'"", ',', ""'flvhd'"", ']', 'for', 't', 'in', 'seq', ':', 'if', 'yk_streams', '.', 'get', '(', 't', ')', ':', 'preferred', '=', 'yk_streams', '[', 't', ']', 'break', '#total_size in the json could be incorrect(F.I. 0)', 'size', '=', '0', 'for', 'url', 'in', 'preferred', '[', '0', ']', ':', '_', ',', '_', ',', 'seg_size', '=', 'url_info', '(', 'url', ')', 'size', '+=', 'seg_size', '#fallback to flvhd is not quite possible', 'if', 're', '.', 'search', '(', ""r'fid=[0-9A-Z\\-]*.flv'"", ',', 'preferred', '[', '0', ']', '[', '0', ']', ')', ':', 'ext', '=', ""'flv'"", 'else', ':', 'ext', '=', ""'mp4'"", 'print_info', '(', 'site_info', ',', 'title', ',', 'ext', ',', 'size', ')', 'if', 'not', 'info_only', ':', 'download_urls', '(', 'preferred', '[', '0', ']', ',', 'title', ',', 'ext', ',', 'size', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ')', 'else', ':', 'raise', 'NotImplementedError', '(', 'sourceType', ')', 'if', 'not', 'info_only', 'and', 'not', 'dry_run', ':', 'if', 'not', 'kwargs', '[', ""'caption'"", ']', ':', 'print', '(', ""'Skipping danmaku.'"", ')', 'return', 'try', ':', 'title', '=', 'get_filename', '(', 'title', ')', 'print', '(', ""'Downloading %s ...\\n'"", '%', '(', 'title', '+', ""'.cmt.json'"", ')', ')', 'cmt', '=', 'get_srt_json', '(', 'vid', ')', 'with', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'output_dir', ',', 'title', '+', ""'.cmt.json'"", ')', ',', ""'w'"", ',', 'encoding', '=', ""'utf-8'"", ')', 'as', 'x', ':', 'x', '.', 'write', '(', 'cmt', ')', 'except', ':', 'pass']","str, str, str, bool, bool ->None

    Download Acfun video by vid.

    Call Acfun API, decide which site to use, and pass the job to its
    extractor.","['str', 'str', 'str', 'bool', 'bool', '-', '>', 'None']",python,test,"['str', 'str', 'str', 'bool', 'bool', '-', '>', 'none']",str str str bool bool - > none,"['def', 'acfun_download_by_vid', '(', 'vid', ',', 'title', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'true', ',', 'info_only', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', '#first call the main parasing api', 'info', '=', 'json', '.', 'loads', '(', 'get_content', '(', ""'http://www.acfun.cn/video/getvideo.aspx?id='"", '+', 'vid', ')', ')', 'sourcetype', '=', 'info', '[', ""'sourcetype'"", ']', '#decide sourceid to know which extractor to use', 'if', ""'sourceid'"", 'in', 'info', ':', 'sourceid', '=', 'info', '[', ""'sourceid'"", ']', ""# danmakuid = info['danmakuid']"", '#call extractor decided by sourceid', 'if', 'sourcetype', '==', ""'sina'"", ':', 'sina_download_by_vid', '(', 'sourceid', ',', 'title', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ')', 'elif', 'sourcetype', '==', ""'youku'"", ':', 'youku_download_by_vid', '(', 'sourceid', ',', 'title', '=', 'title', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ',', '*', '*', 'kwargs', ')', 'elif', 'sourcetype', '==', ""'tudou'"", ':', 'tudou_download_by_iid', '(', 'sourceid', ',', 'title', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ')', 'elif', 'sourcetype', '==', ""'qq'"", ':', 'qq_download_by_vid', '(', 'sourceid', ',', 'title', ',', 'true', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ')', 'elif', 'sourcetype', '==', ""'letv'"", ':', 'letvcloud_download_by_vu', '(', 'sourceid', ',', ""'2d8c027396'"", ',', 'title', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ')', 'elif', 'sourcetype', '==', ""'zhuzhan'"", ':', '#as in jul.28.2016, acfun is using embsig to anti hotlink so we need to pass this', ""#in mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player"", '#old code removed', 'url', '=', ""'http://www.acfun.cn/v/ac'"", '+', 'vid', 'yk_streams', '=', 'youku_acfun_proxy', '(', 'info', '[', ""'sourceid'"", ']', ',', 'info', '[', ""'encode'"", ']', ',', 'url', ')', 'seq', '=', '[', ""'mp4hd3'"", ',', ""'mp4hd2'"", ',', ""'mp4hd'"", ',', ""'flvhd'"", ']', 'for', 't', 'in', 'seq', ':', 'if', 'yk_streams', '.', 'get', '(', 't', ')', ':', 'preferred', '=', 'yk_streams', '[', 't', ']', 'break', '#total_size in the json could be incorrect(f.i. 0)', 'size', '=', '0', 'for', 'url', 'in', 'preferred', '[', '0', ']', ':', '_', ',', '_', ',', 'seg_size', '=', 'url_info', '(', 'url', ')', 'size', '+=', 'seg_size', '#fallback to flvhd is not quite possible', 'if', 're', '.', 'search', '(', ""r'fid=[0-9a-z\\-]*.flv'"", ',', 'preferred', '[', '0', ']', '[', '0', ']', ')', ':', 'ext', '=', ""'flv'"", 'else', ':', 'ext', '=', ""'mp4'"", 'print_info', '(', 'site_info', ',', 'title', ',', 'ext', ',', 'size', ')', 'if', 'not', 'info_only', ':', 'download_urls', '(', 'preferred', '[', '0', ']', ',', 'title', ',', 'ext', ',', 'size', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ')', 'else', ':', 'raise', 'notimplementederror', '(', 'sourcetype', ')', 'if', 'not', 'info_only', 'and', 'not', 'dry_run', ':', 'if', 'not', 'kwargs', '[', ""'caption'"", ']', ':', 'print', '(', ""'skipping danmaku.'"", ')', 'return', 'try', ':', 'title', '=', 'get_filename', '(', 'title', ')', 'print', '(', ""'downloading %s ...\\n'"", '%', '(', 'title', '+', ""'.cmt.json'"", ')', ')', 'cmt', '=', 'get_srt_json', '(', 'vid', ')', 'with', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'output_dir', ',', 'title', '+', ""'.cmt.json'"", ')', ',', ""'w'"", ',', 'encoding', '=', ""'utf-8'"", ')', 'as', 'x', ':', 'x', '.', 'write', '(', 'cmt', ')', 'except', ':', 'pass']","def acfun_download_by_vid ( vid , title , output_dir = '.' , merge = true , info_only = false , * * kwargs ) : #first call the main parasing api info = json . loads ( get_content ( 'http://www.acfun.cn/video/getvideo.aspx?id=' + vid ) ) sourcetype = info [ 'sourcetype' ] #decide sourceid to know which extractor to use if 'sourceid' in info : sourceid = info [ 'sourceid' ] # danmakuid = info['danmakuid'] #call extractor decided by sourceid if sourcetype == 'sina' : sina_download_by_vid ( sourceid , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourcetype == 'youku' : youku_download_by_vid ( sourceid , title = title , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs ) elif sourcetype == 'tudou' : tudou_download_by_iid ( sourceid , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourcetype == 'qq' : qq_download_by_vid ( sourceid , title , true , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourcetype == 'letv' : letvcloud_download_by_vu ( sourceid , '2d8c027396' , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourcetype == 'zhuzhan' : #as in jul.28.2016, acfun is using embsig to anti hotlink so we need to pass this #in mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player #old code removed url = 'http://www.acfun.cn/v/ac' + vid yk_streams = youku_acfun_proxy ( info [ 'sourceid' ] , info [ 'encode' ] , url ) seq = [ 'mp4hd3' , 'mp4hd2' , 'mp4hd' , 'flvhd' ] for t in seq : if yk_streams . get ( t ) : preferred = yk_streams [ t ] break #total_size in the json could be incorrect(f.i. 0) size = 0 for url in preferred [ 0 ] : _ , _ , seg_size = url_info ( url ) size += seg_size #fallback to flvhd is not quite possible if re . search ( r'fid=[0-9a-z\-]*.flv' , preferred [ 0 ] [ 0 ] ) : ext = 'flv' else : ext = 'mp4' print_info ( site_info , title , ext , size ) if not info_only : download_urls ( preferred [ 0 ] , title , ext , size , output_dir = output_dir , merge = merge ) else : raise notimplementederror ( sourcetype ) if not info_only and not dry_run : if not kwargs [ 'caption' ] : print ( 'skipping danmaku.' ) return try : title = get_filename ( title ) print ( 'downloading %s ...\n' % ( title + '.cmt.json' ) ) cmt = get_srt_json ( vid ) with open ( os . path . join ( output_dir , title + '.cmt.json' ) , 'w' , encoding = 'utf-8' ) as x : x . write ( cmt ) except : pass"
35,soimort/you-get,src/you_get/__main__.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/__main__.py#L24-L85,"def main_dev(**kwargs):
    """"""Main entry point.
    you-get-dev
    """"""

    # Get (branch, commit) if running from a git repo.
    head = git.get_head(kwargs['repo_path'])

    # Get options and arguments.
    try:
        opts, args = getopt.getopt(sys.argv[1:], _short_options, _options)
    except getopt.GetoptError as e:
        log.wtf(""""""
    [Fatal] {}.
    Try '{} --help' for more options."""""".format(e, script_name))

    if not opts and not args:
        # Display help.
        print(_help)
        # Enter GUI mode.
        #from .gui import gui_main
        #gui_main()
    else:
        conf = {}
        for opt, arg in opts:
            if opt in ('-h', '--help'):
                # Display help.
                print(_help)

            elif opt in ('-V', '--version'):
                # Display version.
                log.println(""you-get:"", log.BOLD)
                log.println(""    version:  {}"".format(__version__))
                if head is not None:
                    log.println(""    branch:   {}\n    commit:   {}"".format(*head))
                else:
                    log.println(""    branch:   {}\n    commit:   {}"".format(""(stable)"", ""(tag v{})"".format(__version__)))

                log.println(""    platform: {}"".format(platform.platform()))
                log.println(""    python:   {}"".format(sys.version.split('\n')[0]))

            elif opt in ('-g', '--gui'):
                # Run using GUI.
                conf['gui'] = True

            elif opt in ('-f', '--force'):
                # Force download.
                conf['force'] = True

            elif opt in ('-l', '--playlist', '--playlists'):
                # Download playlist whenever possible.
                conf['playlist'] = True

        if args:
            if 'gui' in conf and conf['gui']:
                # Enter GUI mode.
                from .gui import gui_main
                gui_main(*args, **conf)
            else:
                # Enter console mode.
                from .console import console_main
                console_main(*args, **conf)","['def', 'main_dev', '(', '*', '*', 'kwargs', ')', ':', '# Get (branch, commit) if running from a git repo.', 'head', '=', 'git', '.', 'get_head', '(', 'kwargs', '[', ""'repo_path'"", ']', ')', '# Get options and arguments.', 'try', ':', 'opts', ',', 'args', '=', 'getopt', '.', 'getopt', '(', 'sys', '.', 'argv', '[', '1', ':', ']', ',', '_short_options', ',', '_options', ')', 'except', 'getopt', '.', 'GetoptError', 'as', 'e', ':', 'log', '.', 'wtf', '(', '""""""\n    [Fatal] {}.\n    Try \'{} --help\' for more options.""""""', '.', 'format', '(', 'e', ',', 'script_name', ')', ')', 'if', 'not', 'opts', 'and', 'not', 'args', ':', '# Display help.', 'print', '(', '_help', ')', '# Enter GUI mode.', '#from .gui import gui_main', '#gui_main()', 'else', ':', 'conf', '=', '{', '}', 'for', 'opt', ',', 'arg', 'in', 'opts', ':', 'if', 'opt', 'in', '(', ""'-h'"", ',', ""'--help'"", ')', ':', '# Display help.', 'print', '(', '_help', ')', 'elif', 'opt', 'in', '(', ""'-V'"", ',', ""'--version'"", ')', ':', '# Display version.', 'log', '.', 'println', '(', '""you-get:""', ',', 'log', '.', 'BOLD', ')', 'log', '.', 'println', '(', '""    version:  {}""', '.', 'format', '(', '__version__', ')', ')', 'if', 'head', 'is', 'not', 'None', ':', 'log', '.', 'println', '(', '""    branch:   {}\\n    commit:   {}""', '.', 'format', '(', '*', 'head', ')', ')', 'else', ':', 'log', '.', 'println', '(', '""    branch:   {}\\n    commit:   {}""', '.', 'format', '(', '""(stable)""', ',', '""(tag v{})""', '.', 'format', '(', '__version__', ')', ')', ')', 'log', '.', 'println', '(', '""    platform: {}""', '.', 'format', '(', 'platform', '.', 'platform', '(', ')', ')', ')', 'log', '.', 'println', '(', '""    python:   {}""', '.', 'format', '(', 'sys', '.', 'version', '.', 'split', '(', ""'\\n'"", ')', '[', '0', ']', ')', ')', 'elif', 'opt', 'in', '(', ""'-g'"", ',', ""'--gui'"", ')', ':', '# Run using GUI.', 'conf', '[', ""'gui'"", ']', '=', 'True', 'elif', 'opt', 'in', '(', ""'-f'"", ',', ""'--force'"", ')', ':', '# Force download.', 'conf', '[', ""'force'"", ']', '=', 'True', 'elif', 'opt', 'in', '(', ""'-l'"", ',', ""'--playlist'"", ',', ""'--playlists'"", ')', ':', '# Download playlist whenever possible.', 'conf', '[', ""'playlist'"", ']', '=', 'True', 'if', 'args', ':', 'if', ""'gui'"", 'in', 'conf', 'and', 'conf', '[', ""'gui'"", ']', ':', '# Enter GUI mode.', 'from', '.', 'gui', 'import', 'gui_main', 'gui_main', '(', '*', 'args', ',', '*', '*', 'conf', ')', 'else', ':', '# Enter console mode.', 'from', '.', 'console', 'import', 'console_main', 'console_main', '(', '*', 'args', ',', '*', '*', 'conf', ')']","Main entry point.
    you-get-dev","['Main', 'entry', 'point', '.', 'you', '-', 'get', '-', 'dev']",python,test,"['main', 'entry', 'point', '.', 'you', '-', 'get', '-', 'dev']",main entry point . you - get - dev,"['def', 'main_dev', '(', '*', '*', 'kwargs', ')', ':', '# get (branch, commit) if running from a git repo.', 'head', '=', 'git', '.', 'get_head', '(', 'kwargs', '[', ""'repo_path'"", ']', ')', '# get options and arguments.', 'try', ':', 'opts', ',', 'args', '=', 'getopt', '.', 'getopt', '(', 'sys', '.', 'argv', '[', '1', ':', ']', ',', '_short_options', ',', '_options', ')', 'except', 'getopt', '.', 'getopterror', 'as', 'e', ':', 'log', '.', 'wtf', '(', '""""""\n    [fatal] {}.\n    try \'{} --help\' for more options.""""""', '.', 'format', '(', 'e', ',', 'script_name', ')', ')', 'if', 'not', 'opts', 'and', 'not', 'args', ':', '# display help.', 'print', '(', '_help', ')', '# enter gui mode.', '#from .gui import gui_main', '#gui_main()', 'else', ':', 'conf', '=', '{', '}', 'for', 'opt', ',', 'arg', 'in', 'opts', ':', 'if', 'opt', 'in', '(', ""'-h'"", ',', ""'--help'"", ')', ':', '# display help.', 'print', '(', '_help', ')', 'elif', 'opt', 'in', '(', ""'-v'"", ',', ""'--version'"", ')', ':', '# display version.', 'log', '.', 'println', '(', '""you-get:""', ',', 'log', '.', 'bold', ')', 'log', '.', 'println', '(', '""    version:  {}""', '.', 'format', '(', '__version__', ')', ')', 'if', 'head', 'is', 'not', 'none', ':', 'log', '.', 'println', '(', '""    branch:   {}\\n    commit:   {}""', '.', 'format', '(', '*', 'head', ')', ')', 'else', ':', 'log', '.', 'println', '(', '""    branch:   {}\\n    commit:   {}""', '.', 'format', '(', '""(stable)""', ',', '""(tag v{})""', '.', 'format', '(', '__version__', ')', ')', ')', 'log', '.', 'println', '(', '""    platform: {}""', '.', 'format', '(', 'platform', '.', 'platform', '(', ')', ')', ')', 'log', '.', 'println', '(', '""    python:   {}""', '.', 'format', '(', 'sys', '.', 'version', '.', 'split', '(', ""'\\n'"", ')', '[', '0', ']', ')', ')', 'elif', 'opt', 'in', '(', ""'-g'"", ',', ""'--gui'"", ')', ':', '# run using gui.', 'conf', '[', ""'gui'"", ']', '=', 'true', 'elif', 'opt', 'in', '(', ""'-f'"", ',', ""'--force'"", ')', ':', '# force download.', 'conf', '[', ""'force'"", ']', '=', 'true', 'elif', 'opt', 'in', '(', ""'-l'"", ',', ""'--playlist'"", ',', ""'--playlists'"", ')', ':', '# download playlist whenever possible.', 'conf', '[', ""'playlist'"", ']', '=', 'true', 'if', 'args', ':', 'if', ""'gui'"", 'in', 'conf', 'and', 'conf', '[', ""'gui'"", ']', ':', '# enter gui mode.', 'from', '.', 'gui', 'import', 'gui_main', 'gui_main', '(', '*', 'args', ',', '*', '*', 'conf', ')', 'else', ':', '# enter console mode.', 'from', '.', 'console', 'import', 'console_main', 'console_main', '(', '*', 'args', ',', '*', '*', 'conf', ')']","def main_dev ( * * kwargs ) : # get (branch, commit) if running from a git repo. head = git . get_head ( kwargs [ 'repo_path' ] ) # get options and arguments. try : opts , args = getopt . getopt ( sys . argv [ 1 : ] , _short_options , _options ) except getopt . getopterror as e : log . wtf ( """"""
    [fatal] {}.
    try '{} --help' for more options."""""" . format ( e , script_name ) ) if not opts and not args : # display help. print ( _help ) # enter gui mode. #from .gui import gui_main #gui_main() else : conf = { } for opt , arg in opts : if opt in ( '-h' , '--help' ) : # display help. print ( _help ) elif opt in ( '-v' , '--version' ) : # display version. log . println ( ""you-get:"" , log . bold ) log . println ( ""    version:  {}"" . format ( __version__ ) ) if head is not none : log . println ( ""    branch:   {}\n    commit:   {}"" . format ( * head ) ) else : log . println ( ""    branch:   {}\n    commit:   {}"" . format ( ""(stable)"" , ""(tag v{})"" . format ( __version__ ) ) ) log . println ( ""    platform: {}"" . format ( platform . platform ( ) ) ) log . println ( ""    python:   {}"" . format ( sys . version . split ( '\n' ) [ 0 ] ) ) elif opt in ( '-g' , '--gui' ) : # run using gui. conf [ 'gui' ] = true elif opt in ( '-f' , '--force' ) : # force download. conf [ 'force' ] = true elif opt in ( '-l' , '--playlist' , '--playlists' ) : # download playlist whenever possible. conf [ 'playlist' ] = true if args : if 'gui' in conf and conf [ 'gui' ] : # enter gui mode. from . gui import gui_main gui_main ( * args , * * conf ) else : # enter console mode. from . console import console_main console_main ( * args , * * conf )"
36,soimort/you-get,src/you_get/processor/ffmpeg.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/processor/ffmpeg.py#L220-L262,"def ffmpeg_download_stream(files, title, ext, params={}, output_dir='.', stream=True):
    """"""str, str->True
    WARNING: NOT THE SAME PARMS AS OTHER FUNCTIONS!!!!!!
    You can basicly download anything with this function
    but better leave it alone with
    """"""
    output = title + '.' + ext

    if not (output_dir == '.'):
        output = output_dir + '/' + output

    print('Downloading streaming content with FFmpeg, press q to stop recording...')
    if stream:
        ffmpeg_params = [FFMPEG] + ['-y', '-re', '-i']
    else:
        ffmpeg_params = [FFMPEG] + ['-y', '-i']
    ffmpeg_params.append(files)  #not the same here!!!!

    if FFMPEG == 'avconv':  #who cares?
        ffmpeg_params += ['-c', 'copy', output]
    else:
        ffmpeg_params += ['-c', 'copy', '-bsf:a', 'aac_adtstoasc']

    if params is not None:
        if len(params) > 0:
            for k, v in params:
                ffmpeg_params.append(k)
                ffmpeg_params.append(v)

    ffmpeg_params.append(output)

    print(' '.join(ffmpeg_params))

    try:
        a = subprocess.Popen(ffmpeg_params, stdin= subprocess.PIPE)
        a.communicate()
    except KeyboardInterrupt:
        try:
            a.stdin.write('q'.encode('utf-8'))
        except:
            pass

    return True","['def', 'ffmpeg_download_stream', '(', 'files', ',', 'title', ',', 'ext', ',', 'params', '=', '{', '}', ',', 'output_dir', '=', ""'.'"", ',', 'stream', '=', 'True', ')', ':', 'output', '=', 'title', '+', ""'.'"", '+', 'ext', 'if', 'not', '(', 'output_dir', '==', ""'.'"", ')', ':', 'output', '=', 'output_dir', '+', ""'/'"", '+', 'output', 'print', '(', ""'Downloading streaming content with FFmpeg, press q to stop recording...'"", ')', 'if', 'stream', ':', 'ffmpeg_params', '=', '[', 'FFMPEG', ']', '+', '[', ""'-y'"", ',', ""'-re'"", ',', ""'-i'"", ']', 'else', ':', 'ffmpeg_params', '=', '[', 'FFMPEG', ']', '+', '[', ""'-y'"", ',', ""'-i'"", ']', 'ffmpeg_params', '.', 'append', '(', 'files', ')', '#not the same here!!!!', 'if', 'FFMPEG', '==', ""'avconv'"", ':', '#who cares?', 'ffmpeg_params', '+=', '[', ""'-c'"", ',', ""'copy'"", ',', 'output', ']', 'else', ':', 'ffmpeg_params', '+=', '[', ""'-c'"", ',', ""'copy'"", ',', ""'-bsf:a'"", ',', ""'aac_adtstoasc'"", ']', 'if', 'params', 'is', 'not', 'None', ':', 'if', 'len', '(', 'params', ')', '>', '0', ':', 'for', 'k', ',', 'v', 'in', 'params', ':', 'ffmpeg_params', '.', 'append', '(', 'k', ')', 'ffmpeg_params', '.', 'append', '(', 'v', ')', 'ffmpeg_params', '.', 'append', '(', 'output', ')', 'print', '(', ""' '"", '.', 'join', '(', 'ffmpeg_params', ')', ')', 'try', ':', 'a', '=', 'subprocess', '.', 'Popen', '(', 'ffmpeg_params', ',', 'stdin', '=', 'subprocess', '.', 'PIPE', ')', 'a', '.', 'communicate', '(', ')', 'except', 'KeyboardInterrupt', ':', 'try', ':', 'a', '.', 'stdin', '.', 'write', '(', ""'q'"", '.', 'encode', '(', ""'utf-8'"", ')', ')', 'except', ':', 'pass', 'return', 'True']","str, str->True
    WARNING: NOT THE SAME PARMS AS OTHER FUNCTIONS!!!!!!
    You can basicly download anything with this function
    but better leave it alone with","['str', 'str', '-', '>', 'True', 'WARNING', ':', 'NOT', 'THE', 'SAME', 'PARMS', 'AS', 'OTHER', 'FUNCTIONS!!!!!!', 'You', 'can', 'basicly', 'download', 'anything', 'with', 'this', 'function', 'but', 'better', 'leave', 'it', 'alone', 'with']",python,test,"['str', 'str', '-', '>', 'true', 'warning', ':', 'not', 'the', 'same', 'parms', 'as', 'other', 'functions!!!!!!', 'you', 'can', 'basicly', 'download', 'anything', 'with', 'this', 'function', 'but', 'better', 'leave', 'it', 'alone', 'with']",str str - > true warning : not the same parms as other functions!!!!!! you can basicly download anything with this function but better leave it alone with,"['def', 'ffmpeg_download_stream', '(', 'files', ',', 'title', ',', 'ext', ',', 'params', '=', '{', '}', ',', 'output_dir', '=', ""'.'"", ',', 'stream', '=', 'true', ')', ':', 'output', '=', 'title', '+', ""'.'"", '+', 'ext', 'if', 'not', '(', 'output_dir', '==', ""'.'"", ')', ':', 'output', '=', 'output_dir', '+', ""'/'"", '+', 'output', 'print', '(', ""'downloading streaming content with ffmpeg, press q to stop recording...'"", ')', 'if', 'stream', ':', 'ffmpeg_params', '=', '[', 'ffmpeg', ']', '+', '[', ""'-y'"", ',', ""'-re'"", ',', ""'-i'"", ']', 'else', ':', 'ffmpeg_params', '=', '[', 'ffmpeg', ']', '+', '[', ""'-y'"", ',', ""'-i'"", ']', 'ffmpeg_params', '.', 'append', '(', 'files', ')', '#not the same here!!!!', 'if', 'ffmpeg', '==', ""'avconv'"", ':', '#who cares?', 'ffmpeg_params', '+=', '[', ""'-c'"", ',', ""'copy'"", ',', 'output', ']', 'else', ':', 'ffmpeg_params', '+=', '[', ""'-c'"", ',', ""'copy'"", ',', ""'-bsf:a'"", ',', ""'aac_adtstoasc'"", ']', 'if', 'params', 'is', 'not', 'none', ':', 'if', 'len', '(', 'params', ')', '>', '0', ':', 'for', 'k', ',', 'v', 'in', 'params', ':', 'ffmpeg_params', '.', 'append', '(', 'k', ')', 'ffmpeg_params', '.', 'append', '(', 'v', ')', 'ffmpeg_params', '.', 'append', '(', 'output', ')', 'print', '(', ""' '"", '.', 'join', '(', 'ffmpeg_params', ')', ')', 'try', ':', 'a', '=', 'subprocess', '.', 'popen', '(', 'ffmpeg_params', ',', 'stdin', '=', 'subprocess', '.', 'pipe', ')', 'a', '.', 'communicate', '(', ')', 'except', 'keyboardinterrupt', ':', 'try', ':', 'a', '.', 'stdin', '.', 'write', '(', ""'q'"", '.', 'encode', '(', ""'utf-8'"", ')', ')', 'except', ':', 'pass', 'return', 'true']","def ffmpeg_download_stream ( files , title , ext , params = { } , output_dir = '.' , stream = true ) : output = title + '.' + ext if not ( output_dir == '.' ) : output = output_dir + '/' + output print ( 'downloading streaming content with ffmpeg, press q to stop recording...' ) if stream : ffmpeg_params = [ ffmpeg ] + [ '-y' , '-re' , '-i' ] else : ffmpeg_params = [ ffmpeg ] + [ '-y' , '-i' ] ffmpeg_params . append ( files ) #not the same here!!!! if ffmpeg == 'avconv' : #who cares? ffmpeg_params += [ '-c' , 'copy' , output ] else : ffmpeg_params += [ '-c' , 'copy' , '-bsf:a' , 'aac_adtstoasc' ] if params is not none : if len ( params ) > 0 : for k , v in params : ffmpeg_params . append ( k ) ffmpeg_params . append ( v ) ffmpeg_params . append ( output ) print ( ' ' . join ( ffmpeg_params ) ) try : a = subprocess . popen ( ffmpeg_params , stdin = subprocess . pipe ) a . communicate ( ) except keyboardinterrupt : try : a . stdin . write ( 'q' . encode ( 'utf-8' ) ) except : pass return true"
37,soimort/you-get,src/you_get/common.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L224-L249,"def match1(text, *patterns):
    """"""Scans through a string for substrings matched some patterns (first-subgroups only).

    Args:
        text: A string to be scanned.
        patterns: Arbitrary number of regex patterns.

    Returns:
        When only one pattern is given, returns a string (None if no match found).
        When more than one pattern are given, returns a list of strings ([] if no match found).
    """"""

    if len(patterns) == 1:
        pattern = patterns[0]
        match = re.search(pattern, text)
        if match:
            return match.group(1)
        else:
            return None
    else:
        ret = []
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                ret.append(match.group(1))
        return ret","['def', 'match1', '(', 'text', ',', '*', 'patterns', ')', ':', 'if', 'len', '(', 'patterns', ')', '==', '1', ':', 'pattern', '=', 'patterns', '[', '0', ']', 'match', '=', 're', '.', 'search', '(', 'pattern', ',', 'text', ')', 'if', 'match', ':', 'return', 'match', '.', 'group', '(', '1', ')', 'else', ':', 'return', 'None', 'else', ':', 'ret', '=', '[', ']', 'for', 'pattern', 'in', 'patterns', ':', 'match', '=', 're', '.', 'search', '(', 'pattern', ',', 'text', ')', 'if', 'match', ':', 'ret', '.', 'append', '(', 'match', '.', 'group', '(', '1', ')', ')', 'return', 'ret']","Scans through a string for substrings matched some patterns (first-subgroups only).

    Args:
        text: A string to be scanned.
        patterns: Arbitrary number of regex patterns.

    Returns:
        When only one pattern is given, returns a string (None if no match found).
        When more than one pattern are given, returns a list of strings ([] if no match found).","['Scans', 'through', 'a', 'string', 'for', 'substrings', 'matched', 'some', 'patterns', '(', 'first', '-', 'subgroups', 'only', ')', '.']",python,test,"['scans', 'through', 'a', 'string', 'for', 'substrings', 'matched', 'some', 'patterns', '(', 'first', '-', 'subgroups', 'only', ')', '.']",scans through a string for substrings matched some patterns ( first - subgroups only ) .,"['def', 'match1', '(', 'text', ',', '*', 'patterns', ')', ':', 'if', 'len', '(', 'patterns', ')', '==', '1', ':', 'pattern', '=', 'patterns', '[', '0', ']', 'match', '=', 're', '.', 'search', '(', 'pattern', ',', 'text', ')', 'if', 'match', ':', 'return', 'match', '.', 'group', '(', '1', ')', 'else', ':', 'return', 'none', 'else', ':', 'ret', '=', '[', ']', 'for', 'pattern', 'in', 'patterns', ':', 'match', '=', 're', '.', 'search', '(', 'pattern', ',', 'text', ')', 'if', 'match', ':', 'ret', '.', 'append', '(', 'match', '.', 'group', '(', '1', ')', ')', 'return', 'ret']","def match1 ( text , * patterns ) : if len ( patterns ) == 1 : pattern = patterns [ 0 ] match = re . search ( pattern , text ) if match : return match . group ( 1 ) else : return none else : ret = [ ] for pattern in patterns : match = re . search ( pattern , text ) if match : ret . append ( match . group ( 1 ) ) return ret"
38,soimort/you-get,src/you_get/common.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L252-L268,"def matchall(text, patterns):
    """"""Scans through a string for substrings matched some patterns.

    Args:
        text: A string to be scanned.
        patterns: a list of regex pattern.

    Returns:
        a list if matched. empty if not.
    """"""

    ret = []
    for pattern in patterns:
        match = re.findall(pattern, text)
        ret += match

    return ret","['def', 'matchall', '(', 'text', ',', 'patterns', ')', ':', 'ret', '=', '[', ']', 'for', 'pattern', 'in', 'patterns', ':', 'match', '=', 're', '.', 'findall', '(', 'pattern', ',', 'text', ')', 'ret', '+=', 'match', 'return', 'ret']","Scans through a string for substrings matched some patterns.

    Args:
        text: A string to be scanned.
        patterns: a list of regex pattern.

    Returns:
        a list if matched. empty if not.","['Scans', 'through', 'a', 'string', 'for', 'substrings', 'matched', 'some', 'patterns', '.']",python,test,"['scans', 'through', 'a', 'string', 'for', 'substrings', 'matched', 'some', 'patterns', '.']",scans through a string for substrings matched some patterns .,"['def', 'matchall', '(', 'text', ',', 'patterns', ')', ':', 'ret', '=', '[', ']', 'for', 'pattern', 'in', 'patterns', ':', 'match', '=', 're', '.', 'findall', '(', 'pattern', ',', 'text', ')', 'ret', '+=', 'match', 'return', 'ret']","def matchall ( text , patterns ) : ret = [ ] for pattern in patterns : match = re . findall ( pattern , text ) ret += match return ret"
39,soimort/you-get,src/you_get/common.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L285-L299,"def parse_query_param(url, param):
    """"""Parses the query string of a URL and returns the value of a parameter.

    Args:
        url: A URL.
        param: A string representing the name of the parameter.

    Returns:
        The value of the parameter.
    """"""

    try:
        return parse.parse_qs(parse.urlparse(url).query)[param][0]
    except:
        return None","['def', 'parse_query_param', '(', 'url', ',', 'param', ')', ':', 'try', ':', 'return', 'parse', '.', 'parse_qs', '(', 'parse', '.', 'urlparse', '(', 'url', ')', '.', 'query', ')', '[', 'param', ']', '[', '0', ']', 'except', ':', 'return', 'None']","Parses the query string of a URL and returns the value of a parameter.

    Args:
        url: A URL.
        param: A string representing the name of the parameter.

    Returns:
        The value of the parameter.","['Parses', 'the', 'query', 'string', 'of', 'a', 'URL', 'and', 'returns', 'the', 'value', 'of', 'a', 'parameter', '.']",python,test,"['parses', 'the', 'query', 'string', 'of', 'a', 'url', 'and', 'returns', 'the', 'value', 'of', 'a', 'parameter', '.']",parses the query string of a url and returns the value of a parameter .,"['def', 'parse_query_param', '(', 'url', ',', 'param', ')', ':', 'try', ':', 'return', 'parse', '.', 'parse_qs', '(', 'parse', '.', 'urlparse', '(', 'url', ')', '.', 'query', ')', '[', 'param', ']', '[', '0', ']', 'except', ':', 'return', 'none']","def parse_query_param ( url , param ) : try : return parse . parse_qs ( parse . urlparse ( url ) . query ) [ param ] [ 0 ] except : return none"
40,soimort/you-get,src/you_get/common.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L319-L326,"def ungzip(data):
    """"""Decompresses data for Content-Encoding: gzip.
    """"""
    from io import BytesIO
    import gzip
    buffer = BytesIO(data)
    f = gzip.GzipFile(fileobj=buffer)
    return f.read()","['def', 'ungzip', '(', 'data', ')', ':', 'from', 'io', 'import', 'BytesIO', 'import', 'gzip', 'buffer', '=', 'BytesIO', '(', 'data', ')', 'f', '=', 'gzip', '.', 'GzipFile', '(', 'fileobj', '=', 'buffer', ')', 'return', 'f', '.', 'read', '(', ')']",Decompresses data for Content-Encoding: gzip.,"['Decompresses', 'data', 'for', 'Content', '-', 'Encoding', ':', 'gzip', '.']",python,test,"['decompresses', 'data', 'for', 'content', '-', 'encoding', ':', 'gzip', '.']",decompresses data for content - encoding : gzip .,"['def', 'ungzip', '(', 'data', ')', ':', 'from', 'io', 'import', 'bytesio', 'import', 'gzip', 'buffer', '=', 'bytesio', '(', 'data', ')', 'f', '=', 'gzip', '.', 'gzipfile', '(', 'fileobj', '=', 'buffer', ')', 'return', 'f', '.', 'read', '(', ')']",def ungzip ( data ) : from io import bytesio import gzip buffer = bytesio ( data ) f = gzip . gzipfile ( fileobj = buffer ) return f . read ( )
41,soimort/you-get,src/you_get/common.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L329-L335,"def undeflate(data):
    """"""Decompresses data for Content-Encoding: deflate.
    (the zlib compression is used.)
    """"""
    import zlib
    decompressobj = zlib.decompressobj(-zlib.MAX_WBITS)
    return decompressobj.decompress(data)+decompressobj.flush()","['def', 'undeflate', '(', 'data', ')', ':', 'import', 'zlib', 'decompressobj', '=', 'zlib', '.', 'decompressobj', '(', '-', 'zlib', '.', 'MAX_WBITS', ')', 'return', 'decompressobj', '.', 'decompress', '(', 'data', ')', '+', 'decompressobj', '.', 'flush', '(', ')']","Decompresses data for Content-Encoding: deflate.
    (the zlib compression is used.)","['Decompresses', 'data', 'for', 'Content', '-', 'Encoding', ':', 'deflate', '.', '(', 'the', 'zlib', 'compression', 'is', 'used', '.', ')']",python,test,"['decompresses', 'data', 'for', 'content', '-', 'encoding', ':', 'deflate', '.', '(', 'the', 'zlib', 'compression', 'is', 'used', '.', ')']",decompresses data for content - encoding : deflate . ( the zlib compression is used . ),"['def', 'undeflate', '(', 'data', ')', ':', 'import', 'zlib', 'decompressobj', '=', 'zlib', '.', 'decompressobj', '(', '-', 'zlib', '.', 'max_wbits', ')', 'return', 'decompressobj', '.', 'decompress', '(', 'data', ')', '+', 'decompressobj', '.', 'flush', '(', ')']",def undeflate ( data ) : import zlib decompressobj = zlib . decompressobj ( - zlib . max_wbits ) return decompressobj . decompress ( data ) + decompressobj . flush ( )
42,soimort/you-get,src/you_get/common.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L415-L454,"def get_content(url, headers={}, decoded=True):
    """"""Gets the content of a URL via sending a HTTP GET request.

    Args:
        url: A URL.
        headers: Request headers used by the client.
        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.

    Returns:
        The content as a string.
    """"""

    logging.debug('get_content: %s' % url)

    req = request.Request(url, headers=headers)
    if cookies:
        cookies.add_cookie_header(req)
        req.headers.update(req.unredirected_hdrs)

    response = urlopen_with_retry(req)
    data = response.read()

    # Handle HTTP compression for gzip and deflate (zlib)
    content_encoding = response.getheader('Content-Encoding')
    if content_encoding == 'gzip':
        data = ungzip(data)
    elif content_encoding == 'deflate':
        data = undeflate(data)

    # Decode the response body
    if decoded:
        charset = match1(
            response.getheader('Content-Type', ''), r'charset=([\w-]+)'
        )
        if charset is not None:
            data = data.decode(charset, 'ignore')
        else:
            data = data.decode('utf-8', 'ignore')

    return data","['def', 'get_content', '(', 'url', ',', 'headers', '=', '{', '}', ',', 'decoded', '=', 'True', ')', ':', 'logging', '.', 'debug', '(', ""'get_content: %s'"", '%', 'url', ')', 'req', '=', 'request', '.', 'Request', '(', 'url', ',', 'headers', '=', 'headers', ')', 'if', 'cookies', ':', 'cookies', '.', 'add_cookie_header', '(', 'req', ')', 'req', '.', 'headers', '.', 'update', '(', 'req', '.', 'unredirected_hdrs', ')', 'response', '=', 'urlopen_with_retry', '(', 'req', ')', 'data', '=', 'response', '.', 'read', '(', ')', '# Handle HTTP compression for gzip and deflate (zlib)', 'content_encoding', '=', 'response', '.', 'getheader', '(', ""'Content-Encoding'"", ')', 'if', 'content_encoding', '==', ""'gzip'"", ':', 'data', '=', 'ungzip', '(', 'data', ')', 'elif', 'content_encoding', '==', ""'deflate'"", ':', 'data', '=', 'undeflate', '(', 'data', ')', '# Decode the response body', 'if', 'decoded', ':', 'charset', '=', 'match1', '(', 'response', '.', 'getheader', '(', ""'Content-Type'"", ',', ""''"", ')', ',', ""r'charset=([\\w-]+)'"", ')', 'if', 'charset', 'is', 'not', 'None', ':', 'data', '=', 'data', '.', 'decode', '(', 'charset', ',', ""'ignore'"", ')', 'else', ':', 'data', '=', 'data', '.', 'decode', '(', ""'utf-8'"", ',', ""'ignore'"", ')', 'return', 'data']","Gets the content of a URL via sending a HTTP GET request.

    Args:
        url: A URL.
        headers: Request headers used by the client.
        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.

    Returns:
        The content as a string.","['Gets', 'the', 'content', 'of', 'a', 'URL', 'via', 'sending', 'a', 'HTTP', 'GET', 'request', '.']",python,test,"['gets', 'the', 'content', 'of', 'a', 'url', 'via', 'sending', 'a', 'http', 'get', 'request', '.']",gets the content of a url via sending a http get request .,"['def', 'get_content', '(', 'url', ',', 'headers', '=', '{', '}', ',', 'decoded', '=', 'true', ')', ':', 'logging', '.', 'debug', '(', ""'get_content: %s'"", '%', 'url', ')', 'req', '=', 'request', '.', 'request', '(', 'url', ',', 'headers', '=', 'headers', ')', 'if', 'cookies', ':', 'cookies', '.', 'add_cookie_header', '(', 'req', ')', 'req', '.', 'headers', '.', 'update', '(', 'req', '.', 'unredirected_hdrs', ')', 'response', '=', 'urlopen_with_retry', '(', 'req', ')', 'data', '=', 'response', '.', 'read', '(', ')', '# handle http compression for gzip and deflate (zlib)', 'content_encoding', '=', 'response', '.', 'getheader', '(', ""'content-encoding'"", ')', 'if', 'content_encoding', '==', ""'gzip'"", ':', 'data', '=', 'ungzip', '(', 'data', ')', 'elif', 'content_encoding', '==', ""'deflate'"", ':', 'data', '=', 'undeflate', '(', 'data', ')', '# decode the response body', 'if', 'decoded', ':', 'charset', '=', 'match1', '(', 'response', '.', 'getheader', '(', ""'content-type'"", ',', ""''"", ')', ',', ""r'charset=([\\w-]+)'"", ')', 'if', 'charset', 'is', 'not', 'none', ':', 'data', '=', 'data', '.', 'decode', '(', 'charset', ',', ""'ignore'"", ')', 'else', ':', 'data', '=', 'data', '.', 'decode', '(', ""'utf-8'"", ',', ""'ignore'"", ')', 'return', 'data']","def get_content ( url , headers = { } , decoded = true ) : logging . debug ( 'get_content: %s' % url ) req = request . request ( url , headers = headers ) if cookies : cookies . add_cookie_header ( req ) req . headers . update ( req . unredirected_hdrs ) response = urlopen_with_retry ( req ) data = response . read ( ) # handle http compression for gzip and deflate (zlib) content_encoding = response . getheader ( 'content-encoding' ) if content_encoding == 'gzip' : data = ungzip ( data ) elif content_encoding == 'deflate' : data = undeflate ( data ) # decode the response body if decoded : charset = match1 ( response . getheader ( 'content-type' , '' ) , r'charset=([\w-]+)' ) if charset is not none : data = data . decode ( charset , 'ignore' ) else : data = data . decode ( 'utf-8' , 'ignore' ) return data"
43,soimort/you-get,src/you_get/common.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L457-L501,"def post_content(url, headers={}, post_data={}, decoded=True, **kwargs):
    """"""Post the content of a URL via sending a HTTP POST request.

    Args:
        url: A URL.
        headers: Request headers used by the client.
        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.

    Returns:
        The content as a string.
    """"""
    if kwargs.get('post_data_raw'):
        logging.debug('post_content: %s\npost_data_raw: %s' % (url, kwargs['post_data_raw']))
    else:
        logging.debug('post_content: %s\npost_data: %s' % (url, post_data))

    req = request.Request(url, headers=headers)
    if cookies:
        cookies.add_cookie_header(req)
        req.headers.update(req.unredirected_hdrs)
    if kwargs.get('post_data_raw'):
        post_data_enc = bytes(kwargs['post_data_raw'], 'utf-8')
    else:
        post_data_enc = bytes(parse.urlencode(post_data), 'utf-8')
    response = urlopen_with_retry(req, data=post_data_enc)
    data = response.read()

    # Handle HTTP compression for gzip and deflate (zlib)
    content_encoding = response.getheader('Content-Encoding')
    if content_encoding == 'gzip':
        data = ungzip(data)
    elif content_encoding == 'deflate':
        data = undeflate(data)

    # Decode the response body
    if decoded:
        charset = match1(
            response.getheader('Content-Type'), r'charset=([\w-]+)'
        )
        if charset is not None:
            data = data.decode(charset)
        else:
            data = data.decode('utf-8')

    return data","['def', 'post_content', '(', 'url', ',', 'headers', '=', '{', '}', ',', 'post_data', '=', '{', '}', ',', 'decoded', '=', 'True', ',', '*', '*', 'kwargs', ')', ':', 'if', 'kwargs', '.', 'get', '(', ""'post_data_raw'"", ')', ':', 'logging', '.', 'debug', '(', ""'post_content: %s\\npost_data_raw: %s'"", '%', '(', 'url', ',', 'kwargs', '[', ""'post_data_raw'"", ']', ')', ')', 'else', ':', 'logging', '.', 'debug', '(', ""'post_content: %s\\npost_data: %s'"", '%', '(', 'url', ',', 'post_data', ')', ')', 'req', '=', 'request', '.', 'Request', '(', 'url', ',', 'headers', '=', 'headers', ')', 'if', 'cookies', ':', 'cookies', '.', 'add_cookie_header', '(', 'req', ')', 'req', '.', 'headers', '.', 'update', '(', 'req', '.', 'unredirected_hdrs', ')', 'if', 'kwargs', '.', 'get', '(', ""'post_data_raw'"", ')', ':', 'post_data_enc', '=', 'bytes', '(', 'kwargs', '[', ""'post_data_raw'"", ']', ',', ""'utf-8'"", ')', 'else', ':', 'post_data_enc', '=', 'bytes', '(', 'parse', '.', 'urlencode', '(', 'post_data', ')', ',', ""'utf-8'"", ')', 'response', '=', 'urlopen_with_retry', '(', 'req', ',', 'data', '=', 'post_data_enc', ')', 'data', '=', 'response', '.', 'read', '(', ')', '# Handle HTTP compression for gzip and deflate (zlib)', 'content_encoding', '=', 'response', '.', 'getheader', '(', ""'Content-Encoding'"", ')', 'if', 'content_encoding', '==', ""'gzip'"", ':', 'data', '=', 'ungzip', '(', 'data', ')', 'elif', 'content_encoding', '==', ""'deflate'"", ':', 'data', '=', 'undeflate', '(', 'data', ')', '# Decode the response body', 'if', 'decoded', ':', 'charset', '=', 'match1', '(', 'response', '.', 'getheader', '(', ""'Content-Type'"", ')', ',', ""r'charset=([\\w-]+)'"", ')', 'if', 'charset', 'is', 'not', 'None', ':', 'data', '=', 'data', '.', 'decode', '(', 'charset', ')', 'else', ':', 'data', '=', 'data', '.', 'decode', '(', ""'utf-8'"", ')', 'return', 'data']","Post the content of a URL via sending a HTTP POST request.

    Args:
        url: A URL.
        headers: Request headers used by the client.
        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.

    Returns:
        The content as a string.","['Post', 'the', 'content', 'of', 'a', 'URL', 'via', 'sending', 'a', 'HTTP', 'POST', 'request', '.']",python,test,"['post', 'the', 'content', 'of', 'a', 'url', 'via', 'sending', 'a', 'http', 'post', 'request', '.']",post the content of a url via sending a http post request .,"['def', 'post_content', '(', 'url', ',', 'headers', '=', '{', '}', ',', 'post_data', '=', '{', '}', ',', 'decoded', '=', 'true', ',', '*', '*', 'kwargs', ')', ':', 'if', 'kwargs', '.', 'get', '(', ""'post_data_raw'"", ')', ':', 'logging', '.', 'debug', '(', ""'post_content: %s\\npost_data_raw: %s'"", '%', '(', 'url', ',', 'kwargs', '[', ""'post_data_raw'"", ']', ')', ')', 'else', ':', 'logging', '.', 'debug', '(', ""'post_content: %s\\npost_data: %s'"", '%', '(', 'url', ',', 'post_data', ')', ')', 'req', '=', 'request', '.', 'request', '(', 'url', ',', 'headers', '=', 'headers', ')', 'if', 'cookies', ':', 'cookies', '.', 'add_cookie_header', '(', 'req', ')', 'req', '.', 'headers', '.', 'update', '(', 'req', '.', 'unredirected_hdrs', ')', 'if', 'kwargs', '.', 'get', '(', ""'post_data_raw'"", ')', ':', 'post_data_enc', '=', 'bytes', '(', 'kwargs', '[', ""'post_data_raw'"", ']', ',', ""'utf-8'"", ')', 'else', ':', 'post_data_enc', '=', 'bytes', '(', 'parse', '.', 'urlencode', '(', 'post_data', ')', ',', ""'utf-8'"", ')', 'response', '=', 'urlopen_with_retry', '(', 'req', ',', 'data', '=', 'post_data_enc', ')', 'data', '=', 'response', '.', 'read', '(', ')', '# handle http compression for gzip and deflate (zlib)', 'content_encoding', '=', 'response', '.', 'getheader', '(', ""'content-encoding'"", ')', 'if', 'content_encoding', '==', ""'gzip'"", ':', 'data', '=', 'ungzip', '(', 'data', ')', 'elif', 'content_encoding', '==', ""'deflate'"", ':', 'data', '=', 'undeflate', '(', 'data', ')', '# decode the response body', 'if', 'decoded', ':', 'charset', '=', 'match1', '(', 'response', '.', 'getheader', '(', ""'content-type'"", ')', ',', ""r'charset=([\\w-]+)'"", ')', 'if', 'charset', 'is', 'not', 'none', ':', 'data', '=', 'data', '.', 'decode', '(', 'charset', ')', 'else', ':', 'data', '=', 'data', '.', 'decode', '(', ""'utf-8'"", ')', 'return', 'data']","def post_content ( url , headers = { } , post_data = { } , decoded = true , * * kwargs ) : if kwargs . get ( 'post_data_raw' ) : logging . debug ( 'post_content: %s\npost_data_raw: %s' % ( url , kwargs [ 'post_data_raw' ] ) ) else : logging . debug ( 'post_content: %s\npost_data: %s' % ( url , post_data ) ) req = request . request ( url , headers = headers ) if cookies : cookies . add_cookie_header ( req ) req . headers . update ( req . unredirected_hdrs ) if kwargs . get ( 'post_data_raw' ) : post_data_enc = bytes ( kwargs [ 'post_data_raw' ] , 'utf-8' ) else : post_data_enc = bytes ( parse . urlencode ( post_data ) , 'utf-8' ) response = urlopen_with_retry ( req , data = post_data_enc ) data = response . read ( ) # handle http compression for gzip and deflate (zlib) content_encoding = response . getheader ( 'content-encoding' ) if content_encoding == 'gzip' : data = ungzip ( data ) elif content_encoding == 'deflate' : data = undeflate ( data ) # decode the response body if decoded : charset = match1 ( response . getheader ( 'content-type' ) , r'charset=([\w-]+)' ) if charset is not none : data = data . decode ( charset ) else : data = data . decode ( 'utf-8' ) return data"
44,soimort/you-get,src/you_get/common.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L1216-L1226,"def parse_host(host):
    """"""Parses host name and port number from a string.
    """"""
    if re.match(r'^(\d+)$', host) is not None:
        return (""0.0.0.0"", int(host))
    if re.match(r'^(\w+)://', host) is None:
        host = ""//"" + host
    o = parse.urlparse(host)
    hostname = o.hostname or ""0.0.0.0""
    port = o.port or 0
    return (hostname, port)","['def', 'parse_host', '(', 'host', ')', ':', 'if', 're', '.', 'match', '(', ""r'^(\\d+)$'"", ',', 'host', ')', 'is', 'not', 'None', ':', 'return', '(', '""0.0.0.0""', ',', 'int', '(', 'host', ')', ')', 'if', 're', '.', 'match', '(', ""r'^(\\w+)://'"", ',', 'host', ')', 'is', 'None', ':', 'host', '=', '""//""', '+', 'host', 'o', '=', 'parse', '.', 'urlparse', '(', 'host', ')', 'hostname', '=', 'o', '.', 'hostname', 'or', '""0.0.0.0""', 'port', '=', 'o', '.', 'port', 'or', '0', 'return', '(', 'hostname', ',', 'port', ')']",Parses host name and port number from a string.,"['Parses', 'host', 'name', 'and', 'port', 'number', 'from', 'a', 'string', '.']",python,test,"['parses', 'host', 'name', 'and', 'port', 'number', 'from', 'a', 'string', '.']",parses host name and port number from a string .,"['def', 'parse_host', '(', 'host', ')', ':', 'if', 're', '.', 'match', '(', ""r'^(\\d+)$'"", ',', 'host', ')', 'is', 'not', 'none', ':', 'return', '(', '""0.0.0.0""', ',', 'int', '(', 'host', ')', ')', 'if', 're', '.', 'match', '(', ""r'^(\\w+)://'"", ',', 'host', ')', 'is', 'none', ':', 'host', '=', '""//""', '+', 'host', 'o', '=', 'parse', '.', 'urlparse', '(', 'host', ')', 'hostname', '=', 'o', '.', 'hostname', 'or', '""0.0.0.0""', 'port', '=', 'o', '.', 'port', 'or', '0', 'return', '(', 'hostname', ',', 'port', ')']","def parse_host ( host ) : if re . match ( r'^(\d+)$' , host ) is not none : return ( ""0.0.0.0"" , int ( host ) ) if re . match ( r'^(\w+)://' , host ) is none : host = ""//"" + host o = parse . urlparse ( host ) hostname = o . hostname or ""0.0.0.0"" port = o . port or 0 return ( hostname , port )"
45,soimort/you-get,src/you_get/common.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L1258-L1273,"def print_more_compatible(*args, **kwargs):
    import builtins as __builtin__
    """"""Overload default print function as py (<3.3) does not support 'flush' keyword.
    Although the function name can be same as print to get itself overloaded automatically,
    I'd rather leave it with a different name and only overload it when importing to make less confusion.
    """"""
    # nothing happens on py3.3 and later
    if sys.version_info[:2] >= (3, 3):
        return __builtin__.print(*args, **kwargs)

    # in lower pyver (e.g. 3.2.x), remove 'flush' keyword and flush it as requested
    doFlush = kwargs.pop('flush', False)
    ret = __builtin__.print(*args, **kwargs)
    if doFlush:
        kwargs.get('file', sys.stdout).flush()
    return ret","['def', 'print_more_compatible', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'import', 'builtins', 'as', '__builtin__', '# nothing happens on py3.3 and later', 'if', 'sys', '.', 'version_info', '[', ':', '2', ']', '>=', '(', '3', ',', '3', ')', ':', 'return', '__builtin__', '.', 'print', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', ""# in lower pyver (e.g. 3.2.x), remove 'flush' keyword and flush it as requested"", 'doFlush', '=', 'kwargs', '.', 'pop', '(', ""'flush'"", ',', 'False', ')', 'ret', '=', '__builtin__', '.', 'print', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', 'if', 'doFlush', ':', 'kwargs', '.', 'get', '(', ""'file'"", ',', 'sys', '.', 'stdout', ')', '.', 'flush', '(', ')', 'return', 'ret']","Overload default print function as py (<3.3) does not support 'flush' keyword.
    Although the function name can be same as print to get itself overloaded automatically,
    I'd rather leave it with a different name and only overload it when importing to make less confusion.","['Overload', 'default', 'print', 'function', 'as', 'py', '(', '<3', '.', '3', ')', 'does', 'not', 'support', 'flush', 'keyword', '.', 'Although', 'the', 'function', 'name', 'can', 'be', 'same', 'as', 'print', 'to', 'get', 'itself', 'overloaded', 'automatically', 'I', 'd', 'rather', 'leave', 'it', 'with', 'a', 'different', 'name', 'and', 'only', 'overload', 'it', 'when', 'importing', 'to', 'make', 'less', 'confusion', '.']",python,test,"['overload', 'default', 'print', 'function', 'as', 'py', '(', '<3', '.', '3', ')', 'does', 'not', 'support', 'flush', 'keyword', '.', 'although', 'the', 'function', 'name', 'can', 'be', 'same', 'as', 'print', 'to', 'get', 'itself', 'overloaded', 'automatically', 'i', 'd', 'rather', 'leave', 'it', 'with', 'a', 'different', 'name', 'and', 'only', 'overload', 'it', 'when', 'importing', 'to', 'make', 'less', 'confusion', '.']",overload default print function as py ( <3 . 3 ) does not support flush keyword . although the function name can be same as print to get itself overloaded automatically i d rather leave it with a different name and only overload it when importing to make less confusion .,"['def', 'print_more_compatible', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'import', 'builtins', 'as', '__builtin__', '# nothing happens on py3.3 and later', 'if', 'sys', '.', 'version_info', '[', ':', '2', ']', '>=', '(', '3', ',', '3', ')', ':', 'return', '__builtin__', '.', 'print', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', ""# in lower pyver (e.g. 3.2.x), remove 'flush' keyword and flush it as requested"", 'doflush', '=', 'kwargs', '.', 'pop', '(', ""'flush'"", ',', 'false', ')', 'ret', '=', '__builtin__', '.', 'print', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', 'if', 'doflush', ':', 'kwargs', '.', 'get', '(', ""'file'"", ',', 'sys', '.', 'stdout', ')', '.', 'flush', '(', ')', 'return', 'ret']","def print_more_compatible ( * args , * * kwargs ) : import builtins as __builtin__ # nothing happens on py3.3 and later if sys . version_info [ : 2 ] >= ( 3 , 3 ) : return __builtin__ . print ( * args , * * kwargs ) # in lower pyver (e.g. 3.2.x), remove 'flush' keyword and flush it as requested doflush = kwargs . pop ( 'flush' , false ) ret = __builtin__ . print ( * args , * * kwargs ) if doflush : kwargs . get ( 'file' , sys . stdout ) . flush ( ) return ret"
46,soimort/you-get,src/you_get/extractors/showroom.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/showroom.py#L11-L24,"def showroom_get_roomid_by_room_url_key(room_url_key):
    """"""str->str""""""
    fake_headers_mobile = {
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Charset': 'UTF-8,*;q=0.5',
        'Accept-Encoding': 'gzip,deflate,sdch',
        'Accept-Language': 'en-US,en;q=0.8',
        'User-Agent': 'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36'
    }
    webpage_url = 'https://www.showroom-live.com/' + room_url_key
    html = get_content(webpage_url, headers = fake_headers_mobile)
    roomid = match1(html, r'room\?room_id\=(\d+)')
    assert roomid
    return roomid","['def', 'showroom_get_roomid_by_room_url_key', '(', 'room_url_key', ')', ':', 'fake_headers_mobile', '=', '{', ""'Accept'"", ':', ""'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'"", ',', ""'Accept-Charset'"", ':', ""'UTF-8,*;q=0.5'"", ',', ""'Accept-Encoding'"", ':', ""'gzip,deflate,sdch'"", ',', ""'Accept-Language'"", ':', ""'en-US,en;q=0.8'"", ',', ""'User-Agent'"", ':', ""'Mozilla/5.0 (Linux; Android 4.4.2; Nexus 4 Build/KOT49H) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.114 Mobile Safari/537.36'"", '}', 'webpage_url', '=', ""'https://www.showroom-live.com/'"", '+', 'room_url_key', 'html', '=', 'get_content', '(', 'webpage_url', ',', 'headers', '=', 'fake_headers_mobile', ')', 'roomid', '=', 'match1', '(', 'html', ',', ""r'room\\?room_id\\=(\\d+)'"", ')', 'assert', 'roomid', 'return', 'roomid']",str->str,"['str', '-', '>', 'str']",python,test,"['str', '-', '>', 'str']",str - > str,"['def', 'showroom_get_roomid_by_room_url_key', '(', 'room_url_key', ')', ':', 'fake_headers_mobile', '=', '{', ""'accept'"", ':', ""'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'"", ',', ""'accept-charset'"", ':', ""'utf-8,*;q=0.5'"", ',', ""'accept-encoding'"", ':', ""'gzip,deflate,sdch'"", ',', ""'accept-language'"", ':', ""'en-us,en;q=0.8'"", ',', ""'user-agent'"", ':', ""'mozilla/5.0 (linux; android 4.4.2; nexus 4 build/kot49h) applewebkit/537.36 (khtml, like gecko) chrome/34.0.1847.114 mobile safari/537.36'"", '}', 'webpage_url', '=', ""'https://www.showroom-live.com/'"", '+', 'room_url_key', 'html', '=', 'get_content', '(', 'webpage_url', ',', 'headers', '=', 'fake_headers_mobile', ')', 'roomid', '=', 'match1', '(', 'html', ',', ""r'room\\?room_id\\=(\\d+)'"", ')', 'assert', 'roomid', 'return', 'roomid']","def showroom_get_roomid_by_room_url_key ( room_url_key ) : fake_headers_mobile = { 'accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8' , 'accept-charset' : 'utf-8,*;q=0.5' , 'accept-encoding' : 'gzip,deflate,sdch' , 'accept-language' : 'en-us,en;q=0.8' , 'user-agent' : 'mozilla/5.0 (linux; android 4.4.2; nexus 4 build/kot49h) applewebkit/537.36 (khtml, like gecko) chrome/34.0.1847.114 mobile safari/537.36' } webpage_url = 'https://www.showroom-live.com/' + room_url_key html = get_content ( webpage_url , headers = fake_headers_mobile ) roomid = match1 ( html , r'room\?room_id\=(\d+)' ) assert roomid return roomid"
47,soimort/you-get,src/you_get/extractors/showroom.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/showroom.py#L26-L56,"def showroom_download_by_room_id(room_id, output_dir = '.', merge = False, info_only = False, **kwargs):
    '''Source: Android mobile'''
    while True:
        timestamp = str(int(time() * 1000))
        api_endpoint = 'https://www.showroom-live.com/api/live/streaming_url?room_id={room_id}&_={timestamp}'.format(room_id = room_id, timestamp = timestamp)
        html = get_content(api_endpoint)
        html = json.loads(html)
        #{'streaming_url_list': [{'url': 'rtmp://52.197.69.198:1935/liveedge', 'id': 1, 'label': 'original spec(low latency)', 'is_default': True, 'type': 'rtmp', 'stream_name': '7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed'}, {'url': 'http://52.197.69.198:1935/liveedge/7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed/playlist.m3u8', 'is_default': True, 'id': 2, 'type': 'hls', 'label': 'original spec'}, {'url': 'rtmp://52.197.69.198:1935/liveedge', 'id': 3, 'label': 'low spec(low latency)', 'is_default': False, 'type': 'rtmp', 'stream_name': '7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed_low'}, {'url': 'http://52.197.69.198:1935/liveedge/7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed_low/playlist.m3u8', 'is_default': False, 'id': 4, 'type': 'hls', 'label': 'low spec'}]}
        if len(html) >= 1:
            break
        log.w('The live show is currently offline.')
        sleep(1)

    #This is mainly for testing the M3U FFmpeg parser so I would ignore any non-m3u ones
    stream_url = [i['url'] for i in html['streaming_url_list'] if i['is_default'] and i['type'] == 'hls'][0]

    assert stream_url

    #title
    title = ''
    profile_api = 'https://www.showroom-live.com/api/room/profile?room_id={room_id}'.format(room_id = room_id)
    html = loads(get_content(profile_api))
    try:
        title = html['main_name']
    except KeyError:
        title = 'Showroom_{room_id}'.format(room_id = room_id)

    type_, ext, size = url_info(stream_url)
    print_info(site_info, title, type_, size)
    if not info_only:
        download_url_ffmpeg(url=stream_url, title=title, ext= 'mp4', output_dir=output_dir)","['def', 'showroom_download_by_room_id', '(', 'room_id', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'False', ',', 'info_only', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', 'while', 'True', ':', 'timestamp', '=', 'str', '(', 'int', '(', 'time', '(', ')', '*', '1000', ')', ')', 'api_endpoint', '=', ""'https://www.showroom-live.com/api/live/streaming_url?room_id={room_id}&_={timestamp}'"", '.', 'format', '(', 'room_id', '=', 'room_id', ',', 'timestamp', '=', 'timestamp', ')', 'html', '=', 'get_content', '(', 'api_endpoint', ')', 'html', '=', 'json', '.', 'loads', '(', 'html', ')', ""#{'streaming_url_list': [{'url': 'rtmp://52.197.69.198:1935/liveedge', 'id': 1, 'label': 'original spec(low latency)', 'is_default': True, 'type': 'rtmp', 'stream_name': '7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed'}, {'url': 'http://52.197.69.198:1935/liveedge/7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed/playlist.m3u8', 'is_default': True, 'id': 2, 'type': 'hls', 'label': 'original spec'}, {'url': 'rtmp://52.197.69.198:1935/liveedge', 'id': 3, 'label': 'low spec(low latency)', 'is_default': False, 'type': 'rtmp', 'stream_name': '7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed_low'}, {'url': 'http://52.197.69.198:1935/liveedge/7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed_low/playlist.m3u8', 'is_default': False, 'id': 4, 'type': 'hls', 'label': 'low spec'}]}"", 'if', 'len', '(', 'html', ')', '>=', '1', ':', 'break', 'log', '.', 'w', '(', ""'The live show is currently offline.'"", ')', 'sleep', '(', '1', ')', '#This is mainly for testing the M3U FFmpeg parser so I would ignore any non-m3u ones', 'stream_url', '=', '[', 'i', '[', ""'url'"", ']', 'for', 'i', 'in', 'html', '[', ""'streaming_url_list'"", ']', 'if', 'i', '[', ""'is_default'"", ']', 'and', 'i', '[', ""'type'"", ']', '==', ""'hls'"", ']', '[', '0', ']', 'assert', 'stream_url', '#title', 'title', '=', ""''"", 'profile_api', '=', ""'https://www.showroom-live.com/api/room/profile?room_id={room_id}'"", '.', 'format', '(', 'room_id', '=', 'room_id', ')', 'html', '=', 'loads', '(', 'get_content', '(', 'profile_api', ')', ')', 'try', ':', 'title', '=', 'html', '[', ""'main_name'"", ']', 'except', 'KeyError', ':', 'title', '=', ""'Showroom_{room_id}'"", '.', 'format', '(', 'room_id', '=', 'room_id', ')', 'type_', ',', 'ext', ',', 'size', '=', 'url_info', '(', 'stream_url', ')', 'print_info', '(', 'site_info', ',', 'title', ',', 'type_', ',', 'size', ')', 'if', 'not', 'info_only', ':', 'download_url_ffmpeg', '(', 'url', '=', 'stream_url', ',', 'title', '=', 'title', ',', 'ext', '=', ""'mp4'"", ',', 'output_dir', '=', 'output_dir', ')']",Source: Android mobile,"['Source', ':', 'Android', 'mobile']",python,test,"['source', ':', 'android', 'mobile']",source : android mobile,"['def', 'showroom_download_by_room_id', '(', 'room_id', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'false', ',', 'info_only', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', 'while', 'true', ':', 'timestamp', '=', 'str', '(', 'int', '(', 'time', '(', ')', '*', '1000', ')', ')', 'api_endpoint', '=', ""'https://www.showroom-live.com/api/live/streaming_url?room_id={room_id}&_={timestamp}'"", '.', 'format', '(', 'room_id', '=', 'room_id', ',', 'timestamp', '=', 'timestamp', ')', 'html', '=', 'get_content', '(', 'api_endpoint', ')', 'html', '=', 'json', '.', 'loads', '(', 'html', ')', ""#{'streaming_url_list': [{'url': 'rtmp://52.197.69.198:1935/liveedge', 'id': 1, 'label': 'original spec(low latency)', 'is_default': true, 'type': 'rtmp', 'stream_name': '7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed'}, {'url': 'http://52.197.69.198:1935/liveedge/7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed/playlist.m3u8', 'is_default': true, 'id': 2, 'type': 'hls', 'label': 'original spec'}, {'url': 'rtmp://52.197.69.198:1935/liveedge', 'id': 3, 'label': 'low spec(low latency)', 'is_default': false, 'type': 'rtmp', 'stream_name': '7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed_low'}, {'url': 'http://52.197.69.198:1935/liveedge/7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed_low/playlist.m3u8', 'is_default': false, 'id': 4, 'type': 'hls', 'label': 'low spec'}]}"", 'if', 'len', '(', 'html', ')', '>=', '1', ':', 'break', 'log', '.', 'w', '(', ""'the live show is currently offline.'"", ')', 'sleep', '(', '1', ')', '#this is mainly for testing the m3u ffmpeg parser so i would ignore any non-m3u ones', 'stream_url', '=', '[', 'i', '[', ""'url'"", ']', 'for', 'i', 'in', 'html', '[', ""'streaming_url_list'"", ']', 'if', 'i', '[', ""'is_default'"", ']', 'and', 'i', '[', ""'type'"", ']', '==', ""'hls'"", ']', '[', '0', ']', 'assert', 'stream_url', '#title', 'title', '=', ""''"", 'profile_api', '=', ""'https://www.showroom-live.com/api/room/profile?room_id={room_id}'"", '.', 'format', '(', 'room_id', '=', 'room_id', ')', 'html', '=', 'loads', '(', 'get_content', '(', 'profile_api', ')', ')', 'try', ':', 'title', '=', 'html', '[', ""'main_name'"", ']', 'except', 'keyerror', ':', 'title', '=', ""'showroom_{room_id}'"", '.', 'format', '(', 'room_id', '=', 'room_id', ')', 'type_', ',', 'ext', ',', 'size', '=', 'url_info', '(', 'stream_url', ')', 'print_info', '(', 'site_info', ',', 'title', ',', 'type_', ',', 'size', ')', 'if', 'not', 'info_only', ':', 'download_url_ffmpeg', '(', 'url', '=', 'stream_url', ',', 'title', '=', 'title', ',', 'ext', '=', ""'mp4'"", ',', 'output_dir', '=', 'output_dir', ')']","def showroom_download_by_room_id ( room_id , output_dir = '.' , merge = false , info_only = false , * * kwargs ) : while true : timestamp = str ( int ( time ( ) * 1000 ) ) api_endpoint = 'https://www.showroom-live.com/api/live/streaming_url?room_id={room_id}&_={timestamp}' . format ( room_id = room_id , timestamp = timestamp ) html = get_content ( api_endpoint ) html = json . loads ( html ) #{'streaming_url_list': [{'url': 'rtmp://52.197.69.198:1935/liveedge', 'id': 1, 'label': 'original spec(low latency)', 'is_default': true, 'type': 'rtmp', 'stream_name': '7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed'}, {'url': 'http://52.197.69.198:1935/liveedge/7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed/playlist.m3u8', 'is_default': true, 'id': 2, 'type': 'hls', 'label': 'original spec'}, {'url': 'rtmp://52.197.69.198:1935/liveedge', 'id': 3, 'label': 'low spec(low latency)', 'is_default': false, 'type': 'rtmp', 'stream_name': '7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed_low'}, {'url': 'http://52.197.69.198:1935/liveedge/7656a6d5baa1d77075c971f6d8b6dc61b979fc913dc5fe7cc1318281793436ed_low/playlist.m3u8', 'is_default': false, 'id': 4, 'type': 'hls', 'label': 'low spec'}]} if len ( html ) >= 1 : break log . w ( 'the live show is currently offline.' ) sleep ( 1 ) #this is mainly for testing the m3u ffmpeg parser so i would ignore any non-m3u ones stream_url = [ i [ 'url' ] for i in html [ 'streaming_url_list' ] if i [ 'is_default' ] and i [ 'type' ] == 'hls' ] [ 0 ] assert stream_url #title title = '' profile_api = 'https://www.showroom-live.com/api/room/profile?room_id={room_id}' . format ( room_id = room_id ) html = loads ( get_content ( profile_api ) ) try : title = html [ 'main_name' ] except keyerror : title = 'showroom_{room_id}' . format ( room_id = room_id ) type_ , ext , size = url_info ( stream_url ) print_info ( site_info , title , type_ , size ) if not info_only : download_url_ffmpeg ( url = stream_url , title = title , ext = 'mp4' , output_dir = output_dir )"
48,soimort/you-get,src/you_get/extractors/wanmen.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/wanmen.py#L18-L25,"def _wanmen_get_title_by_json_topic_part(json_content, tIndex, pIndex):
    """"""JSON, int, int, int->str
    
    Get a proper title with courseid+topicID+partID.""""""

    return '_'.join([json_content[0]['name'],
                    json_content[0]['Topics'][tIndex]['name'],
                    json_content[0]['Topics'][tIndex]['Parts'][pIndex]['name']])","['def', '_wanmen_get_title_by_json_topic_part', '(', 'json_content', ',', 'tIndex', ',', 'pIndex', ')', ':', 'return', ""'_'"", '.', 'join', '(', '[', 'json_content', '[', '0', ']', '[', ""'name'"", ']', ',', 'json_content', '[', '0', ']', '[', ""'Topics'"", ']', '[', 'tIndex', ']', '[', ""'name'"", ']', ',', 'json_content', '[', '0', ']', '[', ""'Topics'"", ']', '[', 'tIndex', ']', '[', ""'Parts'"", ']', '[', 'pIndex', ']', '[', ""'name'"", ']', ']', ')']","JSON, int, int, int->str
    
    Get a proper title with courseid+topicID+partID.","['JSON', 'int', 'int', 'int', '-', '>', 'str', 'Get', 'a', 'proper', 'title', 'with', 'courseid', '+', 'topicID', '+', 'partID', '.']",python,test,"['json', 'int', 'int', 'int', '-', '>', 'str', 'get', 'a', 'proper', 'title', 'with', 'courseid', '+', 'topicid', '+', 'partid', '.']",json int int int - > str get a proper title with courseid + topicid + partid .,"['def', '_wanmen_get_title_by_json_topic_part', '(', 'json_content', ',', 'tindex', ',', 'pindex', ')', ':', 'return', ""'_'"", '.', 'join', '(', '[', 'json_content', '[', '0', ']', '[', ""'name'"", ']', ',', 'json_content', '[', '0', ']', '[', ""'topics'"", ']', '[', 'tindex', ']', '[', ""'name'"", ']', ',', 'json_content', '[', '0', ']', '[', ""'topics'"", ']', '[', 'tindex', ']', '[', ""'parts'"", ']', '[', 'pindex', ']', '[', ""'name'"", ']', ']', ')']","def _wanmen_get_title_by_json_topic_part ( json_content , tindex , pindex ) : return '_' . join ( [ json_content [ 0 ] [ 'name' ] , json_content [ 0 ] [ 'topics' ] [ tindex ] [ 'name' ] , json_content [ 0 ] [ 'topics' ] [ tindex ] [ 'parts' ] [ pindex ] [ 'name' ] ] )"
49,soimort/you-get,src/you_get/extractors/wanmen.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/wanmen.py#L37-L51,"def wanmen_download_by_course(json_api_content, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""int->None
    
    Download a WHOLE course.
    Reuse the API call to save time.""""""

    for tIndex in range(len(json_api_content[0]['Topics'])):
        for pIndex in range(len(json_api_content[0]['Topics'][tIndex]['Parts'])):
            wanmen_download_by_course_topic_part(json_api_content,
                                                 tIndex,
                                                 pIndex,
                                                 output_dir=output_dir,
                                                 merge=merge,
                                                 info_only=info_only,
                                                 **kwargs)","['def', 'wanmen_download_by_course', '(', 'json_api_content', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'True', ',', 'info_only', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', 'for', 'tIndex', 'in', 'range', '(', 'len', '(', 'json_api_content', '[', '0', ']', '[', ""'Topics'"", ']', ')', ')', ':', 'for', 'pIndex', 'in', 'range', '(', 'len', '(', 'json_api_content', '[', '0', ']', '[', ""'Topics'"", ']', '[', 'tIndex', ']', '[', ""'Parts'"", ']', ')', ')', ':', 'wanmen_download_by_course_topic_part', '(', 'json_api_content', ',', 'tIndex', ',', 'pIndex', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ',', '*', '*', 'kwargs', ')']","int->None
    
    Download a WHOLE course.
    Reuse the API call to save time.","['int', '-', '>', 'None', 'Download', 'a', 'WHOLE', 'course', '.', 'Reuse', 'the', 'API', 'call', 'to', 'save', 'time', '.']",python,test,"['int', '-', '>', 'none', 'download', 'a', 'whole', 'course', '.', 'reuse', 'the', 'api', 'call', 'to', 'save', 'time', '.']",int - > none download a whole course . reuse the api call to save time .,"['def', 'wanmen_download_by_course', '(', 'json_api_content', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'true', ',', 'info_only', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', 'for', 'tindex', 'in', 'range', '(', 'len', '(', 'json_api_content', '[', '0', ']', '[', ""'topics'"", ']', ')', ')', ':', 'for', 'pindex', 'in', 'range', '(', 'len', '(', 'json_api_content', '[', '0', ']', '[', ""'topics'"", ']', '[', 'tindex', ']', '[', ""'parts'"", ']', ')', ')', ':', 'wanmen_download_by_course_topic_part', '(', 'json_api_content', ',', 'tindex', ',', 'pindex', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ',', '*', '*', 'kwargs', ')']","def wanmen_download_by_course ( json_api_content , output_dir = '.' , merge = true , info_only = false , * * kwargs ) : for tindex in range ( len ( json_api_content [ 0 ] [ 'topics' ] ) ) : for pindex in range ( len ( json_api_content [ 0 ] [ 'topics' ] [ tindex ] [ 'parts' ] ) ) : wanmen_download_by_course_topic_part ( json_api_content , tindex , pindex , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs )"
50,soimort/you-get,src/you_get/extractors/wanmen.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/wanmen.py#L69-L84,"def wanmen_download_by_course_topic_part(json_api_content, tIndex, pIndex, output_dir='.', merge=True, info_only=False, **kwargs):
    """"""int, int, int->None
    
    Download ONE PART of the course.""""""

    html = json_api_content

    title = _wanmen_get_title_by_json_topic_part(html, 
                                                  tIndex, 
                                                  pIndex)

    bokeccID = _wanmen_get_boke_id_by_json_topic_part(html,
                                                      tIndex, 
                                                     pIndex)

    bokecc_download_by_id(vid = bokeccID, title = title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)","['def', 'wanmen_download_by_course_topic_part', '(', 'json_api_content', ',', 'tIndex', ',', 'pIndex', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'True', ',', 'info_only', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', 'html', '=', 'json_api_content', 'title', '=', '_wanmen_get_title_by_json_topic_part', '(', 'html', ',', 'tIndex', ',', 'pIndex', ')', 'bokeccID', '=', '_wanmen_get_boke_id_by_json_topic_part', '(', 'html', ',', 'tIndex', ',', 'pIndex', ')', 'bokecc_download_by_id', '(', 'vid', '=', 'bokeccID', ',', 'title', '=', 'title', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ',', '*', '*', 'kwargs', ')']","int, int, int->None
    
    Download ONE PART of the course.","['int', 'int', 'int', '-', '>', 'None', 'Download', 'ONE', 'PART', 'of', 'the', 'course', '.']",python,test,"['int', 'int', 'int', '-', '>', 'none', 'download', 'one', 'part', 'of', 'the', 'course', '.']",int int int - > none download one part of the course .,"['def', 'wanmen_download_by_course_topic_part', '(', 'json_api_content', ',', 'tindex', ',', 'pindex', ',', 'output_dir', '=', ""'.'"", ',', 'merge', '=', 'true', ',', 'info_only', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', 'html', '=', 'json_api_content', 'title', '=', '_wanmen_get_title_by_json_topic_part', '(', 'html', ',', 'tindex', ',', 'pindex', ')', 'bokeccid', '=', '_wanmen_get_boke_id_by_json_topic_part', '(', 'html', ',', 'tindex', ',', 'pindex', ')', 'bokecc_download_by_id', '(', 'vid', '=', 'bokeccid', ',', 'title', '=', 'title', ',', 'output_dir', '=', 'output_dir', ',', 'merge', '=', 'merge', ',', 'info_only', '=', 'info_only', ',', '*', '*', 'kwargs', ')']","def wanmen_download_by_course_topic_part ( json_api_content , tindex , pindex , output_dir = '.' , merge = true , info_only = false , * * kwargs ) : html = json_api_content title = _wanmen_get_title_by_json_topic_part ( html , tindex , pindex ) bokeccid = _wanmen_get_boke_id_by_json_topic_part ( html , tindex , pindex ) bokecc_download_by_id ( vid = bokeccid , title = title , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs )"
51,soimort/you-get,src/you_get/extractors/bigthink.py,https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/bigthink.py#L22-L49,"def get_streams_by_id(account_number, video_id):
        """"""
        int, int->list
        
        Get the height of the videos.
        
        Since brightcove is using 3 kinds of links: rtmp, http and https,
        we will be using the HTTPS one to make it secure.
        
        If somehow akamaihd.net is blocked by the Great Fucking Wall,
        change the ""startswith https"" to http.
        """"""
        endpoint = 'https://edge.api.brightcove.com/playback/v1/accounts/{account_number}/videos/{video_id}'.format(account_number = account_number, video_id = video_id)
        fake_header_id = fake_headers
        #is this somehow related to the time? Magic....
        fake_header_id['Accept'] ='application/json;pk=BCpkADawqM1cc6wmJQC2tvoXZt4mrB7bFfi6zGt9QnOzprPZcGLE9OMGJwspQwKfuFYuCjAAJ53JdjI8zGFx1ll4rxhYJ255AXH1BQ10rnm34weknpfG-sippyQ'

        html = get_content(endpoint, headers= fake_header_id)
        html_json = json.loads(html)

        link_list = []

        for i in html_json['sources']:
            if 'src' in i:  #to avoid KeyError
                if i['src'].startswith('https'):
                    link_list.append((str(i['height']), i['src']))

        return link_list","['def', 'get_streams_by_id', '(', 'account_number', ',', 'video_id', ')', ':', 'endpoint', '=', ""'https://edge.api.brightcove.com/playback/v1/accounts/{account_number}/videos/{video_id}'"", '.', 'format', '(', 'account_number', '=', 'account_number', ',', 'video_id', '=', 'video_id', ')', 'fake_header_id', '=', 'fake_headers', '#is this somehow related to the time? Magic....', 'fake_header_id', '[', ""'Accept'"", ']', '=', ""'application/json;pk=BCpkADawqM1cc6wmJQC2tvoXZt4mrB7bFfi6zGt9QnOzprPZcGLE9OMGJwspQwKfuFYuCjAAJ53JdjI8zGFx1ll4rxhYJ255AXH1BQ10rnm34weknpfG-sippyQ'"", 'html', '=', 'get_content', '(', 'endpoint', ',', 'headers', '=', 'fake_header_id', ')', 'html_json', '=', 'json', '.', 'loads', '(', 'html', ')', 'link_list', '=', '[', ']', 'for', 'i', 'in', 'html_json', '[', ""'sources'"", ']', ':', 'if', ""'src'"", 'in', 'i', ':', '#to avoid KeyError', 'if', 'i', '[', ""'src'"", ']', '.', 'startswith', '(', ""'https'"", ')', ':', 'link_list', '.', 'append', '(', '(', 'str', '(', 'i', '[', ""'height'"", ']', ')', ',', 'i', '[', ""'src'"", ']', ')', ')', 'return', 'link_list']","int, int->list
        
        Get the height of the videos.
        
        Since brightcove is using 3 kinds of links: rtmp, http and https,
        we will be using the HTTPS one to make it secure.
        
        If somehow akamaihd.net is blocked by the Great Fucking Wall,
        change the ""startswith https"" to http.","['int', 'int', '-', '>', 'list', 'Get', 'the', 'height', 'of', 'the', 'videos', '.', 'Since', 'brightcove', 'is', 'using', '3', 'kinds', 'of', 'links', ':', 'rtmp', 'http', 'and', 'https', 'we', 'will', 'be', 'using', 'the', 'HTTPS', 'one', 'to', 'make', 'it', 'secure', '.', 'If', 'somehow', 'akamaihd', '.', 'net', 'is', 'blocked', 'by', 'the', 'Great', 'Fucking', 'Wall', 'change', 'the', 'startswith', 'https', 'to', 'http', '.']",python,test,"['int', 'int', '-', '>', 'list', 'get', 'the', 'height', 'of', 'the', 'videos', '.', 'since', 'brightcove', 'is', 'using', '3', 'kinds', 'of', 'links', ':', 'rtmp', 'http', 'and', 'https', 'we', 'will', 'be', 'using', 'the', 'https', 'one', 'to', 'make', 'it', 'secure', '.', 'if', 'somehow', 'akamaihd', '.', 'net', 'is', 'blocked', 'by', 'the', 'great', 'fucking', 'wall', 'change', 'the', 'startswith', 'https', 'to', 'http', '.']",int int - > list get the height of the videos . since brightcove is using 3 kinds of links : rtmp http and https we will be using the https one to make it secure . if somehow akamaihd . net is blocked by the great fucking wall change the startswith https to http .,"['def', 'get_streams_by_id', '(', 'account_number', ',', 'video_id', ')', ':', 'endpoint', '=', ""'https://edge.api.brightcove.com/playback/v1/accounts/{account_number}/videos/{video_id}'"", '.', 'format', '(', 'account_number', '=', 'account_number', ',', 'video_id', '=', 'video_id', ')', 'fake_header_id', '=', 'fake_headers', '#is this somehow related to the time? magic....', 'fake_header_id', '[', ""'accept'"", ']', '=', ""'application/json;pk=bcpkadawqm1cc6wmjqc2tvoxzt4mrb7bffi6zgt9qnozprpzcgle9omgjwspqwkfufyucjaaj53jdji8zgfx1ll4rxhyj255axh1bq10rnm34weknpfg-sippyq'"", 'html', '=', 'get_content', '(', 'endpoint', ',', 'headers', '=', 'fake_header_id', ')', 'html_json', '=', 'json', '.', 'loads', '(', 'html', ')', 'link_list', '=', '[', ']', 'for', 'i', 'in', 'html_json', '[', ""'sources'"", ']', ':', 'if', ""'src'"", 'in', 'i', ':', '#to avoid keyerror', 'if', 'i', '[', ""'src'"", ']', '.', 'startswith', '(', ""'https'"", ')', ':', 'link_list', '.', 'append', '(', '(', 'str', '(', 'i', '[', ""'height'"", ']', ')', ',', 'i', '[', ""'src'"", ']', ')', ')', 'return', 'link_list']","def get_streams_by_id ( account_number , video_id ) : endpoint = 'https://edge.api.brightcove.com/playback/v1/accounts/{account_number}/videos/{video_id}' . format ( account_number = account_number , video_id = video_id ) fake_header_id = fake_headers #is this somehow related to the time? magic.... fake_header_id [ 'accept' ] = 'application/json;pk=bcpkadawqm1cc6wmjqc2tvoxzt4mrb7bffi6zgt9qnozprpzcgle9omgjwspqwkfufyucjaaj53jdji8zgfx1ll4rxhyj255axh1bq10rnm34weknpfg-sippyq' html = get_content ( endpoint , headers = fake_header_id ) html_json = json . loads ( html ) link_list = [ ] for i in html_json [ 'sources' ] : if 'src' in i : #to avoid keyerror if i [ 'src' ] . startswith ( 'https' ) : link_list . append ( ( str ( i [ 'height' ] ) , i [ 'src' ] ) ) return link_list"
52,apache/airflow,airflow/executors/base_executor.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/base_executor.py#L97-L105,"def has_task(self, task_instance):
        """"""
        Checks if a task is either queued or running in this executor

        :param task_instance: TaskInstance
        :return: True if the task is known to this executor
        """"""
        if task_instance.key in self.queued_tasks or task_instance.key in self.running:
            return True","['def', 'has_task', '(', 'self', ',', 'task_instance', ')', ':', 'if', 'task_instance', '.', 'key', 'in', 'self', '.', 'queued_tasks', 'or', 'task_instance', '.', 'key', 'in', 'self', '.', 'running', ':', 'return', 'True']","Checks if a task is either queued or running in this executor

        :param task_instance: TaskInstance
        :return: True if the task is known to this executor","['Checks', 'if', 'a', 'task', 'is', 'either', 'queued', 'or', 'running', 'in', 'this', 'executor']",python,test,"['checks', 'if', 'a', 'task', 'is', 'either', 'queued', 'or', 'running', 'in', 'this', 'executor']",checks if a task is either queued or running in this executor,"['def', 'has_task', '(', 'self', ',', 'task_instance', ')', ':', 'if', 'task_instance', '.', 'key', 'in', 'self', '.', 'queued_tasks', 'or', 'task_instance', '.', 'key', 'in', 'self', '.', 'running', ':', 'return', 'true']","def has_task ( self , task_instance ) : if task_instance . key in self . queued_tasks or task_instance . key in self . running : return true"
53,apache/airflow,airflow/executors/base_executor.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/base_executor.py#L160-L179,"def get_event_buffer(self, dag_ids=None):
        """"""
        Returns and flush the event buffer. In case dag_ids is specified
        it will only return and flush events for the given dag_ids. Otherwise
        it returns and flushes all

        :param dag_ids: to dag_ids to return events for, if None returns all
        :return: a dict of events
        """"""
        cleared_events = dict()
        if dag_ids is None:
            cleared_events = self.event_buffer
            self.event_buffer = dict()
        else:
            for key in list(self.event_buffer.keys()):
                dag_id, _, _, _ = key
                if dag_id in dag_ids:
                    cleared_events[key] = self.event_buffer.pop(key)

        return cleared_events","['def', 'get_event_buffer', '(', 'self', ',', 'dag_ids', '=', 'None', ')', ':', 'cleared_events', '=', 'dict', '(', ')', 'if', 'dag_ids', 'is', 'None', ':', 'cleared_events', '=', 'self', '.', 'event_buffer', 'self', '.', 'event_buffer', '=', 'dict', '(', ')', 'else', ':', 'for', 'key', 'in', 'list', '(', 'self', '.', 'event_buffer', '.', 'keys', '(', ')', ')', ':', 'dag_id', ',', '_', ',', '_', ',', '_', '=', 'key', 'if', 'dag_id', 'in', 'dag_ids', ':', 'cleared_events', '[', 'key', ']', '=', 'self', '.', 'event_buffer', '.', 'pop', '(', 'key', ')', 'return', 'cleared_events']","Returns and flush the event buffer. In case dag_ids is specified
        it will only return and flush events for the given dag_ids. Otherwise
        it returns and flushes all

        :param dag_ids: to dag_ids to return events for, if None returns all
        :return: a dict of events","['Returns', 'and', 'flush', 'the', 'event', 'buffer', '.', 'In', 'case', 'dag_ids', 'is', 'specified', 'it', 'will', 'only', 'return', 'and', 'flush', 'events', 'for', 'the', 'given', 'dag_ids', '.', 'Otherwise', 'it', 'returns', 'and', 'flushes', 'all']",python,test,"['returns', 'and', 'flush', 'the', 'event', 'buffer', '.', 'in', 'case', 'dag_ids', 'is', 'specified', 'it', 'will', 'only', 'return', 'and', 'flush', 'events', 'for', 'the', 'given', 'dag_ids', '.', 'otherwise', 'it', 'returns', 'and', 'flushes', 'all']",returns and flush the event buffer . in case dag_ids is specified it will only return and flush events for the given dag_ids . otherwise it returns and flushes all,"['def', 'get_event_buffer', '(', 'self', ',', 'dag_ids', '=', 'none', ')', ':', 'cleared_events', '=', 'dict', '(', ')', 'if', 'dag_ids', 'is', 'none', ':', 'cleared_events', '=', 'self', '.', 'event_buffer', 'self', '.', 'event_buffer', '=', 'dict', '(', ')', 'else', ':', 'for', 'key', 'in', 'list', '(', 'self', '.', 'event_buffer', '.', 'keys', '(', ')', ')', ':', 'dag_id', ',', '_', ',', '_', ',', '_', '=', 'key', 'if', 'dag_id', 'in', 'dag_ids', ':', 'cleared_events', '[', 'key', ']', '=', 'self', '.', 'event_buffer', '.', 'pop', '(', 'key', ')', 'return', 'cleared_events']","def get_event_buffer ( self , dag_ids = none ) : cleared_events = dict ( ) if dag_ids is none : cleared_events = self . event_buffer self . event_buffer = dict ( ) else : for key in list ( self . event_buffer . keys ( ) ) : dag_id , _ , _ , _ = key if dag_id in dag_ids : cleared_events [ key ] = self . event_buffer . pop ( key ) return cleared_events"
54,apache/airflow,airflow/contrib/hooks/snowflake_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/snowflake_hook.py#L47-L96,"def _get_conn_params(self):
        """"""
        one method to fetch connection params as a dict
        used in get_uri() and get_connection()
        """"""
        conn = self.get_connection(self.snowflake_conn_id)
        account = conn.extra_dejson.get('account', None)
        warehouse = conn.extra_dejson.get('warehouse', None)
        database = conn.extra_dejson.get('database', None)
        region = conn.extra_dejson.get(""region"", None)
        role = conn.extra_dejson.get('role', None)

        conn_config = {
            ""user"": conn.login,
            ""password"": conn.password or '',
            ""schema"": conn.schema or '',
            ""database"": self.database or database or '',
            ""account"": self.account or account or '',
            ""warehouse"": self.warehouse or warehouse or '',
            ""region"": self.region or region or '',
            ""role"": self.role or role or '',
        }

        """"""
        If private_key_file is specified in the extra json, load the contents of the file as a private
        key and specify that in the connection configuration. The connection password then becomes the
        passphrase for the private key. If your private key file is not encrypted (not recommended), then
        leave the password empty.
        """"""
        private_key_file = conn.extra_dejson.get('private_key_file', None)
        if private_key_file:
            with open(private_key_file, ""rb"") as key:
                passphrase = None
                if conn.password:
                    passphrase = conn.password.strip().encode()

                p_key = serialization.load_pem_private_key(
                    key.read(),
                    password=passphrase,
                    backend=default_backend()
                )

            pkb = p_key.private_bytes(encoding=serialization.Encoding.DER,
                                      format=serialization.PrivateFormat.PKCS8,
                                      encryption_algorithm=serialization.NoEncryption())

            conn_config['private_key'] = pkb
            conn_config.pop('password', None)

        return conn_config","['def', '_get_conn_params', '(', 'self', ')', ':', 'conn', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'snowflake_conn_id', ')', 'account', '=', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'account'"", ',', 'None', ')', 'warehouse', '=', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'warehouse'"", ',', 'None', ')', 'database', '=', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'database'"", ',', 'None', ')', 'region', '=', 'conn', '.', 'extra_dejson', '.', 'get', '(', '""region""', ',', 'None', ')', 'role', '=', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'role'"", ',', 'None', ')', 'conn_config', '=', '{', '""user""', ':', 'conn', '.', 'login', ',', '""password""', ':', 'conn', '.', 'password', 'or', ""''"", ',', '""schema""', ':', 'conn', '.', 'schema', 'or', ""''"", ',', '""database""', ':', 'self', '.', 'database', 'or', 'database', 'or', ""''"", ',', '""account""', ':', 'self', '.', 'account', 'or', 'account', 'or', ""''"", ',', '""warehouse""', ':', 'self', '.', 'warehouse', 'or', 'warehouse', 'or', ""''"", ',', '""region""', ':', 'self', '.', 'region', 'or', 'region', 'or', ""''"", ',', '""role""', ':', 'self', '.', 'role', 'or', 'role', 'or', ""''"", ',', '}', '""""""\n        If private_key_file is specified in the extra json, load the contents of the file as a private\n        key and specify that in the connection configuration. The connection password then becomes the\n        passphrase for the private key. If your private key file is not encrypted (not recommended), then\n        leave the password empty.\n        """"""', 'private_key_file', '=', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'private_key_file'"", ',', 'None', ')', 'if', 'private_key_file', ':', 'with', 'open', '(', 'private_key_file', ',', '""rb""', ')', 'as', 'key', ':', 'passphrase', '=', 'None', 'if', 'conn', '.', 'password', ':', 'passphrase', '=', 'conn', '.', 'password', '.', 'strip', '(', ')', '.', 'encode', '(', ')', 'p_key', '=', 'serialization', '.', 'load_pem_private_key', '(', 'key', '.', 'read', '(', ')', ',', 'password', '=', 'passphrase', ',', 'backend', '=', 'default_backend', '(', ')', ')', 'pkb', '=', 'p_key', '.', 'private_bytes', '(', 'encoding', '=', 'serialization', '.', 'Encoding', '.', 'DER', ',', 'format', '=', 'serialization', '.', 'PrivateFormat', '.', 'PKCS8', ',', 'encryption_algorithm', '=', 'serialization', '.', 'NoEncryption', '(', ')', ')', 'conn_config', '[', ""'private_key'"", ']', '=', 'pkb', 'conn_config', '.', 'pop', '(', ""'password'"", ',', 'None', ')', 'return', 'conn_config']","one method to fetch connection params as a dict
        used in get_uri() and get_connection()","['one', 'method', 'to', 'fetch', 'connection', 'params', 'as', 'a', 'dict', 'used', 'in', 'get_uri', '()', 'and', 'get_connection', '()']",python,test,"['one', 'method', 'to', 'fetch', 'connection', 'params', 'as', 'a', 'dict', 'used', 'in', 'get_uri', '()', 'and', 'get_connection', '()']",one method to fetch connection params as a dict used in get_uri () and get_connection (),"['def', '_get_conn_params', '(', 'self', ')', ':', 'conn', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'snowflake_conn_id', ')', 'account', '=', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'account'"", ',', 'none', ')', 'warehouse', '=', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'warehouse'"", ',', 'none', ')', 'database', '=', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'database'"", ',', 'none', ')', 'region', '=', 'conn', '.', 'extra_dejson', '.', 'get', '(', '""region""', ',', 'none', ')', 'role', '=', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'role'"", ',', 'none', ')', 'conn_config', '=', '{', '""user""', ':', 'conn', '.', 'login', ',', '""password""', ':', 'conn', '.', 'password', 'or', ""''"", ',', '""schema""', ':', 'conn', '.', 'schema', 'or', ""''"", ',', '""database""', ':', 'self', '.', 'database', 'or', 'database', 'or', ""''"", ',', '""account""', ':', 'self', '.', 'account', 'or', 'account', 'or', ""''"", ',', '""warehouse""', ':', 'self', '.', 'warehouse', 'or', 'warehouse', 'or', ""''"", ',', '""region""', ':', 'self', '.', 'region', 'or', 'region', 'or', ""''"", ',', '""role""', ':', 'self', '.', 'role', 'or', 'role', 'or', ""''"", ',', '}', '""""""\n        if private_key_file is specified in the extra json, load the contents of the file as a private\n        key and specify that in the connection configuration. the connection password then becomes the\n        passphrase for the private key. if your private key file is not encrypted (not recommended), then\n        leave the password empty.\n        """"""', 'private_key_file', '=', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'private_key_file'"", ',', 'none', ')', 'if', 'private_key_file', ':', 'with', 'open', '(', 'private_key_file', ',', '""rb""', ')', 'as', 'key', ':', 'passphrase', '=', 'none', 'if', 'conn', '.', 'password', ':', 'passphrase', '=', 'conn', '.', 'password', '.', 'strip', '(', ')', '.', 'encode', '(', ')', 'p_key', '=', 'serialization', '.', 'load_pem_private_key', '(', 'key', '.', 'read', '(', ')', ',', 'password', '=', 'passphrase', ',', 'backend', '=', 'default_backend', '(', ')', ')', 'pkb', '=', 'p_key', '.', 'private_bytes', '(', 'encoding', '=', 'serialization', '.', 'encoding', '.', 'der', ',', 'format', '=', 'serialization', '.', 'privateformat', '.', 'pkcs8', ',', 'encryption_algorithm', '=', 'serialization', '.', 'noencryption', '(', ')', ')', 'conn_config', '[', ""'private_key'"", ']', '=', 'pkb', 'conn_config', '.', 'pop', '(', ""'password'"", ',', 'none', ')', 'return', 'conn_config']","def _get_conn_params ( self ) : conn = self . get_connection ( self . snowflake_conn_id ) account = conn . extra_dejson . get ( 'account' , none ) warehouse = conn . extra_dejson . get ( 'warehouse' , none ) database = conn . extra_dejson . get ( 'database' , none ) region = conn . extra_dejson . get ( ""region"" , none ) role = conn . extra_dejson . get ( 'role' , none ) conn_config = { ""user"" : conn . login , ""password"" : conn . password or '' , ""schema"" : conn . schema or '' , ""database"" : self . database or database or '' , ""account"" : self . account or account or '' , ""warehouse"" : self . warehouse or warehouse or '' , ""region"" : self . region or region or '' , ""role"" : self . role or role or '' , } """"""
        if private_key_file is specified in the extra json, load the contents of the file as a private
        key and specify that in the connection configuration. the connection password then becomes the
        passphrase for the private key. if your private key file is not encrypted (not recommended), then
        leave the password empty.
        """""" private_key_file = conn . extra_dejson . get ( 'private_key_file' , none ) if private_key_file : with open ( private_key_file , ""rb"" ) as key : passphrase = none if conn . password : passphrase = conn . password . strip ( ) . encode ( ) p_key = serialization . load_pem_private_key ( key . read ( ) , password = passphrase , backend = default_backend ( ) ) pkb = p_key . private_bytes ( encoding = serialization . encoding . der , format = serialization . privateformat . pkcs8 , encryption_algorithm = serialization . noencryption ( ) ) conn_config [ 'private_key' ] = pkb conn_config . pop ( 'password' , none ) return conn_config"
55,apache/airflow,airflow/contrib/hooks/snowflake_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/snowflake_hook.py#L98-L105,"def get_uri(self):
        """"""
        override DbApiHook get_uri method for get_sqlalchemy_engine()
        """"""
        conn_config = self._get_conn_params()
        uri = 'snowflake://{user}:{password}@{account}/{database}/'
        uri += '{schema}?warehouse={warehouse}&role={role}'
        return uri.format(**conn_config)","['def', 'get_uri', '(', 'self', ')', ':', 'conn_config', '=', 'self', '.', '_get_conn_params', '(', ')', 'uri', '=', ""'snowflake://{user}:{password}@{account}/{database}/'"", 'uri', '+=', ""'{schema}?warehouse={warehouse}&role={role}'"", 'return', 'uri', '.', 'format', '(', '*', '*', 'conn_config', ')']",override DbApiHook get_uri method for get_sqlalchemy_engine(),"['override', 'DbApiHook', 'get_uri', 'method', 'for', 'get_sqlalchemy_engine', '()']",python,test,"['override', 'dbapihook', 'get_uri', 'method', 'for', 'get_sqlalchemy_engine', '()']",override dbapihook get_uri method for get_sqlalchemy_engine (),"['def', 'get_uri', '(', 'self', ')', ':', 'conn_config', '=', 'self', '.', '_get_conn_params', '(', ')', 'uri', '=', ""'snowflake://{user}:{password}@{account}/{database}/'"", 'uri', '+=', ""'{schema}?warehouse={warehouse}&role={role}'"", 'return', 'uri', '.', 'format', '(', '*', '*', 'conn_config', ')']",def get_uri ( self ) : conn_config = self . _get_conn_params ( ) uri = 'snowflake://{user}:{password}@{account}/{database}/' uri += '{schema}?warehouse={warehouse}&role={role}' return uri . format ( * * conn_config )
56,apache/airflow,airflow/contrib/hooks/snowflake_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/snowflake_hook.py#L107-L113,"def get_conn(self):
        """"""
        Returns a snowflake.connection object
        """"""
        conn_config = self._get_conn_params()
        conn = snowflake.connector.connect(**conn_config)
        return conn","['def', 'get_conn', '(', 'self', ')', ':', 'conn_config', '=', 'self', '.', '_get_conn_params', '(', ')', 'conn', '=', 'snowflake', '.', 'connector', '.', 'connect', '(', '*', '*', 'conn_config', ')', 'return', 'conn']",Returns a snowflake.connection object,"['Returns', 'a', 'snowflake', '.', 'connection', 'object']",python,test,"['returns', 'a', 'snowflake', '.', 'connection', 'object']",returns a snowflake . connection object,"['def', 'get_conn', '(', 'self', ')', ':', 'conn_config', '=', 'self', '.', '_get_conn_params', '(', ')', 'conn', '=', 'snowflake', '.', 'connector', '.', 'connect', '(', '*', '*', 'conn_config', ')', 'return', 'conn']",def get_conn ( self ) : conn_config = self . _get_conn_params ( ) conn = snowflake . connector . connect ( * * conn_config ) return conn
57,apache/airflow,airflow/contrib/hooks/snowflake_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/snowflake_hook.py#L115-L129,"def _get_aws_credentials(self):
        """"""
        returns aws_access_key_id, aws_secret_access_key
        from extra

        intended to be used by external import and export statements
        """"""
        if self.snowflake_conn_id:
            connection_object = self.get_connection(self.snowflake_conn_id)
            if 'aws_secret_access_key' in connection_object.extra_dejson:
                aws_access_key_id = connection_object.extra_dejson.get(
                    'aws_access_key_id')
                aws_secret_access_key = connection_object.extra_dejson.get(
                    'aws_secret_access_key')
        return aws_access_key_id, aws_secret_access_key","['def', '_get_aws_credentials', '(', 'self', ')', ':', 'if', 'self', '.', 'snowflake_conn_id', ':', 'connection_object', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'snowflake_conn_id', ')', 'if', ""'aws_secret_access_key'"", 'in', 'connection_object', '.', 'extra_dejson', ':', 'aws_access_key_id', '=', 'connection_object', '.', 'extra_dejson', '.', 'get', '(', ""'aws_access_key_id'"", ')', 'aws_secret_access_key', '=', 'connection_object', '.', 'extra_dejson', '.', 'get', '(', ""'aws_secret_access_key'"", ')', 'return', 'aws_access_key_id', ',', 'aws_secret_access_key']","returns aws_access_key_id, aws_secret_access_key
        from extra

        intended to be used by external import and export statements","['returns', 'aws_access_key_id', 'aws_secret_access_key', 'from', 'extra']",python,test,"['returns', 'aws_access_key_id', 'aws_secret_access_key', 'from', 'extra']",returns aws_access_key_id aws_secret_access_key from extra,"['def', '_get_aws_credentials', '(', 'self', ')', ':', 'if', 'self', '.', 'snowflake_conn_id', ':', 'connection_object', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'snowflake_conn_id', ')', 'if', ""'aws_secret_access_key'"", 'in', 'connection_object', '.', 'extra_dejson', ':', 'aws_access_key_id', '=', 'connection_object', '.', 'extra_dejson', '.', 'get', '(', ""'aws_access_key_id'"", ')', 'aws_secret_access_key', '=', 'connection_object', '.', 'extra_dejson', '.', 'get', '(', ""'aws_secret_access_key'"", ')', 'return', 'aws_access_key_id', ',', 'aws_secret_access_key']","def _get_aws_credentials ( self ) : if self . snowflake_conn_id : connection_object = self . get_connection ( self . snowflake_conn_id ) if 'aws_secret_access_key' in connection_object . extra_dejson : aws_access_key_id = connection_object . extra_dejson . get ( 'aws_access_key_id' ) aws_secret_access_key = connection_object . extra_dejson . get ( 'aws_secret_access_key' ) return aws_access_key_id , aws_secret_access_key"
58,apache/airflow,airflow/contrib/hooks/grpc_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/grpc_hook.py#L112-L123,"def _get_field(self, field_name, default=None):
        """"""
        Fetches a field from extras, and returns it. This is some Airflow
        magic. The grpc hook type adds custom UI elements
        to the hook page, which allow admins to specify scopes, credential pem files, etc.
        They get formatted as shown below.
        """"""
        full_field_name = 'extra__grpc__{}'.format(field_name)
        if full_field_name in self.extras:
            return self.extras[full_field_name]
        else:
            return default","['def', '_get_field', '(', 'self', ',', 'field_name', ',', 'default', '=', 'None', ')', ':', 'full_field_name', '=', ""'extra__grpc__{}'"", '.', 'format', '(', 'field_name', ')', 'if', 'full_field_name', 'in', 'self', '.', 'extras', ':', 'return', 'self', '.', 'extras', '[', 'full_field_name', ']', 'else', ':', 'return', 'default']","Fetches a field from extras, and returns it. This is some Airflow
        magic. The grpc hook type adds custom UI elements
        to the hook page, which allow admins to specify scopes, credential pem files, etc.
        They get formatted as shown below.","['Fetches', 'a', 'field', 'from', 'extras', 'and', 'returns', 'it', '.', 'This', 'is', 'some', 'Airflow', 'magic', '.', 'The', 'grpc', 'hook', 'type', 'adds', 'custom', 'UI', 'elements', 'to', 'the', 'hook', 'page', 'which', 'allow', 'admins', 'to', 'specify', 'scopes', 'credential', 'pem', 'files', 'etc', '.', 'They', 'get', 'formatted', 'as', 'shown', 'below', '.']",python,test,"['fetches', 'a', 'field', 'from', 'extras', 'and', 'returns', 'it', '.', 'this', 'is', 'some', 'airflow', 'magic', '.', 'the', 'grpc', 'hook', 'type', 'adds', 'custom', 'ui', 'elements', 'to', 'the', 'hook', 'page', 'which', 'allow', 'admins', 'to', 'specify', 'scopes', 'credential', 'pem', 'files', 'etc', '.', 'they', 'get', 'formatted', 'as', 'shown', 'below', '.']",fetches a field from extras and returns it . this is some airflow magic . the grpc hook type adds custom ui elements to the hook page which allow admins to specify scopes credential pem files etc . they get formatted as shown below .,"['def', '_get_field', '(', 'self', ',', 'field_name', ',', 'default', '=', 'none', ')', ':', 'full_field_name', '=', ""'extra__grpc__{}'"", '.', 'format', '(', 'field_name', ')', 'if', 'full_field_name', 'in', 'self', '.', 'extras', ':', 'return', 'self', '.', 'extras', '[', 'full_field_name', ']', 'else', ':', 'return', 'default']","def _get_field ( self , field_name , default = none ) : full_field_name = 'extra__grpc__{}' . format ( field_name ) if full_field_name in self . extras : return self . extras [ full_field_name ] else : return default"
59,apache/airflow,airflow/hooks/postgres_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/postgres_hook.py#L63-L83,"def copy_expert(self, sql, filename, open=open):
        """"""
        Executes SQL using psycopg2 copy_expert method.
        Necessary to execute COPY command without access to a superuser.

        Note: if this method is called with a ""COPY FROM"" statement and
        the specified input file does not exist, it creates an empty
        file and no data is loaded, but the operation succeeds.
        So if users want to be aware when the input file does not exist,
        they have to check its existence by themselves.
        """"""
        if not os.path.isfile(filename):
            with open(filename, 'w'):
                pass

        with open(filename, 'r+') as f:
            with closing(self.get_conn()) as conn:
                with closing(conn.cursor()) as cur:
                    cur.copy_expert(sql, f)
                    f.truncate(f.tell())
                    conn.commit()","['def', 'copy_expert', '(', 'self', ',', 'sql', ',', 'filename', ',', 'open', '=', 'open', ')', ':', 'if', 'not', 'os', '.', 'path', '.', 'isfile', '(', 'filename', ')', ':', 'with', 'open', '(', 'filename', ',', ""'w'"", ')', ':', 'pass', 'with', 'open', '(', 'filename', ',', ""'r+'"", ')', 'as', 'f', ':', 'with', 'closing', '(', 'self', '.', 'get_conn', '(', ')', ')', 'as', 'conn', ':', 'with', 'closing', '(', 'conn', '.', 'cursor', '(', ')', ')', 'as', 'cur', ':', 'cur', '.', 'copy_expert', '(', 'sql', ',', 'f', ')', 'f', '.', 'truncate', '(', 'f', '.', 'tell', '(', ')', ')', 'conn', '.', 'commit', '(', ')']","Executes SQL using psycopg2 copy_expert method.
        Necessary to execute COPY command without access to a superuser.

        Note: if this method is called with a ""COPY FROM"" statement and
        the specified input file does not exist, it creates an empty
        file and no data is loaded, but the operation succeeds.
        So if users want to be aware when the input file does not exist,
        they have to check its existence by themselves.","['Executes', 'SQL', 'using', 'psycopg2', 'copy_expert', 'method', '.', 'Necessary', 'to', 'execute', 'COPY', 'command', 'without', 'access', 'to', 'a', 'superuser', '.']",python,test,"['executes', 'sql', 'using', 'psycopg2', 'copy_expert', 'method', '.', 'necessary', 'to', 'execute', 'copy', 'command', 'without', 'access', 'to', 'a', 'superuser', '.']",executes sql using psycopg2 copy_expert method . necessary to execute copy command without access to a superuser .,"['def', 'copy_expert', '(', 'self', ',', 'sql', ',', 'filename', ',', 'open', '=', 'open', ')', ':', 'if', 'not', 'os', '.', 'path', '.', 'isfile', '(', 'filename', ')', ':', 'with', 'open', '(', 'filename', ',', ""'w'"", ')', ':', 'pass', 'with', 'open', '(', 'filename', ',', ""'r+'"", ')', 'as', 'f', ':', 'with', 'closing', '(', 'self', '.', 'get_conn', '(', ')', ')', 'as', 'conn', ':', 'with', 'closing', '(', 'conn', '.', 'cursor', '(', ')', ')', 'as', 'cur', ':', 'cur', '.', 'copy_expert', '(', 'sql', ',', 'f', ')', 'f', '.', 'truncate', '(', 'f', '.', 'tell', '(', ')', ')', 'conn', '.', 'commit', '(', ')']","def copy_expert ( self , sql , filename , open = open ) : if not os . path . isfile ( filename ) : with open ( filename , 'w' ) : pass with open ( filename , 'r+' ) as f : with closing ( self . get_conn ( ) ) as conn : with closing ( conn . cursor ( ) ) as cur : cur . copy_expert ( sql , f ) f . truncate ( f . tell ( ) ) conn . commit ( )"
60,apache/airflow,airflow/hooks/postgres_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/postgres_hook.py#L85-L89,"def bulk_load(self, table, tmp_file):
        """"""
        Loads a tab-delimited file into a database table
        """"""
        self.copy_expert(""COPY {table} FROM STDIN"".format(table=table), tmp_file)","['def', 'bulk_load', '(', 'self', ',', 'table', ',', 'tmp_file', ')', ':', 'self', '.', 'copy_expert', '(', '""COPY {table} FROM STDIN""', '.', 'format', '(', 'table', '=', 'table', ')', ',', 'tmp_file', ')']",Loads a tab-delimited file into a database table,"['Loads', 'a', 'tab', '-', 'delimited', 'file', 'into', 'a', 'database', 'table']",python,test,"['loads', 'a', 'tab', '-', 'delimited', 'file', 'into', 'a', 'database', 'table']",loads a tab - delimited file into a database table,"['def', 'bulk_load', '(', 'self', ',', 'table', ',', 'tmp_file', ')', ':', 'self', '.', 'copy_expert', '(', '""copy {table} from stdin""', '.', 'format', '(', 'table', '=', 'table', ')', ',', 'tmp_file', ')']","def bulk_load ( self , table , tmp_file ) : self . copy_expert ( ""copy {table} from stdin"" . format ( table = table ) , tmp_file )"
61,apache/airflow,airflow/hooks/postgres_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/postgres_hook.py#L91-L95,"def bulk_dump(self, table, tmp_file):
        """"""
        Dumps a database table into a tab-delimited file
        """"""
        self.copy_expert(""COPY {table} TO STDOUT"".format(table=table), tmp_file)","['def', 'bulk_dump', '(', 'self', ',', 'table', ',', 'tmp_file', ')', ':', 'self', '.', 'copy_expert', '(', '""COPY {table} TO STDOUT""', '.', 'format', '(', 'table', '=', 'table', ')', ',', 'tmp_file', ')']",Dumps a database table into a tab-delimited file,"['Dumps', 'a', 'database', 'table', 'into', 'a', 'tab', '-', 'delimited', 'file']",python,test,"['dumps', 'a', 'database', 'table', 'into', 'a', 'tab', '-', 'delimited', 'file']",dumps a database table into a tab - delimited file,"['def', 'bulk_dump', '(', 'self', ',', 'table', ',', 'tmp_file', ')', ':', 'self', '.', 'copy_expert', '(', '""copy {table} to stdout""', '.', 'format', '(', 'table', '=', 'table', ')', ',', 'tmp_file', ')']","def bulk_dump ( self , table , tmp_file ) : self . copy_expert ( ""copy {table} to stdout"" . format ( table = table ) , tmp_file )"
62,apache/airflow,airflow/contrib/operators/file_to_gcs.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/file_to_gcs.py#L68-L82,"def execute(self, context):
        """"""
        Uploads the file to Google cloud storage
        """"""
        hook = GoogleCloudStorageHook(
            google_cloud_storage_conn_id=self.google_cloud_storage_conn_id,
            delegate_to=self.delegate_to)

        hook.upload(
            bucket_name=self.bucket,
            object_name=self.dst,
            mime_type=self.mime_type,
            filename=self.src,
            gzip=self.gzip,
        )","['def', 'execute', '(', 'self', ',', 'context', ')', ':', 'hook', '=', 'GoogleCloudStorageHook', '(', 'google_cloud_storage_conn_id', '=', 'self', '.', 'google_cloud_storage_conn_id', ',', 'delegate_to', '=', 'self', '.', 'delegate_to', ')', 'hook', '.', 'upload', '(', 'bucket_name', '=', 'self', '.', 'bucket', ',', 'object_name', '=', 'self', '.', 'dst', ',', 'mime_type', '=', 'self', '.', 'mime_type', ',', 'filename', '=', 'self', '.', 'src', ',', 'gzip', '=', 'self', '.', 'gzip', ',', ')']",Uploads the file to Google cloud storage,"['Uploads', 'the', 'file', 'to', 'Google', 'cloud', 'storage']",python,test,"['uploads', 'the', 'file', 'to', 'google', 'cloud', 'storage']",uploads the file to google cloud storage,"['def', 'execute', '(', 'self', ',', 'context', ')', ':', 'hook', '=', 'googlecloudstoragehook', '(', 'google_cloud_storage_conn_id', '=', 'self', '.', 'google_cloud_storage_conn_id', ',', 'delegate_to', '=', 'self', '.', 'delegate_to', ')', 'hook', '.', 'upload', '(', 'bucket_name', '=', 'self', '.', 'bucket', ',', 'object_name', '=', 'self', '.', 'dst', ',', 'mime_type', '=', 'self', '.', 'mime_type', ',', 'filename', '=', 'self', '.', 'src', ',', 'gzip', '=', 'self', '.', 'gzip', ',', ')']","def execute ( self , context ) : hook = googlecloudstoragehook ( google_cloud_storage_conn_id = self . google_cloud_storage_conn_id , delegate_to = self . delegate_to ) hook . upload ( bucket_name = self . bucket , object_name = self . dst , mime_type = self . mime_type , filename = self . src , gzip = self . gzip , )"
63,apache/airflow,airflow/macros/hive.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/macros/hive.py#L23-L55,"def max_partition(
        table, schema=""default"", field=None, filter_map=None,
        metastore_conn_id='metastore_default'):
    """"""
    Gets the max partition for a table.

    :param schema: The hive schema the table lives in
    :type schema: str
    :param table: The hive table you are interested in, supports the dot
        notation as in ""my_database.my_table"", if a dot is found,
        the schema param is disregarded
    :type table: str
    :param metastore_conn_id: The hive connection you are interested in.
        If your default is set you don't need to use this parameter.
    :type metastore_conn_id: str
    :param filter_map: partition_key:partition_value map used for partition filtering,
                       e.g. {'key1': 'value1', 'key2': 'value2'}.
                       Only partitions matching all partition_key:partition_value
                       pairs will be considered as candidates of max partition.
    :type filter_map: map
    :param field: the field to get the max value from. If there's only
        one partition field, this will be inferred
    :type field: str

    >>> max_partition('airflow.static_babynames_partitioned')
    '2015-01-01'
    """"""
    from airflow.hooks.hive_hooks import HiveMetastoreHook
    if '.' in table:
        schema, table = table.split('.')
    hh = HiveMetastoreHook(metastore_conn_id=metastore_conn_id)
    return hh.max_partition(
        schema=schema, table_name=table, field=field, filter_map=filter_map)","['def', 'max_partition', '(', 'table', ',', 'schema', '=', '""default""', ',', 'field', '=', 'None', ',', 'filter_map', '=', 'None', ',', 'metastore_conn_id', '=', ""'metastore_default'"", ')', ':', 'from', 'airflow', '.', 'hooks', '.', 'hive_hooks', 'import', 'HiveMetastoreHook', 'if', ""'.'"", 'in', 'table', ':', 'schema', ',', 'table', '=', 'table', '.', 'split', '(', ""'.'"", ')', 'hh', '=', 'HiveMetastoreHook', '(', 'metastore_conn_id', '=', 'metastore_conn_id', ')', 'return', 'hh', '.', 'max_partition', '(', 'schema', '=', 'schema', ',', 'table_name', '=', 'table', ',', 'field', '=', 'field', ',', 'filter_map', '=', 'filter_map', ')']","Gets the max partition for a table.

    :param schema: The hive schema the table lives in
    :type schema: str
    :param table: The hive table you are interested in, supports the dot
        notation as in ""my_database.my_table"", if a dot is found,
        the schema param is disregarded
    :type table: str
    :param metastore_conn_id: The hive connection you are interested in.
        If your default is set you don't need to use this parameter.
    :type metastore_conn_id: str
    :param filter_map: partition_key:partition_value map used for partition filtering,
                       e.g. {'key1': 'value1', 'key2': 'value2'}.
                       Only partitions matching all partition_key:partition_value
                       pairs will be considered as candidates of max partition.
    :type filter_map: map
    :param field: the field to get the max value from. If there's only
        one partition field, this will be inferred
    :type field: str

    >>> max_partition('airflow.static_babynames_partitioned')
    '2015-01-01'","['Gets', 'the', 'max', 'partition', 'for', 'a', 'table', '.']",python,test,"['gets', 'the', 'max', 'partition', 'for', 'a', 'table', '.']",gets the max partition for a table .,"['def', 'max_partition', '(', 'table', ',', 'schema', '=', '""default""', ',', 'field', '=', 'none', ',', 'filter_map', '=', 'none', ',', 'metastore_conn_id', '=', ""'metastore_default'"", ')', ':', 'from', 'airflow', '.', 'hooks', '.', 'hive_hooks', 'import', 'hivemetastorehook', 'if', ""'.'"", 'in', 'table', ':', 'schema', ',', 'table', '=', 'table', '.', 'split', '(', ""'.'"", ')', 'hh', '=', 'hivemetastorehook', '(', 'metastore_conn_id', '=', 'metastore_conn_id', ')', 'return', 'hh', '.', 'max_partition', '(', 'schema', '=', 'schema', ',', 'table_name', '=', 'table', ',', 'field', '=', 'field', ',', 'filter_map', '=', 'filter_map', ')']","def max_partition ( table , schema = ""default"" , field = none , filter_map = none , metastore_conn_id = 'metastore_default' ) : from airflow . hooks . hive_hooks import hivemetastorehook if '.' in table : schema , table = table . split ( '.' ) hh = hivemetastorehook ( metastore_conn_id = metastore_conn_id ) return hh . max_partition ( schema = schema , table_name = table , field = field , filter_map = filter_map )"
64,apache/airflow,airflow/macros/hive.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/macros/hive.py#L58-L80,"def _closest_date(target_dt, date_list, before_target=None):
    """"""
    This function finds the date in a list closest to the target date.
    An optional parameter can be given to get the closest before or after.

    :param target_dt: The target date
    :type target_dt: datetime.date
    :param date_list: The list of dates to search
    :type date_list: list[datetime.date]
    :param before_target: closest before or after the target
    :type before_target: bool or None
    :returns: The closest date
    :rtype: datetime.date or None
    """"""
    fb = lambda d: target_dt - d if d <= target_dt else datetime.timedelta.max
    fa = lambda d: d - target_dt if d >= target_dt else datetime.timedelta.max
    fnone = lambda d: target_dt - d if d < target_dt else d - target_dt
    if before_target is None:
        return min(date_list, key=fnone).date()
    if before_target:
        return min(date_list, key=fb).date()
    else:
        return min(date_list, key=fa).date()","['def', '_closest_date', '(', 'target_dt', ',', 'date_list', ',', 'before_target', '=', 'None', ')', ':', 'fb', '=', 'lambda', 'd', ':', 'target_dt', '-', 'd', 'if', 'd', '<=', 'target_dt', 'else', 'datetime', '.', 'timedelta', '.', 'max', 'fa', '=', 'lambda', 'd', ':', 'd', '-', 'target_dt', 'if', 'd', '>=', 'target_dt', 'else', 'datetime', '.', 'timedelta', '.', 'max', 'fnone', '=', 'lambda', 'd', ':', 'target_dt', '-', 'd', 'if', 'd', '<', 'target_dt', 'else', 'd', '-', 'target_dt', 'if', 'before_target', 'is', 'None', ':', 'return', 'min', '(', 'date_list', ',', 'key', '=', 'fnone', ')', '.', 'date', '(', ')', 'if', 'before_target', ':', 'return', 'min', '(', 'date_list', ',', 'key', '=', 'fb', ')', '.', 'date', '(', ')', 'else', ':', 'return', 'min', '(', 'date_list', ',', 'key', '=', 'fa', ')', '.', 'date', '(', ')']","This function finds the date in a list closest to the target date.
    An optional parameter can be given to get the closest before or after.

    :param target_dt: The target date
    :type target_dt: datetime.date
    :param date_list: The list of dates to search
    :type date_list: list[datetime.date]
    :param before_target: closest before or after the target
    :type before_target: bool or None
    :returns: The closest date
    :rtype: datetime.date or None","['This', 'function', 'finds', 'the', 'date', 'in', 'a', 'list', 'closest', 'to', 'the', 'target', 'date', '.', 'An', 'optional', 'parameter', 'can', 'be', 'given', 'to', 'get', 'the', 'closest', 'before', 'or', 'after', '.']",python,test,"['this', 'function', 'finds', 'the', 'date', 'in', 'a', 'list', 'closest', 'to', 'the', 'target', 'date', '.', 'an', 'optional', 'parameter', 'can', 'be', 'given', 'to', 'get', 'the', 'closest', 'before', 'or', 'after', '.']",this function finds the date in a list closest to the target date . an optional parameter can be given to get the closest before or after .,"['def', '_closest_date', '(', 'target_dt', ',', 'date_list', ',', 'before_target', '=', 'none', ')', ':', 'fb', '=', 'lambda', 'd', ':', 'target_dt', '-', 'd', 'if', 'd', '<=', 'target_dt', 'else', 'datetime', '.', 'timedelta', '.', 'max', 'fa', '=', 'lambda', 'd', ':', 'd', '-', 'target_dt', 'if', 'd', '>=', 'target_dt', 'else', 'datetime', '.', 'timedelta', '.', 'max', 'fnone', '=', 'lambda', 'd', ':', 'target_dt', '-', 'd', 'if', 'd', '<', 'target_dt', 'else', 'd', '-', 'target_dt', 'if', 'before_target', 'is', 'none', ':', 'return', 'min', '(', 'date_list', ',', 'key', '=', 'fnone', ')', '.', 'date', '(', ')', 'if', 'before_target', ':', 'return', 'min', '(', 'date_list', ',', 'key', '=', 'fb', ')', '.', 'date', '(', ')', 'else', ':', 'return', 'min', '(', 'date_list', ',', 'key', '=', 'fa', ')', '.', 'date', '(', ')']","def _closest_date ( target_dt , date_list , before_target = none ) : fb = lambda d : target_dt - d if d <= target_dt else datetime . timedelta . max fa = lambda d : d - target_dt if d >= target_dt else datetime . timedelta . max fnone = lambda d : target_dt - d if d < target_dt else d - target_dt if before_target is none : return min ( date_list , key = fnone ) . date ( ) if before_target : return min ( date_list , key = fb ) . date ( ) else : return min ( date_list , key = fa ) . date ( )"
65,apache/airflow,airflow/macros/hive.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/macros/hive.py#L83-L118,"def closest_ds_partition(
        table, ds, before=True, schema=""default"",
        metastore_conn_id='metastore_default'):
    """"""
    This function finds the date in a list closest to the target date.
    An optional parameter can be given to get the closest before or after.

    :param table: A hive table name
    :type table: str
    :param ds: A datestamp ``%Y-%m-%d`` e.g. ``yyyy-mm-dd``
    :type ds: list[datetime.date]
    :param before: closest before (True), after (False) or either side of ds
    :type before: bool or None
    :returns: The closest date
    :rtype: str or None

    >>> tbl = 'airflow.static_babynames_partitioned'
    >>> closest_ds_partition(tbl, '2015-01-02')
    '2015-01-01'
    """"""
    from airflow.hooks.hive_hooks import HiveMetastoreHook
    if '.' in table:
        schema, table = table.split('.')
    hh = HiveMetastoreHook(metastore_conn_id=metastore_conn_id)
    partitions = hh.get_partitions(schema=schema, table_name=table)
    if not partitions:
        return None
    part_vals = [list(p.values())[0] for p in partitions]
    if ds in part_vals:
        return ds
    else:
        parts = [datetime.datetime.strptime(pv, '%Y-%m-%d')
                 for pv in part_vals]
        target_dt = datetime.datetime.strptime(ds, '%Y-%m-%d')
        closest_ds = _closest_date(target_dt, parts, before_target=before)
        return closest_ds.isoformat()","['def', 'closest_ds_partition', '(', 'table', ',', 'ds', ',', 'before', '=', 'True', ',', 'schema', '=', '""default""', ',', 'metastore_conn_id', '=', ""'metastore_default'"", ')', ':', 'from', 'airflow', '.', 'hooks', '.', 'hive_hooks', 'import', 'HiveMetastoreHook', 'if', ""'.'"", 'in', 'table', ':', 'schema', ',', 'table', '=', 'table', '.', 'split', '(', ""'.'"", ')', 'hh', '=', 'HiveMetastoreHook', '(', 'metastore_conn_id', '=', 'metastore_conn_id', ')', 'partitions', '=', 'hh', '.', 'get_partitions', '(', 'schema', '=', 'schema', ',', 'table_name', '=', 'table', ')', 'if', 'not', 'partitions', ':', 'return', 'None', 'part_vals', '=', '[', 'list', '(', 'p', '.', 'values', '(', ')', ')', '[', '0', ']', 'for', 'p', 'in', 'partitions', ']', 'if', 'ds', 'in', 'part_vals', ':', 'return', 'ds', 'else', ':', 'parts', '=', '[', 'datetime', '.', 'datetime', '.', 'strptime', '(', 'pv', ',', ""'%Y-%m-%d'"", ')', 'for', 'pv', 'in', 'part_vals', ']', 'target_dt', '=', 'datetime', '.', 'datetime', '.', 'strptime', '(', 'ds', ',', ""'%Y-%m-%d'"", ')', 'closest_ds', '=', '_closest_date', '(', 'target_dt', ',', 'parts', ',', 'before_target', '=', 'before', ')', 'return', 'closest_ds', '.', 'isoformat', '(', ')']","This function finds the date in a list closest to the target date.
    An optional parameter can be given to get the closest before or after.

    :param table: A hive table name
    :type table: str
    :param ds: A datestamp ``%Y-%m-%d`` e.g. ``yyyy-mm-dd``
    :type ds: list[datetime.date]
    :param before: closest before (True), after (False) or either side of ds
    :type before: bool or None
    :returns: The closest date
    :rtype: str or None

    >>> tbl = 'airflow.static_babynames_partitioned'
    >>> closest_ds_partition(tbl, '2015-01-02')
    '2015-01-01'","['This', 'function', 'finds', 'the', 'date', 'in', 'a', 'list', 'closest', 'to', 'the', 'target', 'date', '.', 'An', 'optional', 'parameter', 'can', 'be', 'given', 'to', 'get', 'the', 'closest', 'before', 'or', 'after', '.']",python,test,"['this', 'function', 'finds', 'the', 'date', 'in', 'a', 'list', 'closest', 'to', 'the', 'target', 'date', '.', 'an', 'optional', 'parameter', 'can', 'be', 'given', 'to', 'get', 'the', 'closest', 'before', 'or', 'after', '.']",this function finds the date in a list closest to the target date . an optional parameter can be given to get the closest before or after .,"['def', 'closest_ds_partition', '(', 'table', ',', 'ds', ',', 'before', '=', 'true', ',', 'schema', '=', '""default""', ',', 'metastore_conn_id', '=', ""'metastore_default'"", ')', ':', 'from', 'airflow', '.', 'hooks', '.', 'hive_hooks', 'import', 'hivemetastorehook', 'if', ""'.'"", 'in', 'table', ':', 'schema', ',', 'table', '=', 'table', '.', 'split', '(', ""'.'"", ')', 'hh', '=', 'hivemetastorehook', '(', 'metastore_conn_id', '=', 'metastore_conn_id', ')', 'partitions', '=', 'hh', '.', 'get_partitions', '(', 'schema', '=', 'schema', ',', 'table_name', '=', 'table', ')', 'if', 'not', 'partitions', ':', 'return', 'none', 'part_vals', '=', '[', 'list', '(', 'p', '.', 'values', '(', ')', ')', '[', '0', ']', 'for', 'p', 'in', 'partitions', ']', 'if', 'ds', 'in', 'part_vals', ':', 'return', 'ds', 'else', ':', 'parts', '=', '[', 'datetime', '.', 'datetime', '.', 'strptime', '(', 'pv', ',', ""'%y-%m-%d'"", ')', 'for', 'pv', 'in', 'part_vals', ']', 'target_dt', '=', 'datetime', '.', 'datetime', '.', 'strptime', '(', 'ds', ',', ""'%y-%m-%d'"", ')', 'closest_ds', '=', '_closest_date', '(', 'target_dt', ',', 'parts', ',', 'before_target', '=', 'before', ')', 'return', 'closest_ds', '.', 'isoformat', '(', ')']","def closest_ds_partition ( table , ds , before = true , schema = ""default"" , metastore_conn_id = 'metastore_default' ) : from airflow . hooks . hive_hooks import hivemetastorehook if '.' in table : schema , table = table . split ( '.' ) hh = hivemetastorehook ( metastore_conn_id = metastore_conn_id ) partitions = hh . get_partitions ( schema = schema , table_name = table ) if not partitions : return none part_vals = [ list ( p . values ( ) ) [ 0 ] for p in partitions ] if ds in part_vals : return ds else : parts = [ datetime . datetime . strptime ( pv , '%y-%m-%d' ) for pv in part_vals ] target_dt = datetime . datetime . strptime ( ds , '%y-%m-%d' ) closest_ds = _closest_date ( target_dt , parts , before_target = before ) return closest_ds . isoformat ( )"
66,apache/airflow,airflow/hooks/mysql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/mysql_hook.py#L62-L105,"def get_conn(self):
        """"""
        Returns a mysql connection object
        """"""
        conn = self.get_connection(self.mysql_conn_id)
        conn_config = {
            ""user"": conn.login,
            ""passwd"": conn.password or '',
            ""host"": conn.host or 'localhost',
            ""db"": self.schema or conn.schema or ''
        }

        if not conn.port:
            conn_config[""port""] = 3306
        else:
            conn_config[""port""] = int(conn.port)

        if conn.extra_dejson.get('charset', False):
            conn_config[""charset""] = conn.extra_dejson[""charset""]
            if (conn_config[""charset""]).lower() == 'utf8' or\
                    (conn_config[""charset""]).lower() == 'utf-8':
                conn_config[""use_unicode""] = True
        if conn.extra_dejson.get('cursor', False):
            if (conn.extra_dejson[""cursor""]).lower() == 'sscursor':
                conn_config[""cursorclass""] = MySQLdb.cursors.SSCursor
            elif (conn.extra_dejson[""cursor""]).lower() == 'dictcursor':
                conn_config[""cursorclass""] = MySQLdb.cursors.DictCursor
            elif (conn.extra_dejson[""cursor""]).lower() == 'ssdictcursor':
                conn_config[""cursorclass""] = MySQLdb.cursors.SSDictCursor
        local_infile = conn.extra_dejson.get('local_infile', False)
        if conn.extra_dejson.get('ssl', False):
            # SSL parameter for MySQL has to be a dictionary and in case
            # of extra/dejson we can get string if extra is passed via
            # URL parameters
            dejson_ssl = conn.extra_dejson['ssl']
            if isinstance(dejson_ssl, six.string_types):
                dejson_ssl = json.loads(dejson_ssl)
            conn_config['ssl'] = dejson_ssl
        if conn.extra_dejson.get('unix_socket'):
            conn_config['unix_socket'] = conn.extra_dejson['unix_socket']
        if local_infile:
            conn_config[""local_infile""] = 1
        conn = MySQLdb.connect(**conn_config)
        return conn","['def', 'get_conn', '(', 'self', ')', ':', 'conn', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'mysql_conn_id', ')', 'conn_config', '=', '{', '""user""', ':', 'conn', '.', 'login', ',', '""passwd""', ':', 'conn', '.', 'password', 'or', ""''"", ',', '""host""', ':', 'conn', '.', 'host', 'or', ""'localhost'"", ',', '""db""', ':', 'self', '.', 'schema', 'or', 'conn', '.', 'schema', 'or', ""''"", '}', 'if', 'not', 'conn', '.', 'port', ':', 'conn_config', '[', '""port""', ']', '=', '3306', 'else', ':', 'conn_config', '[', '""port""', ']', '=', 'int', '(', 'conn', '.', 'port', ')', 'if', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'charset'"", ',', 'False', ')', ':', 'conn_config', '[', '""charset""', ']', '=', 'conn', '.', 'extra_dejson', '[', '""charset""', ']', 'if', '(', 'conn_config', '[', '""charset""', ']', ')', '.', 'lower', '(', ')', '==', ""'utf8'"", 'or', '(', 'conn_config', '[', '""charset""', ']', ')', '.', 'lower', '(', ')', '==', ""'utf-8'"", ':', 'conn_config', '[', '""use_unicode""', ']', '=', 'True', 'if', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'cursor'"", ',', 'False', ')', ':', 'if', '(', 'conn', '.', 'extra_dejson', '[', '""cursor""', ']', ')', '.', 'lower', '(', ')', '==', ""'sscursor'"", ':', 'conn_config', '[', '""cursorclass""', ']', '=', 'MySQLdb', '.', 'cursors', '.', 'SSCursor', 'elif', '(', 'conn', '.', 'extra_dejson', '[', '""cursor""', ']', ')', '.', 'lower', '(', ')', '==', ""'dictcursor'"", ':', 'conn_config', '[', '""cursorclass""', ']', '=', 'MySQLdb', '.', 'cursors', '.', 'DictCursor', 'elif', '(', 'conn', '.', 'extra_dejson', '[', '""cursor""', ']', ')', '.', 'lower', '(', ')', '==', ""'ssdictcursor'"", ':', 'conn_config', '[', '""cursorclass""', ']', '=', 'MySQLdb', '.', 'cursors', '.', 'SSDictCursor', 'local_infile', '=', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'local_infile'"", ',', 'False', ')', 'if', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'ssl'"", ',', 'False', ')', ':', '# SSL parameter for MySQL has to be a dictionary and in case', '# of extra/dejson we can get string if extra is passed via', '# URL parameters', 'dejson_ssl', '=', 'conn', '.', 'extra_dejson', '[', ""'ssl'"", ']', 'if', 'isinstance', '(', 'dejson_ssl', ',', 'six', '.', 'string_types', ')', ':', 'dejson_ssl', '=', 'json', '.', 'loads', '(', 'dejson_ssl', ')', 'conn_config', '[', ""'ssl'"", ']', '=', 'dejson_ssl', 'if', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'unix_socket'"", ')', ':', 'conn_config', '[', ""'unix_socket'"", ']', '=', 'conn', '.', 'extra_dejson', '[', ""'unix_socket'"", ']', 'if', 'local_infile', ':', 'conn_config', '[', '""local_infile""', ']', '=', '1', 'conn', '=', 'MySQLdb', '.', 'connect', '(', '*', '*', 'conn_config', ')', 'return', 'conn']",Returns a mysql connection object,"['Returns', 'a', 'mysql', 'connection', 'object']",python,test,"['returns', 'a', 'mysql', 'connection', 'object']",returns a mysql connection object,"['def', 'get_conn', '(', 'self', ')', ':', 'conn', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'mysql_conn_id', ')', 'conn_config', '=', '{', '""user""', ':', 'conn', '.', 'login', ',', '""passwd""', ':', 'conn', '.', 'password', 'or', ""''"", ',', '""host""', ':', 'conn', '.', 'host', 'or', ""'localhost'"", ',', '""db""', ':', 'self', '.', 'schema', 'or', 'conn', '.', 'schema', 'or', ""''"", '}', 'if', 'not', 'conn', '.', 'port', ':', 'conn_config', '[', '""port""', ']', '=', '3306', 'else', ':', 'conn_config', '[', '""port""', ']', '=', 'int', '(', 'conn', '.', 'port', ')', 'if', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'charset'"", ',', 'false', ')', ':', 'conn_config', '[', '""charset""', ']', '=', 'conn', '.', 'extra_dejson', '[', '""charset""', ']', 'if', '(', 'conn_config', '[', '""charset""', ']', ')', '.', 'lower', '(', ')', '==', ""'utf8'"", 'or', '(', 'conn_config', '[', '""charset""', ']', ')', '.', 'lower', '(', ')', '==', ""'utf-8'"", ':', 'conn_config', '[', '""use_unicode""', ']', '=', 'true', 'if', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'cursor'"", ',', 'false', ')', ':', 'if', '(', 'conn', '.', 'extra_dejson', '[', '""cursor""', ']', ')', '.', 'lower', '(', ')', '==', ""'sscursor'"", ':', 'conn_config', '[', '""cursorclass""', ']', '=', 'mysqldb', '.', 'cursors', '.', 'sscursor', 'elif', '(', 'conn', '.', 'extra_dejson', '[', '""cursor""', ']', ')', '.', 'lower', '(', ')', '==', ""'dictcursor'"", ':', 'conn_config', '[', '""cursorclass""', ']', '=', 'mysqldb', '.', 'cursors', '.', 'dictcursor', 'elif', '(', 'conn', '.', 'extra_dejson', '[', '""cursor""', ']', ')', '.', 'lower', '(', ')', '==', ""'ssdictcursor'"", ':', 'conn_config', '[', '""cursorclass""', ']', '=', 'mysqldb', '.', 'cursors', '.', 'ssdictcursor', 'local_infile', '=', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'local_infile'"", ',', 'false', ')', 'if', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'ssl'"", ',', 'false', ')', ':', '# ssl parameter for mysql has to be a dictionary and in case', '# of extra/dejson we can get string if extra is passed via', '# url parameters', 'dejson_ssl', '=', 'conn', '.', 'extra_dejson', '[', ""'ssl'"", ']', 'if', 'isinstance', '(', 'dejson_ssl', ',', 'six', '.', 'string_types', ')', ':', 'dejson_ssl', '=', 'json', '.', 'loads', '(', 'dejson_ssl', ')', 'conn_config', '[', ""'ssl'"", ']', '=', 'dejson_ssl', 'if', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'unix_socket'"", ')', ':', 'conn_config', '[', ""'unix_socket'"", ']', '=', 'conn', '.', 'extra_dejson', '[', ""'unix_socket'"", ']', 'if', 'local_infile', ':', 'conn_config', '[', '""local_infile""', ']', '=', '1', 'conn', '=', 'mysqldb', '.', 'connect', '(', '*', '*', 'conn_config', ')', 'return', 'conn']","def get_conn ( self ) : conn = self . get_connection ( self . mysql_conn_id ) conn_config = { ""user"" : conn . login , ""passwd"" : conn . password or '' , ""host"" : conn . host or 'localhost' , ""db"" : self . schema or conn . schema or '' } if not conn . port : conn_config [ ""port"" ] = 3306 else : conn_config [ ""port"" ] = int ( conn . port ) if conn . extra_dejson . get ( 'charset' , false ) : conn_config [ ""charset"" ] = conn . extra_dejson [ ""charset"" ] if ( conn_config [ ""charset"" ] ) . lower ( ) == 'utf8' or ( conn_config [ ""charset"" ] ) . lower ( ) == 'utf-8' : conn_config [ ""use_unicode"" ] = true if conn . extra_dejson . get ( 'cursor' , false ) : if ( conn . extra_dejson [ ""cursor"" ] ) . lower ( ) == 'sscursor' : conn_config [ ""cursorclass"" ] = mysqldb . cursors . sscursor elif ( conn . extra_dejson [ ""cursor"" ] ) . lower ( ) == 'dictcursor' : conn_config [ ""cursorclass"" ] = mysqldb . cursors . dictcursor elif ( conn . extra_dejson [ ""cursor"" ] ) . lower ( ) == 'ssdictcursor' : conn_config [ ""cursorclass"" ] = mysqldb . cursors . ssdictcursor local_infile = conn . extra_dejson . get ( 'local_infile' , false ) if conn . extra_dejson . get ( 'ssl' , false ) : # ssl parameter for mysql has to be a dictionary and in case # of extra/dejson we can get string if extra is passed via # url parameters dejson_ssl = conn . extra_dejson [ 'ssl' ] if isinstance ( dejson_ssl , six . string_types ) : dejson_ssl = json . loads ( dejson_ssl ) conn_config [ 'ssl' ] = dejson_ssl if conn . extra_dejson . get ( 'unix_socket' ) : conn_config [ 'unix_socket' ] = conn . extra_dejson [ 'unix_socket' ] if local_infile : conn_config [ ""local_infile"" ] = 1 conn = mysqldb . connect ( * * conn_config ) return conn"
67,apache/airflow,airflow/hooks/mysql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/mysql_hook.py#L107-L117,"def bulk_load(self, table, tmp_file):
        """"""
        Loads a tab-delimited file into a database table
        """"""
        conn = self.get_conn()
        cur = conn.cursor()
        cur.execute(""""""
            LOAD DATA LOCAL INFILE '{tmp_file}'
            INTO TABLE {table}
            """""".format(tmp_file=tmp_file, table=table))
        conn.commit()","['def', 'bulk_load', '(', 'self', ',', 'table', ',', 'tmp_file', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'cur', '=', 'conn', '.', 'cursor', '(', ')', 'cur', '.', 'execute', '(', '""""""\n            LOAD DATA LOCAL INFILE \'{tmp_file}\'\n            INTO TABLE {table}\n            """"""', '.', 'format', '(', 'tmp_file', '=', 'tmp_file', ',', 'table', '=', 'table', ')', ')', 'conn', '.', 'commit', '(', ')']",Loads a tab-delimited file into a database table,"['Loads', 'a', 'tab', '-', 'delimited', 'file', 'into', 'a', 'database', 'table']",python,test,"['loads', 'a', 'tab', '-', 'delimited', 'file', 'into', 'a', 'database', 'table']",loads a tab - delimited file into a database table,"['def', 'bulk_load', '(', 'self', ',', 'table', ',', 'tmp_file', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'cur', '=', 'conn', '.', 'cursor', '(', ')', 'cur', '.', 'execute', '(', '""""""\n            load data local infile \'{tmp_file}\'\n            into table {table}\n            """"""', '.', 'format', '(', 'tmp_file', '=', 'tmp_file', ',', 'table', '=', 'table', ')', ')', 'conn', '.', 'commit', '(', ')']","def bulk_load ( self , table , tmp_file ) : conn = self . get_conn ( ) cur = conn . cursor ( ) cur . execute ( """"""
            load data local infile '{tmp_file}'
            into table {table}
            """""" . format ( tmp_file = tmp_file , table = table ) ) conn . commit ( )"
68,apache/airflow,airflow/contrib/sensors/gcs_sensor.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/gcs_sensor.py#L242-L310,"def is_bucket_updated(self, current_num_objects):
        """"""
        Checks whether new objects have been uploaded and the inactivity_period
        has passed and updates the state of the sensor accordingly.

        :param current_num_objects: number of objects in bucket during last poke.
        :type current_num_objects: int
        """"""

        if current_num_objects > self.previous_num_objects:
            # When new objects arrived, reset the inactivity_seconds
            # previous_num_objects for the next poke.
            self.log.info(
                '''
                New objects found at {} resetting last_activity_time.
                '''.format(os.path.join(self.bucket, self.prefix)))
            self.last_activity_time = get_time()
            self.inactivity_seconds = 0
            self.previous_num_objects = current_num_objects
        elif current_num_objects < self.previous_num_objects:
            # During the last poke interval objects were deleted.
            if self.allow_delete:
                self.previous_num_objects = current_num_objects
                self.last_activity_time = get_time()
                self.log.warning(
                    '''
                    Objects were deleted during the last
                    poke interval. Updating the file counter and
                    resetting last_activity_time.
                    '''
                )
            else:
                raise RuntimeError(
                    '''
                    Illegal behavior: objects were deleted in {} between pokes.
                    '''.format(os.path.join(self.bucket, self.prefix))
                )
        else:
            if self.last_activity_time:
                self.inactivity_seconds = (
                    get_time() - self.last_activity_time).total_seconds()
            else:
                # Handles the first poke where last inactivity time is None.
                self.last_activity_time = get_time()
                self.inactivity_seconds = 0

            if self.inactivity_seconds >= self.inactivity_period:
                if current_num_objects >= self.min_objects:
                    self.log.info(
                        '''
                        SUCCESS:
                        Sensor found {} objects at {}.
                        Waited at least {} seconds, with no new objects dropped.
                        '''.format(
                            current_num_objects,
                            os.path.join(self.bucket, self.prefix),
                            self.inactivity_period))
                    return True

                warn_msg = \
                    '''
                    FAILURE:
                    Inactivity Period passed,
                    not enough objects found in {}
                    '''.format(
                        os.path.join(self.bucket, self.prefix))
                self.log.warning(warn_msg)
                return False
            return False","['def', 'is_bucket_updated', '(', 'self', ',', 'current_num_objects', ')', ':', 'if', 'current_num_objects', '>', 'self', '.', 'previous_num_objects', ':', '# When new objects arrived, reset the inactivity_seconds', '# previous_num_objects for the next poke.', 'self', '.', 'log', '.', 'info', '(', ""'''\n                New objects found at {} resetting last_activity_time.\n                '''"", '.', 'format', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'bucket', ',', 'self', '.', 'prefix', ')', ')', ')', 'self', '.', 'last_activity_time', '=', 'get_time', '(', ')', 'self', '.', 'inactivity_seconds', '=', '0', 'self', '.', 'previous_num_objects', '=', 'current_num_objects', 'elif', 'current_num_objects', '<', 'self', '.', 'previous_num_objects', ':', '# During the last poke interval objects were deleted.', 'if', 'self', '.', 'allow_delete', ':', 'self', '.', 'previous_num_objects', '=', 'current_num_objects', 'self', '.', 'last_activity_time', '=', 'get_time', '(', ')', 'self', '.', 'log', '.', 'warning', '(', ""'''\n                    Objects were deleted during the last\n                    poke interval. Updating the file counter and\n                    resetting last_activity_time.\n                    '''"", ')', 'else', ':', 'raise', 'RuntimeError', '(', ""'''\n                    Illegal behavior: objects were deleted in {} between pokes.\n                    '''"", '.', 'format', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'bucket', ',', 'self', '.', 'prefix', ')', ')', ')', 'else', ':', 'if', 'self', '.', 'last_activity_time', ':', 'self', '.', 'inactivity_seconds', '=', '(', 'get_time', '(', ')', '-', 'self', '.', 'last_activity_time', ')', '.', 'total_seconds', '(', ')', 'else', ':', '# Handles the first poke where last inactivity time is None.', 'self', '.', 'last_activity_time', '=', 'get_time', '(', ')', 'self', '.', 'inactivity_seconds', '=', '0', 'if', 'self', '.', 'inactivity_seconds', '>=', 'self', '.', 'inactivity_period', ':', 'if', 'current_num_objects', '>=', 'self', '.', 'min_objects', ':', 'self', '.', 'log', '.', 'info', '(', ""'''\n                        SUCCESS:\n                        Sensor found {} objects at {}.\n                        Waited at least {} seconds, with no new objects dropped.\n                        '''"", '.', 'format', '(', 'current_num_objects', ',', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'bucket', ',', 'self', '.', 'prefix', ')', ',', 'self', '.', 'inactivity_period', ')', ')', 'return', 'True', 'warn_msg', '=', ""'''\n                    FAILURE:\n                    Inactivity Period passed,\n                    not enough objects found in {}\n                    '''"", '.', 'format', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'bucket', ',', 'self', '.', 'prefix', ')', ')', 'self', '.', 'log', '.', 'warning', '(', 'warn_msg', ')', 'return', 'False', 'return', 'False']","Checks whether new objects have been uploaded and the inactivity_period
        has passed and updates the state of the sensor accordingly.

        :param current_num_objects: number of objects in bucket during last poke.
        :type current_num_objects: int","['Checks', 'whether', 'new', 'objects', 'have', 'been', 'uploaded', 'and', 'the', 'inactivity_period', 'has', 'passed', 'and', 'updates', 'the', 'state', 'of', 'the', 'sensor', 'accordingly', '.']",python,test,"['checks', 'whether', 'new', 'objects', 'have', 'been', 'uploaded', 'and', 'the', 'inactivity_period', 'has', 'passed', 'and', 'updates', 'the', 'state', 'of', 'the', 'sensor', 'accordingly', '.']",checks whether new objects have been uploaded and the inactivity_period has passed and updates the state of the sensor accordingly .,"['def', 'is_bucket_updated', '(', 'self', ',', 'current_num_objects', ')', ':', 'if', 'current_num_objects', '>', 'self', '.', 'previous_num_objects', ':', '# when new objects arrived, reset the inactivity_seconds', '# previous_num_objects for the next poke.', 'self', '.', 'log', '.', 'info', '(', ""'''\n                new objects found at {} resetting last_activity_time.\n                '''"", '.', 'format', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'bucket', ',', 'self', '.', 'prefix', ')', ')', ')', 'self', '.', 'last_activity_time', '=', 'get_time', '(', ')', 'self', '.', 'inactivity_seconds', '=', '0', 'self', '.', 'previous_num_objects', '=', 'current_num_objects', 'elif', 'current_num_objects', '<', 'self', '.', 'previous_num_objects', ':', '# during the last poke interval objects were deleted.', 'if', 'self', '.', 'allow_delete', ':', 'self', '.', 'previous_num_objects', '=', 'current_num_objects', 'self', '.', 'last_activity_time', '=', 'get_time', '(', ')', 'self', '.', 'log', '.', 'warning', '(', ""'''\n                    objects were deleted during the last\n                    poke interval. updating the file counter and\n                    resetting last_activity_time.\n                    '''"", ')', 'else', ':', 'raise', 'runtimeerror', '(', ""'''\n                    illegal behavior: objects were deleted in {} between pokes.\n                    '''"", '.', 'format', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'bucket', ',', 'self', '.', 'prefix', ')', ')', ')', 'else', ':', 'if', 'self', '.', 'last_activity_time', ':', 'self', '.', 'inactivity_seconds', '=', '(', 'get_time', '(', ')', '-', 'self', '.', 'last_activity_time', ')', '.', 'total_seconds', '(', ')', 'else', ':', '# handles the first poke where last inactivity time is none.', 'self', '.', 'last_activity_time', '=', 'get_time', '(', ')', 'self', '.', 'inactivity_seconds', '=', '0', 'if', 'self', '.', 'inactivity_seconds', '>=', 'self', '.', 'inactivity_period', ':', 'if', 'current_num_objects', '>=', 'self', '.', 'min_objects', ':', 'self', '.', 'log', '.', 'info', '(', ""'''\n                        success:\n                        sensor found {} objects at {}.\n                        waited at least {} seconds, with no new objects dropped.\n                        '''"", '.', 'format', '(', 'current_num_objects', ',', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'bucket', ',', 'self', '.', 'prefix', ')', ',', 'self', '.', 'inactivity_period', ')', ')', 'return', 'true', 'warn_msg', '=', ""'''\n                    failure:\n                    inactivity period passed,\n                    not enough objects found in {}\n                    '''"", '.', 'format', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'bucket', ',', 'self', '.', 'prefix', ')', ')', 'self', '.', 'log', '.', 'warning', '(', 'warn_msg', ')', 'return', 'false', 'return', 'false']","def is_bucket_updated ( self , current_num_objects ) : if current_num_objects > self . previous_num_objects : # when new objects arrived, reset the inactivity_seconds # previous_num_objects for the next poke. self . log . info ( '''
                new objects found at {} resetting last_activity_time.
                ''' . format ( os . path . join ( self . bucket , self . prefix ) ) ) self . last_activity_time = get_time ( ) self . inactivity_seconds = 0 self . previous_num_objects = current_num_objects elif current_num_objects < self . previous_num_objects : # during the last poke interval objects were deleted. if self . allow_delete : self . previous_num_objects = current_num_objects self . last_activity_time = get_time ( ) self . log . warning ( '''
                    objects were deleted during the last
                    poke interval. updating the file counter and
                    resetting last_activity_time.
                    ''' ) else : raise runtimeerror ( '''
                    illegal behavior: objects were deleted in {} between pokes.
                    ''' . format ( os . path . join ( self . bucket , self . prefix ) ) ) else : if self . last_activity_time : self . inactivity_seconds = ( get_time ( ) - self . last_activity_time ) . total_seconds ( ) else : # handles the first poke where last inactivity time is none. self . last_activity_time = get_time ( ) self . inactivity_seconds = 0 if self . inactivity_seconds >= self . inactivity_period : if current_num_objects >= self . min_objects : self . log . info ( '''
                        success:
                        sensor found {} objects at {}.
                        waited at least {} seconds, with no new objects dropped.
                        ''' . format ( current_num_objects , os . path . join ( self . bucket , self . prefix ) , self . inactivity_period ) ) return true warn_msg = '''
                    failure:
                    inactivity period passed,
                    not enough objects found in {}
                    ''' . format ( os . path . join ( self . bucket , self . prefix ) ) self . log . warning ( warn_msg ) return false return false"
69,apache/airflow,airflow/bin/cli.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/bin/cli.py#L88-L103,"def sigquit_handler(sig, frame):
    """"""Helps debug deadlocks by printing stacktraces when this gets a SIGQUIT
    e.g. kill -s QUIT <PID> or CTRL+\
    """"""
    print(""Dumping stack traces for all threads in PID {}"".format(os.getpid()))
    id_to_name = dict([(th.ident, th.name) for th in threading.enumerate()])
    code = []
    for thread_id, stack in sys._current_frames().items():
        code.append(""\n# Thread: {}({})""
                    .format(id_to_name.get(thread_id, """"), thread_id))
        for filename, line_number, name, line in traceback.extract_stack(stack):
            code.append('File: ""{}"", line {}, in {}'
                        .format(filename, line_number, name))
            if line:
                code.append(""  {}"".format(line.strip()))
    print(""\n"".join(code))","['def', 'sigquit_handler', '(', 'sig', ',', 'frame', ')', ':', 'print', '(', '""Dumping stack traces for all threads in PID {}""', '.', 'format', '(', 'os', '.', 'getpid', '(', ')', ')', ')', 'id_to_name', '=', 'dict', '(', '[', '(', 'th', '.', 'ident', ',', 'th', '.', 'name', ')', 'for', 'th', 'in', 'threading', '.', 'enumerate', '(', ')', ']', ')', 'code', '=', '[', ']', 'for', 'thread_id', ',', 'stack', 'in', 'sys', '.', '_current_frames', '(', ')', '.', 'items', '(', ')', ':', 'code', '.', 'append', '(', '""\\n# Thread: {}({})""', '.', 'format', '(', 'id_to_name', '.', 'get', '(', 'thread_id', ',', '""""', ')', ',', 'thread_id', ')', ')', 'for', 'filename', ',', 'line_number', ',', 'name', ',', 'line', 'in', 'traceback', '.', 'extract_stack', '(', 'stack', ')', ':', 'code', '.', 'append', '(', '\'File: ""{}"", line {}, in {}\'', '.', 'format', '(', 'filename', ',', 'line_number', ',', 'name', ')', ')', 'if', 'line', ':', 'code', '.', 'append', '(', '""  {}""', '.', 'format', '(', 'line', '.', 'strip', '(', ')', ')', ')', 'print', '(', '""\\n""', '.', 'join', '(', 'code', ')', ')']","Helps debug deadlocks by printing stacktraces when this gets a SIGQUIT
    e.g. kill -s QUIT <PID> or CTRL+\","['Helps', 'debug', 'deadlocks', 'by', 'printing', 'stacktraces', 'when', 'this', 'gets', 'a', 'SIGQUIT', 'e', '.', 'g', '.', 'kill', '-', 's', 'QUIT', '<PID', '>', 'or', 'CTRL', '+', '\\']",python,test,"['helps', 'debug', 'deadlocks', 'by', 'printing', 'stacktraces', 'when', 'this', 'gets', 'a', 'sigquit', 'e', '.', 'g', '.', 'kill', '-', 's', 'quit', '<pid', '>', 'or', 'ctrl', '+', '\\']",helps debug deadlocks by printing stacktraces when this gets a sigquit e . g . kill - s quit <pid > or ctrl + \,"['def', 'sigquit_handler', '(', 'sig', ',', 'frame', ')', ':', 'print', '(', '""dumping stack traces for all threads in pid {}""', '.', 'format', '(', 'os', '.', 'getpid', '(', ')', ')', ')', 'id_to_name', '=', 'dict', '(', '[', '(', 'th', '.', 'ident', ',', 'th', '.', 'name', ')', 'for', 'th', 'in', 'threading', '.', 'enumerate', '(', ')', ']', ')', 'code', '=', '[', ']', 'for', 'thread_id', ',', 'stack', 'in', 'sys', '.', '_current_frames', '(', ')', '.', 'items', '(', ')', ':', 'code', '.', 'append', '(', '""\\n# thread: {}({})""', '.', 'format', '(', 'id_to_name', '.', 'get', '(', 'thread_id', ',', '""""', ')', ',', 'thread_id', ')', ')', 'for', 'filename', ',', 'line_number', ',', 'name', ',', 'line', 'in', 'traceback', '.', 'extract_stack', '(', 'stack', ')', ':', 'code', '.', 'append', '(', '\'file: ""{}"", line {}, in {}\'', '.', 'format', '(', 'filename', ',', 'line_number', ',', 'name', ')', ')', 'if', 'line', ':', 'code', '.', 'append', '(', '""  {}""', '.', 'format', '(', 'line', '.', 'strip', '(', ')', ')', ')', 'print', '(', '""\\n""', '.', 'join', '(', 'code', ')', ')']","def sigquit_handler ( sig , frame ) : print ( ""dumping stack traces for all threads in pid {}"" . format ( os . getpid ( ) ) ) id_to_name = dict ( [ ( th . ident , th . name ) for th in threading . enumerate ( ) ] ) code = [ ] for thread_id , stack in sys . _current_frames ( ) . items ( ) : code . append ( ""\n# thread: {}({})"" . format ( id_to_name . get ( thread_id , """" ) , thread_id ) ) for filename , line_number , name , line in traceback . extract_stack ( stack ) : code . append ( 'file: ""{}"", line {}, in {}' . format ( filename , line_number , name ) ) if line : code . append ( ""  {}"" . format ( line . strip ( ) ) ) print ( ""\n"" . join ( code ) )"
70,apache/airflow,airflow/bin/cli.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/bin/cli.py#L221-L236,"def trigger_dag(args):
    """"""
    Creates a dag run for the specified dag
    :param args:
    :return:
    """"""
    log = LoggingMixin().log
    try:
        message = api_client.trigger_dag(dag_id=args.dag_id,
                                         run_id=args.run_id,
                                         conf=args.conf,
                                         execution_date=args.exec_date)
    except IOError as err:
        log.error(err)
        raise AirflowException(err)
    log.info(message)","['def', 'trigger_dag', '(', 'args', ')', ':', 'log', '=', 'LoggingMixin', '(', ')', '.', 'log', 'try', ':', 'message', '=', 'api_client', '.', 'trigger_dag', '(', 'dag_id', '=', 'args', '.', 'dag_id', ',', 'run_id', '=', 'args', '.', 'run_id', ',', 'conf', '=', 'args', '.', 'conf', ',', 'execution_date', '=', 'args', '.', 'exec_date', ')', 'except', 'IOError', 'as', 'err', ':', 'log', '.', 'error', '(', 'err', ')', 'raise', 'AirflowException', '(', 'err', ')', 'log', '.', 'info', '(', 'message', ')']","Creates a dag run for the specified dag
    :param args:
    :return:","['Creates', 'a', 'dag', 'run', 'for', 'the', 'specified', 'dag', ':', 'param', 'args', ':', ':', 'return', ':']",python,test,"['creates', 'a', 'dag', 'run', 'for', 'the', 'specified', 'dag', ':', 'param', 'args', ':', ':', 'return', ':']",creates a dag run for the specified dag : param args : : return :,"['def', 'trigger_dag', '(', 'args', ')', ':', 'log', '=', 'loggingmixin', '(', ')', '.', 'log', 'try', ':', 'message', '=', 'api_client', '.', 'trigger_dag', '(', 'dag_id', '=', 'args', '.', 'dag_id', ',', 'run_id', '=', 'args', '.', 'run_id', ',', 'conf', '=', 'args', '.', 'conf', ',', 'execution_date', '=', 'args', '.', 'exec_date', ')', 'except', 'ioerror', 'as', 'err', ':', 'log', '.', 'error', '(', 'err', ')', 'raise', 'airflowexception', '(', 'err', ')', 'log', '.', 'info', '(', 'message', ')']","def trigger_dag ( args ) : log = loggingmixin ( ) . log try : message = api_client . trigger_dag ( dag_id = args . dag_id , run_id = args . run_id , conf = args . conf , execution_date = args . exec_date ) except ioerror as err : log . error ( err ) raise airflowexception ( err ) log . info ( message )"
71,apache/airflow,airflow/bin/cli.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/bin/cli.py#L240-L257,"def delete_dag(args):
    """"""
    Deletes all DB records related to the specified dag
    :param args:
    :return:
    """"""
    log = LoggingMixin().log
    if args.yes or input(
            ""This will drop all existing records related to the specified DAG. ""
            ""Proceed? (y/n)"").upper() == ""Y"":
        try:
            message = api_client.delete_dag(dag_id=args.dag_id)
        except IOError as err:
            log.error(err)
            raise AirflowException(err)
        log.info(message)
    else:
        print(""Bail."")","['def', 'delete_dag', '(', 'args', ')', ':', 'log', '=', 'LoggingMixin', '(', ')', '.', 'log', 'if', 'args', '.', 'yes', 'or', 'input', '(', '""This will drop all existing records related to the specified DAG. ""', '""Proceed? (y/n)""', ')', '.', 'upper', '(', ')', '==', '""Y""', ':', 'try', ':', 'message', '=', 'api_client', '.', 'delete_dag', '(', 'dag_id', '=', 'args', '.', 'dag_id', ')', 'except', 'IOError', 'as', 'err', ':', 'log', '.', 'error', '(', 'err', ')', 'raise', 'AirflowException', '(', 'err', ')', 'log', '.', 'info', '(', 'message', ')', 'else', ':', 'print', '(', '""Bail.""', ')']","Deletes all DB records related to the specified dag
    :param args:
    :return:","['Deletes', 'all', 'DB', 'records', 'related', 'to', 'the', 'specified', 'dag', ':', 'param', 'args', ':', ':', 'return', ':']",python,test,"['deletes', 'all', 'db', 'records', 'related', 'to', 'the', 'specified', 'dag', ':', 'param', 'args', ':', ':', 'return', ':']",deletes all db records related to the specified dag : param args : : return :,"['def', 'delete_dag', '(', 'args', ')', ':', 'log', '=', 'loggingmixin', '(', ')', '.', 'log', 'if', 'args', '.', 'yes', 'or', 'input', '(', '""this will drop all existing records related to the specified dag. ""', '""proceed? (y/n)""', ')', '.', 'upper', '(', ')', '==', '""y""', ':', 'try', ':', 'message', '=', 'api_client', '.', 'delete_dag', '(', 'dag_id', '=', 'args', '.', 'dag_id', ')', 'except', 'ioerror', 'as', 'err', ':', 'log', '.', 'error', '(', 'err', ')', 'raise', 'airflowexception', '(', 'err', ')', 'log', '.', 'info', '(', 'message', ')', 'else', ':', 'print', '(', '""bail.""', ')']","def delete_dag ( args ) : log = loggingmixin ( ) . log if args . yes or input ( ""this will drop all existing records related to the specified dag. "" ""proceed? (y/n)"" ) . upper ( ) == ""y"" : try : message = api_client . delete_dag ( dag_id = args . dag_id ) except ioerror as err : log . error ( err ) raise airflowexception ( err ) log . info ( message ) else : print ( ""bail."" )"
72,apache/airflow,airflow/bin/cli.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/bin/cli.py#L527-L550,"def task_failed_deps(args):
    """"""
    Returns the unmet dependencies for a task instance from the perspective of the
    scheduler (i.e. why a task instance doesn't get scheduled and then queued by the
    scheduler, and then run by an executor).
    >>> airflow task_failed_deps tutorial sleep 2015-01-01
    Task instance dependencies not met:
    Dagrun Running: Task instance's dagrun did not exist: Unknown reason
    Trigger Rule: Task's trigger rule 'all_success' requires all upstream tasks
    to have succeeded, but found 1 non-success(es).
    """"""
    dag = get_dag(args)
    task = dag.get_task(task_id=args.task_id)
    ti = TaskInstance(task, args.execution_date)

    dep_context = DepContext(deps=SCHEDULER_DEPS)
    failed_deps = list(ti.get_failed_dep_statuses(dep_context=dep_context))
    # TODO, Do we want to print or log this
    if failed_deps:
        print(""Task instance dependencies not met:"")
        for dep in failed_deps:
            print(""{}: {}"".format(dep.dep_name, dep.reason))
    else:
        print(""Task instance dependencies are all met."")","['def', 'task_failed_deps', '(', 'args', ')', ':', 'dag', '=', 'get_dag', '(', 'args', ')', 'task', '=', 'dag', '.', 'get_task', '(', 'task_id', '=', 'args', '.', 'task_id', ')', 'ti', '=', 'TaskInstance', '(', 'task', ',', 'args', '.', 'execution_date', ')', 'dep_context', '=', 'DepContext', '(', 'deps', '=', 'SCHEDULER_DEPS', ')', 'failed_deps', '=', 'list', '(', 'ti', '.', 'get_failed_dep_statuses', '(', 'dep_context', '=', 'dep_context', ')', ')', '# TODO, Do we want to print or log this', 'if', 'failed_deps', ':', 'print', '(', '""Task instance dependencies not met:""', ')', 'for', 'dep', 'in', 'failed_deps', ':', 'print', '(', '""{}: {}""', '.', 'format', '(', 'dep', '.', 'dep_name', ',', 'dep', '.', 'reason', ')', ')', 'else', ':', 'print', '(', '""Task instance dependencies are all met.""', ')']","Returns the unmet dependencies for a task instance from the perspective of the
    scheduler (i.e. why a task instance doesn't get scheduled and then queued by the
    scheduler, and then run by an executor).
    >>> airflow task_failed_deps tutorial sleep 2015-01-01
    Task instance dependencies not met:
    Dagrun Running: Task instance's dagrun did not exist: Unknown reason
    Trigger Rule: Task's trigger rule 'all_success' requires all upstream tasks
    to have succeeded, but found 1 non-success(es).","['Returns', 'the', 'unmet', 'dependencies', 'for', 'a', 'task', 'instance', 'from', 'the', 'perspective', 'of', 'the', 'scheduler', '(', 'i', '.', 'e', '.', 'why', 'a', 'task', 'instance', 'doesn', 't', 'get', 'scheduled', 'and', 'then', 'queued', 'by', 'the', 'scheduler', 'and', 'then', 'run', 'by', 'an', 'executor', ')', '.', '>>>', 'airflow', 'task_failed_deps', 'tutorial', 'sleep', '2015', '-', '01', '-', '01', 'Task', 'instance', 'dependencies', 'not', 'met', ':', 'Dagrun', 'Running', ':', 'Task', 'instance', 's', 'dagrun', 'did', 'not', 'exist', ':', 'Unknown', 'reason', 'Trigger', 'Rule', ':', 'Task', 's', 'trigger', 'rule', 'all_success', 'requires', 'all', 'upstream', 'tasks', 'to', 'have', 'succeeded', 'but', 'found', '1', 'non', '-', 'success', '(', 'es', ')', '.']",python,test,"['returns', 'the', 'unmet', 'dependencies', 'for', 'a', 'task', 'instance', 'from', 'the', 'perspective', 'of', 'the', 'scheduler', '(', 'i', '.', 'e', '.', 'why', 'a', 'task', 'instance', 'doesn', 't', 'get', 'scheduled', 'and', 'then', 'queued', 'by', 'the', 'scheduler', 'and', 'then', 'run', 'by', 'an', 'executor', ')', '.', '>>>', 'airflow', 'task_failed_deps', 'tutorial', 'sleep', '2015', '-', '01', '-', '01', 'task', 'instance', 'dependencies', 'not', 'met', ':', 'dagrun', 'running', ':', 'task', 'instance', 's', 'dagrun', 'did', 'not', 'exist', ':', 'unknown', 'reason', 'trigger', 'rule', ':', 'task', 's', 'trigger', 'rule', 'all_success', 'requires', 'all', 'upstream', 'tasks', 'to', 'have', 'succeeded', 'but', 'found', '1', 'non', '-', 'success', '(', 'es', ')', '.']",returns the unmet dependencies for a task instance from the perspective of the scheduler ( i . e . why a task instance doesn t get scheduled and then queued by the scheduler and then run by an executor ) . >>> airflow task_failed_deps tutorial sleep 2015 - 01 - 01 task instance dependencies not met : dagrun running : task instance s dagrun did not exist : unknown reason trigger rule : task s trigger rule all_success requires all upstream tasks to have succeeded but found 1 non - success ( es ) .,"['def', 'task_failed_deps', '(', 'args', ')', ':', 'dag', '=', 'get_dag', '(', 'args', ')', 'task', '=', 'dag', '.', 'get_task', '(', 'task_id', '=', 'args', '.', 'task_id', ')', 'ti', '=', 'taskinstance', '(', 'task', ',', 'args', '.', 'execution_date', ')', 'dep_context', '=', 'depcontext', '(', 'deps', '=', 'scheduler_deps', ')', 'failed_deps', '=', 'list', '(', 'ti', '.', 'get_failed_dep_statuses', '(', 'dep_context', '=', 'dep_context', ')', ')', '# todo, do we want to print or log this', 'if', 'failed_deps', ':', 'print', '(', '""task instance dependencies not met:""', ')', 'for', 'dep', 'in', 'failed_deps', ':', 'print', '(', '""{}: {}""', '.', 'format', '(', 'dep', '.', 'dep_name', ',', 'dep', '.', 'reason', ')', ')', 'else', ':', 'print', '(', '""task instance dependencies are all met.""', ')']","def task_failed_deps ( args ) : dag = get_dag ( args ) task = dag . get_task ( task_id = args . task_id ) ti = taskinstance ( task , args . execution_date ) dep_context = depcontext ( deps = scheduler_deps ) failed_deps = list ( ti . get_failed_dep_statuses ( dep_context = dep_context ) ) # todo, do we want to print or log this if failed_deps : print ( ""task instance dependencies not met:"" ) for dep in failed_deps : print ( ""{}: {}"" . format ( dep . dep_name , dep . reason ) ) else : print ( ""task instance dependencies are all met."" )"
73,apache/airflow,airflow/bin/cli.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/bin/cli.py#L554-L563,"def task_state(args):
    """"""
    Returns the state of a TaskInstance at the command line.
    >>> airflow task_state tutorial sleep 2015-01-01
    success
    """"""
    dag = get_dag(args)
    task = dag.get_task(task_id=args.task_id)
    ti = TaskInstance(task, args.execution_date)
    print(ti.current_state())","['def', 'task_state', '(', 'args', ')', ':', 'dag', '=', 'get_dag', '(', 'args', ')', 'task', '=', 'dag', '.', 'get_task', '(', 'task_id', '=', 'args', '.', 'task_id', ')', 'ti', '=', 'TaskInstance', '(', 'task', ',', 'args', '.', 'execution_date', ')', 'print', '(', 'ti', '.', 'current_state', '(', ')', ')']","Returns the state of a TaskInstance at the command line.
    >>> airflow task_state tutorial sleep 2015-01-01
    success","['Returns', 'the', 'state', 'of', 'a', 'TaskInstance', 'at', 'the', 'command', 'line', '.', '>>>', 'airflow', 'task_state', 'tutorial', 'sleep', '2015', '-', '01', '-', '01', 'success']",python,test,"['returns', 'the', 'state', 'of', 'a', 'taskinstance', 'at', 'the', 'command', 'line', '.', '>>>', 'airflow', 'task_state', 'tutorial', 'sleep', '2015', '-', '01', '-', '01', 'success']",returns the state of a taskinstance at the command line . >>> airflow task_state tutorial sleep 2015 - 01 - 01 success,"['def', 'task_state', '(', 'args', ')', ':', 'dag', '=', 'get_dag', '(', 'args', ')', 'task', '=', 'dag', '.', 'get_task', '(', 'task_id', '=', 'args', '.', 'task_id', ')', 'ti', '=', 'taskinstance', '(', 'task', ',', 'args', '.', 'execution_date', ')', 'print', '(', 'ti', '.', 'current_state', '(', ')', ')']","def task_state ( args ) : dag = get_dag ( args ) task = dag . get_task ( task_id = args . task_id ) ti = taskinstance ( task , args . execution_date ) print ( ti . current_state ( ) )"
74,apache/airflow,airflow/bin/cli.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/bin/cli.py#L567-L575,"def dag_state(args):
    """"""
    Returns the state of a DagRun at the command line.
    >>> airflow dag_state tutorial 2015-01-01T00:00:00.000000
    running
    """"""
    dag = get_dag(args)
    dr = DagRun.find(dag.dag_id, execution_date=args.execution_date)
    print(dr[0].state if len(dr) > 0 else None)","['def', 'dag_state', '(', 'args', ')', ':', 'dag', '=', 'get_dag', '(', 'args', ')', 'dr', '=', 'DagRun', '.', 'find', '(', 'dag', '.', 'dag_id', ',', 'execution_date', '=', 'args', '.', 'execution_date', ')', 'print', '(', 'dr', '[', '0', ']', '.', 'state', 'if', 'len', '(', 'dr', ')', '>', '0', 'else', 'None', ')']","Returns the state of a DagRun at the command line.
    >>> airflow dag_state tutorial 2015-01-01T00:00:00.000000
    running","['Returns', 'the', 'state', 'of', 'a', 'DagRun', 'at', 'the', 'command', 'line', '.', '>>>', 'airflow', 'dag_state', 'tutorial', '2015', '-', '01', '-', '01T00', ':', '00', ':', '00', '.', '000000', 'running']",python,test,"['returns', 'the', 'state', 'of', 'a', 'dagrun', 'at', 'the', 'command', 'line', '.', '>>>', 'airflow', 'dag_state', 'tutorial', '2015', '-', '01', '-', '01t00', ':', '00', ':', '00', '.', '000000', 'running']",returns the state of a dagrun at the command line . >>> airflow dag_state tutorial 2015 - 01 - 01t00 : 00 : 00 . 000000 running,"['def', 'dag_state', '(', 'args', ')', ':', 'dag', '=', 'get_dag', '(', 'args', ')', 'dr', '=', 'dagrun', '.', 'find', '(', 'dag', '.', 'dag_id', ',', 'execution_date', '=', 'args', '.', 'execution_date', ')', 'print', '(', 'dr', '[', '0', ']', '.', 'state', 'if', 'len', '(', 'dr', ')', '>', '0', 'else', 'none', ')']","def dag_state ( args ) : dag = get_dag ( args ) dr = dagrun . find ( dag . dag_id , execution_date = args . execution_date ) print ( dr [ 0 ] . state if len ( dr ) > 0 else none )"
75,apache/airflow,airflow/bin/cli.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/bin/cli.py#L579-L600,"def next_execution(args):
    """"""
    Returns the next execution datetime of a DAG at the command line.
    >>> airflow next_execution tutorial
    2018-08-31 10:38:00
    """"""
    dag = get_dag(args)

    if dag.is_paused:
        print(""[INFO] Please be reminded this DAG is PAUSED now."")

    if dag.latest_execution_date:
        next_execution_dttm = dag.following_schedule(dag.latest_execution_date)

        if next_execution_dttm is None:
            print(""[WARN] No following schedule can be found. "" +
                  ""This DAG may have schedule interval '@once' or `None`."")

        print(next_execution_dttm)
    else:
        print(""[WARN] Only applicable when there is execution record found for the DAG."")
        print(None)","['def', 'next_execution', '(', 'args', ')', ':', 'dag', '=', 'get_dag', '(', 'args', ')', 'if', 'dag', '.', 'is_paused', ':', 'print', '(', '""[INFO] Please be reminded this DAG is PAUSED now.""', ')', 'if', 'dag', '.', 'latest_execution_date', ':', 'next_execution_dttm', '=', 'dag', '.', 'following_schedule', '(', 'dag', '.', 'latest_execution_date', ')', 'if', 'next_execution_dttm', 'is', 'None', ':', 'print', '(', '""[WARN] No following schedule can be found. ""', '+', '""This DAG may have schedule interval \'@once\' or `None`.""', ')', 'print', '(', 'next_execution_dttm', ')', 'else', ':', 'print', '(', '""[WARN] Only applicable when there is execution record found for the DAG.""', ')', 'print', '(', 'None', ')']","Returns the next execution datetime of a DAG at the command line.
    >>> airflow next_execution tutorial
    2018-08-31 10:38:00","['Returns', 'the', 'next', 'execution', 'datetime', 'of', 'a', 'DAG', 'at', 'the', 'command', 'line', '.', '>>>', 'airflow', 'next_execution', 'tutorial', '2018', '-', '08', '-', '31', '10', ':', '38', ':', '00']",python,test,"['returns', 'the', 'next', 'execution', 'datetime', 'of', 'a', 'dag', 'at', 'the', 'command', 'line', '.', '>>>', 'airflow', 'next_execution', 'tutorial', '2018', '-', '08', '-', '31', '10', ':', '38', ':', '00']",returns the next execution datetime of a dag at the command line . >>> airflow next_execution tutorial 2018 - 08 - 31 10 : 38 : 00,"['def', 'next_execution', '(', 'args', ')', ':', 'dag', '=', 'get_dag', '(', 'args', ')', 'if', 'dag', '.', 'is_paused', ':', 'print', '(', '""[info] please be reminded this dag is paused now.""', ')', 'if', 'dag', '.', 'latest_execution_date', ':', 'next_execution_dttm', '=', 'dag', '.', 'following_schedule', '(', 'dag', '.', 'latest_execution_date', ')', 'if', 'next_execution_dttm', 'is', 'none', ':', 'print', '(', '""[warn] no following schedule can be found. ""', '+', '""this dag may have schedule interval \'@once\' or `none`.""', ')', 'print', '(', 'next_execution_dttm', ')', 'else', ':', 'print', '(', '""[warn] only applicable when there is execution record found for the dag.""', ')', 'print', '(', 'none', ')']","def next_execution ( args ) : dag = get_dag ( args ) if dag . is_paused : print ( ""[info] please be reminded this dag is paused now."" ) if dag . latest_execution_date : next_execution_dttm = dag . following_schedule ( dag . latest_execution_date ) if next_execution_dttm is none : print ( ""[warn] no following schedule can be found. "" + ""this dag may have schedule interval '@once' or `none`."" ) print ( next_execution_dttm ) else : print ( ""[warn] only applicable when there is execution record found for the dag."" ) print ( none )"
76,apache/airflow,airflow/bin/cli.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/bin/cli.py#L763-L868,"def restart_workers(gunicorn_master_proc, num_workers_expected, master_timeout):
    """"""
    Runs forever, monitoring the child processes of @gunicorn_master_proc and
    restarting workers occasionally.
    Each iteration of the loop traverses one edge of this state transition
    diagram, where each state (node) represents
    [ num_ready_workers_running / num_workers_running ]. We expect most time to
    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.
    The horizontal transition at ? happens after the new worker parses all the
    dags (so it could take a while!)
       V ────────────────────────────────────────────────────────────────────────┐
    [n / n] ──TTIN──> [ [n, n+bs) / n + bs ]  ────?───> [n + bs / n + bs] ──TTOU─┘
       ^                          ^───────────────┘
       │
       │      ┌────────────────v
       └──────┴────── [ [0, n) / n ] <─── start
    We change the number of workers by sending TTIN and TTOU to the gunicorn
    master process, which increases and decreases the number of child workers
    respectively. Gunicorn guarantees that on TTOU workers are terminated
    gracefully and that the oldest worker is terminated.
    """"""

    def wait_until_true(fn, timeout=0):
        """"""
        Sleeps until fn is true
        """"""
        t = time.time()
        while not fn():
            if 0 < timeout <= time.time() - t:
                raise AirflowWebServerTimeout(
                    ""No response from gunicorn master within {0} seconds""
                    .format(timeout))
            time.sleep(0.1)

    def start_refresh(gunicorn_master_proc):
        batch_size = conf.getint('webserver', 'worker_refresh_batch_size')
        log.debug('%s doing a refresh of %s workers', state, batch_size)
        sys.stdout.flush()
        sys.stderr.flush()

        excess = 0
        for _ in range(batch_size):
            gunicorn_master_proc.send_signal(signal.SIGTTIN)
            excess += 1
            wait_until_true(lambda: num_workers_expected + excess ==
                            get_num_workers_running(gunicorn_master_proc),
                            master_timeout)

    try:
        wait_until_true(lambda: num_workers_expected ==
                        get_num_workers_running(gunicorn_master_proc),
                        master_timeout)
        while True:
            num_workers_running = get_num_workers_running(gunicorn_master_proc)
            num_ready_workers_running = \
                get_num_ready_workers_running(gunicorn_master_proc)

            state = '[{0} / {1}]'.format(num_ready_workers_running, num_workers_running)

            # Whenever some workers are not ready, wait until all workers are ready
            if num_ready_workers_running < num_workers_running:
                log.debug('%s some workers are starting up, waiting...', state)
                sys.stdout.flush()
                time.sleep(1)

            # Kill a worker gracefully by asking gunicorn to reduce number of workers
            elif num_workers_running > num_workers_expected:
                excess = num_workers_running - num_workers_expected
                log.debug('%s killing %s workers', state, excess)

                for _ in range(excess):
                    gunicorn_master_proc.send_signal(signal.SIGTTOU)
                    excess -= 1
                    wait_until_true(lambda: num_workers_expected + excess ==
                                    get_num_workers_running(gunicorn_master_proc),
                                    master_timeout)

            # Start a new worker by asking gunicorn to increase number of workers
            elif num_workers_running == num_workers_expected:
                refresh_interval = conf.getint('webserver', 'worker_refresh_interval')
                log.debug(
                    '%s sleeping for %ss starting doing a refresh...',
                    state, refresh_interval
                )
                time.sleep(refresh_interval)
                start_refresh(gunicorn_master_proc)

            else:
                # num_ready_workers_running == num_workers_running < num_workers_expected
                log.error((
                    ""%s some workers seem to have died and gunicorn""
                    ""did not restart them as expected""
                ), state)
                time.sleep(10)
                if len(
                    psutil.Process(gunicorn_master_proc.pid).children()
                ) < num_workers_expected:
                    start_refresh(gunicorn_master_proc)
    except (AirflowWebServerTimeout, OSError) as err:
        log.error(err)
        log.error(""Shutting down webserver"")
        try:
            gunicorn_master_proc.terminate()
            gunicorn_master_proc.wait()
        finally:
            sys.exit(1)","['def', 'restart_workers', '(', 'gunicorn_master_proc', ',', 'num_workers_expected', ',', 'master_timeout', ')', ':', 'def', 'wait_until_true', '(', 'fn', ',', 'timeout', '=', '0', ')', ':', '""""""\n        Sleeps until fn is true\n        """"""', 't', '=', 'time', '.', 'time', '(', ')', 'while', 'not', 'fn', '(', ')', ':', 'if', '0', '<', 'timeout', '<=', 'time', '.', 'time', '(', ')', '-', 't', ':', 'raise', 'AirflowWebServerTimeout', '(', '""No response from gunicorn master within {0} seconds""', '.', 'format', '(', 'timeout', ')', ')', 'time', '.', 'sleep', '(', '0.1', ')', 'def', 'start_refresh', '(', 'gunicorn_master_proc', ')', ':', 'batch_size', '=', 'conf', '.', 'getint', '(', ""'webserver'"", ',', ""'worker_refresh_batch_size'"", ')', 'log', '.', 'debug', '(', ""'%s doing a refresh of %s workers'"", ',', 'state', ',', 'batch_size', ')', 'sys', '.', 'stdout', '.', 'flush', '(', ')', 'sys', '.', 'stderr', '.', 'flush', '(', ')', 'excess', '=', '0', 'for', '_', 'in', 'range', '(', 'batch_size', ')', ':', 'gunicorn_master_proc', '.', 'send_signal', '(', 'signal', '.', 'SIGTTIN', ')', 'excess', '+=', '1', 'wait_until_true', '(', 'lambda', ':', 'num_workers_expected', '+', 'excess', '==', 'get_num_workers_running', '(', 'gunicorn_master_proc', ')', ',', 'master_timeout', ')', 'try', ':', 'wait_until_true', '(', 'lambda', ':', 'num_workers_expected', '==', 'get_num_workers_running', '(', 'gunicorn_master_proc', ')', ',', 'master_timeout', ')', 'while', 'True', ':', 'num_workers_running', '=', 'get_num_workers_running', '(', 'gunicorn_master_proc', ')', 'num_ready_workers_running', '=', 'get_num_ready_workers_running', '(', 'gunicorn_master_proc', ')', 'state', '=', ""'[{0} / {1}]'"", '.', 'format', '(', 'num_ready_workers_running', ',', 'num_workers_running', ')', '# Whenever some workers are not ready, wait until all workers are ready', 'if', 'num_ready_workers_running', '<', 'num_workers_running', ':', 'log', '.', 'debug', '(', ""'%s some workers are starting up, waiting...'"", ',', 'state', ')', 'sys', '.', 'stdout', '.', 'flush', '(', ')', 'time', '.', 'sleep', '(', '1', ')', '# Kill a worker gracefully by asking gunicorn to reduce number of workers', 'elif', 'num_workers_running', '>', 'num_workers_expected', ':', 'excess', '=', 'num_workers_running', '-', 'num_workers_expected', 'log', '.', 'debug', '(', ""'%s killing %s workers'"", ',', 'state', ',', 'excess', ')', 'for', '_', 'in', 'range', '(', 'excess', ')', ':', 'gunicorn_master_proc', '.', 'send_signal', '(', 'signal', '.', 'SIGTTOU', ')', 'excess', '-=', '1', 'wait_until_true', '(', 'lambda', ':', 'num_workers_expected', '+', 'excess', '==', 'get_num_workers_running', '(', 'gunicorn_master_proc', ')', ',', 'master_timeout', ')', '# Start a new worker by asking gunicorn to increase number of workers', 'elif', 'num_workers_running', '==', 'num_workers_expected', ':', 'refresh_interval', '=', 'conf', '.', 'getint', '(', ""'webserver'"", ',', ""'worker_refresh_interval'"", ')', 'log', '.', 'debug', '(', ""'%s sleeping for %ss starting doing a refresh...'"", ',', 'state', ',', 'refresh_interval', ')', 'time', '.', 'sleep', '(', 'refresh_interval', ')', 'start_refresh', '(', 'gunicorn_master_proc', ')', 'else', ':', '# num_ready_workers_running == num_workers_running < num_workers_expected', 'log', '.', 'error', '(', '(', '""%s some workers seem to have died and gunicorn""', '""did not restart them as expected""', ')', ',', 'state', ')', 'time', '.', 'sleep', '(', '10', ')', 'if', 'len', '(', 'psutil', '.', 'Process', '(', 'gunicorn_master_proc', '.', 'pid', ')', '.', 'children', '(', ')', ')', '<', 'num_workers_expected', ':', 'start_refresh', '(', 'gunicorn_master_proc', ')', 'except', '(', 'AirflowWebServerTimeout', ',', 'OSError', ')', 'as', 'err', ':', 'log', '.', 'error', '(', 'err', ')', 'log', '.', 'error', '(', '""Shutting down webserver""', ')', 'try', ':', 'gunicorn_master_proc', '.', 'terminate', '(', ')', 'gunicorn_master_proc', '.', 'wait', '(', ')', 'finally', ':', 'sys', '.', 'exit', '(', '1', ')']","Runs forever, monitoring the child processes of @gunicorn_master_proc and
    restarting workers occasionally.
    Each iteration of the loop traverses one edge of this state transition
    diagram, where each state (node) represents
    [ num_ready_workers_running / num_workers_running ]. We expect most time to
    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.
    The horizontal transition at ? happens after the new worker parses all the
    dags (so it could take a while!)
       V ────────────────────────────────────────────────────────────────────────┐
    [n / n] ──TTIN──> [ [n, n+bs) / n + bs ]  ────?───> [n + bs / n + bs] ──TTOU─┘
       ^                          ^───────────────┘
       │
       │      ┌────────────────v
       └──────┴────── [ [0, n) / n ] <─── start
    We change the number of workers by sending TTIN and TTOU to the gunicorn
    master process, which increases and decreases the number of child workers
    respectively. Gunicorn guarantees that on TTOU workers are terminated
    gracefully and that the oldest worker is terminated.","['Runs', 'forever', 'monitoring', 'the', 'child', 'processes', 'of']",python,test,"['runs', 'forever', 'monitoring', 'the', 'child', 'processes', 'of']",runs forever monitoring the child processes of,"['def', 'restart_workers', '(', 'gunicorn_master_proc', ',', 'num_workers_expected', ',', 'master_timeout', ')', ':', 'def', 'wait_until_true', '(', 'fn', ',', 'timeout', '=', '0', ')', ':', '""""""\n        sleeps until fn is true\n        """"""', 't', '=', 'time', '.', 'time', '(', ')', 'while', 'not', 'fn', '(', ')', ':', 'if', '0', '<', 'timeout', '<=', 'time', '.', 'time', '(', ')', '-', 't', ':', 'raise', 'airflowwebservertimeout', '(', '""no response from gunicorn master within {0} seconds""', '.', 'format', '(', 'timeout', ')', ')', 'time', '.', 'sleep', '(', '0.1', ')', 'def', 'start_refresh', '(', 'gunicorn_master_proc', ')', ':', 'batch_size', '=', 'conf', '.', 'getint', '(', ""'webserver'"", ',', ""'worker_refresh_batch_size'"", ')', 'log', '.', 'debug', '(', ""'%s doing a refresh of %s workers'"", ',', 'state', ',', 'batch_size', ')', 'sys', '.', 'stdout', '.', 'flush', '(', ')', 'sys', '.', 'stderr', '.', 'flush', '(', ')', 'excess', '=', '0', 'for', '_', 'in', 'range', '(', 'batch_size', ')', ':', 'gunicorn_master_proc', '.', 'send_signal', '(', 'signal', '.', 'sigttin', ')', 'excess', '+=', '1', 'wait_until_true', '(', 'lambda', ':', 'num_workers_expected', '+', 'excess', '==', 'get_num_workers_running', '(', 'gunicorn_master_proc', ')', ',', 'master_timeout', ')', 'try', ':', 'wait_until_true', '(', 'lambda', ':', 'num_workers_expected', '==', 'get_num_workers_running', '(', 'gunicorn_master_proc', ')', ',', 'master_timeout', ')', 'while', 'true', ':', 'num_workers_running', '=', 'get_num_workers_running', '(', 'gunicorn_master_proc', ')', 'num_ready_workers_running', '=', 'get_num_ready_workers_running', '(', 'gunicorn_master_proc', ')', 'state', '=', ""'[{0} / {1}]'"", '.', 'format', '(', 'num_ready_workers_running', ',', 'num_workers_running', ')', '# whenever some workers are not ready, wait until all workers are ready', 'if', 'num_ready_workers_running', '<', 'num_workers_running', ':', 'log', '.', 'debug', '(', ""'%s some workers are starting up, waiting...'"", ',', 'state', ')', 'sys', '.', 'stdout', '.', 'flush', '(', ')', 'time', '.', 'sleep', '(', '1', ')', '# kill a worker gracefully by asking gunicorn to reduce number of workers', 'elif', 'num_workers_running', '>', 'num_workers_expected', ':', 'excess', '=', 'num_workers_running', '-', 'num_workers_expected', 'log', '.', 'debug', '(', ""'%s killing %s workers'"", ',', 'state', ',', 'excess', ')', 'for', '_', 'in', 'range', '(', 'excess', ')', ':', 'gunicorn_master_proc', '.', 'send_signal', '(', 'signal', '.', 'sigttou', ')', 'excess', '-=', '1', 'wait_until_true', '(', 'lambda', ':', 'num_workers_expected', '+', 'excess', '==', 'get_num_workers_running', '(', 'gunicorn_master_proc', ')', ',', 'master_timeout', ')', '# start a new worker by asking gunicorn to increase number of workers', 'elif', 'num_workers_running', '==', 'num_workers_expected', ':', 'refresh_interval', '=', 'conf', '.', 'getint', '(', ""'webserver'"", ',', ""'worker_refresh_interval'"", ')', 'log', '.', 'debug', '(', ""'%s sleeping for %ss starting doing a refresh...'"", ',', 'state', ',', 'refresh_interval', ')', 'time', '.', 'sleep', '(', 'refresh_interval', ')', 'start_refresh', '(', 'gunicorn_master_proc', ')', 'else', ':', '# num_ready_workers_running == num_workers_running < num_workers_expected', 'log', '.', 'error', '(', '(', '""%s some workers seem to have died and gunicorn""', '""did not restart them as expected""', ')', ',', 'state', ')', 'time', '.', 'sleep', '(', '10', ')', 'if', 'len', '(', 'psutil', '.', 'process', '(', 'gunicorn_master_proc', '.', 'pid', ')', '.', 'children', '(', ')', ')', '<', 'num_workers_expected', ':', 'start_refresh', '(', 'gunicorn_master_proc', ')', 'except', '(', 'airflowwebservertimeout', ',', 'oserror', ')', 'as', 'err', ':', 'log', '.', 'error', '(', 'err', ')', 'log', '.', 'error', '(', '""shutting down webserver""', ')', 'try', ':', 'gunicorn_master_proc', '.', 'terminate', '(', ')', 'gunicorn_master_proc', '.', 'wait', '(', ')', 'finally', ':', 'sys', '.', 'exit', '(', '1', ')']","def restart_workers ( gunicorn_master_proc , num_workers_expected , master_timeout ) : def wait_until_true ( fn , timeout = 0 ) : """"""
        sleeps until fn is true
        """""" t = time . time ( ) while not fn ( ) : if 0 < timeout <= time . time ( ) - t : raise airflowwebservertimeout ( ""no response from gunicorn master within {0} seconds"" . format ( timeout ) ) time . sleep ( 0.1 ) def start_refresh ( gunicorn_master_proc ) : batch_size = conf . getint ( 'webserver' , 'worker_refresh_batch_size' ) log . debug ( '%s doing a refresh of %s workers' , state , batch_size ) sys . stdout . flush ( ) sys . stderr . flush ( ) excess = 0 for _ in range ( batch_size ) : gunicorn_master_proc . send_signal ( signal . sigttin ) excess += 1 wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) try : wait_until_true ( lambda : num_workers_expected == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) while true : num_workers_running = get_num_workers_running ( gunicorn_master_proc ) num_ready_workers_running = get_num_ready_workers_running ( gunicorn_master_proc ) state = '[{0} / {1}]' . format ( num_ready_workers_running , num_workers_running ) # whenever some workers are not ready, wait until all workers are ready if num_ready_workers_running < num_workers_running : log . debug ( '%s some workers are starting up, waiting...' , state ) sys . stdout . flush ( ) time . sleep ( 1 ) # kill a worker gracefully by asking gunicorn to reduce number of workers elif num_workers_running > num_workers_expected : excess = num_workers_running - num_workers_expected log . debug ( '%s killing %s workers' , state , excess ) for _ in range ( excess ) : gunicorn_master_proc . send_signal ( signal . sigttou ) excess -= 1 wait_until_true ( lambda : num_workers_expected + excess == get_num_workers_running ( gunicorn_master_proc ) , master_timeout ) # start a new worker by asking gunicorn to increase number of workers elif num_workers_running == num_workers_expected : refresh_interval = conf . getint ( 'webserver' , 'worker_refresh_interval' ) log . debug ( '%s sleeping for %ss starting doing a refresh...' , state , refresh_interval ) time . sleep ( refresh_interval ) start_refresh ( gunicorn_master_proc ) else : # num_ready_workers_running == num_workers_running < num_workers_expected log . error ( ( ""%s some workers seem to have died and gunicorn"" ""did not restart them as expected"" ) , state ) time . sleep ( 10 ) if len ( psutil . process ( gunicorn_master_proc . pid ) . children ( ) ) < num_workers_expected : start_refresh ( gunicorn_master_proc ) except ( airflowwebservertimeout , oserror ) as err : log . error ( err ) log . error ( ""shutting down webserver"" ) try : gunicorn_master_proc . terminate ( ) gunicorn_master_proc . wait ( ) finally : sys . exit ( 1 )"
77,apache/airflow,airflow/contrib/hooks/gcp_translate_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_translate_hook.py#L34-L43,"def get_conn(self):
        """"""
        Retrieves connection to Cloud Translate

        :return: Google Cloud Translate client object.
        :rtype: Client
        """"""
        if not self._client:
            self._client = Client(credentials=self._get_credentials())
        return self._client","['def', 'get_conn', '(', 'self', ')', ':', 'if', 'not', 'self', '.', '_client', ':', 'self', '.', '_client', '=', 'Client', '(', 'credentials', '=', 'self', '.', '_get_credentials', '(', ')', ')', 'return', 'self', '.', '_client']","Retrieves connection to Cloud Translate

        :return: Google Cloud Translate client object.
        :rtype: Client","['Retrieves', 'connection', 'to', 'Cloud', 'Translate']",python,test,"['retrieves', 'connection', 'to', 'cloud', 'translate']",retrieves connection to cloud translate,"['def', 'get_conn', '(', 'self', ')', ':', 'if', 'not', 'self', '.', '_client', ':', 'self', '.', '_client', '=', 'client', '(', 'credentials', '=', 'self', '.', '_get_credentials', '(', ')', ')', 'return', 'self', '.', '_client']",def get_conn ( self ) : if not self . _client : self . _client = client ( credentials = self . _get_credentials ( ) ) return self . _client
78,apache/airflow,airflow/contrib/hooks/gcp_translate_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_translate_hook.py#L45-L97,"def translate(
        self, values, target_language, format_=None, source_language=None, model=None
    ):
        """"""Translate a string or list of strings.

        See https://cloud.google.com/translate/docs/translating-text

        :type values: str or list
        :param values: String or list of strings to translate.

        :type target_language: str
        :param target_language: The language to translate results into. This
                                is required by the API and defaults to
                                the target language of the current instance.

        :type format_: str
        :param format_: (Optional) One of ``text`` or ``html``, to specify
                        if the input text is plain text or HTML.

        :type source_language: str or None
        :param source_language: (Optional) The language of the text to
                                be translated.

        :type model: str or None
        :param model: (Optional) The model used to translate the text, such
                      as ``'base'`` or ``'nmt'``.

        :rtype: str or list
        :returns: A list of dictionaries for each queried value. Each
                  dictionary typically contains three keys (though not
                  all will be present in all cases)

                  * ``detectedSourceLanguage``: The detected language (as an
                    ISO 639-1 language code) of the text.
                  * ``translatedText``: The translation of the text into the
                    target language.
                  * ``input``: The corresponding input value.
                  * ``model``: The model used to translate the text.

                  If only a single value is passed, then only a single
                  dictionary will be returned.
        :raises: :class:`~exceptions.ValueError` if the number of
                 values and translations differ.
        """"""
        client = self.get_conn()

        return client.translate(
            values=values,
            target_language=target_language,
            format_=format_,
            source_language=source_language,
            model=model,
        )","['def', 'translate', '(', 'self', ',', 'values', ',', 'target_language', ',', 'format_', '=', 'None', ',', 'source_language', '=', 'None', ',', 'model', '=', 'None', ')', ':', 'client', '=', 'self', '.', 'get_conn', '(', ')', 'return', 'client', '.', 'translate', '(', 'values', '=', 'values', ',', 'target_language', '=', 'target_language', ',', 'format_', '=', 'format_', ',', 'source_language', '=', 'source_language', ',', 'model', '=', 'model', ',', ')']","Translate a string or list of strings.

        See https://cloud.google.com/translate/docs/translating-text

        :type values: str or list
        :param values: String or list of strings to translate.

        :type target_language: str
        :param target_language: The language to translate results into. This
                                is required by the API and defaults to
                                the target language of the current instance.

        :type format_: str
        :param format_: (Optional) One of ``text`` or ``html``, to specify
                        if the input text is plain text or HTML.

        :type source_language: str or None
        :param source_language: (Optional) The language of the text to
                                be translated.

        :type model: str or None
        :param model: (Optional) The model used to translate the text, such
                      as ``'base'`` or ``'nmt'``.

        :rtype: str or list
        :returns: A list of dictionaries for each queried value. Each
                  dictionary typically contains three keys (though not
                  all will be present in all cases)

                  * ``detectedSourceLanguage``: The detected language (as an
                    ISO 639-1 language code) of the text.
                  * ``translatedText``: The translation of the text into the
                    target language.
                  * ``input``: The corresponding input value.
                  * ``model``: The model used to translate the text.

                  If only a single value is passed, then only a single
                  dictionary will be returned.
        :raises: :class:`~exceptions.ValueError` if the number of
                 values and translations differ.","['Translate', 'a', 'string', 'or', 'list', 'of', 'strings', '.']",python,test,"['translate', 'a', 'string', 'or', 'list', 'of', 'strings', '.']",translate a string or list of strings .,"['def', 'translate', '(', 'self', ',', 'values', ',', 'target_language', ',', 'format_', '=', 'none', ',', 'source_language', '=', 'none', ',', 'model', '=', 'none', ')', ':', 'client', '=', 'self', '.', 'get_conn', '(', ')', 'return', 'client', '.', 'translate', '(', 'values', '=', 'values', ',', 'target_language', '=', 'target_language', ',', 'format_', '=', 'format_', ',', 'source_language', '=', 'source_language', ',', 'model', '=', 'model', ',', ')']","def translate ( self , values , target_language , format_ = none , source_language = none , model = none ) : client = self . get_conn ( ) return client . translate ( values = values , target_language = target_language , format_ = format_ , source_language = source_language , model = model , )"
79,apache/airflow,airflow/operators/bash_operator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/operators/bash_operator.py#L86-L144,"def execute(self, context):
        """"""
        Execute the bash command in a temporary directory
        which will be cleaned afterwards
        """"""
        self.log.info('Tmp dir root location: \n %s', gettempdir())

        # Prepare env for child process.
        if self.env is None:
            self.env = os.environ.copy()

        airflow_context_vars = context_to_airflow_vars(context, in_env_var_format=True)
        self.log.info('Exporting the following env vars:\n%s',
                      '\n'.join([""{}={}"".format(k, v)
                                 for k, v in
                                 airflow_context_vars.items()]))
        self.env.update(airflow_context_vars)

        self.lineage_data = self.bash_command

        with TemporaryDirectory(prefix='airflowtmp') as tmp_dir:
            with NamedTemporaryFile(dir=tmp_dir, prefix=self.task_id) as tmp_file:
                tmp_file.write(bytes(self.bash_command, 'utf_8'))
                tmp_file.flush()
                script_location = os.path.abspath(tmp_file.name)
                self.log.info('Temporary script location: %s', script_location)

                def pre_exec():
                    # Restore default signal disposition and invoke setsid
                    for sig in ('SIGPIPE', 'SIGXFZ', 'SIGXFSZ'):
                        if hasattr(signal, sig):
                            signal.signal(getattr(signal, sig), signal.SIG_DFL)
                    os.setsid()

                self.log.info('Running command: %s', self.bash_command)
                sub_process = Popen(
                    ['bash', tmp_file.name],
                    stdout=PIPE,
                    stderr=STDOUT,
                    cwd=tmp_dir,
                    env=self.env,
                    preexec_fn=pre_exec)

                self.sub_process = sub_process

                self.log.info('Output:')
                line = ''
                for raw_line in iter(sub_process.stdout.readline, b''):
                    line = raw_line.decode(self.output_encoding).rstrip()
                    self.log.info(line)

                sub_process.wait()

                self.log.info('Command exited with return code %s', sub_process.returncode)

                if sub_process.returncode:
                    raise AirflowException('Bash command failed')

        return line","['def', 'execute', '(', 'self', ',', 'context', ')', ':', 'self', '.', 'log', '.', 'info', '(', ""'Tmp dir root location: \\n %s'"", ',', 'gettempdir', '(', ')', ')', '# Prepare env for child process.', 'if', 'self', '.', 'env', 'is', 'None', ':', 'self', '.', 'env', '=', 'os', '.', 'environ', '.', 'copy', '(', ')', 'airflow_context_vars', '=', 'context_to_airflow_vars', '(', 'context', ',', 'in_env_var_format', '=', 'True', ')', 'self', '.', 'log', '.', 'info', '(', ""'Exporting the following env vars:\\n%s'"", ',', ""'\\n'"", '.', 'join', '(', '[', '""{}={}""', '.', 'format', '(', 'k', ',', 'v', ')', 'for', 'k', ',', 'v', 'in', 'airflow_context_vars', '.', 'items', '(', ')', ']', ')', ')', 'self', '.', 'env', '.', 'update', '(', 'airflow_context_vars', ')', 'self', '.', 'lineage_data', '=', 'self', '.', 'bash_command', 'with', 'TemporaryDirectory', '(', 'prefix', '=', ""'airflowtmp'"", ')', 'as', 'tmp_dir', ':', 'with', 'NamedTemporaryFile', '(', 'dir', '=', 'tmp_dir', ',', 'prefix', '=', 'self', '.', 'task_id', ')', 'as', 'tmp_file', ':', 'tmp_file', '.', 'write', '(', 'bytes', '(', 'self', '.', 'bash_command', ',', ""'utf_8'"", ')', ')', 'tmp_file', '.', 'flush', '(', ')', 'script_location', '=', 'os', '.', 'path', '.', 'abspath', '(', 'tmp_file', '.', 'name', ')', 'self', '.', 'log', '.', 'info', '(', ""'Temporary script location: %s'"", ',', 'script_location', ')', 'def', 'pre_exec', '(', ')', ':', '# Restore default signal disposition and invoke setsid', 'for', 'sig', 'in', '(', ""'SIGPIPE'"", ',', ""'SIGXFZ'"", ',', ""'SIGXFSZ'"", ')', ':', 'if', 'hasattr', '(', 'signal', ',', 'sig', ')', ':', 'signal', '.', 'signal', '(', 'getattr', '(', 'signal', ',', 'sig', ')', ',', 'signal', '.', 'SIG_DFL', ')', 'os', '.', 'setsid', '(', ')', 'self', '.', 'log', '.', 'info', '(', ""'Running command: %s'"", ',', 'self', '.', 'bash_command', ')', 'sub_process', '=', 'Popen', '(', '[', ""'bash'"", ',', 'tmp_file', '.', 'name', ']', ',', 'stdout', '=', 'PIPE', ',', 'stderr', '=', 'STDOUT', ',', 'cwd', '=', 'tmp_dir', ',', 'env', '=', 'self', '.', 'env', ',', 'preexec_fn', '=', 'pre_exec', ')', 'self', '.', 'sub_process', '=', 'sub_process', 'self', '.', 'log', '.', 'info', '(', ""'Output:'"", ')', 'line', '=', ""''"", 'for', 'raw_line', 'in', 'iter', '(', 'sub_process', '.', 'stdout', '.', 'readline', ',', ""b''"", ')', ':', 'line', '=', 'raw_line', '.', 'decode', '(', 'self', '.', 'output_encoding', ')', '.', 'rstrip', '(', ')', 'self', '.', 'log', '.', 'info', '(', 'line', ')', 'sub_process', '.', 'wait', '(', ')', 'self', '.', 'log', '.', 'info', '(', ""'Command exited with return code %s'"", ',', 'sub_process', '.', 'returncode', ')', 'if', 'sub_process', '.', 'returncode', ':', 'raise', 'AirflowException', '(', ""'Bash command failed'"", ')', 'return', 'line']","Execute the bash command in a temporary directory
        which will be cleaned afterwards","['Execute', 'the', 'bash', 'command', 'in', 'a', 'temporary', 'directory', 'which', 'will', 'be', 'cleaned', 'afterwards']",python,test,"['execute', 'the', 'bash', 'command', 'in', 'a', 'temporary', 'directory', 'which', 'will', 'be', 'cleaned', 'afterwards']",execute the bash command in a temporary directory which will be cleaned afterwards,"['def', 'execute', '(', 'self', ',', 'context', ')', ':', 'self', '.', 'log', '.', 'info', '(', ""'tmp dir root location: \\n %s'"", ',', 'gettempdir', '(', ')', ')', '# prepare env for child process.', 'if', 'self', '.', 'env', 'is', 'none', ':', 'self', '.', 'env', '=', 'os', '.', 'environ', '.', 'copy', '(', ')', 'airflow_context_vars', '=', 'context_to_airflow_vars', '(', 'context', ',', 'in_env_var_format', '=', 'true', ')', 'self', '.', 'log', '.', 'info', '(', ""'exporting the following env vars:\\n%s'"", ',', ""'\\n'"", '.', 'join', '(', '[', '""{}={}""', '.', 'format', '(', 'k', ',', 'v', ')', 'for', 'k', ',', 'v', 'in', 'airflow_context_vars', '.', 'items', '(', ')', ']', ')', ')', 'self', '.', 'env', '.', 'update', '(', 'airflow_context_vars', ')', 'self', '.', 'lineage_data', '=', 'self', '.', 'bash_command', 'with', 'temporarydirectory', '(', 'prefix', '=', ""'airflowtmp'"", ')', 'as', 'tmp_dir', ':', 'with', 'namedtemporaryfile', '(', 'dir', '=', 'tmp_dir', ',', 'prefix', '=', 'self', '.', 'task_id', ')', 'as', 'tmp_file', ':', 'tmp_file', '.', 'write', '(', 'bytes', '(', 'self', '.', 'bash_command', ',', ""'utf_8'"", ')', ')', 'tmp_file', '.', 'flush', '(', ')', 'script_location', '=', 'os', '.', 'path', '.', 'abspath', '(', 'tmp_file', '.', 'name', ')', 'self', '.', 'log', '.', 'info', '(', ""'temporary script location: %s'"", ',', 'script_location', ')', 'def', 'pre_exec', '(', ')', ':', '# restore default signal disposition and invoke setsid', 'for', 'sig', 'in', '(', ""'sigpipe'"", ',', ""'sigxfz'"", ',', ""'sigxfsz'"", ')', ':', 'if', 'hasattr', '(', 'signal', ',', 'sig', ')', ':', 'signal', '.', 'signal', '(', 'getattr', '(', 'signal', ',', 'sig', ')', ',', 'signal', '.', 'sig_dfl', ')', 'os', '.', 'setsid', '(', ')', 'self', '.', 'log', '.', 'info', '(', ""'running command: %s'"", ',', 'self', '.', 'bash_command', ')', 'sub_process', '=', 'popen', '(', '[', ""'bash'"", ',', 'tmp_file', '.', 'name', ']', ',', 'stdout', '=', 'pipe', ',', 'stderr', '=', 'stdout', ',', 'cwd', '=', 'tmp_dir', ',', 'env', '=', 'self', '.', 'env', ',', 'preexec_fn', '=', 'pre_exec', ')', 'self', '.', 'sub_process', '=', 'sub_process', 'self', '.', 'log', '.', 'info', '(', ""'output:'"", ')', 'line', '=', ""''"", 'for', 'raw_line', 'in', 'iter', '(', 'sub_process', '.', 'stdout', '.', 'readline', ',', ""b''"", ')', ':', 'line', '=', 'raw_line', '.', 'decode', '(', 'self', '.', 'output_encoding', ')', '.', 'rstrip', '(', ')', 'self', '.', 'log', '.', 'info', '(', 'line', ')', 'sub_process', '.', 'wait', '(', ')', 'self', '.', 'log', '.', 'info', '(', ""'command exited with return code %s'"", ',', 'sub_process', '.', 'returncode', ')', 'if', 'sub_process', '.', 'returncode', ':', 'raise', 'airflowexception', '(', ""'bash command failed'"", ')', 'return', 'line']","def execute ( self , context ) : self . log . info ( 'tmp dir root location: \n %s' , gettempdir ( ) ) # prepare env for child process. if self . env is none : self . env = os . environ . copy ( ) airflow_context_vars = context_to_airflow_vars ( context , in_env_var_format = true ) self . log . info ( 'exporting the following env vars:\n%s' , '\n' . join ( [ ""{}={}"" . format ( k , v ) for k , v in airflow_context_vars . items ( ) ] ) ) self . env . update ( airflow_context_vars ) self . lineage_data = self . bash_command with temporarydirectory ( prefix = 'airflowtmp' ) as tmp_dir : with namedtemporaryfile ( dir = tmp_dir , prefix = self . task_id ) as tmp_file : tmp_file . write ( bytes ( self . bash_command , 'utf_8' ) ) tmp_file . flush ( ) script_location = os . path . abspath ( tmp_file . name ) self . log . info ( 'temporary script location: %s' , script_location ) def pre_exec ( ) : # restore default signal disposition and invoke setsid for sig in ( 'sigpipe' , 'sigxfz' , 'sigxfsz' ) : if hasattr ( signal , sig ) : signal . signal ( getattr ( signal , sig ) , signal . sig_dfl ) os . setsid ( ) self . log . info ( 'running command: %s' , self . bash_command ) sub_process = popen ( [ 'bash' , tmp_file . name ] , stdout = pipe , stderr = stdout , cwd = tmp_dir , env = self . env , preexec_fn = pre_exec ) self . sub_process = sub_process self . log . info ( 'output:' ) line = '' for raw_line in iter ( sub_process . stdout . readline , b'' ) : line = raw_line . decode ( self . output_encoding ) . rstrip ( ) self . log . info ( line ) sub_process . wait ( ) self . log . info ( 'command exited with return code %s' , sub_process . returncode ) if sub_process . returncode : raise airflowexception ( 'bash command failed' ) return line"
80,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L97-L112,"def get_instance(self, instance, project_id=None):
        """"""
        Retrieves a resource containing information about a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: A Cloud SQL instance resource.
        :rtype: dict
        """"""
        return self.get_conn().instances().get(
            project=project_id,
            instance=instance
        ).execute(num_retries=self.num_retries)","['def', 'get_instance', '(', 'self', ',', 'instance', ',', 'project_id', '=', 'None', ')', ':', 'return', 'self', '.', 'get_conn', '(', ')', '.', 'instances', '(', ')', '.', 'get', '(', 'project', '=', 'project_id', ',', 'instance', '=', 'instance', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')']","Retrieves a resource containing information about a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: A Cloud SQL instance resource.
        :rtype: dict","['Retrieves', 'a', 'resource', 'containing', 'information', 'about', 'a', 'Cloud', 'SQL', 'instance', '.']",python,test,"['retrieves', 'a', 'resource', 'containing', 'information', 'about', 'a', 'cloud', 'sql', 'instance', '.']",retrieves a resource containing information about a cloud sql instance .,"['def', 'get_instance', '(', 'self', ',', 'instance', ',', 'project_id', '=', 'none', ')', ':', 'return', 'self', '.', 'get_conn', '(', ')', '.', 'instances', '(', ')', '.', 'get', '(', 'project', '=', 'project_id', ',', 'instance', '=', 'instance', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')']","def get_instance ( self , instance , project_id = none ) : return self . get_conn ( ) . instances ( ) . get ( project = project_id , instance = instance ) . execute ( num_retries = self . num_retries )"
81,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L115-L133,"def create_instance(self, body, project_id=None):
        """"""
        Creates a new Cloud SQL instance.

        :param body: Body required by the Cloud SQL insert API, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().instances().insert(
            project=project_id,
            body=body
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)","['def', 'create_instance', '(', 'self', ',', 'body', ',', 'project_id', '=', 'None', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'instances', '(', ')', '.', 'insert', '(', 'project', '=', 'project_id', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ')']","Creates a new Cloud SQL instance.

        :param body: Body required by the Cloud SQL insert API, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None","['Creates', 'a', 'new', 'Cloud', 'SQL', 'instance', '.']",python,test,"['creates', 'a', 'new', 'cloud', 'sql', 'instance', '.']",creates a new cloud sql instance .,"['def', 'create_instance', '(', 'self', ',', 'body', ',', 'project_id', '=', 'none', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'instances', '(', ')', '.', 'insert', '(', 'project', '=', 'project_id', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ')']","def create_instance ( self , body , project_id = none ) : response = self . get_conn ( ) . instances ( ) . insert ( project = project_id , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )"
82,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L136-L160,"def patch_instance(self, body, instance, project_id=None):
        """"""
        Updates settings of a Cloud SQL instance.

        Caution: This is not a partial update, so you must include values for
        all the settings that you want to retain.

        :param body: Body required by the Cloud SQL patch API, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.
        :type body: dict
        :param instance: Cloud SQL instance ID. This does not include the project ID.
        :type instance: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().instances().patch(
            project=project_id,
            instance=instance,
            body=body
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)","['def', 'patch_instance', '(', 'self', ',', 'body', ',', 'instance', ',', 'project_id', '=', 'None', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'instances', '(', ')', '.', 'patch', '(', 'project', '=', 'project_id', ',', 'instance', '=', 'instance', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ')']","Updates settings of a Cloud SQL instance.

        Caution: This is not a partial update, so you must include values for
        all the settings that you want to retain.

        :param body: Body required by the Cloud SQL patch API, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.
        :type body: dict
        :param instance: Cloud SQL instance ID. This does not include the project ID.
        :type instance: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None","['Updates', 'settings', 'of', 'a', 'Cloud', 'SQL', 'instance', '.']",python,test,"['updates', 'settings', 'of', 'a', 'cloud', 'sql', 'instance', '.']",updates settings of a cloud sql instance .,"['def', 'patch_instance', '(', 'self', ',', 'body', ',', 'instance', ',', 'project_id', '=', 'none', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'instances', '(', ')', '.', 'patch', '(', 'project', '=', 'project_id', ',', 'instance', '=', 'instance', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ')']","def patch_instance ( self , body , instance , project_id = none ) : response = self . get_conn ( ) . instances ( ) . patch ( project = project_id , instance = instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )"
83,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L163-L180,"def delete_instance(self, instance, project_id=None):
        """"""
        Deletes a Cloud SQL instance.

        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :param instance: Cloud SQL instance ID. This does not include the project ID.
        :type instance: str
        :return: None
        """"""
        response = self.get_conn().instances().delete(
            project=project_id,
            instance=instance,
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)","['def', 'delete_instance', '(', 'self', ',', 'instance', ',', 'project_id', '=', 'None', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'instances', '(', ')', '.', 'delete', '(', 'project', '=', 'project_id', ',', 'instance', '=', 'instance', ',', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ')']","Deletes a Cloud SQL instance.

        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :param instance: Cloud SQL instance ID. This does not include the project ID.
        :type instance: str
        :return: None","['Deletes', 'a', 'Cloud', 'SQL', 'instance', '.']",python,test,"['deletes', 'a', 'cloud', 'sql', 'instance', '.']",deletes a cloud sql instance .,"['def', 'delete_instance', '(', 'self', ',', 'instance', ',', 'project_id', '=', 'none', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'instances', '(', ')', '.', 'delete', '(', 'project', '=', 'project_id', ',', 'instance', '=', 'instance', ',', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ')']","def delete_instance ( self , instance , project_id = none ) : response = self . get_conn ( ) . instances ( ) . delete ( project = project_id , instance = instance , ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )"
84,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L183-L202,"def get_database(self, instance, database, project_id=None):
        """"""
        Retrieves a database resource from a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param database: Name of the database in the instance.
        :type database: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: A Cloud SQL database resource, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.
        :rtype: dict
        """"""
        return self.get_conn().databases().get(
            project=project_id,
            instance=instance,
            database=database
        ).execute(num_retries=self.num_retries)","['def', 'get_database', '(', 'self', ',', 'instance', ',', 'database', ',', 'project_id', '=', 'None', ')', ':', 'return', 'self', '.', 'get_conn', '(', ')', '.', 'databases', '(', ')', '.', 'get', '(', 'project', '=', 'project_id', ',', 'instance', '=', 'instance', ',', 'database', '=', 'database', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')']","Retrieves a database resource from a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param database: Name of the database in the instance.
        :type database: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: A Cloud SQL database resource, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.
        :rtype: dict","['Retrieves', 'a', 'database', 'resource', 'from', 'a', 'Cloud', 'SQL', 'instance', '.']",python,test,"['retrieves', 'a', 'database', 'resource', 'from', 'a', 'cloud', 'sql', 'instance', '.']",retrieves a database resource from a cloud sql instance .,"['def', 'get_database', '(', 'self', ',', 'instance', ',', 'database', ',', 'project_id', '=', 'none', ')', ':', 'return', 'self', '.', 'get_conn', '(', ')', '.', 'databases', '(', ')', '.', 'get', '(', 'project', '=', 'project_id', ',', 'instance', '=', 'instance', ',', 'database', '=', 'database', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')']","def get_database ( self , instance , database , project_id = none ) : return self . get_conn ( ) . databases ( ) . get ( project = project_id , instance = instance , database = database ) . execute ( num_retries = self . num_retries )"
85,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L205-L226,"def create_database(self, instance, body, project_id=None):
        """"""
        Creates a new database inside a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param body: The request body, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().databases().insert(
            project=project_id,
            instance=instance,
            body=body
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)","['def', 'create_database', '(', 'self', ',', 'instance', ',', 'body', ',', 'project_id', '=', 'None', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'databases', '(', ')', '.', 'insert', '(', 'project', '=', 'project_id', ',', 'instance', '=', 'instance', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ')']","Creates a new database inside a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param body: The request body, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None","['Creates', 'a', 'new', 'database', 'inside', 'a', 'Cloud', 'SQL', 'instance', '.']",python,test,"['creates', 'a', 'new', 'database', 'inside', 'a', 'cloud', 'sql', 'instance', '.']",creates a new database inside a cloud sql instance .,"['def', 'create_database', '(', 'self', ',', 'instance', ',', 'body', ',', 'project_id', '=', 'none', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'databases', '(', ')', '.', 'insert', '(', 'project', '=', 'project_id', ',', 'instance', '=', 'instance', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ')']","def create_database ( self , instance , body , project_id = none ) : response = self . get_conn ( ) . databases ( ) . insert ( project = project_id , instance = instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )"
86,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L229-L256,"def patch_database(self, instance, database, body, project_id=None):
        """"""
        Updates a database resource inside a Cloud SQL instance.

        This method supports patch semantics.
        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param database: Name of the database to be updated in the instance.
        :type database: str
        :param body: The request body, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().databases().patch(
            project=project_id,
            instance=instance,
            database=database,
            body=body
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)","['def', 'patch_database', '(', 'self', ',', 'instance', ',', 'database', ',', 'body', ',', 'project_id', '=', 'None', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'databases', '(', ')', '.', 'patch', '(', 'project', '=', 'project_id', ',', 'instance', '=', 'instance', ',', 'database', '=', 'database', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ')']","Updates a database resource inside a Cloud SQL instance.

        This method supports patch semantics.
        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param database: Name of the database to be updated in the instance.
        :type database: str
        :param body: The request body, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None","['Updates', 'a', 'database', 'resource', 'inside', 'a', 'Cloud', 'SQL', 'instance', '.']",python,test,"['updates', 'a', 'database', 'resource', 'inside', 'a', 'cloud', 'sql', 'instance', '.']",updates a database resource inside a cloud sql instance .,"['def', 'patch_database', '(', 'self', ',', 'instance', ',', 'database', ',', 'body', ',', 'project_id', '=', 'none', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'databases', '(', ')', '.', 'patch', '(', 'project', '=', 'project_id', ',', 'instance', '=', 'instance', ',', 'database', '=', 'database', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ')']","def patch_database ( self , instance , database , body , project_id = none ) : response = self . get_conn ( ) . databases ( ) . patch ( project = project_id , instance = instance , database = database , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )"
87,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L259-L279,"def delete_database(self, instance, database, project_id=None):
        """"""
        Deletes a database from a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param database: Name of the database to be deleted in the instance.
        :type database: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().databases().delete(
            project=project_id,
            instance=instance,
            database=database
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)","['def', 'delete_database', '(', 'self', ',', 'instance', ',', 'database', ',', 'project_id', '=', 'None', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'databases', '(', ')', '.', 'delete', '(', 'project', '=', 'project_id', ',', 'instance', '=', 'instance', ',', 'database', '=', 'database', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ')']","Deletes a database from a Cloud SQL instance.

        :param instance: Database instance ID. This does not include the project ID.
        :type instance: str
        :param database: Name of the database to be deleted in the instance.
        :type database: str
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None","['Deletes', 'a', 'database', 'from', 'a', 'Cloud', 'SQL', 'instance', '.']",python,test,"['deletes', 'a', 'database', 'from', 'a', 'cloud', 'sql', 'instance', '.']",deletes a database from a cloud sql instance .,"['def', 'delete_database', '(', 'self', ',', 'instance', ',', 'database', ',', 'project_id', '=', 'none', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'databases', '(', ')', '.', 'delete', '(', 'project', '=', 'project_id', ',', 'instance', '=', 'instance', ',', 'database', '=', 'database', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ')']","def delete_database ( self , instance , database , project_id = none ) : response = self . get_conn ( ) . databases ( ) . delete ( project = project_id , instance = instance , database = database ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )"
88,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L282-L310,"def export_instance(self, instance, body, project_id=None):
        """"""
        Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump
        or CSV file.

        :param instance: Database instance ID of the Cloud SQL instance. This does not include the
            project ID.
        :type instance: str
        :param body: The request body, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        try:
            response = self.get_conn().instances().export(
                project=project_id,
                instance=instance,
                body=body
            ).execute(num_retries=self.num_retries)
            operation_name = response[""name""]
            self._wait_for_operation_to_complete(project_id=project_id,
                                                 operation_name=operation_name)
        except HttpError as ex:
            raise AirflowException(
                'Exporting instance {} failed: {}'.format(instance, ex.content)
            )","['def', 'export_instance', '(', 'self', ',', 'instance', ',', 'body', ',', 'project_id', '=', 'None', ')', ':', 'try', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'instances', '(', ')', '.', 'export', '(', 'project', '=', 'project_id', ',', 'instance', '=', 'instance', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ')', 'except', 'HttpError', 'as', 'ex', ':', 'raise', 'AirflowException', '(', ""'Exporting instance {} failed: {}'"", '.', 'format', '(', 'instance', ',', 'ex', '.', 'content', ')', ')']","Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump
        or CSV file.

        :param instance: Database instance ID of the Cloud SQL instance. This does not include the
            project ID.
        :type instance: str
        :param body: The request body, as described in
            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body
        :type body: dict
        :param project_id: Project ID of the project that contains the instance. If set
            to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None","['Exports', 'data', 'from', 'a', 'Cloud', 'SQL', 'instance', 'to', 'a', 'Cloud', 'Storage', 'bucket', 'as', 'a', 'SQL', 'dump', 'or', 'CSV', 'file', '.']",python,test,"['exports', 'data', 'from', 'a', 'cloud', 'sql', 'instance', 'to', 'a', 'cloud', 'storage', 'bucket', 'as', 'a', 'sql', 'dump', 'or', 'csv', 'file', '.']",exports data from a cloud sql instance to a cloud storage bucket as a sql dump or csv file .,"['def', 'export_instance', '(', 'self', ',', 'instance', ',', 'body', ',', 'project_id', '=', 'none', ')', ':', 'try', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'instances', '(', ')', '.', 'export', '(', 'project', '=', 'project_id', ',', 'instance', '=', 'instance', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ')', 'except', 'httperror', 'as', 'ex', ':', 'raise', 'airflowexception', '(', ""'exporting instance {} failed: {}'"", '.', 'format', '(', 'instance', ',', 'ex', '.', 'content', ')', ')']","def export_instance ( self , instance , body , project_id = none ) : try : response = self . get_conn ( ) . instances ( ) . export ( project = project_id , instance = instance , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name ) except httperror as ex : raise airflowexception ( 'exporting instance {} failed: {}' . format ( instance , ex . content ) )"
89,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L343-L368,"def _wait_for_operation_to_complete(self, project_id, operation_name):
        """"""
        Waits for the named operation to complete - checks status of the
        asynchronous call.

        :param project_id: Project ID of the project that contains the instance.
        :type project_id: str
        :param operation_name: Name of the operation.
        :type operation_name: str
        :return: None
        """"""
        service = self.get_conn()
        while True:
            operation_response = service.operations().get(
                project=project_id,
                operation=operation_name,
            ).execute(num_retries=self.num_retries)
            if operation_response.get(""status"") == CloudSqlOperationStatus.DONE:
                error = operation_response.get(""error"")
                if error:
                    # Extracting the errors list as string and trimming square braces
                    error_msg = str(error.get(""errors""))[1:-1]
                    raise AirflowException(error_msg)
                # No meaningful info to return from the response in case of success
                return
            time.sleep(TIME_TO_SLEEP_IN_SECONDS)","['def', '_wait_for_operation_to_complete', '(', 'self', ',', 'project_id', ',', 'operation_name', ')', ':', 'service', '=', 'self', '.', 'get_conn', '(', ')', 'while', 'True', ':', 'operation_response', '=', 'service', '.', 'operations', '(', ')', '.', 'get', '(', 'project', '=', 'project_id', ',', 'operation', '=', 'operation_name', ',', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'if', 'operation_response', '.', 'get', '(', '""status""', ')', '==', 'CloudSqlOperationStatus', '.', 'DONE', ':', 'error', '=', 'operation_response', '.', 'get', '(', '""error""', ')', 'if', 'error', ':', '# Extracting the errors list as string and trimming square braces', 'error_msg', '=', 'str', '(', 'error', '.', 'get', '(', '""errors""', ')', ')', '[', '1', ':', '-', '1', ']', 'raise', 'AirflowException', '(', 'error_msg', ')', '# No meaningful info to return from the response in case of success', 'return', 'time', '.', 'sleep', '(', 'TIME_TO_SLEEP_IN_SECONDS', ')']","Waits for the named operation to complete - checks status of the
        asynchronous call.

        :param project_id: Project ID of the project that contains the instance.
        :type project_id: str
        :param operation_name: Name of the operation.
        :type operation_name: str
        :return: None","['Waits', 'for', 'the', 'named', 'operation', 'to', 'complete', '-', 'checks', 'status', 'of', 'the', 'asynchronous', 'call', '.']",python,test,"['waits', 'for', 'the', 'named', 'operation', 'to', 'complete', '-', 'checks', 'status', 'of', 'the', 'asynchronous', 'call', '.']",waits for the named operation to complete - checks status of the asynchronous call .,"['def', '_wait_for_operation_to_complete', '(', 'self', ',', 'project_id', ',', 'operation_name', ')', ':', 'service', '=', 'self', '.', 'get_conn', '(', ')', 'while', 'true', ':', 'operation_response', '=', 'service', '.', 'operations', '(', ')', '.', 'get', '(', 'project', '=', 'project_id', ',', 'operation', '=', 'operation_name', ',', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'if', 'operation_response', '.', 'get', '(', '""status""', ')', '==', 'cloudsqloperationstatus', '.', 'done', ':', 'error', '=', 'operation_response', '.', 'get', '(', '""error""', ')', 'if', 'error', ':', '# extracting the errors list as string and trimming square braces', 'error_msg', '=', 'str', '(', 'error', '.', 'get', '(', '""errors""', ')', ')', '[', '1', ':', '-', '1', ']', 'raise', 'airflowexception', '(', 'error_msg', ')', '# no meaningful info to return from the response in case of success', 'return', 'time', '.', 'sleep', '(', 'time_to_sleep_in_seconds', ')']","def _wait_for_operation_to_complete ( self , project_id , operation_name ) : service = self . get_conn ( ) while true : operation_response = service . operations ( ) . get ( project = project_id , operation = operation_name , ) . execute ( num_retries = self . num_retries ) if operation_response . get ( ""status"" ) == cloudsqloperationstatus . done : error = operation_response . get ( ""error"" ) if error : # extracting the errors list as string and trimming square braces error_msg = str ( error . get ( ""errors"" ) ) [ 1 : - 1 ] raise airflowexception ( error_msg ) # no meaningful info to return from the response in case of success return time . sleep ( time_to_sleep_in_seconds )"
90,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L524-L565,"def start_proxy(self):
        """"""
        Starts Cloud SQL Proxy.

        You have to remember to stop the proxy if you started it!
        """"""
        self._download_sql_proxy_if_needed()
        if self.sql_proxy_process:
            raise AirflowException(""The sql proxy is already running: {}"".format(
                self.sql_proxy_process))
        else:
            command_to_run = [self.sql_proxy_path]
            command_to_run.extend(self.command_line_parameters)
            try:
                self.log.info(""Creating directory %s"",
                              self.cloud_sql_proxy_socket_directory)
                os.makedirs(self.cloud_sql_proxy_socket_directory)
            except OSError:
                # Needed for python 2 compatibility (exists_ok missing)
                pass
            command_to_run.extend(self._get_credential_parameters())
            self.log.info(""Running the command: `%s`"", "" "".join(command_to_run))
            self.sql_proxy_process = Popen(command_to_run,
                                           stdin=PIPE, stdout=PIPE, stderr=PIPE)
            self.log.info(""The pid of cloud_sql_proxy: %s"", self.sql_proxy_process.pid)
            while True:
                line = self.sql_proxy_process.stderr.readline().decode('utf-8')
                return_code = self.sql_proxy_process.poll()
                if line == '' and return_code is not None:
                    self.sql_proxy_process = None
                    raise AirflowException(
                        ""The cloud_sql_proxy finished early with return code {}!"".format(
                            return_code))
                if line != '':
                    self.log.info(line)
                if ""googleapi: Error"" in line or ""invalid instance name:"" in line:
                    self.stop_proxy()
                    raise AirflowException(
                        ""Error when starting the cloud_sql_proxy {}!"".format(
                            line))
                if ""Ready for new connections"" in line:
                    return","['def', 'start_proxy', '(', 'self', ')', ':', 'self', '.', '_download_sql_proxy_if_needed', '(', ')', 'if', 'self', '.', 'sql_proxy_process', ':', 'raise', 'AirflowException', '(', '""The sql proxy is already running: {}""', '.', 'format', '(', 'self', '.', 'sql_proxy_process', ')', ')', 'else', ':', 'command_to_run', '=', '[', 'self', '.', 'sql_proxy_path', ']', 'command_to_run', '.', 'extend', '(', 'self', '.', 'command_line_parameters', ')', 'try', ':', 'self', '.', 'log', '.', 'info', '(', '""Creating directory %s""', ',', 'self', '.', 'cloud_sql_proxy_socket_directory', ')', 'os', '.', 'makedirs', '(', 'self', '.', 'cloud_sql_proxy_socket_directory', ')', 'except', 'OSError', ':', '# Needed for python 2 compatibility (exists_ok missing)', 'pass', 'command_to_run', '.', 'extend', '(', 'self', '.', '_get_credential_parameters', '(', ')', ')', 'self', '.', 'log', '.', 'info', '(', '""Running the command: `%s`""', ',', '"" ""', '.', 'join', '(', 'command_to_run', ')', ')', 'self', '.', 'sql_proxy_process', '=', 'Popen', '(', 'command_to_run', ',', 'stdin', '=', 'PIPE', ',', 'stdout', '=', 'PIPE', ',', 'stderr', '=', 'PIPE', ')', 'self', '.', 'log', '.', 'info', '(', '""The pid of cloud_sql_proxy: %s""', ',', 'self', '.', 'sql_proxy_process', '.', 'pid', ')', 'while', 'True', ':', 'line', '=', 'self', '.', 'sql_proxy_process', '.', 'stderr', '.', 'readline', '(', ')', '.', 'decode', '(', ""'utf-8'"", ')', 'return_code', '=', 'self', '.', 'sql_proxy_process', '.', 'poll', '(', ')', 'if', 'line', '==', ""''"", 'and', 'return_code', 'is', 'not', 'None', ':', 'self', '.', 'sql_proxy_process', '=', 'None', 'raise', 'AirflowException', '(', '""The cloud_sql_proxy finished early with return code {}!""', '.', 'format', '(', 'return_code', ')', ')', 'if', 'line', '!=', ""''"", ':', 'self', '.', 'log', '.', 'info', '(', 'line', ')', 'if', '""googleapi: Error""', 'in', 'line', 'or', '""invalid instance name:""', 'in', 'line', ':', 'self', '.', 'stop_proxy', '(', ')', 'raise', 'AirflowException', '(', '""Error when starting the cloud_sql_proxy {}!""', '.', 'format', '(', 'line', ')', ')', 'if', '""Ready for new connections""', 'in', 'line', ':', 'return']","Starts Cloud SQL Proxy.

        You have to remember to stop the proxy if you started it!","['Starts', 'Cloud', 'SQL', 'Proxy', '.']",python,test,"['starts', 'cloud', 'sql', 'proxy', '.']",starts cloud sql proxy .,"['def', 'start_proxy', '(', 'self', ')', ':', 'self', '.', '_download_sql_proxy_if_needed', '(', ')', 'if', 'self', '.', 'sql_proxy_process', ':', 'raise', 'airflowexception', '(', '""the sql proxy is already running: {}""', '.', 'format', '(', 'self', '.', 'sql_proxy_process', ')', ')', 'else', ':', 'command_to_run', '=', '[', 'self', '.', 'sql_proxy_path', ']', 'command_to_run', '.', 'extend', '(', 'self', '.', 'command_line_parameters', ')', 'try', ':', 'self', '.', 'log', '.', 'info', '(', '""creating directory %s""', ',', 'self', '.', 'cloud_sql_proxy_socket_directory', ')', 'os', '.', 'makedirs', '(', 'self', '.', 'cloud_sql_proxy_socket_directory', ')', 'except', 'oserror', ':', '# needed for python 2 compatibility (exists_ok missing)', 'pass', 'command_to_run', '.', 'extend', '(', 'self', '.', '_get_credential_parameters', '(', ')', ')', 'self', '.', 'log', '.', 'info', '(', '""running the command: `%s`""', ',', '"" ""', '.', 'join', '(', 'command_to_run', ')', ')', 'self', '.', 'sql_proxy_process', '=', 'popen', '(', 'command_to_run', ',', 'stdin', '=', 'pipe', ',', 'stdout', '=', 'pipe', ',', 'stderr', '=', 'pipe', ')', 'self', '.', 'log', '.', 'info', '(', '""the pid of cloud_sql_proxy: %s""', ',', 'self', '.', 'sql_proxy_process', '.', 'pid', ')', 'while', 'true', ':', 'line', '=', 'self', '.', 'sql_proxy_process', '.', 'stderr', '.', 'readline', '(', ')', '.', 'decode', '(', ""'utf-8'"", ')', 'return_code', '=', 'self', '.', 'sql_proxy_process', '.', 'poll', '(', ')', 'if', 'line', '==', ""''"", 'and', 'return_code', 'is', 'not', 'none', ':', 'self', '.', 'sql_proxy_process', '=', 'none', 'raise', 'airflowexception', '(', '""the cloud_sql_proxy finished early with return code {}!""', '.', 'format', '(', 'return_code', ')', ')', 'if', 'line', '!=', ""''"", ':', 'self', '.', 'log', '.', 'info', '(', 'line', ')', 'if', '""googleapi: error""', 'in', 'line', 'or', '""invalid instance name:""', 'in', 'line', ':', 'self', '.', 'stop_proxy', '(', ')', 'raise', 'airflowexception', '(', '""error when starting the cloud_sql_proxy {}!""', '.', 'format', '(', 'line', ')', ')', 'if', '""ready for new connections""', 'in', 'line', ':', 'return']","def start_proxy ( self ) : self . _download_sql_proxy_if_needed ( ) if self . sql_proxy_process : raise airflowexception ( ""the sql proxy is already running: {}"" . format ( self . sql_proxy_process ) ) else : command_to_run = [ self . sql_proxy_path ] command_to_run . extend ( self . command_line_parameters ) try : self . log . info ( ""creating directory %s"" , self . cloud_sql_proxy_socket_directory ) os . makedirs ( self . cloud_sql_proxy_socket_directory ) except oserror : # needed for python 2 compatibility (exists_ok missing) pass command_to_run . extend ( self . _get_credential_parameters ( ) ) self . log . info ( ""running the command: `%s`"" , "" "" . join ( command_to_run ) ) self . sql_proxy_process = popen ( command_to_run , stdin = pipe , stdout = pipe , stderr = pipe ) self . log . info ( ""the pid of cloud_sql_proxy: %s"" , self . sql_proxy_process . pid ) while true : line = self . sql_proxy_process . stderr . readline ( ) . decode ( 'utf-8' ) return_code = self . sql_proxy_process . poll ( ) if line == '' and return_code is not none : self . sql_proxy_process = none raise airflowexception ( ""the cloud_sql_proxy finished early with return code {}!"" . format ( return_code ) ) if line != '' : self . log . info ( line ) if ""googleapi: error"" in line or ""invalid instance name:"" in line : self . stop_proxy ( ) raise airflowexception ( ""error when starting the cloud_sql_proxy {}!"" . format ( line ) ) if ""ready for new connections"" in line : return"
91,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L567-L599,"def stop_proxy(self):
        """"""
        Stops running proxy.

        You should stop the proxy after you stop using it.
        """"""
        if not self.sql_proxy_process:
            raise AirflowException(""The sql proxy is not started yet"")
        else:
            self.log.info(""Stopping the cloud_sql_proxy pid: %s"",
                          self.sql_proxy_process.pid)
            self.sql_proxy_process.kill()
            self.sql_proxy_process = None
        # Cleanup!
        self.log.info(""Removing the socket directory: %s"",
                      self.cloud_sql_proxy_socket_directory)
        shutil.rmtree(self.cloud_sql_proxy_socket_directory, ignore_errors=True)
        if self.sql_proxy_was_downloaded:
            self.log.info(""Removing downloaded proxy: %s"", self.sql_proxy_path)
            # Silently ignore if the file has already been removed (concurrency)
            try:
                os.remove(self.sql_proxy_path)
            except OSError as e:
                if not e.errno == errno.ENOENT:
                    raise
        else:
            self.log.info(""Skipped removing proxy - it was not downloaded: %s"",
                          self.sql_proxy_path)
        if os.path.isfile(self.credentials_path):
            self.log.info(""Removing generated credentials file %s"",
                          self.credentials_path)
            # Here file cannot be delete by concurrent task (each task has its own copy)
            os.remove(self.credentials_path)","['def', 'stop_proxy', '(', 'self', ')', ':', 'if', 'not', 'self', '.', 'sql_proxy_process', ':', 'raise', 'AirflowException', '(', '""The sql proxy is not started yet""', ')', 'else', ':', 'self', '.', 'log', '.', 'info', '(', '""Stopping the cloud_sql_proxy pid: %s""', ',', 'self', '.', 'sql_proxy_process', '.', 'pid', ')', 'self', '.', 'sql_proxy_process', '.', 'kill', '(', ')', 'self', '.', 'sql_proxy_process', '=', 'None', '# Cleanup!', 'self', '.', 'log', '.', 'info', '(', '""Removing the socket directory: %s""', ',', 'self', '.', 'cloud_sql_proxy_socket_directory', ')', 'shutil', '.', 'rmtree', '(', 'self', '.', 'cloud_sql_proxy_socket_directory', ',', 'ignore_errors', '=', 'True', ')', 'if', 'self', '.', 'sql_proxy_was_downloaded', ':', 'self', '.', 'log', '.', 'info', '(', '""Removing downloaded proxy: %s""', ',', 'self', '.', 'sql_proxy_path', ')', '# Silently ignore if the file has already been removed (concurrency)', 'try', ':', 'os', '.', 'remove', '(', 'self', '.', 'sql_proxy_path', ')', 'except', 'OSError', 'as', 'e', ':', 'if', 'not', 'e', '.', 'errno', '==', 'errno', '.', 'ENOENT', ':', 'raise', 'else', ':', 'self', '.', 'log', '.', 'info', '(', '""Skipped removing proxy - it was not downloaded: %s""', ',', 'self', '.', 'sql_proxy_path', ')', 'if', 'os', '.', 'path', '.', 'isfile', '(', 'self', '.', 'credentials_path', ')', ':', 'self', '.', 'log', '.', 'info', '(', '""Removing generated credentials file %s""', ',', 'self', '.', 'credentials_path', ')', '# Here file cannot be delete by concurrent task (each task has its own copy)', 'os', '.', 'remove', '(', 'self', '.', 'credentials_path', ')']","Stops running proxy.

        You should stop the proxy after you stop using it.","['Stops', 'running', 'proxy', '.']",python,test,"['stops', 'running', 'proxy', '.']",stops running proxy .,"['def', 'stop_proxy', '(', 'self', ')', ':', 'if', 'not', 'self', '.', 'sql_proxy_process', ':', 'raise', 'airflowexception', '(', '""the sql proxy is not started yet""', ')', 'else', ':', 'self', '.', 'log', '.', 'info', '(', '""stopping the cloud_sql_proxy pid: %s""', ',', 'self', '.', 'sql_proxy_process', '.', 'pid', ')', 'self', '.', 'sql_proxy_process', '.', 'kill', '(', ')', 'self', '.', 'sql_proxy_process', '=', 'none', '# cleanup!', 'self', '.', 'log', '.', 'info', '(', '""removing the socket directory: %s""', ',', 'self', '.', 'cloud_sql_proxy_socket_directory', ')', 'shutil', '.', 'rmtree', '(', 'self', '.', 'cloud_sql_proxy_socket_directory', ',', 'ignore_errors', '=', 'true', ')', 'if', 'self', '.', 'sql_proxy_was_downloaded', ':', 'self', '.', 'log', '.', 'info', '(', '""removing downloaded proxy: %s""', ',', 'self', '.', 'sql_proxy_path', ')', '# silently ignore if the file has already been removed (concurrency)', 'try', ':', 'os', '.', 'remove', '(', 'self', '.', 'sql_proxy_path', ')', 'except', 'oserror', 'as', 'e', ':', 'if', 'not', 'e', '.', 'errno', '==', 'errno', '.', 'enoent', ':', 'raise', 'else', ':', 'self', '.', 'log', '.', 'info', '(', '""skipped removing proxy - it was not downloaded: %s""', ',', 'self', '.', 'sql_proxy_path', ')', 'if', 'os', '.', 'path', '.', 'isfile', '(', 'self', '.', 'credentials_path', ')', ':', 'self', '.', 'log', '.', 'info', '(', '""removing generated credentials file %s""', ',', 'self', '.', 'credentials_path', ')', '# here file cannot be delete by concurrent task (each task has its own copy)', 'os', '.', 'remove', '(', 'self', '.', 'credentials_path', ')']","def stop_proxy ( self ) : if not self . sql_proxy_process : raise airflowexception ( ""the sql proxy is not started yet"" ) else : self . log . info ( ""stopping the cloud_sql_proxy pid: %s"" , self . sql_proxy_process . pid ) self . sql_proxy_process . kill ( ) self . sql_proxy_process = none # cleanup! self . log . info ( ""removing the socket directory: %s"" , self . cloud_sql_proxy_socket_directory ) shutil . rmtree ( self . cloud_sql_proxy_socket_directory , ignore_errors = true ) if self . sql_proxy_was_downloaded : self . log . info ( ""removing downloaded proxy: %s"" , self . sql_proxy_path ) # silently ignore if the file has already been removed (concurrency) try : os . remove ( self . sql_proxy_path ) except oserror as e : if not e . errno == errno . enoent : raise else : self . log . info ( ""skipped removing proxy - it was not downloaded: %s"" , self . sql_proxy_path ) if os . path . isfile ( self . credentials_path ) : self . log . info ( ""removing generated credentials file %s"" , self . credentials_path ) # here file cannot be delete by concurrent task (each task has its own copy) os . remove ( self . credentials_path )"
92,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L601-L615,"def get_proxy_version(self):
        """"""
        Returns version of the Cloud SQL Proxy.
        """"""
        self._download_sql_proxy_if_needed()
        command_to_run = [self.sql_proxy_path]
        command_to_run.extend(['--version'])
        command_to_run.extend(self._get_credential_parameters())
        result = subprocess.check_output(command_to_run).decode('utf-8')
        pattern = re.compile(""^.*[V|v]ersion ([^;]*);.*$"")
        m = pattern.match(result)
        if m:
            return m.group(1)
        else:
            return None","['def', 'get_proxy_version', '(', 'self', ')', ':', 'self', '.', '_download_sql_proxy_if_needed', '(', ')', 'command_to_run', '=', '[', 'self', '.', 'sql_proxy_path', ']', 'command_to_run', '.', 'extend', '(', '[', ""'--version'"", ']', ')', 'command_to_run', '.', 'extend', '(', 'self', '.', '_get_credential_parameters', '(', ')', ')', 'result', '=', 'subprocess', '.', 'check_output', '(', 'command_to_run', ')', '.', 'decode', '(', ""'utf-8'"", ')', 'pattern', '=', 're', '.', 'compile', '(', '""^.*[V|v]ersion ([^;]*);.*$""', ')', 'm', '=', 'pattern', '.', 'match', '(', 'result', ')', 'if', 'm', ':', 'return', 'm', '.', 'group', '(', '1', ')', 'else', ':', 'return', 'None']",Returns version of the Cloud SQL Proxy.,"['Returns', 'version', 'of', 'the', 'Cloud', 'SQL', 'Proxy', '.']",python,test,"['returns', 'version', 'of', 'the', 'cloud', 'sql', 'proxy', '.']",returns version of the cloud sql proxy .,"['def', 'get_proxy_version', '(', 'self', ')', ':', 'self', '.', '_download_sql_proxy_if_needed', '(', ')', 'command_to_run', '=', '[', 'self', '.', 'sql_proxy_path', ']', 'command_to_run', '.', 'extend', '(', '[', ""'--version'"", ']', ')', 'command_to_run', '.', 'extend', '(', 'self', '.', '_get_credential_parameters', '(', ')', ')', 'result', '=', 'subprocess', '.', 'check_output', '(', 'command_to_run', ')', '.', 'decode', '(', ""'utf-8'"", ')', 'pattern', '=', 're', '.', 'compile', '(', '""^.*[v|v]ersion ([^;]*);.*$""', ')', 'm', '=', 'pattern', '.', 'match', '(', 'result', ')', 'if', 'm', ':', 'return', 'm', '.', 'group', '(', '1', ')', 'else', ':', 'return', 'none']","def get_proxy_version ( self ) : self . _download_sql_proxy_if_needed ( ) command_to_run = [ self . sql_proxy_path ] command_to_run . extend ( [ '--version' ] ) command_to_run . extend ( self . _get_credential_parameters ( ) ) result = subprocess . check_output ( command_to_run ) . decode ( 'utf-8' ) pattern = re . compile ( ""^.*[v|v]ersion ([^;]*);.*$"" ) m = pattern . match ( result ) if m : return m . group ( 1 ) else : return none"
93,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L895-L908,"def create_connection(self, session=None):
        """"""
        Create connection in the Connection table, according to whether it uses
        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.

        :param session: Session of the SQL Alchemy ORM (automatically generated with
                        decorator).
        """"""
        connection = Connection(conn_id=self.db_conn_id)
        uri = self._generate_connection_uri()
        self.log.info(""Creating connection %s"", self.db_conn_id)
        connection.parse_from_uri(uri)
        session.add(connection)
        session.commit()","['def', 'create_connection', '(', 'self', ',', 'session', '=', 'None', ')', ':', 'connection', '=', 'Connection', '(', 'conn_id', '=', 'self', '.', 'db_conn_id', ')', 'uri', '=', 'self', '.', '_generate_connection_uri', '(', ')', 'self', '.', 'log', '.', 'info', '(', '""Creating connection %s""', ',', 'self', '.', 'db_conn_id', ')', 'connection', '.', 'parse_from_uri', '(', 'uri', ')', 'session', '.', 'add', '(', 'connection', ')', 'session', '.', 'commit', '(', ')']","Create connection in the Connection table, according to whether it uses
        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.

        :param session: Session of the SQL Alchemy ORM (automatically generated with
                        decorator).","['Create', 'connection', 'in', 'the', 'Connection', 'table', 'according', 'to', 'whether', 'it', 'uses', 'proxy', 'TCP', 'UNIX', 'sockets', 'SSL', '.', 'Connection', 'ID', 'will', 'be', 'randomly', 'generated', '.']",python,test,"['create', 'connection', 'in', 'the', 'connection', 'table', 'according', 'to', 'whether', 'it', 'uses', 'proxy', 'tcp', 'unix', 'sockets', 'ssl', '.', 'connection', 'id', 'will', 'be', 'randomly', 'generated', '.']",create connection in the connection table according to whether it uses proxy tcp unix sockets ssl . connection id will be randomly generated .,"['def', 'create_connection', '(', 'self', ',', 'session', '=', 'none', ')', ':', 'connection', '=', 'connection', '(', 'conn_id', '=', 'self', '.', 'db_conn_id', ')', 'uri', '=', 'self', '.', '_generate_connection_uri', '(', ')', 'self', '.', 'log', '.', 'info', '(', '""creating connection %s""', ',', 'self', '.', 'db_conn_id', ')', 'connection', '.', 'parse_from_uri', '(', 'uri', ')', 'session', '.', 'add', '(', 'connection', ')', 'session', '.', 'commit', '(', ')']","def create_connection ( self , session = none ) : connection = connection ( conn_id = self . db_conn_id ) uri = self . _generate_connection_uri ( ) self . log . info ( ""creating connection %s"" , self . db_conn_id ) connection . parse_from_uri ( uri ) session . add ( connection ) session . commit ( )"
94,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L911-L923,"def retrieve_connection(self, session=None):
        """"""
        Retrieves the dynamically created connection from the Connection table.

        :param session: Session of the SQL Alchemy ORM (automatically generated with
                        decorator).
        """"""
        self.log.info(""Retrieving connection %s"", self.db_conn_id)
        connections = session.query(Connection).filter(
            Connection.conn_id == self.db_conn_id)
        if connections.count():
            return connections[0]
        return None","['def', 'retrieve_connection', '(', 'self', ',', 'session', '=', 'None', ')', ':', 'self', '.', 'log', '.', 'info', '(', '""Retrieving connection %s""', ',', 'self', '.', 'db_conn_id', ')', 'connections', '=', 'session', '.', 'query', '(', 'Connection', ')', '.', 'filter', '(', 'Connection', '.', 'conn_id', '==', 'self', '.', 'db_conn_id', ')', 'if', 'connections', '.', 'count', '(', ')', ':', 'return', 'connections', '[', '0', ']', 'return', 'None']","Retrieves the dynamically created connection from the Connection table.

        :param session: Session of the SQL Alchemy ORM (automatically generated with
                        decorator).","['Retrieves', 'the', 'dynamically', 'created', 'connection', 'from', 'the', 'Connection', 'table', '.']",python,test,"['retrieves', 'the', 'dynamically', 'created', 'connection', 'from', 'the', 'connection', 'table', '.']",retrieves the dynamically created connection from the connection table .,"['def', 'retrieve_connection', '(', 'self', ',', 'session', '=', 'none', ')', ':', 'self', '.', 'log', '.', 'info', '(', '""retrieving connection %s""', ',', 'self', '.', 'db_conn_id', ')', 'connections', '=', 'session', '.', 'query', '(', 'connection', ')', '.', 'filter', '(', 'connection', '.', 'conn_id', '==', 'self', '.', 'db_conn_id', ')', 'if', 'connections', '.', 'count', '(', ')', ':', 'return', 'connections', '[', '0', ']', 'return', 'none']","def retrieve_connection ( self , session = none ) : self . log . info ( ""retrieving connection %s"" , self . db_conn_id ) connections = session . query ( connection ) . filter ( connection . conn_id == self . db_conn_id ) if connections . count ( ) : return connections [ 0 ] return none"
95,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L926-L941,"def delete_connection(self, session=None):
        """"""
        Delete the dynamically created connection from the Connection table.

        :param session: Session of the SQL Alchemy ORM (automatically generated with
                        decorator).
        """"""
        self.log.info(""Deleting connection %s"", self.db_conn_id)
        connections = session.query(Connection).filter(
            Connection.conn_id == self.db_conn_id)
        if connections.count():
            connection = connections[0]
            session.delete(connection)
            session.commit()
        else:
            self.log.info(""Connection was already deleted!"")","['def', 'delete_connection', '(', 'self', ',', 'session', '=', 'None', ')', ':', 'self', '.', 'log', '.', 'info', '(', '""Deleting connection %s""', ',', 'self', '.', 'db_conn_id', ')', 'connections', '=', 'session', '.', 'query', '(', 'Connection', ')', '.', 'filter', '(', 'Connection', '.', 'conn_id', '==', 'self', '.', 'db_conn_id', ')', 'if', 'connections', '.', 'count', '(', ')', ':', 'connection', '=', 'connections', '[', '0', ']', 'session', '.', 'delete', '(', 'connection', ')', 'session', '.', 'commit', '(', ')', 'else', ':', 'self', '.', 'log', '.', 'info', '(', '""Connection was already deleted!""', ')']","Delete the dynamically created connection from the Connection table.

        :param session: Session of the SQL Alchemy ORM (automatically generated with
                        decorator).","['Delete', 'the', 'dynamically', 'created', 'connection', 'from', 'the', 'Connection', 'table', '.']",python,test,"['delete', 'the', 'dynamically', 'created', 'connection', 'from', 'the', 'connection', 'table', '.']",delete the dynamically created connection from the connection table .,"['def', 'delete_connection', '(', 'self', ',', 'session', '=', 'none', ')', ':', 'self', '.', 'log', '.', 'info', '(', '""deleting connection %s""', ',', 'self', '.', 'db_conn_id', ')', 'connections', '=', 'session', '.', 'query', '(', 'connection', ')', '.', 'filter', '(', 'connection', '.', 'conn_id', '==', 'self', '.', 'db_conn_id', ')', 'if', 'connections', '.', 'count', '(', ')', ':', 'connection', '=', 'connections', '[', '0', ']', 'session', '.', 'delete', '(', 'connection', ')', 'session', '.', 'commit', '(', ')', 'else', ':', 'self', '.', 'log', '.', 'info', '(', '""connection was already deleted!""', ')']","def delete_connection ( self , session = none ) : self . log . info ( ""deleting connection %s"" , self . db_conn_id ) connections = session . query ( connection ) . filter ( connection . conn_id == self . db_conn_id ) if connections . count ( ) : connection = connections [ 0 ] session . delete ( connection ) session . commit ( ) else : self . log . info ( ""connection was already deleted!"" )"
96,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L943-L959,"def get_sqlproxy_runner(self):
        """"""
        Retrieve Cloud SQL Proxy runner. It is used to manage the proxy
        lifecycle per task.

        :return: The Cloud SQL Proxy runner.
        :rtype: CloudSqlProxyRunner
        """"""
        if not self.use_proxy:
            raise AirflowException(""Proxy runner can only be retrieved in case of use_proxy = True"")
        return CloudSqlProxyRunner(
            path_prefix=self.sql_proxy_unique_path,
            instance_specification=self._get_sqlproxy_instance_specification(),
            project_id=self.project_id,
            sql_proxy_version=self.sql_proxy_version,
            sql_proxy_binary_path=self.sql_proxy_binary_path
        )","['def', 'get_sqlproxy_runner', '(', 'self', ')', ':', 'if', 'not', 'self', '.', 'use_proxy', ':', 'raise', 'AirflowException', '(', '""Proxy runner can only be retrieved in case of use_proxy = True""', ')', 'return', 'CloudSqlProxyRunner', '(', 'path_prefix', '=', 'self', '.', 'sql_proxy_unique_path', ',', 'instance_specification', '=', 'self', '.', '_get_sqlproxy_instance_specification', '(', ')', ',', 'project_id', '=', 'self', '.', 'project_id', ',', 'sql_proxy_version', '=', 'self', '.', 'sql_proxy_version', ',', 'sql_proxy_binary_path', '=', 'self', '.', 'sql_proxy_binary_path', ')']","Retrieve Cloud SQL Proxy runner. It is used to manage the proxy
        lifecycle per task.

        :return: The Cloud SQL Proxy runner.
        :rtype: CloudSqlProxyRunner","['Retrieve', 'Cloud', 'SQL', 'Proxy', 'runner', '.', 'It', 'is', 'used', 'to', 'manage', 'the', 'proxy', 'lifecycle', 'per', 'task', '.']",python,test,"['retrieve', 'cloud', 'sql', 'proxy', 'runner', '.', 'it', 'is', 'used', 'to', 'manage', 'the', 'proxy', 'lifecycle', 'per', 'task', '.']",retrieve cloud sql proxy runner . it is used to manage the proxy lifecycle per task .,"['def', 'get_sqlproxy_runner', '(', 'self', ')', ':', 'if', 'not', 'self', '.', 'use_proxy', ':', 'raise', 'airflowexception', '(', '""proxy runner can only be retrieved in case of use_proxy = true""', ')', 'return', 'cloudsqlproxyrunner', '(', 'path_prefix', '=', 'self', '.', 'sql_proxy_unique_path', ',', 'instance_specification', '=', 'self', '.', '_get_sqlproxy_instance_specification', '(', ')', ',', 'project_id', '=', 'self', '.', 'project_id', ',', 'sql_proxy_version', '=', 'self', '.', 'sql_proxy_version', ',', 'sql_proxy_binary_path', '=', 'self', '.', 'sql_proxy_binary_path', ')']","def get_sqlproxy_runner ( self ) : if not self . use_proxy : raise airflowexception ( ""proxy runner can only be retrieved in case of use_proxy = true"" ) return cloudsqlproxyrunner ( path_prefix = self . sql_proxy_unique_path , instance_specification = self . _get_sqlproxy_instance_specification ( ) , project_id = self . project_id , sql_proxy_version = self . sql_proxy_version , sql_proxy_binary_path = self . sql_proxy_binary_path )"
97,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L961-L972,"def get_database_hook(self):
        """"""
        Retrieve database hook. This is the actual Postgres or MySQL database hook
        that uses proxy or connects directly to the Google Cloud SQL database.
        """"""
        if self.database_type == 'postgres':
            self.db_hook = PostgresHook(postgres_conn_id=self.db_conn_id,
                                        schema=self.database)
        else:
            self.db_hook = MySqlHook(mysql_conn_id=self.db_conn_id,
                                     schema=self.database)
        return self.db_hook","['def', 'get_database_hook', '(', 'self', ')', ':', 'if', 'self', '.', 'database_type', '==', ""'postgres'"", ':', 'self', '.', 'db_hook', '=', 'PostgresHook', '(', 'postgres_conn_id', '=', 'self', '.', 'db_conn_id', ',', 'schema', '=', 'self', '.', 'database', ')', 'else', ':', 'self', '.', 'db_hook', '=', 'MySqlHook', '(', 'mysql_conn_id', '=', 'self', '.', 'db_conn_id', ',', 'schema', '=', 'self', '.', 'database', ')', 'return', 'self', '.', 'db_hook']","Retrieve database hook. This is the actual Postgres or MySQL database hook
        that uses proxy or connects directly to the Google Cloud SQL database.","['Retrieve', 'database', 'hook', '.', 'This', 'is', 'the', 'actual', 'Postgres', 'or', 'MySQL', 'database', 'hook', 'that', 'uses', 'proxy', 'or', 'connects', 'directly', 'to', 'the', 'Google', 'Cloud', 'SQL', 'database', '.']",python,test,"['retrieve', 'database', 'hook', '.', 'this', 'is', 'the', 'actual', 'postgres', 'or', 'mysql', 'database', 'hook', 'that', 'uses', 'proxy', 'or', 'connects', 'directly', 'to', 'the', 'google', 'cloud', 'sql', 'database', '.']",retrieve database hook . this is the actual postgres or mysql database hook that uses proxy or connects directly to the google cloud sql database .,"['def', 'get_database_hook', '(', 'self', ')', ':', 'if', 'self', '.', 'database_type', '==', ""'postgres'"", ':', 'self', '.', 'db_hook', '=', 'postgreshook', '(', 'postgres_conn_id', '=', 'self', '.', 'db_conn_id', ',', 'schema', '=', 'self', '.', 'database', ')', 'else', ':', 'self', '.', 'db_hook', '=', 'mysqlhook', '(', 'mysql_conn_id', '=', 'self', '.', 'db_conn_id', ',', 'schema', '=', 'self', '.', 'database', ')', 'return', 'self', '.', 'db_hook']","def get_database_hook ( self ) : if self . database_type == 'postgres' : self . db_hook = postgreshook ( postgres_conn_id = self . db_conn_id , schema = self . database ) else : self . db_hook = mysqlhook ( mysql_conn_id = self . db_conn_id , schema = self . database ) return self . db_hook"
98,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L974-L982,"def cleanup_database_hook(self):
        """"""
        Clean up database hook after it was used.
        """"""
        if self.database_type == 'postgres':
            if hasattr(self.db_hook,
                       'conn') and self.db_hook.conn and self.db_hook.conn.notices:
                for output in self.db_hook.conn.notices:
                    self.log.info(output)","['def', 'cleanup_database_hook', '(', 'self', ')', ':', 'if', 'self', '.', 'database_type', '==', ""'postgres'"", ':', 'if', 'hasattr', '(', 'self', '.', 'db_hook', ',', ""'conn'"", ')', 'and', 'self', '.', 'db_hook', '.', 'conn', 'and', 'self', '.', 'db_hook', '.', 'conn', '.', 'notices', ':', 'for', 'output', 'in', 'self', '.', 'db_hook', '.', 'conn', '.', 'notices', ':', 'self', '.', 'log', '.', 'info', '(', 'output', ')']",Clean up database hook after it was used.,"['Clean', 'up', 'database', 'hook', 'after', 'it', 'was', 'used', '.']",python,test,"['clean', 'up', 'database', 'hook', 'after', 'it', 'was', 'used', '.']",clean up database hook after it was used .,"['def', 'cleanup_database_hook', '(', 'self', ')', ':', 'if', 'self', '.', 'database_type', '==', ""'postgres'"", ':', 'if', 'hasattr', '(', 'self', '.', 'db_hook', ',', ""'conn'"", ')', 'and', 'self', '.', 'db_hook', '.', 'conn', 'and', 'self', '.', 'db_hook', '.', 'conn', '.', 'notices', ':', 'for', 'output', 'in', 'self', '.', 'db_hook', '.', 'conn', '.', 'notices', ':', 'self', '.', 'log', '.', 'info', '(', 'output', ')']","def cleanup_database_hook ( self ) : if self . database_type == 'postgres' : if hasattr ( self . db_hook , 'conn' ) and self . db_hook . conn and self . db_hook . conn . notices : for output in self . db_hook . conn . notices : self . log . info ( output )"
99,apache/airflow,airflow/contrib/hooks/gcp_sql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_sql_hook.py#L984-L990,"def reserve_free_tcp_port(self):
        """"""
        Reserve free TCP port to be used by Cloud SQL Proxy
        """"""
        self.reserved_tcp_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.reserved_tcp_socket.bind(('127.0.0.1', 0))
        self.sql_proxy_tcp_port = self.reserved_tcp_socket.getsockname()[1]","['def', 'reserve_free_tcp_port', '(', 'self', ')', ':', 'self', '.', 'reserved_tcp_socket', '=', 'socket', '.', 'socket', '(', 'socket', '.', 'AF_INET', ',', 'socket', '.', 'SOCK_STREAM', ')', 'self', '.', 'reserved_tcp_socket', '.', 'bind', '(', '(', ""'127.0.0.1'"", ',', '0', ')', ')', 'self', '.', 'sql_proxy_tcp_port', '=', 'self', '.', 'reserved_tcp_socket', '.', 'getsockname', '(', ')', '[', '1', ']']",Reserve free TCP port to be used by Cloud SQL Proxy,"['Reserve', 'free', 'TCP', 'port', 'to', 'be', 'used', 'by', 'Cloud', 'SQL', 'Proxy']",python,test,"['reserve', 'free', 'tcp', 'port', 'to', 'be', 'used', 'by', 'cloud', 'sql', 'proxy']",reserve free tcp port to be used by cloud sql proxy,"['def', 'reserve_free_tcp_port', '(', 'self', ')', ':', 'self', '.', 'reserved_tcp_socket', '=', 'socket', '.', 'socket', '(', 'socket', '.', 'af_inet', ',', 'socket', '.', 'sock_stream', ')', 'self', '.', 'reserved_tcp_socket', '.', 'bind', '(', '(', ""'127.0.0.1'"", ',', '0', ')', ')', 'self', '.', 'sql_proxy_tcp_port', '=', 'self', '.', 'reserved_tcp_socket', '.', 'getsockname', '(', ')', '[', '1', ']']","def reserve_free_tcp_port ( self ) : self . reserved_tcp_socket = socket . socket ( socket . af_inet , socket . sock_stream ) self . reserved_tcp_socket . bind ( ( '127.0.0.1' , 0 ) ) self . sql_proxy_tcp_port = self . reserved_tcp_socket . getsockname ( ) [ 1 ]"
100,apache/airflow,airflow/contrib/operators/mlengine_operator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mlengine_operator.py#L29-L62,"def _normalize_mlengine_job_id(job_id):
    """"""
    Replaces invalid MLEngine job_id characters with '_'.

    This also adds a leading 'z' in case job_id starts with an invalid
    character.

    Args:
        job_id: A job_id str that may have invalid characters.

    Returns:
        A valid job_id representation.
    """"""

    # Add a prefix when a job_id starts with a digit or a template
    match = re.search(r'\d|\{{2}', job_id)
    if match and match.start() == 0:
        job = 'z_{}'.format(job_id)
    else:
        job = job_id

    # Clean up 'bad' characters except templates
    tracker = 0
    cleansed_job_id = ''
    for m in re.finditer(r'\{{2}.+?\}{2}', job):
        cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_',
                                  job[tracker:m.start()])
        cleansed_job_id += job[m.start():m.end()]
        tracker = m.end()

    # Clean up last substring or the full string if no templates
    cleansed_job_id += re.sub(r'[^0-9a-zA-Z]+', '_', job[tracker:])

    return cleansed_job_id","['def', '_normalize_mlengine_job_id', '(', 'job_id', ')', ':', '# Add a prefix when a job_id starts with a digit or a template', 'match', '=', 're', '.', 'search', '(', ""r'\\d|\\{{2}'"", ',', 'job_id', ')', 'if', 'match', 'and', 'match', '.', 'start', '(', ')', '==', '0', ':', 'job', '=', ""'z_{}'"", '.', 'format', '(', 'job_id', ')', 'else', ':', 'job', '=', 'job_id', ""# Clean up 'bad' characters except templates"", 'tracker', '=', '0', 'cleansed_job_id', '=', ""''"", 'for', 'm', 'in', 're', '.', 'finditer', '(', ""r'\\{{2}.+?\\}{2}'"", ',', 'job', ')', ':', 'cleansed_job_id', '+=', 're', '.', 'sub', '(', ""r'[^0-9a-zA-Z]+'"", ',', ""'_'"", ',', 'job', '[', 'tracker', ':', 'm', '.', 'start', '(', ')', ']', ')', 'cleansed_job_id', '+=', 'job', '[', 'm', '.', 'start', '(', ')', ':', 'm', '.', 'end', '(', ')', ']', 'tracker', '=', 'm', '.', 'end', '(', ')', '# Clean up last substring or the full string if no templates', 'cleansed_job_id', '+=', 're', '.', 'sub', '(', ""r'[^0-9a-zA-Z]+'"", ',', ""'_'"", ',', 'job', '[', 'tracker', ':', ']', ')', 'return', 'cleansed_job_id']","Replaces invalid MLEngine job_id characters with '_'.

    This also adds a leading 'z' in case job_id starts with an invalid
    character.

    Args:
        job_id: A job_id str that may have invalid characters.

    Returns:
        A valid job_id representation.","['Replaces', 'invalid', 'MLEngine', 'job_id', 'characters', 'with', '_', '.']",python,test,"['replaces', 'invalid', 'mlengine', 'job_id', 'characters', 'with', '_', '.']",replaces invalid mlengine job_id characters with _ .,"['def', '_normalize_mlengine_job_id', '(', 'job_id', ')', ':', '# add a prefix when a job_id starts with a digit or a template', 'match', '=', 're', '.', 'search', '(', ""r'\\d|\\{{2}'"", ',', 'job_id', ')', 'if', 'match', 'and', 'match', '.', 'start', '(', ')', '==', '0', ':', 'job', '=', ""'z_{}'"", '.', 'format', '(', 'job_id', ')', 'else', ':', 'job', '=', 'job_id', ""# clean up 'bad' characters except templates"", 'tracker', '=', '0', 'cleansed_job_id', '=', ""''"", 'for', 'm', 'in', 're', '.', 'finditer', '(', ""r'\\{{2}.+?\\}{2}'"", ',', 'job', ')', ':', 'cleansed_job_id', '+=', 're', '.', 'sub', '(', ""r'[^0-9a-za-z]+'"", ',', ""'_'"", ',', 'job', '[', 'tracker', ':', 'm', '.', 'start', '(', ')', ']', ')', 'cleansed_job_id', '+=', 'job', '[', 'm', '.', 'start', '(', ')', ':', 'm', '.', 'end', '(', ')', ']', 'tracker', '=', 'm', '.', 'end', '(', ')', '# clean up last substring or the full string if no templates', 'cleansed_job_id', '+=', 're', '.', 'sub', '(', ""r'[^0-9a-za-z]+'"", ',', ""'_'"", ',', 'job', '[', 'tracker', ':', ']', ')', 'return', 'cleansed_job_id']","def _normalize_mlengine_job_id ( job_id ) : # add a prefix when a job_id starts with a digit or a template match = re . search ( r'\d|\{{2}' , job_id ) if match and match . start ( ) == 0 : job = 'z_{}' . format ( job_id ) else : job = job_id # clean up 'bad' characters except templates tracker = 0 cleansed_job_id = '' for m in re . finditer ( r'\{{2}.+?\}{2}' , job ) : cleansed_job_id += re . sub ( r'[^0-9a-za-z]+' , '_' , job [ tracker : m . start ( ) ] ) cleansed_job_id += job [ m . start ( ) : m . end ( ) ] tracker = m . end ( ) # clean up last substring or the full string if no templates cleansed_job_id += re . sub ( r'[^0-9a-za-z]+' , '_' , job [ tracker : ] ) return cleansed_job_id"
101,apache/airflow,airflow/contrib/sensors/ftp_sensor.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/ftp_sensor.py#L69-L76,"def _get_error_code(self, e):
        """"""Extract error code from ftp exception""""""
        try:
            matches = self.error_code_pattern.match(str(e))
            code = int(matches.group(0))
            return code
        except ValueError:
            return e","['def', '_get_error_code', '(', 'self', ',', 'e', ')', ':', 'try', ':', 'matches', '=', 'self', '.', 'error_code_pattern', '.', 'match', '(', 'str', '(', 'e', ')', ')', 'code', '=', 'int', '(', 'matches', '.', 'group', '(', '0', ')', ')', 'return', 'code', 'except', 'ValueError', ':', 'return', 'e']",Extract error code from ftp exception,"['Extract', 'error', 'code', 'from', 'ftp', 'exception']",python,test,"['extract', 'error', 'code', 'from', 'ftp', 'exception']",extract error code from ftp exception,"['def', '_get_error_code', '(', 'self', ',', 'e', ')', ':', 'try', ':', 'matches', '=', 'self', '.', 'error_code_pattern', '.', 'match', '(', 'str', '(', 'e', ')', ')', 'code', '=', 'int', '(', 'matches', '.', 'group', '(', '0', ')', ')', 'return', 'code', 'except', 'valueerror', ':', 'return', 'e']","def _get_error_code ( self , e ) : try : matches = self . error_code_pattern . match ( str ( e ) ) code = int ( matches . group ( 0 ) ) return code except valueerror : return e"
102,apache/airflow,airflow/sensors/__init__.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/sensors/__init__.py#L22-L28,"def _integrate_plugins():
    """"""Integrate plugins to the context""""""
    import sys
    from airflow.plugins_manager import sensors_modules
    for sensors_module in sensors_modules:
        sys.modules[sensors_module.__name__] = sensors_module
        globals()[sensors_module._name] = sensors_module","['def', '_integrate_plugins', '(', ')', ':', 'import', 'sys', 'from', 'airflow', '.', 'plugins_manager', 'import', 'sensors_modules', 'for', 'sensors_module', 'in', 'sensors_modules', ':', 'sys', '.', 'modules', '[', 'sensors_module', '.', '__name__', ']', '=', 'sensors_module', 'globals', '(', ')', '[', 'sensors_module', '.', '_name', ']', '=', 'sensors_module']",Integrate plugins to the context,"['Integrate', 'plugins', 'to', 'the', 'context']",python,test,"['integrate', 'plugins', 'to', 'the', 'context']",integrate plugins to the context,"['def', '_integrate_plugins', '(', ')', ':', 'import', 'sys', 'from', 'airflow', '.', 'plugins_manager', 'import', 'sensors_modules', 'for', 'sensors_module', 'in', 'sensors_modules', ':', 'sys', '.', 'modules', '[', 'sensors_module', '.', '__name__', ']', '=', 'sensors_module', 'globals', '(', ')', '[', 'sensors_module', '.', '_name', ']', '=', 'sensors_module']",def _integrate_plugins ( ) : import sys from airflow . plugins_manager import sensors_modules for sensors_module in sensors_modules : sys . modules [ sensors_module . __name__ ] = sensors_module globals ( ) [ sensors_module . _name ] = sensors_module
103,apache/airflow,scripts/perf/scheduler_ops_metrics.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L138-L148,"def clear_dag_runs():
    """"""
    Remove any existing DAG runs for the perf test DAGs.
    """"""
    session = settings.Session()
    drs = session.query(DagRun).filter(
        DagRun.dag_id.in_(DAG_IDS),
    ).all()
    for dr in drs:
        logging.info('Deleting DagRun :: {}'.format(dr))
        session.delete(dr)","['def', 'clear_dag_runs', '(', ')', ':', 'session', '=', 'settings', '.', 'Session', '(', ')', 'drs', '=', 'session', '.', 'query', '(', 'DagRun', ')', '.', 'filter', '(', 'DagRun', '.', 'dag_id', '.', 'in_', '(', 'DAG_IDS', ')', ',', ')', '.', 'all', '(', ')', 'for', 'dr', 'in', 'drs', ':', 'logging', '.', 'info', '(', ""'Deleting DagRun :: {}'"", '.', 'format', '(', 'dr', ')', ')', 'session', '.', 'delete', '(', 'dr', ')']",Remove any existing DAG runs for the perf test DAGs.,"['Remove', 'any', 'existing', 'DAG', 'runs', 'for', 'the', 'perf', 'test', 'DAGs', '.']",python,test,"['remove', 'any', 'existing', 'dag', 'runs', 'for', 'the', 'perf', 'test', 'dags', '.']",remove any existing dag runs for the perf test dags .,"['def', 'clear_dag_runs', '(', ')', ':', 'session', '=', 'settings', '.', 'session', '(', ')', 'drs', '=', 'session', '.', 'query', '(', 'dagrun', ')', '.', 'filter', '(', 'dagrun', '.', 'dag_id', '.', 'in_', '(', 'dag_ids', ')', ',', ')', '.', 'all', '(', ')', 'for', 'dr', 'in', 'drs', ':', 'logging', '.', 'info', '(', ""'deleting dagrun :: {}'"", '.', 'format', '(', 'dr', ')', ')', 'session', '.', 'delete', '(', 'dr', ')']","def clear_dag_runs ( ) : session = settings . session ( ) drs = session . query ( dagrun ) . filter ( dagrun . dag_id . in_ ( dag_ids ) , ) . all ( ) for dr in drs : logging . info ( 'deleting dagrun :: {}' . format ( dr ) ) session . delete ( dr )"
104,apache/airflow,scripts/perf/scheduler_ops_metrics.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L151-L166,"def clear_dag_task_instances():
    """"""
    Remove any existing task instances for the perf test DAGs.
    """"""
    session = settings.Session()
    TI = TaskInstance
    tis = (
        session
        .query(TI)
        .filter(TI.dag_id.in_(DAG_IDS))
        .all()
    )
    for ti in tis:
        logging.info('Deleting TaskInstance :: {}'.format(ti))
        session.delete(ti)
    session.commit()","['def', 'clear_dag_task_instances', '(', ')', ':', 'session', '=', 'settings', '.', 'Session', '(', ')', 'TI', '=', 'TaskInstance', 'tis', '=', '(', 'session', '.', 'query', '(', 'TI', ')', '.', 'filter', '(', 'TI', '.', 'dag_id', '.', 'in_', '(', 'DAG_IDS', ')', ')', '.', 'all', '(', ')', ')', 'for', 'ti', 'in', 'tis', ':', 'logging', '.', 'info', '(', ""'Deleting TaskInstance :: {}'"", '.', 'format', '(', 'ti', ')', ')', 'session', '.', 'delete', '(', 'ti', ')', 'session', '.', 'commit', '(', ')']",Remove any existing task instances for the perf test DAGs.,"['Remove', 'any', 'existing', 'task', 'instances', 'for', 'the', 'perf', 'test', 'DAGs', '.']",python,test,"['remove', 'any', 'existing', 'task', 'instances', 'for', 'the', 'perf', 'test', 'dags', '.']",remove any existing task instances for the perf test dags .,"['def', 'clear_dag_task_instances', '(', ')', ':', 'session', '=', 'settings', '.', 'session', '(', ')', 'ti', '=', 'taskinstance', 'tis', '=', '(', 'session', '.', 'query', '(', 'ti', ')', '.', 'filter', '(', 'ti', '.', 'dag_id', '.', 'in_', '(', 'dag_ids', ')', ')', '.', 'all', '(', ')', ')', 'for', 'ti', 'in', 'tis', ':', 'logging', '.', 'info', '(', ""'deleting taskinstance :: {}'"", '.', 'format', '(', 'ti', ')', ')', 'session', '.', 'delete', '(', 'ti', ')', 'session', '.', 'commit', '(', ')']",def clear_dag_task_instances ( ) : session = settings . session ( ) ti = taskinstance tis = ( session . query ( ti ) . filter ( ti . dag_id . in_ ( dag_ids ) ) . all ( ) ) for ti in tis : logging . info ( 'deleting taskinstance :: {}' . format ( ti ) ) session . delete ( ti ) session . commit ( )
105,apache/airflow,scripts/perf/scheduler_ops_metrics.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L169-L179,"def set_dags_paused_state(is_paused):
    """"""
    Toggle the pause state of the DAGs in the test.
    """"""
    session = settings.Session()
    dms = session.query(DagModel).filter(
        DagModel.dag_id.in_(DAG_IDS))
    for dm in dms:
        logging.info('Setting DAG :: {} is_paused={}'.format(dm, is_paused))
        dm.is_paused = is_paused
    session.commit()","['def', 'set_dags_paused_state', '(', 'is_paused', ')', ':', 'session', '=', 'settings', '.', 'Session', '(', ')', 'dms', '=', 'session', '.', 'query', '(', 'DagModel', ')', '.', 'filter', '(', 'DagModel', '.', 'dag_id', '.', 'in_', '(', 'DAG_IDS', ')', ')', 'for', 'dm', 'in', 'dms', ':', 'logging', '.', 'info', '(', ""'Setting DAG :: {} is_paused={}'"", '.', 'format', '(', 'dm', ',', 'is_paused', ')', ')', 'dm', '.', 'is_paused', '=', 'is_paused', 'session', '.', 'commit', '(', ')']",Toggle the pause state of the DAGs in the test.,"['Toggle', 'the', 'pause', 'state', 'of', 'the', 'DAGs', 'in', 'the', 'test', '.']",python,test,"['toggle', 'the', 'pause', 'state', 'of', 'the', 'dags', 'in', 'the', 'test', '.']",toggle the pause state of the dags in the test .,"['def', 'set_dags_paused_state', '(', 'is_paused', ')', ':', 'session', '=', 'settings', '.', 'session', '(', ')', 'dms', '=', 'session', '.', 'query', '(', 'dagmodel', ')', '.', 'filter', '(', 'dagmodel', '.', 'dag_id', '.', 'in_', '(', 'dag_ids', ')', ')', 'for', 'dm', 'in', 'dms', ':', 'logging', '.', 'info', '(', ""'setting dag :: {} is_paused={}'"", '.', 'format', '(', 'dm', ',', 'is_paused', ')', ')', 'dm', '.', 'is_paused', '=', 'is_paused', 'session', '.', 'commit', '(', ')']","def set_dags_paused_state ( is_paused ) : session = settings . session ( ) dms = session . query ( dagmodel ) . filter ( dagmodel . dag_id . in_ ( dag_ids ) ) for dm in dms : logging . info ( 'setting dag :: {} is_paused={}' . format ( dm , is_paused ) ) dm . is_paused = is_paused session . commit ( )"
106,apache/airflow,scripts/perf/scheduler_ops_metrics.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L65-L101,"def print_stats(self):
        """"""
        Print operational metrics for the scheduler test.
        """"""
        session = settings.Session()
        TI = TaskInstance
        tis = (
            session
            .query(TI)
            .filter(TI.dag_id.in_(DAG_IDS))
            .all()
        )
        successful_tis = [x for x in tis if x.state == State.SUCCESS]
        ti_perf = [(ti.dag_id, ti.task_id, ti.execution_date,
                    (ti.queued_dttm - self.start_date).total_seconds(),
                    (ti.start_date - self.start_date).total_seconds(),
                    (ti.end_date - self.start_date).total_seconds(),
                    ti.duration) for ti in successful_tis]
        ti_perf_df = pd.DataFrame(ti_perf, columns=['dag_id', 'task_id',
                                                    'execution_date',
                                                    'queue_delay',
                                                    'start_delay', 'land_time',
                                                    'duration'])

        print('Performance Results')
        print('###################')
        for dag_id in DAG_IDS:
            print('DAG {}'.format(dag_id))
            print(ti_perf_df[ti_perf_df['dag_id'] == dag_id])
        print('###################')
        if len(tis) > len(successful_tis):
            print(""WARNING!! The following task instances haven't completed"")
            print(pd.DataFrame([(ti.dag_id, ti.task_id, ti.execution_date, ti.state)
                  for ti in filter(lambda x: x.state != State.SUCCESS, tis)],
                  columns=['dag_id', 'task_id', 'execution_date', 'state']))

        session.commit()","['def', 'print_stats', '(', 'self', ')', ':', 'session', '=', 'settings', '.', 'Session', '(', ')', 'TI', '=', 'TaskInstance', 'tis', '=', '(', 'session', '.', 'query', '(', 'TI', ')', '.', 'filter', '(', 'TI', '.', 'dag_id', '.', 'in_', '(', 'DAG_IDS', ')', ')', '.', 'all', '(', ')', ')', 'successful_tis', '=', '[', 'x', 'for', 'x', 'in', 'tis', 'if', 'x', '.', 'state', '==', 'State', '.', 'SUCCESS', ']', 'ti_perf', '=', '[', '(', 'ti', '.', 'dag_id', ',', 'ti', '.', 'task_id', ',', 'ti', '.', 'execution_date', ',', '(', 'ti', '.', 'queued_dttm', '-', 'self', '.', 'start_date', ')', '.', 'total_seconds', '(', ')', ',', '(', 'ti', '.', 'start_date', '-', 'self', '.', 'start_date', ')', '.', 'total_seconds', '(', ')', ',', '(', 'ti', '.', 'end_date', '-', 'self', '.', 'start_date', ')', '.', 'total_seconds', '(', ')', ',', 'ti', '.', 'duration', ')', 'for', 'ti', 'in', 'successful_tis', ']', 'ti_perf_df', '=', 'pd', '.', 'DataFrame', '(', 'ti_perf', ',', 'columns', '=', '[', ""'dag_id'"", ',', ""'task_id'"", ',', ""'execution_date'"", ',', ""'queue_delay'"", ',', ""'start_delay'"", ',', ""'land_time'"", ',', ""'duration'"", ']', ')', 'print', '(', ""'Performance Results'"", ')', 'print', '(', ""'###################'"", ')', 'for', 'dag_id', 'in', 'DAG_IDS', ':', 'print', '(', ""'DAG {}'"", '.', 'format', '(', 'dag_id', ')', ')', 'print', '(', 'ti_perf_df', '[', 'ti_perf_df', '[', ""'dag_id'"", ']', '==', 'dag_id', ']', ')', 'print', '(', ""'###################'"", ')', 'if', 'len', '(', 'tis', ')', '>', 'len', '(', 'successful_tis', ')', ':', 'print', '(', '""WARNING!! The following task instances haven\'t completed""', ')', 'print', '(', 'pd', '.', 'DataFrame', '(', '[', '(', 'ti', '.', 'dag_id', ',', 'ti', '.', 'task_id', ',', 'ti', '.', 'execution_date', ',', 'ti', '.', 'state', ')', 'for', 'ti', 'in', 'filter', '(', 'lambda', 'x', ':', 'x', '.', 'state', '!=', 'State', '.', 'SUCCESS', ',', 'tis', ')', ']', ',', 'columns', '=', '[', ""'dag_id'"", ',', ""'task_id'"", ',', ""'execution_date'"", ',', ""'state'"", ']', ')', ')', 'session', '.', 'commit', '(', ')']",Print operational metrics for the scheduler test.,"['Print', 'operational', 'metrics', 'for', 'the', 'scheduler', 'test', '.']",python,test,"['print', 'operational', 'metrics', 'for', 'the', 'scheduler', 'test', '.']",print operational metrics for the scheduler test .,"['def', 'print_stats', '(', 'self', ')', ':', 'session', '=', 'settings', '.', 'session', '(', ')', 'ti', '=', 'taskinstance', 'tis', '=', '(', 'session', '.', 'query', '(', 'ti', ')', '.', 'filter', '(', 'ti', '.', 'dag_id', '.', 'in_', '(', 'dag_ids', ')', ')', '.', 'all', '(', ')', ')', 'successful_tis', '=', '[', 'x', 'for', 'x', 'in', 'tis', 'if', 'x', '.', 'state', '==', 'state', '.', 'success', ']', 'ti_perf', '=', '[', '(', 'ti', '.', 'dag_id', ',', 'ti', '.', 'task_id', ',', 'ti', '.', 'execution_date', ',', '(', 'ti', '.', 'queued_dttm', '-', 'self', '.', 'start_date', ')', '.', 'total_seconds', '(', ')', ',', '(', 'ti', '.', 'start_date', '-', 'self', '.', 'start_date', ')', '.', 'total_seconds', '(', ')', ',', '(', 'ti', '.', 'end_date', '-', 'self', '.', 'start_date', ')', '.', 'total_seconds', '(', ')', ',', 'ti', '.', 'duration', ')', 'for', 'ti', 'in', 'successful_tis', ']', 'ti_perf_df', '=', 'pd', '.', 'dataframe', '(', 'ti_perf', ',', 'columns', '=', '[', ""'dag_id'"", ',', ""'task_id'"", ',', ""'execution_date'"", ',', ""'queue_delay'"", ',', ""'start_delay'"", ',', ""'land_time'"", ',', ""'duration'"", ']', ')', 'print', '(', ""'performance results'"", ')', 'print', '(', ""'###################'"", ')', 'for', 'dag_id', 'in', 'dag_ids', ':', 'print', '(', ""'dag {}'"", '.', 'format', '(', 'dag_id', ')', ')', 'print', '(', 'ti_perf_df', '[', 'ti_perf_df', '[', ""'dag_id'"", ']', '==', 'dag_id', ']', ')', 'print', '(', ""'###################'"", ')', 'if', 'len', '(', 'tis', ')', '>', 'len', '(', 'successful_tis', ')', ':', 'print', '(', '""warning!! the following task instances haven\'t completed""', ')', 'print', '(', 'pd', '.', 'dataframe', '(', '[', '(', 'ti', '.', 'dag_id', ',', 'ti', '.', 'task_id', ',', 'ti', '.', 'execution_date', ',', 'ti', '.', 'state', ')', 'for', 'ti', 'in', 'filter', '(', 'lambda', 'x', ':', 'x', '.', 'state', '!=', 'state', '.', 'success', ',', 'tis', ')', ']', ',', 'columns', '=', '[', ""'dag_id'"", ',', ""'task_id'"", ',', ""'execution_date'"", ',', ""'state'"", ']', ')', ')', 'session', '.', 'commit', '(', ')']","def print_stats ( self ) : session = settings . session ( ) ti = taskinstance tis = ( session . query ( ti ) . filter ( ti . dag_id . in_ ( dag_ids ) ) . all ( ) ) successful_tis = [ x for x in tis if x . state == state . success ] ti_perf = [ ( ti . dag_id , ti . task_id , ti . execution_date , ( ti . queued_dttm - self . start_date ) . total_seconds ( ) , ( ti . start_date - self . start_date ) . total_seconds ( ) , ( ti . end_date - self . start_date ) . total_seconds ( ) , ti . duration ) for ti in successful_tis ] ti_perf_df = pd . dataframe ( ti_perf , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'queue_delay' , 'start_delay' , 'land_time' , 'duration' ] ) print ( 'performance results' ) print ( '###################' ) for dag_id in dag_ids : print ( 'dag {}' . format ( dag_id ) ) print ( ti_perf_df [ ti_perf_df [ 'dag_id' ] == dag_id ] ) print ( '###################' ) if len ( tis ) > len ( successful_tis ) : print ( ""warning!! the following task instances haven't completed"" ) print ( pd . dataframe ( [ ( ti . dag_id , ti . task_id , ti . execution_date , ti . state ) for ti in filter ( lambda x : x . state != state . success , tis ) ] , columns = [ 'dag_id' , 'task_id' , 'execution_date' , 'state' ] ) ) session . commit ( )"
107,apache/airflow,scripts/perf/scheduler_ops_metrics.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/scripts/perf/scheduler_ops_metrics.py#L103-L135,"def heartbeat(self):
        """"""
        Override the scheduler heartbeat to determine when the test is complete
        """"""
        super(SchedulerMetricsJob, self).heartbeat()
        session = settings.Session()
        # Get all the relevant task instances
        TI = TaskInstance
        successful_tis = (
            session
            .query(TI)
            .filter(TI.dag_id.in_(DAG_IDS))
            .filter(TI.state.in_([State.SUCCESS]))
            .all()
        )
        session.commit()

        dagbag = DagBag(SUBDIR)
        dags = [dagbag.dags[dag_id] for dag_id in DAG_IDS]
        # the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.
        num_task_instances = sum([(timezone.utcnow() - task.start_date).days
                                 for dag in dags for task in dag.tasks])

        if (len(successful_tis) == num_task_instances or
                (timezone.utcnow() - self.start_date).total_seconds() >
                MAX_RUNTIME_SECS):
            if len(successful_tis) == num_task_instances:
                self.log.info(""All tasks processed! Printing stats."")
            else:
                self.log.info(""Test timeout reached. Printing available stats."")
            self.print_stats()
            set_dags_paused_state(True)
            sys.exit()","['def', 'heartbeat', '(', 'self', ')', ':', 'super', '(', 'SchedulerMetricsJob', ',', 'self', ')', '.', 'heartbeat', '(', ')', 'session', '=', 'settings', '.', 'Session', '(', ')', '# Get all the relevant task instances', 'TI', '=', 'TaskInstance', 'successful_tis', '=', '(', 'session', '.', 'query', '(', 'TI', ')', '.', 'filter', '(', 'TI', '.', 'dag_id', '.', 'in_', '(', 'DAG_IDS', ')', ')', '.', 'filter', '(', 'TI', '.', 'state', '.', 'in_', '(', '[', 'State', '.', 'SUCCESS', ']', ')', ')', '.', 'all', '(', ')', ')', 'session', '.', 'commit', '(', ')', 'dagbag', '=', 'DagBag', '(', 'SUBDIR', ')', 'dags', '=', '[', 'dagbag', '.', 'dags', '[', 'dag_id', ']', 'for', 'dag_id', 'in', 'DAG_IDS', ']', '# the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.', 'num_task_instances', '=', 'sum', '(', '[', '(', 'timezone', '.', 'utcnow', '(', ')', '-', 'task', '.', 'start_date', ')', '.', 'days', 'for', 'dag', 'in', 'dags', 'for', 'task', 'in', 'dag', '.', 'tasks', ']', ')', 'if', '(', 'len', '(', 'successful_tis', ')', '==', 'num_task_instances', 'or', '(', 'timezone', '.', 'utcnow', '(', ')', '-', 'self', '.', 'start_date', ')', '.', 'total_seconds', '(', ')', '>', 'MAX_RUNTIME_SECS', ')', ':', 'if', 'len', '(', 'successful_tis', ')', '==', 'num_task_instances', ':', 'self', '.', 'log', '.', 'info', '(', '""All tasks processed! Printing stats.""', ')', 'else', ':', 'self', '.', 'log', '.', 'info', '(', '""Test timeout reached. Printing available stats.""', ')', 'self', '.', 'print_stats', '(', ')', 'set_dags_paused_state', '(', 'True', ')', 'sys', '.', 'exit', '(', ')']",Override the scheduler heartbeat to determine when the test is complete,"['Override', 'the', 'scheduler', 'heartbeat', 'to', 'determine', 'when', 'the', 'test', 'is', 'complete']",python,test,"['override', 'the', 'scheduler', 'heartbeat', 'to', 'determine', 'when', 'the', 'test', 'is', 'complete']",override the scheduler heartbeat to determine when the test is complete,"['def', 'heartbeat', '(', 'self', ')', ':', 'super', '(', 'schedulermetricsjob', ',', 'self', ')', '.', 'heartbeat', '(', ')', 'session', '=', 'settings', '.', 'session', '(', ')', '# get all the relevant task instances', 'ti', '=', 'taskinstance', 'successful_tis', '=', '(', 'session', '.', 'query', '(', 'ti', ')', '.', 'filter', '(', 'ti', '.', 'dag_id', '.', 'in_', '(', 'dag_ids', ')', ')', '.', 'filter', '(', 'ti', '.', 'state', '.', 'in_', '(', '[', 'state', '.', 'success', ']', ')', ')', '.', 'all', '(', ')', ')', 'session', '.', 'commit', '(', ')', 'dagbag', '=', 'dagbag', '(', 'subdir', ')', 'dags', '=', '[', 'dagbag', '.', 'dags', '[', 'dag_id', ']', 'for', 'dag_id', 'in', 'dag_ids', ']', '# the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval.', 'num_task_instances', '=', 'sum', '(', '[', '(', 'timezone', '.', 'utcnow', '(', ')', '-', 'task', '.', 'start_date', ')', '.', 'days', 'for', 'dag', 'in', 'dags', 'for', 'task', 'in', 'dag', '.', 'tasks', ']', ')', 'if', '(', 'len', '(', 'successful_tis', ')', '==', 'num_task_instances', 'or', '(', 'timezone', '.', 'utcnow', '(', ')', '-', 'self', '.', 'start_date', ')', '.', 'total_seconds', '(', ')', '>', 'max_runtime_secs', ')', ':', 'if', 'len', '(', 'successful_tis', ')', '==', 'num_task_instances', ':', 'self', '.', 'log', '.', 'info', '(', '""all tasks processed! printing stats.""', ')', 'else', ':', 'self', '.', 'log', '.', 'info', '(', '""test timeout reached. printing available stats.""', ')', 'self', '.', 'print_stats', '(', ')', 'set_dags_paused_state', '(', 'true', ')', 'sys', '.', 'exit', '(', ')']","def heartbeat ( self ) : super ( schedulermetricsjob , self ) . heartbeat ( ) session = settings . session ( ) # get all the relevant task instances ti = taskinstance successful_tis = ( session . query ( ti ) . filter ( ti . dag_id . in_ ( dag_ids ) ) . filter ( ti . state . in_ ( [ state . success ] ) ) . all ( ) ) session . commit ( ) dagbag = dagbag ( subdir ) dags = [ dagbag . dags [ dag_id ] for dag_id in dag_ids ] # the tasks in perf_dag_1 and per_dag_2 have a daily schedule interval. num_task_instances = sum ( [ ( timezone . utcnow ( ) - task . start_date ) . days for dag in dags for task in dag . tasks ] ) if ( len ( successful_tis ) == num_task_instances or ( timezone . utcnow ( ) - self . start_date ) . total_seconds ( ) > max_runtime_secs ) : if len ( successful_tis ) == num_task_instances : self . log . info ( ""all tasks processed! printing stats."" ) else : self . log . info ( ""test timeout reached. printing available stats."" ) self . print_stats ( ) set_dags_paused_state ( true ) sys . exit ( )"
108,apache/airflow,airflow/contrib/hooks/aws_lambda_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_lambda_hook.py#L53-L68,"def invoke_lambda(self, payload):
        """"""
        Invoke Lambda Function
        """"""

        awslambda_conn = self.get_conn()

        response = awslambda_conn.invoke(
            FunctionName=self.function_name,
            InvocationType=self.invocation_type,
            LogType=self.log_type,
            Payload=payload,
            Qualifier=self.qualifier
        )

        return response","['def', 'invoke_lambda', '(', 'self', ',', 'payload', ')', ':', 'awslambda_conn', '=', 'self', '.', 'get_conn', '(', ')', 'response', '=', 'awslambda_conn', '.', 'invoke', '(', 'FunctionName', '=', 'self', '.', 'function_name', ',', 'InvocationType', '=', 'self', '.', 'invocation_type', ',', 'LogType', '=', 'self', '.', 'log_type', ',', 'Payload', '=', 'payload', ',', 'Qualifier', '=', 'self', '.', 'qualifier', ')', 'return', 'response']",Invoke Lambda Function,"['Invoke', 'Lambda', 'Function']",python,test,"['invoke', 'lambda', 'function']",invoke lambda function,"['def', 'invoke_lambda', '(', 'self', ',', 'payload', ')', ':', 'awslambda_conn', '=', 'self', '.', 'get_conn', '(', ')', 'response', '=', 'awslambda_conn', '.', 'invoke', '(', 'functionname', '=', 'self', '.', 'function_name', ',', 'invocationtype', '=', 'self', '.', 'invocation_type', ',', 'logtype', '=', 'self', '.', 'log_type', ',', 'payload', '=', 'payload', ',', 'qualifier', '=', 'self', '.', 'qualifier', ')', 'return', 'response']","def invoke_lambda ( self , payload ) : awslambda_conn = self . get_conn ( ) response = awslambda_conn . invoke ( functionname = self . function_name , invocationtype = self . invocation_type , logtype = self . log_type , payload = payload , qualifier = self . qualifier ) return response"
109,apache/airflow,airflow/api/common/experimental/get_dag_run_state.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/api/common/experimental/get_dag_run_state.py#L24-L44,"def get_dag_run_state(dag_id, execution_date):
    """"""Return the task object identified by the given dag_id and task_id.""""""

    dagbag = DagBag()

    # Check DAG exists.
    if dag_id not in dagbag.dags:
        error_message = ""Dag id {} not found"".format(dag_id)
        raise DagNotFound(error_message)

    # Get DAG object and check Task Exists
    dag = dagbag.get_dag(dag_id)

    # Get DagRun object and check that it exists
    dagrun = dag.get_dagrun(execution_date=execution_date)
    if not dagrun:
        error_message = ('Dag Run for date {} not found in dag {}'
                         .format(execution_date, dag_id))
        raise DagRunNotFound(error_message)

    return {'state': dagrun.get_state()}","['def', 'get_dag_run_state', '(', 'dag_id', ',', 'execution_date', ')', ':', 'dagbag', '=', 'DagBag', '(', ')', '# Check DAG exists.', 'if', 'dag_id', 'not', 'in', 'dagbag', '.', 'dags', ':', 'error_message', '=', '""Dag id {} not found""', '.', 'format', '(', 'dag_id', ')', 'raise', 'DagNotFound', '(', 'error_message', ')', '# Get DAG object and check Task Exists', 'dag', '=', 'dagbag', '.', 'get_dag', '(', 'dag_id', ')', '# Get DagRun object and check that it exists', 'dagrun', '=', 'dag', '.', 'get_dagrun', '(', 'execution_date', '=', 'execution_date', ')', 'if', 'not', 'dagrun', ':', 'error_message', '=', '(', ""'Dag Run for date {} not found in dag {}'"", '.', 'format', '(', 'execution_date', ',', 'dag_id', ')', ')', 'raise', 'DagRunNotFound', '(', 'error_message', ')', 'return', '{', ""'state'"", ':', 'dagrun', '.', 'get_state', '(', ')', '}']",Return the task object identified by the given dag_id and task_id.,"['Return', 'the', 'task', 'object', 'identified', 'by', 'the', 'given', 'dag_id', 'and', 'task_id', '.']",python,test,"['return', 'the', 'task', 'object', 'identified', 'by', 'the', 'given', 'dag_id', 'and', 'task_id', '.']",return the task object identified by the given dag_id and task_id .,"['def', 'get_dag_run_state', '(', 'dag_id', ',', 'execution_date', ')', ':', 'dagbag', '=', 'dagbag', '(', ')', '# check dag exists.', 'if', 'dag_id', 'not', 'in', 'dagbag', '.', 'dags', ':', 'error_message', '=', '""dag id {} not found""', '.', 'format', '(', 'dag_id', ')', 'raise', 'dagnotfound', '(', 'error_message', ')', '# get dag object and check task exists', 'dag', '=', 'dagbag', '.', 'get_dag', '(', 'dag_id', ')', '# get dagrun object and check that it exists', 'dagrun', '=', 'dag', '.', 'get_dagrun', '(', 'execution_date', '=', 'execution_date', ')', 'if', 'not', 'dagrun', ':', 'error_message', '=', '(', ""'dag run for date {} not found in dag {}'"", '.', 'format', '(', 'execution_date', ',', 'dag_id', ')', ')', 'raise', 'dagrunnotfound', '(', 'error_message', ')', 'return', '{', ""'state'"", ':', 'dagrun', '.', 'get_state', '(', ')', '}']","def get_dag_run_state ( dag_id , execution_date ) : dagbag = dagbag ( ) # check dag exists. if dag_id not in dagbag . dags : error_message = ""dag id {} not found"" . format ( dag_id ) raise dagnotfound ( error_message ) # get dag object and check task exists dag = dagbag . get_dag ( dag_id ) # get dagrun object and check that it exists dagrun = dag . get_dagrun ( execution_date = execution_date ) if not dagrun : error_message = ( 'dag run for date {} not found in dag {}' . format ( execution_date , dag_id ) ) raise dagrunnotfound ( error_message ) return { 'state' : dagrun . get_state ( ) }"
110,apache/airflow,airflow/contrib/utils/mlengine_operator_utils.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/utils/mlengine_operator_utils.py#L32-L246,"def create_evaluate_ops(task_prefix,
                        data_format,
                        input_paths,
                        prediction_path,
                        metric_fn_and_keys,
                        validate_fn,
                        batch_prediction_job_id=None,
                        project_id=None,
                        region=None,
                        dataflow_options=None,
                        model_uri=None,
                        model_name=None,
                        version_name=None,
                        dag=None):
    """"""
    Creates Operators needed for model evaluation and returns.

    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by
    calling MLEngineBatchPredictionOperator, then summarize and validate
    the result via Cloud Dataflow using DataFlowPythonOperator.

    For details and pricing about Batch prediction, please refer to the website
    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict
    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/

    It returns three chained operators for prediction, summary, and validation,
    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,
    respectively.
    (<prefix> should contain only alphanumeric characters or hyphen.)

    The upstream and downstream can be set accordingly like:
      pred, _, val = create_evaluate_ops(...)
      pred.set_upstream(upstream_op)
      ...
      downstream_op.set_upstream(val)

    Callers will provide two python callables, metric_fn and validate_fn, in
    order to customize the evaluation behavior as they wish.
    - metric_fn receives a dictionary per instance derived from json in the
      batch prediction result. The keys might vary depending on the model.
      It should return a tuple of metrics.
    - validation_fn receives a dictionary of the averaged metrics that metric_fn
      generated over all instances.
      The key/value of the dictionary matches to what's given by
      metric_fn_and_keys arg.
      The dictionary contains an additional metric, 'count' to represent the
      total number of instances received for evaluation.
      The function would raise an exception to mark the task as failed, in a
      case the validation result is not okay to proceed (i.e. to set the trained
      version as default).

    Typical examples are like this:

    def get_metric_fn_and_keys():
        import math  # imports should be outside of the metric_fn below.
        def error_and_squared_error(inst):
            label = float(inst['input_label'])
            classes = float(inst['classes'])  # 0 or 1
            err = abs(classes-label)
            squared_err = math.pow(classes-label, 2)
            return (err, squared_err)  # returns a tuple.
        return error_and_squared_error, ['err', 'mse']  # key order must match.

    def validate_err_and_count(summary):
        if summary['err'] > 0.2:
            raise ValueError('Too high err>0.2; summary=%s' % summary)
        if summary['mse'] > 0.05:
            raise ValueError('Too high mse>0.05; summary=%s' % summary)
        if summary['count'] < 1000:
            raise ValueError('Too few instances<1000; summary=%s' % summary)
        return summary

    For the details on the other BatchPrediction-related arguments (project_id,
    job_id, region, data_format, input_paths, prediction_path, model_uri),
    please refer to MLEngineBatchPredictionOperator too.

    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and
        hyphen are allowed (no underscores), since this will be used as dataflow
        job name, which doesn't allow other characters.
    :type task_prefix: str

    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'
    :type data_format: str

    :param input_paths: a list of input paths to be sent to BatchPrediction.
    :type input_paths: list[str]

    :param prediction_path: GCS path to put the prediction results in.
    :type prediction_path: str

    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:
        - metric_fn is a function that accepts a dictionary (for an instance),
          and returns a tuple of metric(s) that it calculates.
        - metric_keys is a list of strings to denote the key of each metric.
    :type metric_fn_and_keys: tuple of a function and a list[str]

    :param validate_fn: a function to validate whether the averaged metric(s) is
        good enough to push the model.
    :type validate_fn: function

    :param batch_prediction_job_id: the id to use for the Cloud ML Batch
        prediction job. Passed directly to the MLEngineBatchPredictionOperator as
        the job_id argument.
    :type batch_prediction_job_id: str

    :param project_id: the Google Cloud Platform project id in which to execute
        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s
        `default_args['project_id']` will be used.
    :type project_id: str

    :param region: the Google Cloud Platform region in which to execute Cloud ML
        Batch Prediction and Dataflow jobs. If None, then the `dag`'s
        `default_args['region']` will be used.
    :type region: str

    :param dataflow_options: options to run Dataflow jobs. If None, then the
        `dag`'s `default_args['dataflow_default_options']` will be used.
    :type dataflow_options: dictionary

    :param model_uri: GCS path of the model exported by Tensorflow using
        tensorflow.estimator.export_savedmodel(). It cannot be used with
        model_name or version_name below. See MLEngineBatchPredictionOperator for
        more detail.
    :type model_uri: str

    :param model_name: Used to indicate a model to use for prediction. Can be
        used in combination with version_name, but cannot be used together with
        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,
        then the `dag`'s `default_args['model_name']` will be used.
    :type model_name: str

    :param version_name: Used to indicate a model version to use for prediction,
        in combination with model_name. Cannot be used together with model_uri.
        See MLEngineBatchPredictionOperator for more detail. If None, then the
        `dag`'s `default_args['version_name']` will be used.
    :type version_name: str

    :param dag: The `DAG` to use for all Operators.
    :type dag: airflow.models.DAG

    :returns: a tuple of three operators, (prediction, summary, validation)
    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,
                  PythonOperator)
    """"""

    # Verify that task_prefix doesn't have any special characters except hyphen
    # '-', which is the only allowed non-alphanumeric character by Dataflow.
    if not re.match(r""^[a-zA-Z][-A-Za-z0-9]*$"", task_prefix):
        raise AirflowException(
            ""Malformed task_id for DataFlowPythonOperator (only alphanumeric ""
            ""and hyphens are allowed but got: "" + task_prefix)

    metric_fn, metric_keys = metric_fn_and_keys
    if not callable(metric_fn):
        raise AirflowException(""`metric_fn` param must be callable."")
    if not callable(validate_fn):
        raise AirflowException(""`validate_fn` param must be callable."")

    if dag is not None and dag.default_args is not None:
        default_args = dag.default_args
        project_id = project_id or default_args.get('project_id')
        region = region or default_args.get('region')
        model_name = model_name or default_args.get('model_name')
        version_name = version_name or default_args.get('version_name')
        dataflow_options = dataflow_options or \
            default_args.get('dataflow_default_options')

    evaluate_prediction = MLEngineBatchPredictionOperator(
        task_id=(task_prefix + ""-prediction""),
        project_id=project_id,
        job_id=batch_prediction_job_id,
        region=region,
        data_format=data_format,
        input_paths=input_paths,
        output_path=prediction_path,
        uri=model_uri,
        model_name=model_name,
        version_name=version_name,
        dag=dag)

    metric_fn_encoded = base64.b64encode(dill.dumps(metric_fn, recurse=True))
    evaluate_summary = DataFlowPythonOperator(
        task_id=(task_prefix + ""-summary""),
        py_options=[""-m""],
        py_file=""airflow.contrib.utils.mlengine_prediction_summary"",
        dataflow_default_options=dataflow_options,
        options={
            ""prediction_path"": prediction_path,
            ""metric_fn_encoded"": metric_fn_encoded,
            ""metric_keys"": ','.join(metric_keys)
        },
        dag=dag)
    evaluate_summary.set_upstream(evaluate_prediction)

    def apply_validate_fn(*args, **kwargs):
        prediction_path = kwargs[""templates_dict""][""prediction_path""]
        scheme, bucket, obj, _, _ = urlsplit(prediction_path)
        if scheme != ""gs"" or not bucket or not obj:
            raise ValueError(""Wrong format prediction_path: %s"",
                             prediction_path)
        summary = os.path.join(obj.strip(""/""),
                               ""prediction.summary.json"")
        gcs_hook = GoogleCloudStorageHook()
        summary = json.loads(gcs_hook.download(bucket, summary))
        return validate_fn(summary)

    evaluate_validation = PythonOperator(
        task_id=(task_prefix + ""-validation""),
        python_callable=apply_validate_fn,
        provide_context=True,
        templates_dict={""prediction_path"": prediction_path},
        dag=dag)
    evaluate_validation.set_upstream(evaluate_summary)

    return evaluate_prediction, evaluate_summary, evaluate_validation","['def', 'create_evaluate_ops', '(', 'task_prefix', ',', 'data_format', ',', 'input_paths', ',', 'prediction_path', ',', 'metric_fn_and_keys', ',', 'validate_fn', ',', 'batch_prediction_job_id', '=', 'None', ',', 'project_id', '=', 'None', ',', 'region', '=', 'None', ',', 'dataflow_options', '=', 'None', ',', 'model_uri', '=', 'None', ',', 'model_name', '=', 'None', ',', 'version_name', '=', 'None', ',', 'dag', '=', 'None', ')', ':', ""# Verify that task_prefix doesn't have any special characters except hyphen"", ""# '-', which is the only allowed non-alphanumeric character by Dataflow."", 'if', 'not', 're', '.', 'match', '(', 'r""^[a-zA-Z][-A-Za-z0-9]*$""', ',', 'task_prefix', ')', ':', 'raise', 'AirflowException', '(', '""Malformed task_id for DataFlowPythonOperator (only alphanumeric ""', '""and hyphens are allowed but got: ""', '+', 'task_prefix', ')', 'metric_fn', ',', 'metric_keys', '=', 'metric_fn_and_keys', 'if', 'not', 'callable', '(', 'metric_fn', ')', ':', 'raise', 'AirflowException', '(', '""`metric_fn` param must be callable.""', ')', 'if', 'not', 'callable', '(', 'validate_fn', ')', ':', 'raise', 'AirflowException', '(', '""`validate_fn` param must be callable.""', ')', 'if', 'dag', 'is', 'not', 'None', 'and', 'dag', '.', 'default_args', 'is', 'not', 'None', ':', 'default_args', '=', 'dag', '.', 'default_args', 'project_id', '=', 'project_id', 'or', 'default_args', '.', 'get', '(', ""'project_id'"", ')', 'region', '=', 'region', 'or', 'default_args', '.', 'get', '(', ""'region'"", ')', 'model_name', '=', 'model_name', 'or', 'default_args', '.', 'get', '(', ""'model_name'"", ')', 'version_name', '=', 'version_name', 'or', 'default_args', '.', 'get', '(', ""'version_name'"", ')', 'dataflow_options', '=', 'dataflow_options', 'or', 'default_args', '.', 'get', '(', ""'dataflow_default_options'"", ')', 'evaluate_prediction', '=', 'MLEngineBatchPredictionOperator', '(', 'task_id', '=', '(', 'task_prefix', '+', '""-prediction""', ')', ',', 'project_id', '=', 'project_id', ',', 'job_id', '=', 'batch_prediction_job_id', ',', 'region', '=', 'region', ',', 'data_format', '=', 'data_format', ',', 'input_paths', '=', 'input_paths', ',', 'output_path', '=', 'prediction_path', ',', 'uri', '=', 'model_uri', ',', 'model_name', '=', 'model_name', ',', 'version_name', '=', 'version_name', ',', 'dag', '=', 'dag', ')', 'metric_fn_encoded', '=', 'base64', '.', 'b64encode', '(', 'dill', '.', 'dumps', '(', 'metric_fn', ',', 'recurse', '=', 'True', ')', ')', 'evaluate_summary', '=', 'DataFlowPythonOperator', '(', 'task_id', '=', '(', 'task_prefix', '+', '""-summary""', ')', ',', 'py_options', '=', '[', '""-m""', ']', ',', 'py_file', '=', '""airflow.contrib.utils.mlengine_prediction_summary""', ',', 'dataflow_default_options', '=', 'dataflow_options', ',', 'options', '=', '{', '""prediction_path""', ':', 'prediction_path', ',', '""metric_fn_encoded""', ':', 'metric_fn_encoded', ',', '""metric_keys""', ':', ""','"", '.', 'join', '(', 'metric_keys', ')', '}', ',', 'dag', '=', 'dag', ')', 'evaluate_summary', '.', 'set_upstream', '(', 'evaluate_prediction', ')', 'def', 'apply_validate_fn', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'prediction_path', '=', 'kwargs', '[', '""templates_dict""', ']', '[', '""prediction_path""', ']', 'scheme', ',', 'bucket', ',', 'obj', ',', '_', ',', '_', '=', 'urlsplit', '(', 'prediction_path', ')', 'if', 'scheme', '!=', '""gs""', 'or', 'not', 'bucket', 'or', 'not', 'obj', ':', 'raise', 'ValueError', '(', '""Wrong format prediction_path: %s""', ',', 'prediction_path', ')', 'summary', '=', 'os', '.', 'path', '.', 'join', '(', 'obj', '.', 'strip', '(', '""/""', ')', ',', '""prediction.summary.json""', ')', 'gcs_hook', '=', 'GoogleCloudStorageHook', '(', ')', 'summary', '=', 'json', '.', 'loads', '(', 'gcs_hook', '.', 'download', '(', 'bucket', ',', 'summary', ')', ')', 'return', 'validate_fn', '(', 'summary', ')', 'evaluate_validation', '=', 'PythonOperator', '(', 'task_id', '=', '(', 'task_prefix', '+', '""-validation""', ')', ',', 'python_callable', '=', 'apply_validate_fn', ',', 'provide_context', '=', 'True', ',', 'templates_dict', '=', '{', '""prediction_path""', ':', 'prediction_path', '}', ',', 'dag', '=', 'dag', ')', 'evaluate_validation', '.', 'set_upstream', '(', 'evaluate_summary', ')', 'return', 'evaluate_prediction', ',', 'evaluate_summary', ',', 'evaluate_validation']","Creates Operators needed for model evaluation and returns.

    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by
    calling MLEngineBatchPredictionOperator, then summarize and validate
    the result via Cloud Dataflow using DataFlowPythonOperator.

    For details and pricing about Batch prediction, please refer to the website
    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict
    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/

    It returns three chained operators for prediction, summary, and validation,
    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,
    respectively.
    (<prefix> should contain only alphanumeric characters or hyphen.)

    The upstream and downstream can be set accordingly like:
      pred, _, val = create_evaluate_ops(...)
      pred.set_upstream(upstream_op)
      ...
      downstream_op.set_upstream(val)

    Callers will provide two python callables, metric_fn and validate_fn, in
    order to customize the evaluation behavior as they wish.
    - metric_fn receives a dictionary per instance derived from json in the
      batch prediction result. The keys might vary depending on the model.
      It should return a tuple of metrics.
    - validation_fn receives a dictionary of the averaged metrics that metric_fn
      generated over all instances.
      The key/value of the dictionary matches to what's given by
      metric_fn_and_keys arg.
      The dictionary contains an additional metric, 'count' to represent the
      total number of instances received for evaluation.
      The function would raise an exception to mark the task as failed, in a
      case the validation result is not okay to proceed (i.e. to set the trained
      version as default).

    Typical examples are like this:

    def get_metric_fn_and_keys():
        import math  # imports should be outside of the metric_fn below.
        def error_and_squared_error(inst):
            label = float(inst['input_label'])
            classes = float(inst['classes'])  # 0 or 1
            err = abs(classes-label)
            squared_err = math.pow(classes-label, 2)
            return (err, squared_err)  # returns a tuple.
        return error_and_squared_error, ['err', 'mse']  # key order must match.

    def validate_err_and_count(summary):
        if summary['err'] > 0.2:
            raise ValueError('Too high err>0.2; summary=%s' % summary)
        if summary['mse'] > 0.05:
            raise ValueError('Too high mse>0.05; summary=%s' % summary)
        if summary['count'] < 1000:
            raise ValueError('Too few instances<1000; summary=%s' % summary)
        return summary

    For the details on the other BatchPrediction-related arguments (project_id,
    job_id, region, data_format, input_paths, prediction_path, model_uri),
    please refer to MLEngineBatchPredictionOperator too.

    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and
        hyphen are allowed (no underscores), since this will be used as dataflow
        job name, which doesn't allow other characters.
    :type task_prefix: str

    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'
    :type data_format: str

    :param input_paths: a list of input paths to be sent to BatchPrediction.
    :type input_paths: list[str]

    :param prediction_path: GCS path to put the prediction results in.
    :type prediction_path: str

    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:
        - metric_fn is a function that accepts a dictionary (for an instance),
          and returns a tuple of metric(s) that it calculates.
        - metric_keys is a list of strings to denote the key of each metric.
    :type metric_fn_and_keys: tuple of a function and a list[str]

    :param validate_fn: a function to validate whether the averaged metric(s) is
        good enough to push the model.
    :type validate_fn: function

    :param batch_prediction_job_id: the id to use for the Cloud ML Batch
        prediction job. Passed directly to the MLEngineBatchPredictionOperator as
        the job_id argument.
    :type batch_prediction_job_id: str

    :param project_id: the Google Cloud Platform project id in which to execute
        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s
        `default_args['project_id']` will be used.
    :type project_id: str

    :param region: the Google Cloud Platform region in which to execute Cloud ML
        Batch Prediction and Dataflow jobs. If None, then the `dag`'s
        `default_args['region']` will be used.
    :type region: str

    :param dataflow_options: options to run Dataflow jobs. If None, then the
        `dag`'s `default_args['dataflow_default_options']` will be used.
    :type dataflow_options: dictionary

    :param model_uri: GCS path of the model exported by Tensorflow using
        tensorflow.estimator.export_savedmodel(). It cannot be used with
        model_name or version_name below. See MLEngineBatchPredictionOperator for
        more detail.
    :type model_uri: str

    :param model_name: Used to indicate a model to use for prediction. Can be
        used in combination with version_name, but cannot be used together with
        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,
        then the `dag`'s `default_args['model_name']` will be used.
    :type model_name: str

    :param version_name: Used to indicate a model version to use for prediction,
        in combination with model_name. Cannot be used together with model_uri.
        See MLEngineBatchPredictionOperator for more detail. If None, then the
        `dag`'s `default_args['version_name']` will be used.
    :type version_name: str

    :param dag: The `DAG` to use for all Operators.
    :type dag: airflow.models.DAG

    :returns: a tuple of three operators, (prediction, summary, validation)
    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,
                  PythonOperator)","['Creates', 'Operators', 'needed', 'for', 'model', 'evaluation', 'and', 'returns', '.']",python,test,"['creates', 'operators', 'needed', 'for', 'model', 'evaluation', 'and', 'returns', '.']",creates operators needed for model evaluation and returns .,"['def', 'create_evaluate_ops', '(', 'task_prefix', ',', 'data_format', ',', 'input_paths', ',', 'prediction_path', ',', 'metric_fn_and_keys', ',', 'validate_fn', ',', 'batch_prediction_job_id', '=', 'none', ',', 'project_id', '=', 'none', ',', 'region', '=', 'none', ',', 'dataflow_options', '=', 'none', ',', 'model_uri', '=', 'none', ',', 'model_name', '=', 'none', ',', 'version_name', '=', 'none', ',', 'dag', '=', 'none', ')', ':', ""# verify that task_prefix doesn't have any special characters except hyphen"", ""# '-', which is the only allowed non-alphanumeric character by dataflow."", 'if', 'not', 're', '.', 'match', '(', 'r""^[a-za-z][-a-za-z0-9]*$""', ',', 'task_prefix', ')', ':', 'raise', 'airflowexception', '(', '""malformed task_id for dataflowpythonoperator (only alphanumeric ""', '""and hyphens are allowed but got: ""', '+', 'task_prefix', ')', 'metric_fn', ',', 'metric_keys', '=', 'metric_fn_and_keys', 'if', 'not', 'callable', '(', 'metric_fn', ')', ':', 'raise', 'airflowexception', '(', '""`metric_fn` param must be callable.""', ')', 'if', 'not', 'callable', '(', 'validate_fn', ')', ':', 'raise', 'airflowexception', '(', '""`validate_fn` param must be callable.""', ')', 'if', 'dag', 'is', 'not', 'none', 'and', 'dag', '.', 'default_args', 'is', 'not', 'none', ':', 'default_args', '=', 'dag', '.', 'default_args', 'project_id', '=', 'project_id', 'or', 'default_args', '.', 'get', '(', ""'project_id'"", ')', 'region', '=', 'region', 'or', 'default_args', '.', 'get', '(', ""'region'"", ')', 'model_name', '=', 'model_name', 'or', 'default_args', '.', 'get', '(', ""'model_name'"", ')', 'version_name', '=', 'version_name', 'or', 'default_args', '.', 'get', '(', ""'version_name'"", ')', 'dataflow_options', '=', 'dataflow_options', 'or', 'default_args', '.', 'get', '(', ""'dataflow_default_options'"", ')', 'evaluate_prediction', '=', 'mlenginebatchpredictionoperator', '(', 'task_id', '=', '(', 'task_prefix', '+', '""-prediction""', ')', ',', 'project_id', '=', 'project_id', ',', 'job_id', '=', 'batch_prediction_job_id', ',', 'region', '=', 'region', ',', 'data_format', '=', 'data_format', ',', 'input_paths', '=', 'input_paths', ',', 'output_path', '=', 'prediction_path', ',', 'uri', '=', 'model_uri', ',', 'model_name', '=', 'model_name', ',', 'version_name', '=', 'version_name', ',', 'dag', '=', 'dag', ')', 'metric_fn_encoded', '=', 'base64', '.', 'b64encode', '(', 'dill', '.', 'dumps', '(', 'metric_fn', ',', 'recurse', '=', 'true', ')', ')', 'evaluate_summary', '=', 'dataflowpythonoperator', '(', 'task_id', '=', '(', 'task_prefix', '+', '""-summary""', ')', ',', 'py_options', '=', '[', '""-m""', ']', ',', 'py_file', '=', '""airflow.contrib.utils.mlengine_prediction_summary""', ',', 'dataflow_default_options', '=', 'dataflow_options', ',', 'options', '=', '{', '""prediction_path""', ':', 'prediction_path', ',', '""metric_fn_encoded""', ':', 'metric_fn_encoded', ',', '""metric_keys""', ':', ""','"", '.', 'join', '(', 'metric_keys', ')', '}', ',', 'dag', '=', 'dag', ')', 'evaluate_summary', '.', 'set_upstream', '(', 'evaluate_prediction', ')', 'def', 'apply_validate_fn', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'prediction_path', '=', 'kwargs', '[', '""templates_dict""', ']', '[', '""prediction_path""', ']', 'scheme', ',', 'bucket', ',', 'obj', ',', '_', ',', '_', '=', 'urlsplit', '(', 'prediction_path', ')', 'if', 'scheme', '!=', '""gs""', 'or', 'not', 'bucket', 'or', 'not', 'obj', ':', 'raise', 'valueerror', '(', '""wrong format prediction_path: %s""', ',', 'prediction_path', ')', 'summary', '=', 'os', '.', 'path', '.', 'join', '(', 'obj', '.', 'strip', '(', '""/""', ')', ',', '""prediction.summary.json""', ')', 'gcs_hook', '=', 'googlecloudstoragehook', '(', ')', 'summary', '=', 'json', '.', 'loads', '(', 'gcs_hook', '.', 'download', '(', 'bucket', ',', 'summary', ')', ')', 'return', 'validate_fn', '(', 'summary', ')', 'evaluate_validation', '=', 'pythonoperator', '(', 'task_id', '=', '(', 'task_prefix', '+', '""-validation""', ')', ',', 'python_callable', '=', 'apply_validate_fn', ',', 'provide_context', '=', 'true', ',', 'templates_dict', '=', '{', '""prediction_path""', ':', 'prediction_path', '}', ',', 'dag', '=', 'dag', ')', 'evaluate_validation', '.', 'set_upstream', '(', 'evaluate_summary', ')', 'return', 'evaluate_prediction', ',', 'evaluate_summary', ',', 'evaluate_validation']","def create_evaluate_ops ( task_prefix , data_format , input_paths , prediction_path , metric_fn_and_keys , validate_fn , batch_prediction_job_id = none , project_id = none , region = none , dataflow_options = none , model_uri = none , model_name = none , version_name = none , dag = none ) : # verify that task_prefix doesn't have any special characters except hyphen # '-', which is the only allowed non-alphanumeric character by dataflow. if not re . match ( r""^[a-za-z][-a-za-z0-9]*$"" , task_prefix ) : raise airflowexception ( ""malformed task_id for dataflowpythonoperator (only alphanumeric "" ""and hyphens are allowed but got: "" + task_prefix ) metric_fn , metric_keys = metric_fn_and_keys if not callable ( metric_fn ) : raise airflowexception ( ""`metric_fn` param must be callable."" ) if not callable ( validate_fn ) : raise airflowexception ( ""`validate_fn` param must be callable."" ) if dag is not none and dag . default_args is not none : default_args = dag . default_args project_id = project_id or default_args . get ( 'project_id' ) region = region or default_args . get ( 'region' ) model_name = model_name or default_args . get ( 'model_name' ) version_name = version_name or default_args . get ( 'version_name' ) dataflow_options = dataflow_options or default_args . get ( 'dataflow_default_options' ) evaluate_prediction = mlenginebatchpredictionoperator ( task_id = ( task_prefix + ""-prediction"" ) , project_id = project_id , job_id = batch_prediction_job_id , region = region , data_format = data_format , input_paths = input_paths , output_path = prediction_path , uri = model_uri , model_name = model_name , version_name = version_name , dag = dag ) metric_fn_encoded = base64 . b64encode ( dill . dumps ( metric_fn , recurse = true ) ) evaluate_summary = dataflowpythonoperator ( task_id = ( task_prefix + ""-summary"" ) , py_options = [ ""-m"" ] , py_file = ""airflow.contrib.utils.mlengine_prediction_summary"" , dataflow_default_options = dataflow_options , options = { ""prediction_path"" : prediction_path , ""metric_fn_encoded"" : metric_fn_encoded , ""metric_keys"" : ',' . join ( metric_keys ) } , dag = dag ) evaluate_summary . set_upstream ( evaluate_prediction ) def apply_validate_fn ( * args , * * kwargs ) : prediction_path = kwargs [ ""templates_dict"" ] [ ""prediction_path"" ] scheme , bucket , obj , _ , _ = urlsplit ( prediction_path ) if scheme != ""gs"" or not bucket or not obj : raise valueerror ( ""wrong format prediction_path: %s"" , prediction_path ) summary = os . path . join ( obj . strip ( ""/"" ) , ""prediction.summary.json"" ) gcs_hook = googlecloudstoragehook ( ) summary = json . loads ( gcs_hook . download ( bucket , summary ) ) return validate_fn ( summary ) evaluate_validation = pythonoperator ( task_id = ( task_prefix + ""-validation"" ) , python_callable = apply_validate_fn , provide_context = true , templates_dict = { ""prediction_path"" : prediction_path } , dag = dag ) evaluate_validation . set_upstream ( evaluate_summary ) return evaluate_prediction , evaluate_summary , evaluate_validation"
111,apache/airflow,airflow/utils/file.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/file.py#L42-L59,"def mkdirs(path, mode):
    """"""
    Creates the directory specified by path, creating intermediate directories
    as necessary. If directory already exists, this is a no-op.

    :param path: The directory to create
    :type path: str
    :param mode: The mode to give to the directory e.g. 0o755, ignores umask
    :type mode: int
    """"""
    try:
        o_umask = os.umask(0)
        os.makedirs(path, mode)
    except OSError:
        if not os.path.isdir(path):
            raise
    finally:
        os.umask(o_umask)","['def', 'mkdirs', '(', 'path', ',', 'mode', ')', ':', 'try', ':', 'o_umask', '=', 'os', '.', 'umask', '(', '0', ')', 'os', '.', 'makedirs', '(', 'path', ',', 'mode', ')', 'except', 'OSError', ':', 'if', 'not', 'os', '.', 'path', '.', 'isdir', '(', 'path', ')', ':', 'raise', 'finally', ':', 'os', '.', 'umask', '(', 'o_umask', ')']","Creates the directory specified by path, creating intermediate directories
    as necessary. If directory already exists, this is a no-op.

    :param path: The directory to create
    :type path: str
    :param mode: The mode to give to the directory e.g. 0o755, ignores umask
    :type mode: int","['Creates', 'the', 'directory', 'specified', 'by', 'path', 'creating', 'intermediate', 'directories', 'as', 'necessary', '.', 'If', 'directory', 'already', 'exists', 'this', 'is', 'a', 'no', '-', 'op', '.']",python,test,"['creates', 'the', 'directory', 'specified', 'by', 'path', 'creating', 'intermediate', 'directories', 'as', 'necessary', '.', 'if', 'directory', 'already', 'exists', 'this', 'is', 'a', 'no', '-', 'op', '.']",creates the directory specified by path creating intermediate directories as necessary . if directory already exists this is a no - op .,"['def', 'mkdirs', '(', 'path', ',', 'mode', ')', ':', 'try', ':', 'o_umask', '=', 'os', '.', 'umask', '(', '0', ')', 'os', '.', 'makedirs', '(', 'path', ',', 'mode', ')', 'except', 'oserror', ':', 'if', 'not', 'os', '.', 'path', '.', 'isdir', '(', 'path', ')', ':', 'raise', 'finally', ':', 'os', '.', 'umask', '(', 'o_umask', ')']","def mkdirs ( path , mode ) : try : o_umask = os . umask ( 0 ) os . makedirs ( path , mode ) except oserror : if not os . path . isdir ( path ) : raise finally : os . umask ( o_umask )"
112,apache/airflow,airflow/operators/check_operator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/operators/check_operator.py#L98-L110,"def _convert_to_float_if_possible(s):
    """"""
    A small helper function to convert a string to a numeric value
    if appropriate

    :param s: the string to be converted
    :type s: str
    """"""
    try:
        ret = float(s)
    except (ValueError, TypeError):
        ret = s
    return ret","['def', '_convert_to_float_if_possible', '(', 's', ')', ':', 'try', ':', 'ret', '=', 'float', '(', 's', ')', 'except', '(', 'ValueError', ',', 'TypeError', ')', ':', 'ret', '=', 's', 'return', 'ret']","A small helper function to convert a string to a numeric value
    if appropriate

    :param s: the string to be converted
    :type s: str","['A', 'small', 'helper', 'function', 'to', 'convert', 'a', 'string', 'to', 'a', 'numeric', 'value', 'if', 'appropriate']",python,test,"['a', 'small', 'helper', 'function', 'to', 'convert', 'a', 'string', 'to', 'a', 'numeric', 'value', 'if', 'appropriate']",a small helper function to convert a string to a numeric value if appropriate,"['def', '_convert_to_float_if_possible', '(', 's', ')', ':', 'try', ':', 'ret', '=', 'float', '(', 's', ')', 'except', '(', 'valueerror', ',', 'typeerror', ')', ':', 'ret', '=', 's', 'return', 'ret']","def _convert_to_float_if_possible ( s ) : try : ret = float ( s ) except ( valueerror , typeerror ) : ret = s return ret"
113,apache/airflow,airflow/utils/timezone.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L52-L64,"def utcnow():
    """"""
    Get the current date and time in UTC
    :return:
    """"""

    # pendulum utcnow() is not used as that sets a TimezoneInfo object
    # instead of a Timezone. This is not pickable and also creates issues
    # when using replace()
    d = dt.datetime.utcnow()
    d = d.replace(tzinfo=utc)

    return d","['def', 'utcnow', '(', ')', ':', '# pendulum utcnow() is not used as that sets a TimezoneInfo object', '# instead of a Timezone. This is not pickable and also creates issues', '# when using replace()', 'd', '=', 'dt', '.', 'datetime', '.', 'utcnow', '(', ')', 'd', '=', 'd', '.', 'replace', '(', 'tzinfo', '=', 'utc', ')', 'return', 'd']","Get the current date and time in UTC
    :return:","['Get', 'the', 'current', 'date', 'and', 'time', 'in', 'UTC', ':', 'return', ':']",python,test,"['get', 'the', 'current', 'date', 'and', 'time', 'in', 'utc', ':', 'return', ':']",get the current date and time in utc : return :,"['def', 'utcnow', '(', ')', ':', '# pendulum utcnow() is not used as that sets a timezoneinfo object', '# instead of a timezone. this is not pickable and also creates issues', '# when using replace()', 'd', '=', 'dt', '.', 'datetime', '.', 'utcnow', '(', ')', 'd', '=', 'd', '.', 'replace', '(', 'tzinfo', '=', 'utc', ')', 'return', 'd']",def utcnow ( ) : # pendulum utcnow() is not used as that sets a timezoneinfo object # instead of a timezone. this is not pickable and also creates issues # when using replace() d = dt . datetime . utcnow ( ) d = d . replace ( tzinfo = utc ) return d
114,apache/airflow,airflow/utils/timezone.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L67-L79,"def utc_epoch():
    """"""
    Gets the epoch in the users timezone
    :return:
    """"""

    # pendulum utcnow() is not used as that sets a TimezoneInfo object
    # instead of a Timezone. This is not pickable and also creates issues
    # when using replace()
    d = dt.datetime(1970, 1, 1)
    d = d.replace(tzinfo=utc)

    return d","['def', 'utc_epoch', '(', ')', ':', '# pendulum utcnow() is not used as that sets a TimezoneInfo object', '# instead of a Timezone. This is not pickable and also creates issues', '# when using replace()', 'd', '=', 'dt', '.', 'datetime', '(', '1970', ',', '1', ',', '1', ')', 'd', '=', 'd', '.', 'replace', '(', 'tzinfo', '=', 'utc', ')', 'return', 'd']","Gets the epoch in the users timezone
    :return:","['Gets', 'the', 'epoch', 'in', 'the', 'users', 'timezone', ':', 'return', ':']",python,test,"['gets', 'the', 'epoch', 'in', 'the', 'users', 'timezone', ':', 'return', ':']",gets the epoch in the users timezone : return :,"['def', 'utc_epoch', '(', ')', ':', '# pendulum utcnow() is not used as that sets a timezoneinfo object', '# instead of a timezone. this is not pickable and also creates issues', '# when using replace()', 'd', '=', 'dt', '.', 'datetime', '(', '1970', ',', '1', ',', '1', ')', 'd', '=', 'd', '.', 'replace', '(', 'tzinfo', '=', 'utc', ')', 'return', 'd']","def utc_epoch ( ) : # pendulum utcnow() is not used as that sets a timezoneinfo object # instead of a timezone. this is not pickable and also creates issues # when using replace() d = dt . datetime ( 1970 , 1 , 1 ) d = d . replace ( tzinfo = utc ) return d"
115,apache/airflow,airflow/utils/timezone.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L82-L95,"def convert_to_utc(value):
    """"""
    Returns the datetime with the default timezone added if timezone
    information was not associated
    :param value: datetime
    :return: datetime with tzinfo
    """"""
    if not value:
        return value

    if not is_localized(value):
        value = pendulum.instance(value, TIMEZONE)

    return value.astimezone(utc)","['def', 'convert_to_utc', '(', 'value', ')', ':', 'if', 'not', 'value', ':', 'return', 'value', 'if', 'not', 'is_localized', '(', 'value', ')', ':', 'value', '=', 'pendulum', '.', 'instance', '(', 'value', ',', 'TIMEZONE', ')', 'return', 'value', '.', 'astimezone', '(', 'utc', ')']","Returns the datetime with the default timezone added if timezone
    information was not associated
    :param value: datetime
    :return: datetime with tzinfo","['Returns', 'the', 'datetime', 'with', 'the', 'default', 'timezone', 'added', 'if', 'timezone', 'information', 'was', 'not', 'associated', ':', 'param', 'value', ':', 'datetime', ':', 'return', ':', 'datetime', 'with', 'tzinfo']",python,test,"['returns', 'the', 'datetime', 'with', 'the', 'default', 'timezone', 'added', 'if', 'timezone', 'information', 'was', 'not', 'associated', ':', 'param', 'value', ':', 'datetime', ':', 'return', ':', 'datetime', 'with', 'tzinfo']",returns the datetime with the default timezone added if timezone information was not associated : param value : datetime : return : datetime with tzinfo,"['def', 'convert_to_utc', '(', 'value', ')', ':', 'if', 'not', 'value', ':', 'return', 'value', 'if', 'not', 'is_localized', '(', 'value', ')', ':', 'value', '=', 'pendulum', '.', 'instance', '(', 'value', ',', 'timezone', ')', 'return', 'value', '.', 'astimezone', '(', 'utc', ')']","def convert_to_utc ( value ) : if not value : return value if not is_localized ( value ) : value = pendulum . instance ( value , timezone ) return value . astimezone ( utc )"
116,apache/airflow,airflow/utils/timezone.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L98-L128,"def make_aware(value, timezone=None):
    """"""
    Make a naive datetime.datetime in a given time zone aware.

    :param value: datetime
    :param timezone: timezone
    :return: localized datetime in settings.TIMEZONE or timezone

    """"""
    if timezone is None:
        timezone = TIMEZONE

    # Check that we won't overwrite the timezone of an aware datetime.
    if is_localized(value):
        raise ValueError(
            ""make_aware expects a naive datetime, got %s"" % value)
    if hasattr(value, 'fold'):
        # In case of python 3.6 we want to do the same that pendulum does for python3.5
        # i.e in case we move clock back we want to schedule the run at the time of the second
        # instance of the same clock time rather than the first one.
        # Fold parameter has no impact in other cases so we can safely set it to 1 here
        value = value.replace(fold=1)
    if hasattr(timezone, 'localize'):
        # This method is available for pytz time zones.
        return timezone.localize(value)
    elif hasattr(timezone, 'convert'):
        # For pendulum
        return timezone.convert(value)
    else:
        # This may be wrong around DST changes!
        return value.replace(tzinfo=timezone)","['def', 'make_aware', '(', 'value', ',', 'timezone', '=', 'None', ')', ':', 'if', 'timezone', 'is', 'None', ':', 'timezone', '=', 'TIMEZONE', ""# Check that we won't overwrite the timezone of an aware datetime."", 'if', 'is_localized', '(', 'value', ')', ':', 'raise', 'ValueError', '(', '""make_aware expects a naive datetime, got %s""', '%', 'value', ')', 'if', 'hasattr', '(', 'value', ',', ""'fold'"", ')', ':', '# In case of python 3.6 we want to do the same that pendulum does for python3.5', '# i.e in case we move clock back we want to schedule the run at the time of the second', '# instance of the same clock time rather than the first one.', '# Fold parameter has no impact in other cases so we can safely set it to 1 here', 'value', '=', 'value', '.', 'replace', '(', 'fold', '=', '1', ')', 'if', 'hasattr', '(', 'timezone', ',', ""'localize'"", ')', ':', '# This method is available for pytz time zones.', 'return', 'timezone', '.', 'localize', '(', 'value', ')', 'elif', 'hasattr', '(', 'timezone', ',', ""'convert'"", ')', ':', '# For pendulum', 'return', 'timezone', '.', 'convert', '(', 'value', ')', 'else', ':', '# This may be wrong around DST changes!', 'return', 'value', '.', 'replace', '(', 'tzinfo', '=', 'timezone', ')']","Make a naive datetime.datetime in a given time zone aware.

    :param value: datetime
    :param timezone: timezone
    :return: localized datetime in settings.TIMEZONE or timezone","['Make', 'a', 'naive', 'datetime', '.', 'datetime', 'in', 'a', 'given', 'time', 'zone', 'aware', '.']",python,test,"['make', 'a', 'naive', 'datetime', '.', 'datetime', 'in', 'a', 'given', 'time', 'zone', 'aware', '.']",make a naive datetime . datetime in a given time zone aware .,"['def', 'make_aware', '(', 'value', ',', 'timezone', '=', 'none', ')', ':', 'if', 'timezone', 'is', 'none', ':', 'timezone', '=', 'timezone', ""# check that we won't overwrite the timezone of an aware datetime."", 'if', 'is_localized', '(', 'value', ')', ':', 'raise', 'valueerror', '(', '""make_aware expects a naive datetime, got %s""', '%', 'value', ')', 'if', 'hasattr', '(', 'value', ',', ""'fold'"", ')', ':', '# in case of python 3.6 we want to do the same that pendulum does for python3.5', '# i.e in case we move clock back we want to schedule the run at the time of the second', '# instance of the same clock time rather than the first one.', '# fold parameter has no impact in other cases so we can safely set it to 1 here', 'value', '=', 'value', '.', 'replace', '(', 'fold', '=', '1', ')', 'if', 'hasattr', '(', 'timezone', ',', ""'localize'"", ')', ':', '# this method is available for pytz time zones.', 'return', 'timezone', '.', 'localize', '(', 'value', ')', 'elif', 'hasattr', '(', 'timezone', ',', ""'convert'"", ')', ':', '# for pendulum', 'return', 'timezone', '.', 'convert', '(', 'value', ')', 'else', ':', '# this may be wrong around dst changes!', 'return', 'value', '.', 'replace', '(', 'tzinfo', '=', 'timezone', ')']","def make_aware ( value , timezone = none ) : if timezone is none : timezone = timezone # check that we won't overwrite the timezone of an aware datetime. if is_localized ( value ) : raise valueerror ( ""make_aware expects a naive datetime, got %s"" % value ) if hasattr ( value , 'fold' ) : # in case of python 3.6 we want to do the same that pendulum does for python3.5 # i.e in case we move clock back we want to schedule the run at the time of the second # instance of the same clock time rather than the first one. # fold parameter has no impact in other cases so we can safely set it to 1 here value = value . replace ( fold = 1 ) if hasattr ( timezone , 'localize' ) : # this method is available for pytz time zones. return timezone . localize ( value ) elif hasattr ( timezone , 'convert' ) : # for pendulum return timezone . convert ( value ) else : # this may be wrong around dst changes! return value . replace ( tzinfo = timezone )"
117,apache/airflow,airflow/utils/timezone.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L131-L157,"def make_naive(value, timezone=None):
    """"""
    Make an aware datetime.datetime naive in a given time zone.

    :param value: datetime
    :param timezone: timezone
    :return: naive datetime
    """"""
    if timezone is None:
        timezone = TIMEZONE

    # Emulate the behavior of astimezone() on Python < 3.6.
    if is_naive(value):
        raise ValueError(""make_naive() cannot be applied to a naive datetime"")

    o = value.astimezone(timezone)

    # cross library compatibility
    naive = dt.datetime(o.year,
                        o.month,
                        o.day,
                        o.hour,
                        o.minute,
                        o.second,
                        o.microsecond)

    return naive","['def', 'make_naive', '(', 'value', ',', 'timezone', '=', 'None', ')', ':', 'if', 'timezone', 'is', 'None', ':', 'timezone', '=', 'TIMEZONE', '# Emulate the behavior of astimezone() on Python < 3.6.', 'if', 'is_naive', '(', 'value', ')', ':', 'raise', 'ValueError', '(', '""make_naive() cannot be applied to a naive datetime""', ')', 'o', '=', 'value', '.', 'astimezone', '(', 'timezone', ')', '# cross library compatibility', 'naive', '=', 'dt', '.', 'datetime', '(', 'o', '.', 'year', ',', 'o', '.', 'month', ',', 'o', '.', 'day', ',', 'o', '.', 'hour', ',', 'o', '.', 'minute', ',', 'o', '.', 'second', ',', 'o', '.', 'microsecond', ')', 'return', 'naive']","Make an aware datetime.datetime naive in a given time zone.

    :param value: datetime
    :param timezone: timezone
    :return: naive datetime","['Make', 'an', 'aware', 'datetime', '.', 'datetime', 'naive', 'in', 'a', 'given', 'time', 'zone', '.']",python,test,"['make', 'an', 'aware', 'datetime', '.', 'datetime', 'naive', 'in', 'a', 'given', 'time', 'zone', '.']",make an aware datetime . datetime naive in a given time zone .,"['def', 'make_naive', '(', 'value', ',', 'timezone', '=', 'none', ')', ':', 'if', 'timezone', 'is', 'none', ':', 'timezone', '=', 'timezone', '# emulate the behavior of astimezone() on python < 3.6.', 'if', 'is_naive', '(', 'value', ')', ':', 'raise', 'valueerror', '(', '""make_naive() cannot be applied to a naive datetime""', ')', 'o', '=', 'value', '.', 'astimezone', '(', 'timezone', ')', '# cross library compatibility', 'naive', '=', 'dt', '.', 'datetime', '(', 'o', '.', 'year', ',', 'o', '.', 'month', ',', 'o', '.', 'day', ',', 'o', '.', 'hour', ',', 'o', '.', 'minute', ',', 'o', '.', 'second', ',', 'o', '.', 'microsecond', ')', 'return', 'naive']","def make_naive ( value , timezone = none ) : if timezone is none : timezone = timezone # emulate the behavior of astimezone() on python < 3.6. if is_naive ( value ) : raise valueerror ( ""make_naive() cannot be applied to a naive datetime"" ) o = value . astimezone ( timezone ) # cross library compatibility naive = dt . datetime ( o . year , o . month , o . day , o . hour , o . minute , o . second , o . microsecond ) return naive"
118,apache/airflow,airflow/utils/timezone.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L160-L169,"def datetime(*args, **kwargs):
    """"""
    Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified

    :return: datetime.datetime
    """"""
    if 'tzinfo' not in kwargs:
        kwargs['tzinfo'] = TIMEZONE

    return dt.datetime(*args, **kwargs)","['def', 'datetime', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'if', ""'tzinfo'"", 'not', 'in', 'kwargs', ':', 'kwargs', '[', ""'tzinfo'"", ']', '=', 'TIMEZONE', 'return', 'dt', '.', 'datetime', '(', '*', 'args', ',', '*', '*', 'kwargs', ')']","Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified

    :return: datetime.datetime","['Wrapper', 'around', 'datetime', '.', 'datetime', 'that', 'adds', 'settings', '.', 'TIMEZONE', 'if', 'tzinfo', 'not', 'specified']",python,test,"['wrapper', 'around', 'datetime', '.', 'datetime', 'that', 'adds', 'settings', '.', 'timezone', 'if', 'tzinfo', 'not', 'specified']",wrapper around datetime . datetime that adds settings . timezone if tzinfo not specified,"['def', 'datetime', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'if', ""'tzinfo'"", 'not', 'in', 'kwargs', ':', 'kwargs', '[', ""'tzinfo'"", ']', '=', 'timezone', 'return', 'dt', '.', 'datetime', '(', '*', 'args', ',', '*', '*', 'kwargs', ')']","def datetime ( * args , * * kwargs ) : if 'tzinfo' not in kwargs : kwargs [ 'tzinfo' ] = timezone return dt . datetime ( * args , * * kwargs )"
119,apache/airflow,airflow/contrib/operators/gcp_container_operator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/gcp_container_operator.py#L282-L308,"def _set_env_from_extras(self, extras):
        """"""
        Sets the environment variable `GOOGLE_APPLICATION_CREDENTIALS` with either:

        - The path to the keyfile from the specified connection id
        - A generated file's path if the user specified JSON in the connection id. The
            file is assumed to be deleted after the process dies due to how mkstemp()
            works.

        The environment variable is used inside the gcloud command to determine correct
        service account to use.
        """"""
        key_path = self._get_field(extras, 'key_path', False)
        keyfile_json_str = self._get_field(extras, 'keyfile_dict', False)

        if not key_path and not keyfile_json_str:
            self.log.info('Using gcloud with application default credentials.')
        elif key_path:
            os.environ[G_APP_CRED] = key_path
        else:
            # Write service account JSON to secure file for gcloud to reference
            service_key = tempfile.NamedTemporaryFile(delete=False)
            service_key.write(keyfile_json_str)
            os.environ[G_APP_CRED] = service_key.name
            # Return file object to have a pointer to close after use,
            # thus deleting from file system.
            return service_key","['def', '_set_env_from_extras', '(', 'self', ',', 'extras', ')', ':', 'key_path', '=', 'self', '.', '_get_field', '(', 'extras', ',', ""'key_path'"", ',', 'False', ')', 'keyfile_json_str', '=', 'self', '.', '_get_field', '(', 'extras', ',', ""'keyfile_dict'"", ',', 'False', ')', 'if', 'not', 'key_path', 'and', 'not', 'keyfile_json_str', ':', 'self', '.', 'log', '.', 'info', '(', ""'Using gcloud with application default credentials.'"", ')', 'elif', 'key_path', ':', 'os', '.', 'environ', '[', 'G_APP_CRED', ']', '=', 'key_path', 'else', ':', '# Write service account JSON to secure file for gcloud to reference', 'service_key', '=', 'tempfile', '.', 'NamedTemporaryFile', '(', 'delete', '=', 'False', ')', 'service_key', '.', 'write', '(', 'keyfile_json_str', ')', 'os', '.', 'environ', '[', 'G_APP_CRED', ']', '=', 'service_key', '.', 'name', '# Return file object to have a pointer to close after use,', '# thus deleting from file system.', 'return', 'service_key']","Sets the environment variable `GOOGLE_APPLICATION_CREDENTIALS` with either:

        - The path to the keyfile from the specified connection id
        - A generated file's path if the user specified JSON in the connection id. The
            file is assumed to be deleted after the process dies due to how mkstemp()
            works.

        The environment variable is used inside the gcloud command to determine correct
        service account to use.","['Sets', 'the', 'environment', 'variable', 'GOOGLE_APPLICATION_CREDENTIALS', 'with', 'either', ':']",python,test,"['sets', 'the', 'environment', 'variable', 'google_application_credentials', 'with', 'either', ':']",sets the environment variable google_application_credentials with either :,"['def', '_set_env_from_extras', '(', 'self', ',', 'extras', ')', ':', 'key_path', '=', 'self', '.', '_get_field', '(', 'extras', ',', ""'key_path'"", ',', 'false', ')', 'keyfile_json_str', '=', 'self', '.', '_get_field', '(', 'extras', ',', ""'keyfile_dict'"", ',', 'false', ')', 'if', 'not', 'key_path', 'and', 'not', 'keyfile_json_str', ':', 'self', '.', 'log', '.', 'info', '(', ""'using gcloud with application default credentials.'"", ')', 'elif', 'key_path', ':', 'os', '.', 'environ', '[', 'g_app_cred', ']', '=', 'key_path', 'else', ':', '# write service account json to secure file for gcloud to reference', 'service_key', '=', 'tempfile', '.', 'namedtemporaryfile', '(', 'delete', '=', 'false', ')', 'service_key', '.', 'write', '(', 'keyfile_json_str', ')', 'os', '.', 'environ', '[', 'g_app_cred', ']', '=', 'service_key', '.', 'name', '# return file object to have a pointer to close after use,', '# thus deleting from file system.', 'return', 'service_key']","def _set_env_from_extras ( self , extras ) : key_path = self . _get_field ( extras , 'key_path' , false ) keyfile_json_str = self . _get_field ( extras , 'keyfile_dict' , false ) if not key_path and not keyfile_json_str : self . log . info ( 'using gcloud with application default credentials.' ) elif key_path : os . environ [ g_app_cred ] = key_path else : # write service account json to secure file for gcloud to reference service_key = tempfile . namedtemporaryfile ( delete = false ) service_key . write ( keyfile_json_str ) os . environ [ g_app_cred ] = service_key . name # return file object to have a pointer to close after use, # thus deleting from file system. return service_key"
120,apache/airflow,airflow/contrib/operators/gcp_container_operator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/gcp_container_operator.py#L310-L322,"def _get_field(self, extras, field, default=None):
        """"""
        Fetches a field from extras, and returns it. This is some Airflow
        magic. The google_cloud_platform hook type adds custom UI elements
        to the hook page, which allow admins to specify service_account,
        key_path, etc. They get formatted as shown below.
        """"""
        long_f = 'extra__google_cloud_platform__{}'.format(field)
        if long_f in extras:
            return extras[long_f]
        else:
            self.log.info('Field %s not found in extras.', field)
            return default","['def', '_get_field', '(', 'self', ',', 'extras', ',', 'field', ',', 'default', '=', 'None', ')', ':', 'long_f', '=', ""'extra__google_cloud_platform__{}'"", '.', 'format', '(', 'field', ')', 'if', 'long_f', 'in', 'extras', ':', 'return', 'extras', '[', 'long_f', ']', 'else', ':', 'self', '.', 'log', '.', 'info', '(', ""'Field %s not found in extras.'"", ',', 'field', ')', 'return', 'default']","Fetches a field from extras, and returns it. This is some Airflow
        magic. The google_cloud_platform hook type adds custom UI elements
        to the hook page, which allow admins to specify service_account,
        key_path, etc. They get formatted as shown below.","['Fetches', 'a', 'field', 'from', 'extras', 'and', 'returns', 'it', '.', 'This', 'is', 'some', 'Airflow', 'magic', '.', 'The', 'google_cloud_platform', 'hook', 'type', 'adds', 'custom', 'UI', 'elements', 'to', 'the', 'hook', 'page', 'which', 'allow', 'admins', 'to', 'specify', 'service_account', 'key_path', 'etc', '.', 'They', 'get', 'formatted', 'as', 'shown', 'below', '.']",python,test,"['fetches', 'a', 'field', 'from', 'extras', 'and', 'returns', 'it', '.', 'this', 'is', 'some', 'airflow', 'magic', '.', 'the', 'google_cloud_platform', 'hook', 'type', 'adds', 'custom', 'ui', 'elements', 'to', 'the', 'hook', 'page', 'which', 'allow', 'admins', 'to', 'specify', 'service_account', 'key_path', 'etc', '.', 'they', 'get', 'formatted', 'as', 'shown', 'below', '.']",fetches a field from extras and returns it . this is some airflow magic . the google_cloud_platform hook type adds custom ui elements to the hook page which allow admins to specify service_account key_path etc . they get formatted as shown below .,"['def', '_get_field', '(', 'self', ',', 'extras', ',', 'field', ',', 'default', '=', 'none', ')', ':', 'long_f', '=', ""'extra__google_cloud_platform__{}'"", '.', 'format', '(', 'field', ')', 'if', 'long_f', 'in', 'extras', ':', 'return', 'extras', '[', 'long_f', ']', 'else', ':', 'self', '.', 'log', '.', 'info', '(', ""'field %s not found in extras.'"", ',', 'field', ')', 'return', 'default']","def _get_field ( self , extras , field , default = none ) : long_f = 'extra__google_cloud_platform__{}' . format ( field ) if long_f in extras : return extras [ long_f ] else : self . log . info ( 'field %s not found in extras.' , field ) return default"
121,apache/airflow,airflow/hooks/druid_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/druid_hook.py#L127-L139,"def get_conn(self):
        """"""
        Establish a connection to druid broker.
        """"""
        conn = self.get_connection(self.druid_broker_conn_id)
        druid_broker_conn = connect(
            host=conn.host,
            port=conn.port,
            path=conn.extra_dejson.get('endpoint', '/druid/v2/sql'),
            scheme=conn.extra_dejson.get('schema', 'http')
        )
        self.log.info('Get the connection to druid broker on %s', conn.host)
        return druid_broker_conn","['def', 'get_conn', '(', 'self', ')', ':', 'conn', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'druid_broker_conn_id', ')', 'druid_broker_conn', '=', 'connect', '(', 'host', '=', 'conn', '.', 'host', ',', 'port', '=', 'conn', '.', 'port', ',', 'path', '=', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'endpoint'"", ',', ""'/druid/v2/sql'"", ')', ',', 'scheme', '=', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'schema'"", ',', ""'http'"", ')', ')', 'self', '.', 'log', '.', 'info', '(', ""'Get the connection to druid broker on %s'"", ',', 'conn', '.', 'host', ')', 'return', 'druid_broker_conn']",Establish a connection to druid broker.,"['Establish', 'a', 'connection', 'to', 'druid', 'broker', '.']",python,test,"['establish', 'a', 'connection', 'to', 'druid', 'broker', '.']",establish a connection to druid broker .,"['def', 'get_conn', '(', 'self', ')', ':', 'conn', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'druid_broker_conn_id', ')', 'druid_broker_conn', '=', 'connect', '(', 'host', '=', 'conn', '.', 'host', ',', 'port', '=', 'conn', '.', 'port', ',', 'path', '=', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'endpoint'"", ',', ""'/druid/v2/sql'"", ')', ',', 'scheme', '=', 'conn', '.', 'extra_dejson', '.', 'get', '(', ""'schema'"", ',', ""'http'"", ')', ')', 'self', '.', 'log', '.', 'info', '(', ""'get the connection to druid broker on %s'"", ',', 'conn', '.', 'host', ')', 'return', 'druid_broker_conn']","def get_conn ( self ) : conn = self . get_connection ( self . druid_broker_conn_id ) druid_broker_conn = connect ( host = conn . host , port = conn . port , path = conn . extra_dejson . get ( 'endpoint' , '/druid/v2/sql' ) , scheme = conn . extra_dejson . get ( 'schema' , 'http' ) ) self . log . info ( 'get the connection to druid broker on %s' , conn . host ) return druid_broker_conn"
122,apache/airflow,airflow/hooks/http_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L53-L83,"def get_conn(self, headers=None):
        """"""
        Returns http session for use with requests

        :param headers: additional headers to be passed through as a dictionary
        :type headers: dict
        """"""
        session = requests.Session()
        if self.http_conn_id:
            conn = self.get_connection(self.http_conn_id)

            if ""://"" in conn.host:
                self.base_url = conn.host
            else:
                # schema defaults to HTTP
                schema = conn.schema if conn.schema else ""http""
                self.base_url = schema + ""://"" + conn.host

            if conn.port:
                self.base_url = self.base_url + "":"" + str(conn.port)
            if conn.login:
                session.auth = (conn.login, conn.password)
            if conn.extra:
                try:
                    session.headers.update(conn.extra_dejson)
                except TypeError:
                    self.log.warn('Connection to %s has invalid extra field.', conn.host)
        if headers:
            session.headers.update(headers)

        return session","['def', 'get_conn', '(', 'self', ',', 'headers', '=', 'None', ')', ':', 'session', '=', 'requests', '.', 'Session', '(', ')', 'if', 'self', '.', 'http_conn_id', ':', 'conn', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'http_conn_id', ')', 'if', '""://""', 'in', 'conn', '.', 'host', ':', 'self', '.', 'base_url', '=', 'conn', '.', 'host', 'else', ':', '# schema defaults to HTTP', 'schema', '=', 'conn', '.', 'schema', 'if', 'conn', '.', 'schema', 'else', '""http""', 'self', '.', 'base_url', '=', 'schema', '+', '""://""', '+', 'conn', '.', 'host', 'if', 'conn', '.', 'port', ':', 'self', '.', 'base_url', '=', 'self', '.', 'base_url', '+', '"":""', '+', 'str', '(', 'conn', '.', 'port', ')', 'if', 'conn', '.', 'login', ':', 'session', '.', 'auth', '=', '(', 'conn', '.', 'login', ',', 'conn', '.', 'password', ')', 'if', 'conn', '.', 'extra', ':', 'try', ':', 'session', '.', 'headers', '.', 'update', '(', 'conn', '.', 'extra_dejson', ')', 'except', 'TypeError', ':', 'self', '.', 'log', '.', 'warn', '(', ""'Connection to %s has invalid extra field.'"", ',', 'conn', '.', 'host', ')', 'if', 'headers', ':', 'session', '.', 'headers', '.', 'update', '(', 'headers', ')', 'return', 'session']","Returns http session for use with requests

        :param headers: additional headers to be passed through as a dictionary
        :type headers: dict","['Returns', 'http', 'session', 'for', 'use', 'with', 'requests']",python,test,"['returns', 'http', 'session', 'for', 'use', 'with', 'requests']",returns http session for use with requests,"['def', 'get_conn', '(', 'self', ',', 'headers', '=', 'none', ')', ':', 'session', '=', 'requests', '.', 'session', '(', ')', 'if', 'self', '.', 'http_conn_id', ':', 'conn', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'http_conn_id', ')', 'if', '""://""', 'in', 'conn', '.', 'host', ':', 'self', '.', 'base_url', '=', 'conn', '.', 'host', 'else', ':', '# schema defaults to http', 'schema', '=', 'conn', '.', 'schema', 'if', 'conn', '.', 'schema', 'else', '""http""', 'self', '.', 'base_url', '=', 'schema', '+', '""://""', '+', 'conn', '.', 'host', 'if', 'conn', '.', 'port', ':', 'self', '.', 'base_url', '=', 'self', '.', 'base_url', '+', '"":""', '+', 'str', '(', 'conn', '.', 'port', ')', 'if', 'conn', '.', 'login', ':', 'session', '.', 'auth', '=', '(', 'conn', '.', 'login', ',', 'conn', '.', 'password', ')', 'if', 'conn', '.', 'extra', ':', 'try', ':', 'session', '.', 'headers', '.', 'update', '(', 'conn', '.', 'extra_dejson', ')', 'except', 'typeerror', ':', 'self', '.', 'log', '.', 'warn', '(', ""'connection to %s has invalid extra field.'"", ',', 'conn', '.', 'host', ')', 'if', 'headers', ':', 'session', '.', 'headers', '.', 'update', '(', 'headers', ')', 'return', 'session']","def get_conn ( self , headers = none ) : session = requests . session ( ) if self . http_conn_id : conn = self . get_connection ( self . http_conn_id ) if ""://"" in conn . host : self . base_url = conn . host else : # schema defaults to http schema = conn . schema if conn . schema else ""http"" self . base_url = schema + ""://"" + conn . host if conn . port : self . base_url = self . base_url + "":"" + str ( conn . port ) if conn . login : session . auth = ( conn . login , conn . password ) if conn . extra : try : session . headers . update ( conn . extra_dejson ) except typeerror : self . log . warn ( 'connection to %s has invalid extra field.' , conn . host ) if headers : session . headers . update ( headers ) return session"
123,apache/airflow,airflow/hooks/http_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L85-L131,"def run(self, endpoint, data=None, headers=None, extra_options=None):
        """"""
        Performs the request

        :param endpoint: the endpoint to be called i.e. resource/v1/query?
        :type endpoint: str
        :param data: payload to be uploaded or request parameters
        :type data: dict
        :param headers: additional headers to be passed through as a dictionary
        :type headers: dict
        :param extra_options: additional options to be used when executing the request
            i.e. {'check_response': False} to avoid checking raising exceptions on non
            2XX or 3XX status codes
        :type extra_options: dict
        """"""
        extra_options = extra_options or {}

        session = self.get_conn(headers)

        if self.base_url and not self.base_url.endswith('/') and \
           endpoint and not endpoint.startswith('/'):
            url = self.base_url + '/' + endpoint
        else:
            url = (self.base_url or '') + (endpoint or '')

        req = None
        if self.method == 'GET':
            # GET uses params
            req = requests.Request(self.method,
                                   url,
                                   params=data,
                                   headers=headers)
        elif self.method == 'HEAD':
            # HEAD doesn't use params
            req = requests.Request(self.method,
                                   url,
                                   headers=headers)
        else:
            # Others use data
            req = requests.Request(self.method,
                                   url,
                                   data=data,
                                   headers=headers)

        prepped_request = session.prepare_request(req)
        self.log.info(""Sending '%s' to url: %s"", self.method, url)
        return self.run_and_check(session, prepped_request, extra_options)","['def', 'run', '(', 'self', ',', 'endpoint', ',', 'data', '=', 'None', ',', 'headers', '=', 'None', ',', 'extra_options', '=', 'None', ')', ':', 'extra_options', '=', 'extra_options', 'or', '{', '}', 'session', '=', 'self', '.', 'get_conn', '(', 'headers', ')', 'if', 'self', '.', 'base_url', 'and', 'not', 'self', '.', 'base_url', '.', 'endswith', '(', ""'/'"", ')', 'and', 'endpoint', 'and', 'not', 'endpoint', '.', 'startswith', '(', ""'/'"", ')', ':', 'url', '=', 'self', '.', 'base_url', '+', ""'/'"", '+', 'endpoint', 'else', ':', 'url', '=', '(', 'self', '.', 'base_url', 'or', ""''"", ')', '+', '(', 'endpoint', 'or', ""''"", ')', 'req', '=', 'None', 'if', 'self', '.', 'method', '==', ""'GET'"", ':', '# GET uses params', 'req', '=', 'requests', '.', 'Request', '(', 'self', '.', 'method', ',', 'url', ',', 'params', '=', 'data', ',', 'headers', '=', 'headers', ')', 'elif', 'self', '.', 'method', '==', ""'HEAD'"", ':', ""# HEAD doesn't use params"", 'req', '=', 'requests', '.', 'Request', '(', 'self', '.', 'method', ',', 'url', ',', 'headers', '=', 'headers', ')', 'else', ':', '# Others use data', 'req', '=', 'requests', '.', 'Request', '(', 'self', '.', 'method', ',', 'url', ',', 'data', '=', 'data', ',', 'headers', '=', 'headers', ')', 'prepped_request', '=', 'session', '.', 'prepare_request', '(', 'req', ')', 'self', '.', 'log', '.', 'info', '(', '""Sending \'%s\' to url: %s""', ',', 'self', '.', 'method', ',', 'url', ')', 'return', 'self', '.', 'run_and_check', '(', 'session', ',', 'prepped_request', ',', 'extra_options', ')']","Performs the request

        :param endpoint: the endpoint to be called i.e. resource/v1/query?
        :type endpoint: str
        :param data: payload to be uploaded or request parameters
        :type data: dict
        :param headers: additional headers to be passed through as a dictionary
        :type headers: dict
        :param extra_options: additional options to be used when executing the request
            i.e. {'check_response': False} to avoid checking raising exceptions on non
            2XX or 3XX status codes
        :type extra_options: dict","['Performs', 'the', 'request']",python,test,"['performs', 'the', 'request']",performs the request,"['def', 'run', '(', 'self', ',', 'endpoint', ',', 'data', '=', 'none', ',', 'headers', '=', 'none', ',', 'extra_options', '=', 'none', ')', ':', 'extra_options', '=', 'extra_options', 'or', '{', '}', 'session', '=', 'self', '.', 'get_conn', '(', 'headers', ')', 'if', 'self', '.', 'base_url', 'and', 'not', 'self', '.', 'base_url', '.', 'endswith', '(', ""'/'"", ')', 'and', 'endpoint', 'and', 'not', 'endpoint', '.', 'startswith', '(', ""'/'"", ')', ':', 'url', '=', 'self', '.', 'base_url', '+', ""'/'"", '+', 'endpoint', 'else', ':', 'url', '=', '(', 'self', '.', 'base_url', 'or', ""''"", ')', '+', '(', 'endpoint', 'or', ""''"", ')', 'req', '=', 'none', 'if', 'self', '.', 'method', '==', ""'get'"", ':', '# get uses params', 'req', '=', 'requests', '.', 'request', '(', 'self', '.', 'method', ',', 'url', ',', 'params', '=', 'data', ',', 'headers', '=', 'headers', ')', 'elif', 'self', '.', 'method', '==', ""'head'"", ':', ""# head doesn't use params"", 'req', '=', 'requests', '.', 'request', '(', 'self', '.', 'method', ',', 'url', ',', 'headers', '=', 'headers', ')', 'else', ':', '# others use data', 'req', '=', 'requests', '.', 'request', '(', 'self', '.', 'method', ',', 'url', ',', 'data', '=', 'data', ',', 'headers', '=', 'headers', ')', 'prepped_request', '=', 'session', '.', 'prepare_request', '(', 'req', ')', 'self', '.', 'log', '.', 'info', '(', '""sending \'%s\' to url: %s""', ',', 'self', '.', 'method', ',', 'url', ')', 'return', 'self', '.', 'run_and_check', '(', 'session', ',', 'prepped_request', ',', 'extra_options', ')']","def run ( self , endpoint , data = none , headers = none , extra_options = none ) : extra_options = extra_options or { } session = self . get_conn ( headers ) if self . base_url and not self . base_url . endswith ( '/' ) and endpoint and not endpoint . startswith ( '/' ) : url = self . base_url + '/' + endpoint else : url = ( self . base_url or '' ) + ( endpoint or '' ) req = none if self . method == 'get' : # get uses params req = requests . request ( self . method , url , params = data , headers = headers ) elif self . method == 'head' : # head doesn't use params req = requests . request ( self . method , url , headers = headers ) else : # others use data req = requests . request ( self . method , url , data = data , headers = headers ) prepped_request = session . prepare_request ( req ) self . log . info ( ""sending '%s' to url: %s"" , self . method , url ) return self . run_and_check ( session , prepped_request , extra_options )"
124,apache/airflow,airflow/hooks/http_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L133-L147,"def check_response(self, response):
        """"""
        Checks the status code and raise an AirflowException exception on non 2XX or 3XX
        status codes

        :param response: A requests response object
        :type response: requests.response
        """"""
        try:
            response.raise_for_status()
        except requests.exceptions.HTTPError:
            self.log.error(""HTTP error: %s"", response.reason)
            if self.method not in ['GET', 'HEAD']:
                self.log.error(response.text)
            raise AirflowException(str(response.status_code) + "":"" + response.reason)","['def', 'check_response', '(', 'self', ',', 'response', ')', ':', 'try', ':', 'response', '.', 'raise_for_status', '(', ')', 'except', 'requests', '.', 'exceptions', '.', 'HTTPError', ':', 'self', '.', 'log', '.', 'error', '(', '""HTTP error: %s""', ',', 'response', '.', 'reason', ')', 'if', 'self', '.', 'method', 'not', 'in', '[', ""'GET'"", ',', ""'HEAD'"", ']', ':', 'self', '.', 'log', '.', 'error', '(', 'response', '.', 'text', ')', 'raise', 'AirflowException', '(', 'str', '(', 'response', '.', 'status_code', ')', '+', '"":""', '+', 'response', '.', 'reason', ')']","Checks the status code and raise an AirflowException exception on non 2XX or 3XX
        status codes

        :param response: A requests response object
        :type response: requests.response","['Checks', 'the', 'status', 'code', 'and', 'raise', 'an', 'AirflowException', 'exception', 'on', 'non', '2XX', 'or', '3XX', 'status', 'codes']",python,test,"['checks', 'the', 'status', 'code', 'and', 'raise', 'an', 'airflowexception', 'exception', 'on', 'non', '2xx', 'or', '3xx', 'status', 'codes']",checks the status code and raise an airflowexception exception on non 2xx or 3xx status codes,"['def', 'check_response', '(', 'self', ',', 'response', ')', ':', 'try', ':', 'response', '.', 'raise_for_status', '(', ')', 'except', 'requests', '.', 'exceptions', '.', 'httperror', ':', 'self', '.', 'log', '.', 'error', '(', '""http error: %s""', ',', 'response', '.', 'reason', ')', 'if', 'self', '.', 'method', 'not', 'in', '[', ""'get'"", ',', ""'head'"", ']', ':', 'self', '.', 'log', '.', 'error', '(', 'response', '.', 'text', ')', 'raise', 'airflowexception', '(', 'str', '(', 'response', '.', 'status_code', ')', '+', '"":""', '+', 'response', '.', 'reason', ')']","def check_response ( self , response ) : try : response . raise_for_status ( ) except requests . exceptions . httperror : self . log . error ( ""http error: %s"" , response . reason ) if self . method not in [ 'get' , 'head' ] : self . log . error ( response . text ) raise airflowexception ( str ( response . status_code ) + "":"" + response . reason )"
125,apache/airflow,airflow/hooks/http_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L149-L181,"def run_and_check(self, session, prepped_request, extra_options):
        """"""
        Grabs extra options like timeout and actually runs the request,
        checking for the result

        :param session: the session to be used to execute the request
        :type session: requests.Session
        :param prepped_request: the prepared request generated in run()
        :type prepped_request: session.prepare_request
        :param extra_options: additional options to be used when executing the request
            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX
            or 3XX status codes
        :type extra_options: dict
        """"""
        extra_options = extra_options or {}

        try:
            response = session.send(
                prepped_request,
                stream=extra_options.get(""stream"", False),
                verify=extra_options.get(""verify"", True),
                proxies=extra_options.get(""proxies"", {}),
                cert=extra_options.get(""cert""),
                timeout=extra_options.get(""timeout""),
                allow_redirects=extra_options.get(""allow_redirects"", True))

            if extra_options.get('check_response', True):
                self.check_response(response)
            return response

        except requests.exceptions.ConnectionError as ex:
            self.log.warn(str(ex) + ' Tenacity will retry to execute the operation')
            raise ex","['def', 'run_and_check', '(', 'self', ',', 'session', ',', 'prepped_request', ',', 'extra_options', ')', ':', 'extra_options', '=', 'extra_options', 'or', '{', '}', 'try', ':', 'response', '=', 'session', '.', 'send', '(', 'prepped_request', ',', 'stream', '=', 'extra_options', '.', 'get', '(', '""stream""', ',', 'False', ')', ',', 'verify', '=', 'extra_options', '.', 'get', '(', '""verify""', ',', 'True', ')', ',', 'proxies', '=', 'extra_options', '.', 'get', '(', '""proxies""', ',', '{', '}', ')', ',', 'cert', '=', 'extra_options', '.', 'get', '(', '""cert""', ')', ',', 'timeout', '=', 'extra_options', '.', 'get', '(', '""timeout""', ')', ',', 'allow_redirects', '=', 'extra_options', '.', 'get', '(', '""allow_redirects""', ',', 'True', ')', ')', 'if', 'extra_options', '.', 'get', '(', ""'check_response'"", ',', 'True', ')', ':', 'self', '.', 'check_response', '(', 'response', ')', 'return', 'response', 'except', 'requests', '.', 'exceptions', '.', 'ConnectionError', 'as', 'ex', ':', 'self', '.', 'log', '.', 'warn', '(', 'str', '(', 'ex', ')', '+', ""' Tenacity will retry to execute the operation'"", ')', 'raise', 'ex']","Grabs extra options like timeout and actually runs the request,
        checking for the result

        :param session: the session to be used to execute the request
        :type session: requests.Session
        :param prepped_request: the prepared request generated in run()
        :type prepped_request: session.prepare_request
        :param extra_options: additional options to be used when executing the request
            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX
            or 3XX status codes
        :type extra_options: dict","['Grabs', 'extra', 'options', 'like', 'timeout', 'and', 'actually', 'runs', 'the', 'request', 'checking', 'for', 'the', 'result']",python,test,"['grabs', 'extra', 'options', 'like', 'timeout', 'and', 'actually', 'runs', 'the', 'request', 'checking', 'for', 'the', 'result']",grabs extra options like timeout and actually runs the request checking for the result,"['def', 'run_and_check', '(', 'self', ',', 'session', ',', 'prepped_request', ',', 'extra_options', ')', ':', 'extra_options', '=', 'extra_options', 'or', '{', '}', 'try', ':', 'response', '=', 'session', '.', 'send', '(', 'prepped_request', ',', 'stream', '=', 'extra_options', '.', 'get', '(', '""stream""', ',', 'false', ')', ',', 'verify', '=', 'extra_options', '.', 'get', '(', '""verify""', ',', 'true', ')', ',', 'proxies', '=', 'extra_options', '.', 'get', '(', '""proxies""', ',', '{', '}', ')', ',', 'cert', '=', 'extra_options', '.', 'get', '(', '""cert""', ')', ',', 'timeout', '=', 'extra_options', '.', 'get', '(', '""timeout""', ')', ',', 'allow_redirects', '=', 'extra_options', '.', 'get', '(', '""allow_redirects""', ',', 'true', ')', ')', 'if', 'extra_options', '.', 'get', '(', ""'check_response'"", ',', 'true', ')', ':', 'self', '.', 'check_response', '(', 'response', ')', 'return', 'response', 'except', 'requests', '.', 'exceptions', '.', 'connectionerror', 'as', 'ex', ':', 'self', '.', 'log', '.', 'warn', '(', 'str', '(', 'ex', ')', '+', ""' tenacity will retry to execute the operation'"", ')', 'raise', 'ex']","def run_and_check ( self , session , prepped_request , extra_options ) : extra_options = extra_options or { } try : response = session . send ( prepped_request , stream = extra_options . get ( ""stream"" , false ) , verify = extra_options . get ( ""verify"" , true ) , proxies = extra_options . get ( ""proxies"" , { } ) , cert = extra_options . get ( ""cert"" ) , timeout = extra_options . get ( ""timeout"" ) , allow_redirects = extra_options . get ( ""allow_redirects"" , true ) ) if extra_options . get ( 'check_response' , true ) : self . check_response ( response ) return response except requests . exceptions . connectionerror as ex : self . log . warn ( str ( ex ) + ' tenacity will retry to execute the operation' ) raise ex"
126,apache/airflow,airflow/hooks/http_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/http_hook.py#L183-L211,"def run_with_advanced_retry(self, _retry_args, *args, **kwargs):
        """"""
        Runs Hook.run() with a Tenacity decorator attached to it. This is useful for
        connectors which might be disturbed by intermittent issues and should not
        instantly fail.

        :param _retry_args: Arguments which define the retry behaviour.
            See Tenacity documentation at https://github.com/jd/tenacity
        :type _retry_args: dict


        :Example::

            hook = HttpHook(http_conn_id='my_conn',method='GET')
            retry_args = dict(
                 wait=tenacity.wait_exponential(),
                 stop=tenacity.stop_after_attempt(10),
                 retry=requests.exceptions.ConnectionError
             )
             hook.run_with_advanced_retry(
                     endpoint='v1/test',
                     _retry_args=retry_args
                 )
        """"""
        self._retry_obj = tenacity.Retrying(
            **_retry_args
        )

        self._retry_obj(self.run, *args, **kwargs)","['def', 'run_with_advanced_retry', '(', 'self', ',', '_retry_args', ',', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'self', '.', '_retry_obj', '=', 'tenacity', '.', 'Retrying', '(', '*', '*', '_retry_args', ')', 'self', '.', '_retry_obj', '(', 'self', '.', 'run', ',', '*', 'args', ',', '*', '*', 'kwargs', ')']","Runs Hook.run() with a Tenacity decorator attached to it. This is useful for
        connectors which might be disturbed by intermittent issues and should not
        instantly fail.

        :param _retry_args: Arguments which define the retry behaviour.
            See Tenacity documentation at https://github.com/jd/tenacity
        :type _retry_args: dict


        :Example::

            hook = HttpHook(http_conn_id='my_conn',method='GET')
            retry_args = dict(
                 wait=tenacity.wait_exponential(),
                 stop=tenacity.stop_after_attempt(10),
                 retry=requests.exceptions.ConnectionError
             )
             hook.run_with_advanced_retry(
                     endpoint='v1/test',
                     _retry_args=retry_args
                 )","['Runs', 'Hook', '.', 'run', '()', 'with', 'a', 'Tenacity', 'decorator', 'attached', 'to', 'it', '.', 'This', 'is', 'useful', 'for', 'connectors', 'which', 'might', 'be', 'disturbed', 'by', 'intermittent', 'issues', 'and', 'should', 'not', 'instantly', 'fail', '.']",python,test,"['runs', 'hook', '.', 'run', '()', 'with', 'a', 'tenacity', 'decorator', 'attached', 'to', 'it', '.', 'this', 'is', 'useful', 'for', 'connectors', 'which', 'might', 'be', 'disturbed', 'by', 'intermittent', 'issues', 'and', 'should', 'not', 'instantly', 'fail', '.']",runs hook . run () with a tenacity decorator attached to it . this is useful for connectors which might be disturbed by intermittent issues and should not instantly fail .,"['def', 'run_with_advanced_retry', '(', 'self', ',', '_retry_args', ',', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'self', '.', '_retry_obj', '=', 'tenacity', '.', 'retrying', '(', '*', '*', '_retry_args', ')', 'self', '.', '_retry_obj', '(', 'self', '.', 'run', ',', '*', 'args', ',', '*', '*', 'kwargs', ')']","def run_with_advanced_retry ( self , _retry_args , * args , * * kwargs ) : self . _retry_obj = tenacity . retrying ( * * _retry_args ) self . _retry_obj ( self . run , * args , * * kwargs )"
127,apache/airflow,airflow/utils/db.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/db.py#L32-L44,"def create_session():
    """"""
    Contextmanager that will create and teardown a session.
    """"""
    session = settings.Session()
    try:
        yield session
        session.commit()
    except Exception:
        session.rollback()
        raise
    finally:
        session.close()","['def', 'create_session', '(', ')', ':', 'session', '=', 'settings', '.', 'Session', '(', ')', 'try', ':', 'yield', 'session', 'session', '.', 'commit', '(', ')', 'except', 'Exception', ':', 'session', '.', 'rollback', '(', ')', 'raise', 'finally', ':', 'session', '.', 'close', '(', ')']",Contextmanager that will create and teardown a session.,"['Contextmanager', 'that', 'will', 'create', 'and', 'teardown', 'a', 'session', '.']",python,test,"['contextmanager', 'that', 'will', 'create', 'and', 'teardown', 'a', 'session', '.']",contextmanager that will create and teardown a session .,"['def', 'create_session', '(', ')', ':', 'session', '=', 'settings', '.', 'session', '(', ')', 'try', ':', 'yield', 'session', 'session', '.', 'commit', '(', ')', 'except', 'exception', ':', 'session', '.', 'rollback', '(', ')', 'raise', 'finally', ':', 'session', '.', 'close', '(', ')']",def create_session ( ) : session = settings . session ( ) try : yield session session . commit ( ) except exception : session . rollback ( ) raise finally : session . close ( )
128,apache/airflow,airflow/utils/db.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/db.py#L47-L70,"def provide_session(func):
    """"""
    Function decorator that provides a session if it isn't provided.
    If you want to reuse a session or run the function as part of a
    database transaction, you pass it to the function, if not this wrapper
    will create one and close it for you.
    """"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        arg_session = 'session'

        func_params = func.__code__.co_varnames
        session_in_args = arg_session in func_params and \
            func_params.index(arg_session) < len(args)
        session_in_kwargs = arg_session in kwargs

        if session_in_kwargs or session_in_args:
            return func(*args, **kwargs)
        else:
            with create_session() as session:
                kwargs[arg_session] = session
                return func(*args, **kwargs)

    return wrapper","['def', 'provide_session', '(', 'func', ')', ':', '@', 'wraps', '(', 'func', ')', 'def', 'wrapper', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'arg_session', '=', ""'session'"", 'func_params', '=', 'func', '.', '__code__', '.', 'co_varnames', 'session_in_args', '=', 'arg_session', 'in', 'func_params', 'and', 'func_params', '.', 'index', '(', 'arg_session', ')', '<', 'len', '(', 'args', ')', 'session_in_kwargs', '=', 'arg_session', 'in', 'kwargs', 'if', 'session_in_kwargs', 'or', 'session_in_args', ':', 'return', 'func', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', 'else', ':', 'with', 'create_session', '(', ')', 'as', 'session', ':', 'kwargs', '[', 'arg_session', ']', '=', 'session', 'return', 'func', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', 'return', 'wrapper']","Function decorator that provides a session if it isn't provided.
    If you want to reuse a session or run the function as part of a
    database transaction, you pass it to the function, if not this wrapper
    will create one and close it for you.","['Function', 'decorator', 'that', 'provides', 'a', 'session', 'if', 'it', 'isn', 't', 'provided', '.', 'If', 'you', 'want', 'to', 'reuse', 'a', 'session', 'or', 'run', 'the', 'function', 'as', 'part', 'of', 'a', 'database', 'transaction', 'you', 'pass', 'it', 'to', 'the', 'function', 'if', 'not', 'this', 'wrapper', 'will', 'create', 'one', 'and', 'close', 'it', 'for', 'you', '.']",python,test,"['function', 'decorator', 'that', 'provides', 'a', 'session', 'if', 'it', 'isn', 't', 'provided', '.', 'if', 'you', 'want', 'to', 'reuse', 'a', 'session', 'or', 'run', 'the', 'function', 'as', 'part', 'of', 'a', 'database', 'transaction', 'you', 'pass', 'it', 'to', 'the', 'function', 'if', 'not', 'this', 'wrapper', 'will', 'create', 'one', 'and', 'close', 'it', 'for', 'you', '.']",function decorator that provides a session if it isn t provided . if you want to reuse a session or run the function as part of a database transaction you pass it to the function if not this wrapper will create one and close it for you .,"['def', 'provide_session', '(', 'func', ')', ':', '@', 'wraps', '(', 'func', ')', 'def', 'wrapper', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'arg_session', '=', ""'session'"", 'func_params', '=', 'func', '.', '__code__', '.', 'co_varnames', 'session_in_args', '=', 'arg_session', 'in', 'func_params', 'and', 'func_params', '.', 'index', '(', 'arg_session', ')', '<', 'len', '(', 'args', ')', 'session_in_kwargs', '=', 'arg_session', 'in', 'kwargs', 'if', 'session_in_kwargs', 'or', 'session_in_args', ':', 'return', 'func', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', 'else', ':', 'with', 'create_session', '(', ')', 'as', 'session', ':', 'kwargs', '[', 'arg_session', ']', '=', 'session', 'return', 'func', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', 'return', 'wrapper']","def provide_session ( func ) : @ wraps ( func ) def wrapper ( * args , * * kwargs ) : arg_session = 'session' func_params = func . __code__ . co_varnames session_in_args = arg_session in func_params and func_params . index ( arg_session ) < len ( args ) session_in_kwargs = arg_session in kwargs if session_in_kwargs or session_in_args : return func ( * args , * * kwargs ) else : with create_session ( ) as session : kwargs [ arg_session ] = session return func ( * args , * * kwargs ) return wrapper"
129,apache/airflow,airflow/utils/db.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/db.py#L312-L331,"def resetdb():
    """"""
    Clear out the database
    """"""
    from airflow import models

    # alembic adds significant import time, so we import it lazily
    from alembic.migration import MigrationContext

    log.info(""Dropping tables that exist"")

    models.base.Base.metadata.drop_all(settings.engine)
    mc = MigrationContext.configure(settings.engine)
    if mc._version.exists(settings.engine):
        mc._version.drop(settings.engine)

    from flask_appbuilder.models.sqla import Base
    Base.metadata.drop_all(settings.engine)

    initdb()","['def', 'resetdb', '(', ')', ':', 'from', 'airflow', 'import', 'models', '# alembic adds significant import time, so we import it lazily', 'from', 'alembic', '.', 'migration', 'import', 'MigrationContext', 'log', '.', 'info', '(', '""Dropping tables that exist""', ')', 'models', '.', 'base', '.', 'Base', '.', 'metadata', '.', 'drop_all', '(', 'settings', '.', 'engine', ')', 'mc', '=', 'MigrationContext', '.', 'configure', '(', 'settings', '.', 'engine', ')', 'if', 'mc', '.', '_version', '.', 'exists', '(', 'settings', '.', 'engine', ')', ':', 'mc', '.', '_version', '.', 'drop', '(', 'settings', '.', 'engine', ')', 'from', 'flask_appbuilder', '.', 'models', '.', 'sqla', 'import', 'Base', 'Base', '.', 'metadata', '.', 'drop_all', '(', 'settings', '.', 'engine', ')', 'initdb', '(', ')']",Clear out the database,"['Clear', 'out', 'the', 'database']",python,test,"['clear', 'out', 'the', 'database']",clear out the database,"['def', 'resetdb', '(', ')', ':', 'from', 'airflow', 'import', 'models', '# alembic adds significant import time, so we import it lazily', 'from', 'alembic', '.', 'migration', 'import', 'migrationcontext', 'log', '.', 'info', '(', '""dropping tables that exist""', ')', 'models', '.', 'base', '.', 'base', '.', 'metadata', '.', 'drop_all', '(', 'settings', '.', 'engine', ')', 'mc', '=', 'migrationcontext', '.', 'configure', '(', 'settings', '.', 'engine', ')', 'if', 'mc', '.', '_version', '.', 'exists', '(', 'settings', '.', 'engine', ')', ':', 'mc', '.', '_version', '.', 'drop', '(', 'settings', '.', 'engine', ')', 'from', 'flask_appbuilder', '.', 'models', '.', 'sqla', 'import', 'base', 'base', '.', 'metadata', '.', 'drop_all', '(', 'settings', '.', 'engine', ')', 'initdb', '(', ')']","def resetdb ( ) : from airflow import models # alembic adds significant import time, so we import it lazily from alembic . migration import migrationcontext log . info ( ""dropping tables that exist"" ) models . base . base . metadata . drop_all ( settings . engine ) mc = migrationcontext . configure ( settings . engine ) if mc . _version . exists ( settings . engine ) : mc . _version . drop ( settings . engine ) from flask_appbuilder . models . sqla import base base . metadata . drop_all ( settings . engine ) initdb ( )"
130,apache/airflow,airflow/contrib/operators/file_to_wasb.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/file_to_wasb.py#L56-L64,"def execute(self, context):
        """"""Upload a file to Azure Blob Storage.""""""
        hook = WasbHook(wasb_conn_id=self.wasb_conn_id)
        self.log.info(
            'Uploading %s to wasb://%s '
            'as %s'.format(self.file_path, self.container_name, self.blob_name)
        )
        hook.load_file(self.file_path, self.container_name,
                       self.blob_name, **self.load_options)","['def', 'execute', '(', 'self', ',', 'context', ')', ':', 'hook', '=', 'WasbHook', '(', 'wasb_conn_id', '=', 'self', '.', 'wasb_conn_id', ')', 'self', '.', 'log', '.', 'info', '(', ""'Uploading %s to wasb://%s '"", ""'as %s'"", '.', 'format', '(', 'self', '.', 'file_path', ',', 'self', '.', 'container_name', ',', 'self', '.', 'blob_name', ')', ')', 'hook', '.', 'load_file', '(', 'self', '.', 'file_path', ',', 'self', '.', 'container_name', ',', 'self', '.', 'blob_name', ',', '*', '*', 'self', '.', 'load_options', ')']",Upload a file to Azure Blob Storage.,"['Upload', 'a', 'file', 'to', 'Azure', 'Blob', 'Storage', '.']",python,test,"['upload', 'a', 'file', 'to', 'azure', 'blob', 'storage', '.']",upload a file to azure blob storage .,"['def', 'execute', '(', 'self', ',', 'context', ')', ':', 'hook', '=', 'wasbhook', '(', 'wasb_conn_id', '=', 'self', '.', 'wasb_conn_id', ')', 'self', '.', 'log', '.', 'info', '(', ""'uploading %s to wasb://%s '"", ""'as %s'"", '.', 'format', '(', 'self', '.', 'file_path', ',', 'self', '.', 'container_name', ',', 'self', '.', 'blob_name', ')', ')', 'hook', '.', 'load_file', '(', 'self', '.', 'file_path', ',', 'self', '.', 'container_name', ',', 'self', '.', 'blob_name', ',', '*', '*', 'self', '.', 'load_options', ')']","def execute ( self , context ) : hook = wasbhook ( wasb_conn_id = self . wasb_conn_id ) self . log . info ( 'uploading %s to wasb://%s ' 'as %s' . format ( self . file_path , self . container_name , self . blob_name ) ) hook . load_file ( self . file_path , self . container_name , self . blob_name , * * self . load_options )"
131,apache/airflow,airflow/hooks/presto_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L46-L60,"def get_conn(self):
        """"""Returns a connection object""""""
        db = self.get_connection(self.presto_conn_id)
        reqkwargs = None
        if db.password is not None:
            reqkwargs = {'auth': HTTPBasicAuth(db.login, db.password)}
        return presto.connect(
            host=db.host,
            port=db.port,
            username=db.login,
            source=db.extra_dejson.get('source', 'airflow'),
            protocol=db.extra_dejson.get('protocol', 'http'),
            catalog=db.extra_dejson.get('catalog', 'hive'),
            requests_kwargs=reqkwargs,
            schema=db.schema)","['def', 'get_conn', '(', 'self', ')', ':', 'db', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'presto_conn_id', ')', 'reqkwargs', '=', 'None', 'if', 'db', '.', 'password', 'is', 'not', 'None', ':', 'reqkwargs', '=', '{', ""'auth'"", ':', 'HTTPBasicAuth', '(', 'db', '.', 'login', ',', 'db', '.', 'password', ')', '}', 'return', 'presto', '.', 'connect', '(', 'host', '=', 'db', '.', 'host', ',', 'port', '=', 'db', '.', 'port', ',', 'username', '=', 'db', '.', 'login', ',', 'source', '=', 'db', '.', 'extra_dejson', '.', 'get', '(', ""'source'"", ',', ""'airflow'"", ')', ',', 'protocol', '=', 'db', '.', 'extra_dejson', '.', 'get', '(', ""'protocol'"", ',', ""'http'"", ')', ',', 'catalog', '=', 'db', '.', 'extra_dejson', '.', 'get', '(', ""'catalog'"", ',', ""'hive'"", ')', ',', 'requests_kwargs', '=', 'reqkwargs', ',', 'schema', '=', 'db', '.', 'schema', ')']",Returns a connection object,"['Returns', 'a', 'connection', 'object']",python,test,"['returns', 'a', 'connection', 'object']",returns a connection object,"['def', 'get_conn', '(', 'self', ')', ':', 'db', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'presto_conn_id', ')', 'reqkwargs', '=', 'none', 'if', 'db', '.', 'password', 'is', 'not', 'none', ':', 'reqkwargs', '=', '{', ""'auth'"", ':', 'httpbasicauth', '(', 'db', '.', 'login', ',', 'db', '.', 'password', ')', '}', 'return', 'presto', '.', 'connect', '(', 'host', '=', 'db', '.', 'host', ',', 'port', '=', 'db', '.', 'port', ',', 'username', '=', 'db', '.', 'login', ',', 'source', '=', 'db', '.', 'extra_dejson', '.', 'get', '(', ""'source'"", ',', ""'airflow'"", ')', ',', 'protocol', '=', 'db', '.', 'extra_dejson', '.', 'get', '(', ""'protocol'"", ',', ""'http'"", ')', ',', 'catalog', '=', 'db', '.', 'extra_dejson', '.', 'get', '(', ""'catalog'"", ',', ""'hive'"", ')', ',', 'requests_kwargs', '=', 'reqkwargs', ',', 'schema', '=', 'db', '.', 'schema', ')']","def get_conn ( self ) : db = self . get_connection ( self . presto_conn_id ) reqkwargs = none if db . password is not none : reqkwargs = { 'auth' : httpbasicauth ( db . login , db . password ) } return presto . connect ( host = db . host , port = db . port , username = db . login , source = db . extra_dejson . get ( 'source' , 'airflow' ) , protocol = db . extra_dejson . get ( 'protocol' , 'http' ) , catalog = db . extra_dejson . get ( 'catalog' , 'hive' ) , requests_kwargs = reqkwargs , schema = db . schema )"
132,apache/airflow,airflow/hooks/presto_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L67-L78,"def _get_pretty_exception_message(e):
        """"""
        Parses some DatabaseError to provide a better error message
        """"""
        if (hasattr(e, 'message') and
            'errorName' in e.message and
                'message' in e.message):
            return ('{name}: {message}'.format(
                    name=e.message['errorName'],
                    message=e.message['message']))
        else:
            return str(e)","['def', '_get_pretty_exception_message', '(', 'e', ')', ':', 'if', '(', 'hasattr', '(', 'e', ',', ""'message'"", ')', 'and', ""'errorName'"", 'in', 'e', '.', 'message', 'and', ""'message'"", 'in', 'e', '.', 'message', ')', ':', 'return', '(', ""'{name}: {message}'"", '.', 'format', '(', 'name', '=', 'e', '.', 'message', '[', ""'errorName'"", ']', ',', 'message', '=', 'e', '.', 'message', '[', ""'message'"", ']', ')', ')', 'else', ':', 'return', 'str', '(', 'e', ')']",Parses some DatabaseError to provide a better error message,"['Parses', 'some', 'DatabaseError', 'to', 'provide', 'a', 'better', 'error', 'message']",python,test,"['parses', 'some', 'databaseerror', 'to', 'provide', 'a', 'better', 'error', 'message']",parses some databaseerror to provide a better error message,"['def', '_get_pretty_exception_message', '(', 'e', ')', ':', 'if', '(', 'hasattr', '(', 'e', ',', ""'message'"", ')', 'and', ""'errorname'"", 'in', 'e', '.', 'message', 'and', ""'message'"", 'in', 'e', '.', 'message', ')', ':', 'return', '(', ""'{name}: {message}'"", '.', 'format', '(', 'name', '=', 'e', '.', 'message', '[', ""'errorname'"", ']', ',', 'message', '=', 'e', '.', 'message', '[', ""'message'"", ']', ')', ')', 'else', ':', 'return', 'str', '(', 'e', ')']","def _get_pretty_exception_message ( e ) : if ( hasattr ( e , 'message' ) and 'errorname' in e . message and 'message' in e . message ) : return ( '{name}: {message}' . format ( name = e . message [ 'errorname' ] , message = e . message [ 'message' ] ) ) else : return str ( e )"
133,apache/airflow,airflow/hooks/presto_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L80-L88,"def get_records(self, hql, parameters=None):
        """"""
        Get a set of records from Presto
        """"""
        try:
            return super().get_records(
                self._strip_sql(hql), parameters)
        except DatabaseError as e:
            raise PrestoException(self._get_pretty_exception_message(e))","['def', 'get_records', '(', 'self', ',', 'hql', ',', 'parameters', '=', 'None', ')', ':', 'try', ':', 'return', 'super', '(', ')', '.', 'get_records', '(', 'self', '.', '_strip_sql', '(', 'hql', ')', ',', 'parameters', ')', 'except', 'DatabaseError', 'as', 'e', ':', 'raise', 'PrestoException', '(', 'self', '.', '_get_pretty_exception_message', '(', 'e', ')', ')']",Get a set of records from Presto,"['Get', 'a', 'set', 'of', 'records', 'from', 'Presto']",python,test,"['get', 'a', 'set', 'of', 'records', 'from', 'presto']",get a set of records from presto,"['def', 'get_records', '(', 'self', ',', 'hql', ',', 'parameters', '=', 'none', ')', ':', 'try', ':', 'return', 'super', '(', ')', '.', 'get_records', '(', 'self', '.', '_strip_sql', '(', 'hql', ')', ',', 'parameters', ')', 'except', 'databaseerror', 'as', 'e', ':', 'raise', 'prestoexception', '(', 'self', '.', '_get_pretty_exception_message', '(', 'e', ')', ')']","def get_records ( self , hql , parameters = none ) : try : return super ( ) . get_records ( self . _strip_sql ( hql ) , parameters ) except databaseerror as e : raise prestoexception ( self . _get_pretty_exception_message ( e ) )"
134,apache/airflow,airflow/hooks/presto_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L101-L118,"def get_pandas_df(self, hql, parameters=None):
        """"""
        Get a pandas dataframe from a sql query.
        """"""
        import pandas
        cursor = self.get_cursor()
        try:
            cursor.execute(self._strip_sql(hql), parameters)
            data = cursor.fetchall()
        except DatabaseError as e:
            raise PrestoException(self._get_pretty_exception_message(e))
        column_descriptions = cursor.description
        if data:
            df = pandas.DataFrame(data)
            df.columns = [c[0] for c in column_descriptions]
        else:
            df = pandas.DataFrame()
        return df","['def', 'get_pandas_df', '(', 'self', ',', 'hql', ',', 'parameters', '=', 'None', ')', ':', 'import', 'pandas', 'cursor', '=', 'self', '.', 'get_cursor', '(', ')', 'try', ':', 'cursor', '.', 'execute', '(', 'self', '.', '_strip_sql', '(', 'hql', ')', ',', 'parameters', ')', 'data', '=', 'cursor', '.', 'fetchall', '(', ')', 'except', 'DatabaseError', 'as', 'e', ':', 'raise', 'PrestoException', '(', 'self', '.', '_get_pretty_exception_message', '(', 'e', ')', ')', 'column_descriptions', '=', 'cursor', '.', 'description', 'if', 'data', ':', 'df', '=', 'pandas', '.', 'DataFrame', '(', 'data', ')', 'df', '.', 'columns', '=', '[', 'c', '[', '0', ']', 'for', 'c', 'in', 'column_descriptions', ']', 'else', ':', 'df', '=', 'pandas', '.', 'DataFrame', '(', ')', 'return', 'df']",Get a pandas dataframe from a sql query.,"['Get', 'a', 'pandas', 'dataframe', 'from', 'a', 'sql', 'query', '.']",python,test,"['get', 'a', 'pandas', 'dataframe', 'from', 'a', 'sql', 'query', '.']",get a pandas dataframe from a sql query .,"['def', 'get_pandas_df', '(', 'self', ',', 'hql', ',', 'parameters', '=', 'none', ')', ':', 'import', 'pandas', 'cursor', '=', 'self', '.', 'get_cursor', '(', ')', 'try', ':', 'cursor', '.', 'execute', '(', 'self', '.', '_strip_sql', '(', 'hql', ')', ',', 'parameters', ')', 'data', '=', 'cursor', '.', 'fetchall', '(', ')', 'except', 'databaseerror', 'as', 'e', ':', 'raise', 'prestoexception', '(', 'self', '.', '_get_pretty_exception_message', '(', 'e', ')', ')', 'column_descriptions', '=', 'cursor', '.', 'description', 'if', 'data', ':', 'df', '=', 'pandas', '.', 'dataframe', '(', 'data', ')', 'df', '.', 'columns', '=', '[', 'c', '[', '0', ']', 'for', 'c', 'in', 'column_descriptions', ']', 'else', ':', 'df', '=', 'pandas', '.', 'dataframe', '(', ')', 'return', 'df']","def get_pandas_df ( self , hql , parameters = none ) : import pandas cursor = self . get_cursor ( ) try : cursor . execute ( self . _strip_sql ( hql ) , parameters ) data = cursor . fetchall ( ) except databaseerror as e : raise prestoexception ( self . _get_pretty_exception_message ( e ) ) column_descriptions = cursor . description if data : df = pandas . dataframe ( data ) df . columns = [ c [ 0 ] for c in column_descriptions ] else : df = pandas . dataframe ( ) return df"
135,apache/airflow,airflow/hooks/presto_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L120-L124,"def run(self, hql, parameters=None):
        """"""
        Execute the statement against Presto. Can be used to create views.
        """"""
        return super().run(self._strip_sql(hql), parameters)","['def', 'run', '(', 'self', ',', 'hql', ',', 'parameters', '=', 'None', ')', ':', 'return', 'super', '(', ')', '.', 'run', '(', 'self', '.', '_strip_sql', '(', 'hql', ')', ',', 'parameters', ')']",Execute the statement against Presto. Can be used to create views.,"['Execute', 'the', 'statement', 'against', 'Presto', '.', 'Can', 'be', 'used', 'to', 'create', 'views', '.']",python,test,"['execute', 'the', 'statement', 'against', 'presto', '.', 'can', 'be', 'used', 'to', 'create', 'views', '.']",execute the statement against presto . can be used to create views .,"['def', 'run', '(', 'self', ',', 'hql', ',', 'parameters', '=', 'none', ')', ':', 'return', 'super', '(', ')', '.', 'run', '(', 'self', '.', '_strip_sql', '(', 'hql', ')', ',', 'parameters', ')']","def run ( self , hql , parameters = none ) : return super ( ) . run ( self . _strip_sql ( hql ) , parameters )"
136,apache/airflow,airflow/hooks/presto_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/presto_hook.py#L129-L140,"def insert_rows(self, table, rows, target_fields=None):
        """"""
        A generic way to insert a set of tuples into a table.

        :param table: Name of the target table
        :type table: str
        :param rows: The rows to insert into the table
        :type rows: iterable of tuples
        :param target_fields: The names of the columns to fill in the table
        :type target_fields: iterable of strings
        """"""
        super().insert_rows(table, rows, target_fields, 0)","['def', 'insert_rows', '(', 'self', ',', 'table', ',', 'rows', ',', 'target_fields', '=', 'None', ')', ':', 'super', '(', ')', '.', 'insert_rows', '(', 'table', ',', 'rows', ',', 'target_fields', ',', '0', ')']","A generic way to insert a set of tuples into a table.

        :param table: Name of the target table
        :type table: str
        :param rows: The rows to insert into the table
        :type rows: iterable of tuples
        :param target_fields: The names of the columns to fill in the table
        :type target_fields: iterable of strings","['A', 'generic', 'way', 'to', 'insert', 'a', 'set', 'of', 'tuples', 'into', 'a', 'table', '.']",python,test,"['a', 'generic', 'way', 'to', 'insert', 'a', 'set', 'of', 'tuples', 'into', 'a', 'table', '.']",a generic way to insert a set of tuples into a table .,"['def', 'insert_rows', '(', 'self', ',', 'table', ',', 'rows', ',', 'target_fields', '=', 'none', ')', ':', 'super', '(', ')', '.', 'insert_rows', '(', 'table', ',', 'rows', ',', 'target_fields', ',', '0', ')']","def insert_rows ( self , table , rows , target_fields = none ) : super ( ) . insert_rows ( table , rows , target_fields , 0 )"
137,apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L50-L60,"def get_conn(self):
        """"""
        Return a cosmos db client.
        """"""
        if self.cosmos_client is not None:
            return self.cosmos_client

        # Initialize the Python Azure Cosmos DB client
        self.cosmos_client = cosmos_client.CosmosClient(self.endpoint_uri, {'masterKey': self.master_key})

        return self.cosmos_client","['def', 'get_conn', '(', 'self', ')', ':', 'if', 'self', '.', 'cosmos_client', 'is', 'not', 'None', ':', 'return', 'self', '.', 'cosmos_client', '# Initialize the Python Azure Cosmos DB client', 'self', '.', 'cosmos_client', '=', 'cosmos_client', '.', 'CosmosClient', '(', 'self', '.', 'endpoint_uri', ',', '{', ""'masterKey'"", ':', 'self', '.', 'master_key', '}', ')', 'return', 'self', '.', 'cosmos_client']",Return a cosmos db client.,"['Return', 'a', 'cosmos', 'db', 'client', '.']",python,test,"['return', 'a', 'cosmos', 'db', 'client', '.']",return a cosmos db client .,"['def', 'get_conn', '(', 'self', ')', ':', 'if', 'self', '.', 'cosmos_client', 'is', 'not', 'none', ':', 'return', 'self', '.', 'cosmos_client', '# initialize the python azure cosmos db client', 'self', '.', 'cosmos_client', '=', 'cosmos_client', '.', 'cosmosclient', '(', 'self', '.', 'endpoint_uri', ',', '{', ""'masterkey'"", ':', 'self', '.', 'master_key', '}', ')', 'return', 'self', '.', 'cosmos_client']","def get_conn ( self ) : if self . cosmos_client is not none : return self . cosmos_client # initialize the python azure cosmos db client self . cosmos_client = cosmos_client . cosmosclient ( self . endpoint_uri , { 'masterkey' : self . master_key } ) return self . cosmos_client"
138,apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L82-L99,"def does_collection_exist(self, collection_name, database_name=None):
        """"""
        Checks if a collection exists in CosmosDB.
        """"""
        if collection_name is None:
            raise AirflowBadRequest(""Collection name cannot be None."")

        existing_container = list(self.get_conn().QueryContainers(
            get_database_link(self.__get_database_name(database_name)), {
                ""query"": ""SELECT * FROM r WHERE r.id=@id"",
                ""parameters"": [
                    {""name"": ""@id"", ""value"": collection_name}
                ]
            }))
        if len(existing_container) == 0:
            return False

        return True","['def', 'does_collection_exist', '(', 'self', ',', 'collection_name', ',', 'database_name', '=', 'None', ')', ':', 'if', 'collection_name', 'is', 'None', ':', 'raise', 'AirflowBadRequest', '(', '""Collection name cannot be None.""', ')', 'existing_container', '=', 'list', '(', 'self', '.', 'get_conn', '(', ')', '.', 'QueryContainers', '(', 'get_database_link', '(', 'self', '.', '__get_database_name', '(', 'database_name', ')', ')', ',', '{', '""query""', ':', '""SELECT * FROM r WHERE r.id=@id""', ',', '""parameters""', ':', '[', '{', '""name""', ':', '""@id""', ',', '""value""', ':', 'collection_name', '}', ']', '}', ')', ')', 'if', 'len', '(', 'existing_container', ')', '==', '0', ':', 'return', 'False', 'return', 'True']",Checks if a collection exists in CosmosDB.,"['Checks', 'if', 'a', 'collection', 'exists', 'in', 'CosmosDB', '.']",python,test,"['checks', 'if', 'a', 'collection', 'exists', 'in', 'cosmosdb', '.']",checks if a collection exists in cosmosdb .,"['def', 'does_collection_exist', '(', 'self', ',', 'collection_name', ',', 'database_name', '=', 'none', ')', ':', 'if', 'collection_name', 'is', 'none', ':', 'raise', 'airflowbadrequest', '(', '""collection name cannot be none.""', ')', 'existing_container', '=', 'list', '(', 'self', '.', 'get_conn', '(', ')', '.', 'querycontainers', '(', 'get_database_link', '(', 'self', '.', '__get_database_name', '(', 'database_name', ')', ')', ',', '{', '""query""', ':', '""select * from r where r.id=@id""', ',', '""parameters""', ':', '[', '{', '""name""', ':', '""@id""', ',', '""value""', ':', 'collection_name', '}', ']', '}', ')', ')', 'if', 'len', '(', 'existing_container', ')', '==', '0', ':', 'return', 'false', 'return', 'true']","def does_collection_exist ( self , collection_name , database_name = none ) : if collection_name is none : raise airflowbadrequest ( ""collection name cannot be none."" ) existing_container = list ( self . get_conn ( ) . querycontainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { ""query"" : ""select * from r where r.id=@id"" , ""parameters"" : [ { ""name"" : ""@id"" , ""value"" : collection_name } ] } ) ) if len ( existing_container ) == 0 : return false return true"
139,apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L101-L122,"def create_collection(self, collection_name, database_name=None):
        """"""
        Creates a new collection in the CosmosDB database.
        """"""
        if collection_name is None:
            raise AirflowBadRequest(""Collection name cannot be None."")

        # We need to check to see if this container already exists so we don't try
        # to create it twice
        existing_container = list(self.get_conn().QueryContainers(
            get_database_link(self.__get_database_name(database_name)), {
                ""query"": ""SELECT * FROM r WHERE r.id=@id"",
                ""parameters"": [
                    {""name"": ""@id"", ""value"": collection_name}
                ]
            }))

        # Only create if we did not find it already existing
        if len(existing_container) == 0:
            self.get_conn().CreateContainer(
                get_database_link(self.__get_database_name(database_name)),
                {""id"": collection_name})","['def', 'create_collection', '(', 'self', ',', 'collection_name', ',', 'database_name', '=', 'None', ')', ':', 'if', 'collection_name', 'is', 'None', ':', 'raise', 'AirflowBadRequest', '(', '""Collection name cannot be None.""', ')', ""# We need to check to see if this container already exists so we don't try"", '# to create it twice', 'existing_container', '=', 'list', '(', 'self', '.', 'get_conn', '(', ')', '.', 'QueryContainers', '(', 'get_database_link', '(', 'self', '.', '__get_database_name', '(', 'database_name', ')', ')', ',', '{', '""query""', ':', '""SELECT * FROM r WHERE r.id=@id""', ',', '""parameters""', ':', '[', '{', '""name""', ':', '""@id""', ',', '""value""', ':', 'collection_name', '}', ']', '}', ')', ')', '# Only create if we did not find it already existing', 'if', 'len', '(', 'existing_container', ')', '==', '0', ':', 'self', '.', 'get_conn', '(', ')', '.', 'CreateContainer', '(', 'get_database_link', '(', 'self', '.', '__get_database_name', '(', 'database_name', ')', ')', ',', '{', '""id""', ':', 'collection_name', '}', ')']",Creates a new collection in the CosmosDB database.,"['Creates', 'a', 'new', 'collection', 'in', 'the', 'CosmosDB', 'database', '.']",python,test,"['creates', 'a', 'new', 'collection', 'in', 'the', 'cosmosdb', 'database', '.']",creates a new collection in the cosmosdb database .,"['def', 'create_collection', '(', 'self', ',', 'collection_name', ',', 'database_name', '=', 'none', ')', ':', 'if', 'collection_name', 'is', 'none', ':', 'raise', 'airflowbadrequest', '(', '""collection name cannot be none.""', ')', ""# we need to check to see if this container already exists so we don't try"", '# to create it twice', 'existing_container', '=', 'list', '(', 'self', '.', 'get_conn', '(', ')', '.', 'querycontainers', '(', 'get_database_link', '(', 'self', '.', '__get_database_name', '(', 'database_name', ')', ')', ',', '{', '""query""', ':', '""select * from r where r.id=@id""', ',', '""parameters""', ':', '[', '{', '""name""', ':', '""@id""', ',', '""value""', ':', 'collection_name', '}', ']', '}', ')', ')', '# only create if we did not find it already existing', 'if', 'len', '(', 'existing_container', ')', '==', '0', ':', 'self', '.', 'get_conn', '(', ')', '.', 'createcontainer', '(', 'get_database_link', '(', 'self', '.', '__get_database_name', '(', 'database_name', ')', ')', ',', '{', '""id""', ':', 'collection_name', '}', ')']","def create_collection ( self , collection_name , database_name = none ) : if collection_name is none : raise airflowbadrequest ( ""collection name cannot be none."" ) # we need to check to see if this container already exists so we don't try # to create it twice existing_container = list ( self . get_conn ( ) . querycontainers ( get_database_link ( self . __get_database_name ( database_name ) ) , { ""query"" : ""select * from r where r.id=@id"" , ""parameters"" : [ { ""name"" : ""@id"" , ""value"" : collection_name } ] } ) ) # only create if we did not find it already existing if len ( existing_container ) == 0 : self . get_conn ( ) . createcontainer ( get_database_link ( self . __get_database_name ( database_name ) ) , { ""id"" : collection_name } )"
140,apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L124-L140,"def does_database_exist(self, database_name):
        """"""
        Checks if a database exists in CosmosDB.
        """"""
        if database_name is None:
            raise AirflowBadRequest(""Database name cannot be None."")

        existing_database = list(self.get_conn().QueryDatabases({
            ""query"": ""SELECT * FROM r WHERE r.id=@id"",
            ""parameters"": [
                {""name"": ""@id"", ""value"": database_name}
            ]
        }))
        if len(existing_database) == 0:
            return False

        return True","['def', 'does_database_exist', '(', 'self', ',', 'database_name', ')', ':', 'if', 'database_name', 'is', 'None', ':', 'raise', 'AirflowBadRequest', '(', '""Database name cannot be None.""', ')', 'existing_database', '=', 'list', '(', 'self', '.', 'get_conn', '(', ')', '.', 'QueryDatabases', '(', '{', '""query""', ':', '""SELECT * FROM r WHERE r.id=@id""', ',', '""parameters""', ':', '[', '{', '""name""', ':', '""@id""', ',', '""value""', ':', 'database_name', '}', ']', '}', ')', ')', 'if', 'len', '(', 'existing_database', ')', '==', '0', ':', 'return', 'False', 'return', 'True']",Checks if a database exists in CosmosDB.,"['Checks', 'if', 'a', 'database', 'exists', 'in', 'CosmosDB', '.']",python,test,"['checks', 'if', 'a', 'database', 'exists', 'in', 'cosmosdb', '.']",checks if a database exists in cosmosdb .,"['def', 'does_database_exist', '(', 'self', ',', 'database_name', ')', ':', 'if', 'database_name', 'is', 'none', ':', 'raise', 'airflowbadrequest', '(', '""database name cannot be none.""', ')', 'existing_database', '=', 'list', '(', 'self', '.', 'get_conn', '(', ')', '.', 'querydatabases', '(', '{', '""query""', ':', '""select * from r where r.id=@id""', ',', '""parameters""', ':', '[', '{', '""name""', ':', '""@id""', ',', '""value""', ':', 'database_name', '}', ']', '}', ')', ')', 'if', 'len', '(', 'existing_database', ')', '==', '0', ':', 'return', 'false', 'return', 'true']","def does_database_exist ( self , database_name ) : if database_name is none : raise airflowbadrequest ( ""database name cannot be none."" ) existing_database = list ( self . get_conn ( ) . querydatabases ( { ""query"" : ""select * from r where r.id=@id"" , ""parameters"" : [ { ""name"" : ""@id"" , ""value"" : database_name } ] } ) ) if len ( existing_database ) == 0 : return false return true"
141,apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L142-L160,"def create_database(self, database_name):
        """"""
        Creates a new database in CosmosDB.
        """"""
        if database_name is None:
            raise AirflowBadRequest(""Database name cannot be None."")

        # We need to check to see if this database already exists so we don't try
        # to create it twice
        existing_database = list(self.get_conn().QueryDatabases({
            ""query"": ""SELECT * FROM r WHERE r.id=@id"",
            ""parameters"": [
                {""name"": ""@id"", ""value"": database_name}
            ]
        }))

        # Only create if we did not find it already existing
        if len(existing_database) == 0:
            self.get_conn().CreateDatabase({""id"": database_name})","['def', 'create_database', '(', 'self', ',', 'database_name', ')', ':', 'if', 'database_name', 'is', 'None', ':', 'raise', 'AirflowBadRequest', '(', '""Database name cannot be None.""', ')', ""# We need to check to see if this database already exists so we don't try"", '# to create it twice', 'existing_database', '=', 'list', '(', 'self', '.', 'get_conn', '(', ')', '.', 'QueryDatabases', '(', '{', '""query""', ':', '""SELECT * FROM r WHERE r.id=@id""', ',', '""parameters""', ':', '[', '{', '""name""', ':', '""@id""', ',', '""value""', ':', 'database_name', '}', ']', '}', ')', ')', '# Only create if we did not find it already existing', 'if', 'len', '(', 'existing_database', ')', '==', '0', ':', 'self', '.', 'get_conn', '(', ')', '.', 'CreateDatabase', '(', '{', '""id""', ':', 'database_name', '}', ')']",Creates a new database in CosmosDB.,"['Creates', 'a', 'new', 'database', 'in', 'CosmosDB', '.']",python,test,"['creates', 'a', 'new', 'database', 'in', 'cosmosdb', '.']",creates a new database in cosmosdb .,"['def', 'create_database', '(', 'self', ',', 'database_name', ')', ':', 'if', 'database_name', 'is', 'none', ':', 'raise', 'airflowbadrequest', '(', '""database name cannot be none.""', ')', ""# we need to check to see if this database already exists so we don't try"", '# to create it twice', 'existing_database', '=', 'list', '(', 'self', '.', 'get_conn', '(', ')', '.', 'querydatabases', '(', '{', '""query""', ':', '""select * from r where r.id=@id""', ',', '""parameters""', ':', '[', '{', '""name""', ':', '""@id""', ',', '""value""', ':', 'database_name', '}', ']', '}', ')', ')', '# only create if we did not find it already existing', 'if', 'len', '(', 'existing_database', ')', '==', '0', ':', 'self', '.', 'get_conn', '(', ')', '.', 'createdatabase', '(', '{', '""id""', ':', 'database_name', '}', ')']","def create_database ( self , database_name ) : if database_name is none : raise airflowbadrequest ( ""database name cannot be none."" ) # we need to check to see if this database already exists so we don't try # to create it twice existing_database = list ( self . get_conn ( ) . querydatabases ( { ""query"" : ""select * from r where r.id=@id"" , ""parameters"" : [ { ""name"" : ""@id"" , ""value"" : database_name } ] } ) ) # only create if we did not find it already existing if len ( existing_database ) == 0 : self . get_conn ( ) . createdatabase ( { ""id"" : database_name } )"
142,apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L162-L169,"def delete_database(self, database_name):
        """"""
        Deletes an existing database in CosmosDB.
        """"""
        if database_name is None:
            raise AirflowBadRequest(""Database name cannot be None."")

        self.get_conn().DeleteDatabase(get_database_link(database_name))","['def', 'delete_database', '(', 'self', ',', 'database_name', ')', ':', 'if', 'database_name', 'is', 'None', ':', 'raise', 'AirflowBadRequest', '(', '""Database name cannot be None.""', ')', 'self', '.', 'get_conn', '(', ')', '.', 'DeleteDatabase', '(', 'get_database_link', '(', 'database_name', ')', ')']",Deletes an existing database in CosmosDB.,"['Deletes', 'an', 'existing', 'database', 'in', 'CosmosDB', '.']",python,test,"['deletes', 'an', 'existing', 'database', 'in', 'cosmosdb', '.']",deletes an existing database in cosmosdb .,"['def', 'delete_database', '(', 'self', ',', 'database_name', ')', ':', 'if', 'database_name', 'is', 'none', ':', 'raise', 'airflowbadrequest', '(', '""database name cannot be none.""', ')', 'self', '.', 'get_conn', '(', ')', '.', 'deletedatabase', '(', 'get_database_link', '(', 'database_name', ')', ')']","def delete_database ( self , database_name ) : if database_name is none : raise airflowbadrequest ( ""database name cannot be none."" ) self . get_conn ( ) . deletedatabase ( get_database_link ( database_name ) )"
143,apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L171-L179,"def delete_collection(self, collection_name, database_name=None):
        """"""
        Deletes an existing collection in the CosmosDB database.
        """"""
        if collection_name is None:
            raise AirflowBadRequest(""Collection name cannot be None."")

        self.get_conn().DeleteContainer(
            get_collection_link(self.__get_database_name(database_name), collection_name))","['def', 'delete_collection', '(', 'self', ',', 'collection_name', ',', 'database_name', '=', 'None', ')', ':', 'if', 'collection_name', 'is', 'None', ':', 'raise', 'AirflowBadRequest', '(', '""Collection name cannot be None.""', ')', 'self', '.', 'get_conn', '(', ')', '.', 'DeleteContainer', '(', 'get_collection_link', '(', 'self', '.', '__get_database_name', '(', 'database_name', ')', ',', 'collection_name', ')', ')']",Deletes an existing collection in the CosmosDB database.,"['Deletes', 'an', 'existing', 'collection', 'in', 'the', 'CosmosDB', 'database', '.']",python,test,"['deletes', 'an', 'existing', 'collection', 'in', 'the', 'cosmosdb', 'database', '.']",deletes an existing collection in the cosmosdb database .,"['def', 'delete_collection', '(', 'self', ',', 'collection_name', ',', 'database_name', '=', 'none', ')', ':', 'if', 'collection_name', 'is', 'none', ':', 'raise', 'airflowbadrequest', '(', '""collection name cannot be none.""', ')', 'self', '.', 'get_conn', '(', ')', '.', 'deletecontainer', '(', 'get_collection_link', '(', 'self', '.', '__get_database_name', '(', 'database_name', ')', ',', 'collection_name', ')', ')']","def delete_collection ( self , collection_name , database_name = none ) : if collection_name is none : raise airflowbadrequest ( ""collection name cannot be none."" ) self . get_conn ( ) . deletecontainer ( get_collection_link ( self . __get_database_name ( database_name ) , collection_name ) )"
144,apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L181-L206,"def upsert_document(self, document, database_name=None, collection_name=None, document_id=None):
        """"""
        Inserts a new document (or updates an existing one) into an existing
        collection in the CosmosDB database.
        """"""
        # Assign unique ID if one isn't provided
        if document_id is None:
            document_id = str(uuid.uuid4())

        if document is None:
            raise AirflowBadRequest(""You cannot insert a None document"")

        # Add document id if isn't found
        if 'id' in document:
            if document['id'] is None:
                document['id'] = document_id
        else:
            document['id'] = document_id

        created_document = self.get_conn().CreateItem(
            get_collection_link(
                self.__get_database_name(database_name),
                self.__get_collection_name(collection_name)),
            document)

        return created_document","['def', 'upsert_document', '(', 'self', ',', 'document', ',', 'database_name', '=', 'None', ',', 'collection_name', '=', 'None', ',', 'document_id', '=', 'None', ')', ':', ""# Assign unique ID if one isn't provided"", 'if', 'document_id', 'is', 'None', ':', 'document_id', '=', 'str', '(', 'uuid', '.', 'uuid4', '(', ')', ')', 'if', 'document', 'is', 'None', ':', 'raise', 'AirflowBadRequest', '(', '""You cannot insert a None document""', ')', ""# Add document id if isn't found"", 'if', ""'id'"", 'in', 'document', ':', 'if', 'document', '[', ""'id'"", ']', 'is', 'None', ':', 'document', '[', ""'id'"", ']', '=', 'document_id', 'else', ':', 'document', '[', ""'id'"", ']', '=', 'document_id', 'created_document', '=', 'self', '.', 'get_conn', '(', ')', '.', 'CreateItem', '(', 'get_collection_link', '(', 'self', '.', '__get_database_name', '(', 'database_name', ')', ',', 'self', '.', '__get_collection_name', '(', 'collection_name', ')', ')', ',', 'document', ')', 'return', 'created_document']","Inserts a new document (or updates an existing one) into an existing
        collection in the CosmosDB database.","['Inserts', 'a', 'new', 'document', '(', 'or', 'updates', 'an', 'existing', 'one', ')', 'into', 'an', 'existing', 'collection', 'in', 'the', 'CosmosDB', 'database', '.']",python,test,"['inserts', 'a', 'new', 'document', '(', 'or', 'updates', 'an', 'existing', 'one', ')', 'into', 'an', 'existing', 'collection', 'in', 'the', 'cosmosdb', 'database', '.']",inserts a new document ( or updates an existing one ) into an existing collection in the cosmosdb database .,"['def', 'upsert_document', '(', 'self', ',', 'document', ',', 'database_name', '=', 'none', ',', 'collection_name', '=', 'none', ',', 'document_id', '=', 'none', ')', ':', ""# assign unique id if one isn't provided"", 'if', 'document_id', 'is', 'none', ':', 'document_id', '=', 'str', '(', 'uuid', '.', 'uuid4', '(', ')', ')', 'if', 'document', 'is', 'none', ':', 'raise', 'airflowbadrequest', '(', '""you cannot insert a none document""', ')', ""# add document id if isn't found"", 'if', ""'id'"", 'in', 'document', ':', 'if', 'document', '[', ""'id'"", ']', 'is', 'none', ':', 'document', '[', ""'id'"", ']', '=', 'document_id', 'else', ':', 'document', '[', ""'id'"", ']', '=', 'document_id', 'created_document', '=', 'self', '.', 'get_conn', '(', ')', '.', 'createitem', '(', 'get_collection_link', '(', 'self', '.', '__get_database_name', '(', 'database_name', ')', ',', 'self', '.', '__get_collection_name', '(', 'collection_name', ')', ')', ',', 'document', ')', 'return', 'created_document']","def upsert_document ( self , document , database_name = none , collection_name = none , document_id = none ) : # assign unique id if one isn't provided if document_id is none : document_id = str ( uuid . uuid4 ( ) ) if document is none : raise airflowbadrequest ( ""you cannot insert a none document"" ) # add document id if isn't found if 'id' in document : if document [ 'id' ] is none : document [ 'id' ] = document_id else : document [ 'id' ] = document_id created_document = self . get_conn ( ) . createitem ( get_collection_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) ) , document ) return created_document"
145,apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L208-L224,"def insert_documents(self, documents, database_name=None, collection_name=None):
        """"""
        Insert a list of new documents into an existing collection in the CosmosDB database.
        """"""
        if documents is None:
            raise AirflowBadRequest(""You cannot insert empty documents"")

        created_documents = []
        for single_document in documents:
            created_documents.append(
                self.get_conn().CreateItem(
                    get_collection_link(
                        self.__get_database_name(database_name),
                        self.__get_collection_name(collection_name)),
                    single_document))

        return created_documents","['def', 'insert_documents', '(', 'self', ',', 'documents', ',', 'database_name', '=', 'None', ',', 'collection_name', '=', 'None', ')', ':', 'if', 'documents', 'is', 'None', ':', 'raise', 'AirflowBadRequest', '(', '""You cannot insert empty documents""', ')', 'created_documents', '=', '[', ']', 'for', 'single_document', 'in', 'documents', ':', 'created_documents', '.', 'append', '(', 'self', '.', 'get_conn', '(', ')', '.', 'CreateItem', '(', 'get_collection_link', '(', 'self', '.', '__get_database_name', '(', 'database_name', ')', ',', 'self', '.', '__get_collection_name', '(', 'collection_name', ')', ')', ',', 'single_document', ')', ')', 'return', 'created_documents']",Insert a list of new documents into an existing collection in the CosmosDB database.,"['Insert', 'a', 'list', 'of', 'new', 'documents', 'into', 'an', 'existing', 'collection', 'in', 'the', 'CosmosDB', 'database', '.']",python,test,"['insert', 'a', 'list', 'of', 'new', 'documents', 'into', 'an', 'existing', 'collection', 'in', 'the', 'cosmosdb', 'database', '.']",insert a list of new documents into an existing collection in the cosmosdb database .,"['def', 'insert_documents', '(', 'self', ',', 'documents', ',', 'database_name', '=', 'none', ',', 'collection_name', '=', 'none', ')', ':', 'if', 'documents', 'is', 'none', ':', 'raise', 'airflowbadrequest', '(', '""you cannot insert empty documents""', ')', 'created_documents', '=', '[', ']', 'for', 'single_document', 'in', 'documents', ':', 'created_documents', '.', 'append', '(', 'self', '.', 'get_conn', '(', ')', '.', 'createitem', '(', 'get_collection_link', '(', 'self', '.', '__get_database_name', '(', 'database_name', ')', ',', 'self', '.', '__get_collection_name', '(', 'collection_name', ')', ')', ',', 'single_document', ')', ')', 'return', 'created_documents']","def insert_documents ( self , documents , database_name = none , collection_name = none ) : if documents is none : raise airflowbadrequest ( ""you cannot insert empty documents"" ) created_documents = [ ] for single_document in documents : created_documents . append ( self . get_conn ( ) . createitem ( get_collection_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) ) , single_document ) ) return created_documents"
146,apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L226-L237,"def delete_document(self, document_id, database_name=None, collection_name=None):
        """"""
        Delete an existing document out of a collection in the CosmosDB database.
        """"""
        if document_id is None:
            raise AirflowBadRequest(""Cannot delete a document without an id"")

        self.get_conn().DeleteItem(
            get_document_link(
                self.__get_database_name(database_name),
                self.__get_collection_name(collection_name),
                document_id))","['def', 'delete_document', '(', 'self', ',', 'document_id', ',', 'database_name', '=', 'None', ',', 'collection_name', '=', 'None', ')', ':', 'if', 'document_id', 'is', 'None', ':', 'raise', 'AirflowBadRequest', '(', '""Cannot delete a document without an id""', ')', 'self', '.', 'get_conn', '(', ')', '.', 'DeleteItem', '(', 'get_document_link', '(', 'self', '.', '__get_database_name', '(', 'database_name', ')', ',', 'self', '.', '__get_collection_name', '(', 'collection_name', ')', ',', 'document_id', ')', ')']",Delete an existing document out of a collection in the CosmosDB database.,"['Delete', 'an', 'existing', 'document', 'out', 'of', 'a', 'collection', 'in', 'the', 'CosmosDB', 'database', '.']",python,test,"['delete', 'an', 'existing', 'document', 'out', 'of', 'a', 'collection', 'in', 'the', 'cosmosdb', 'database', '.']",delete an existing document out of a collection in the cosmosdb database .,"['def', 'delete_document', '(', 'self', ',', 'document_id', ',', 'database_name', '=', 'none', ',', 'collection_name', '=', 'none', ')', ':', 'if', 'document_id', 'is', 'none', ':', 'raise', 'airflowbadrequest', '(', '""cannot delete a document without an id""', ')', 'self', '.', 'get_conn', '(', ')', '.', 'deleteitem', '(', 'get_document_link', '(', 'self', '.', '__get_database_name', '(', 'database_name', ')', ',', 'self', '.', '__get_collection_name', '(', 'collection_name', ')', ',', 'document_id', ')', ')']","def delete_document ( self , document_id , database_name = none , collection_name = none ) : if document_id is none : raise airflowbadrequest ( ""cannot delete a document without an id"" ) self . get_conn ( ) . deleteitem ( get_document_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) , document_id ) )"
147,apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L239-L253,"def get_document(self, document_id, database_name=None, collection_name=None):
        """"""
        Get a document from an existing collection in the CosmosDB database.
        """"""
        if document_id is None:
            raise AirflowBadRequest(""Cannot get a document without an id"")

        try:
            return self.get_conn().ReadItem(
                get_document_link(
                    self.__get_database_name(database_name),
                    self.__get_collection_name(collection_name),
                    document_id))
        except HTTPFailure:
            return None","['def', 'get_document', '(', 'self', ',', 'document_id', ',', 'database_name', '=', 'None', ',', 'collection_name', '=', 'None', ')', ':', 'if', 'document_id', 'is', 'None', ':', 'raise', 'AirflowBadRequest', '(', '""Cannot get a document without an id""', ')', 'try', ':', 'return', 'self', '.', 'get_conn', '(', ')', '.', 'ReadItem', '(', 'get_document_link', '(', 'self', '.', '__get_database_name', '(', 'database_name', ')', ',', 'self', '.', '__get_collection_name', '(', 'collection_name', ')', ',', 'document_id', ')', ')', 'except', 'HTTPFailure', ':', 'return', 'None']",Get a document from an existing collection in the CosmosDB database.,"['Get', 'a', 'document', 'from', 'an', 'existing', 'collection', 'in', 'the', 'CosmosDB', 'database', '.']",python,test,"['get', 'a', 'document', 'from', 'an', 'existing', 'collection', 'in', 'the', 'cosmosdb', 'database', '.']",get a document from an existing collection in the cosmosdb database .,"['def', 'get_document', '(', 'self', ',', 'document_id', ',', 'database_name', '=', 'none', ',', 'collection_name', '=', 'none', ')', ':', 'if', 'document_id', 'is', 'none', ':', 'raise', 'airflowbadrequest', '(', '""cannot get a document without an id""', ')', 'try', ':', 'return', 'self', '.', 'get_conn', '(', ')', '.', 'readitem', '(', 'get_document_link', '(', 'self', '.', '__get_database_name', '(', 'database_name', ')', ',', 'self', '.', '__get_collection_name', '(', 'collection_name', ')', ',', 'document_id', ')', ')', 'except', 'httpfailure', ':', 'return', 'none']","def get_document ( self , document_id , database_name = none , collection_name = none ) : if document_id is none : raise airflowbadrequest ( ""cannot get a document without an id"" ) try : return self . get_conn ( ) . readitem ( get_document_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) , document_id ) ) except httpfailure : return none"
148,apache/airflow,airflow/contrib/hooks/azure_cosmos_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_cosmos_hook.py#L255-L275,"def get_documents(self, sql_string, database_name=None, collection_name=None, partition_key=None):
        """"""
        Get a list of documents from an existing collection in the CosmosDB database via SQL query.
        """"""
        if sql_string is None:
            raise AirflowBadRequest(""SQL query string cannot be None"")

        # Query them in SQL
        query = {'query': sql_string}

        try:
            result_iterable = self.get_conn().QueryItems(
                get_collection_link(
                    self.__get_database_name(database_name),
                    self.__get_collection_name(collection_name)),
                query,
                partition_key)

            return list(result_iterable)
        except HTTPFailure:
            return None","['def', 'get_documents', '(', 'self', ',', 'sql_string', ',', 'database_name', '=', 'None', ',', 'collection_name', '=', 'None', ',', 'partition_key', '=', 'None', ')', ':', 'if', 'sql_string', 'is', 'None', ':', 'raise', 'AirflowBadRequest', '(', '""SQL query string cannot be None""', ')', '# Query them in SQL', 'query', '=', '{', ""'query'"", ':', 'sql_string', '}', 'try', ':', 'result_iterable', '=', 'self', '.', 'get_conn', '(', ')', '.', 'QueryItems', '(', 'get_collection_link', '(', 'self', '.', '__get_database_name', '(', 'database_name', ')', ',', 'self', '.', '__get_collection_name', '(', 'collection_name', ')', ')', ',', 'query', ',', 'partition_key', ')', 'return', 'list', '(', 'result_iterable', ')', 'except', 'HTTPFailure', ':', 'return', 'None']",Get a list of documents from an existing collection in the CosmosDB database via SQL query.,"['Get', 'a', 'list', 'of', 'documents', 'from', 'an', 'existing', 'collection', 'in', 'the', 'CosmosDB', 'database', 'via', 'SQL', 'query', '.']",python,test,"['get', 'a', 'list', 'of', 'documents', 'from', 'an', 'existing', 'collection', 'in', 'the', 'cosmosdb', 'database', 'via', 'sql', 'query', '.']",get a list of documents from an existing collection in the cosmosdb database via sql query .,"['def', 'get_documents', '(', 'self', ',', 'sql_string', ',', 'database_name', '=', 'none', ',', 'collection_name', '=', 'none', ',', 'partition_key', '=', 'none', ')', ':', 'if', 'sql_string', 'is', 'none', ':', 'raise', 'airflowbadrequest', '(', '""sql query string cannot be none""', ')', '# query them in sql', 'query', '=', '{', ""'query'"", ':', 'sql_string', '}', 'try', ':', 'result_iterable', '=', 'self', '.', 'get_conn', '(', ')', '.', 'queryitems', '(', 'get_collection_link', '(', 'self', '.', '__get_database_name', '(', 'database_name', ')', ',', 'self', '.', '__get_collection_name', '(', 'collection_name', ')', ')', ',', 'query', ',', 'partition_key', ')', 'return', 'list', '(', 'result_iterable', ')', 'except', 'httpfailure', ':', 'return', 'none']","def get_documents ( self , sql_string , database_name = none , collection_name = none , partition_key = none ) : if sql_string is none : raise airflowbadrequest ( ""sql query string cannot be none"" ) # query them in sql query = { 'query' : sql_string } try : result_iterable = self . get_conn ( ) . queryitems ( get_collection_link ( self . __get_database_name ( database_name ) , self . __get_collection_name ( collection_name ) ) , query , partition_key ) return list ( result_iterable ) except httpfailure : return none"
149,apache/airflow,airflow/api/common/experimental/get_code.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/api/common/experimental/get_code.py#L25-L42,"def get_code(dag_id):
    """"""Return python code of a given dag_id.""""""
    session = settings.Session()
    DM = models.DagModel
    dag = session.query(DM).filter(DM.dag_id == dag_id).first()
    session.close()
    # Check DAG exists.
    if dag is None:
        error_message = ""Dag id {} not found"".format(dag_id)
        raise DagNotFound(error_message)

    try:
        with wwwutils.open_maybe_zipped(dag.fileloc, 'r') as f:
            code = f.read()
            return code
    except IOError as e:
        error_message = ""Error {} while reading Dag id {} Code"".format(str(e), dag_id)
        raise AirflowException(error_message)","['def', 'get_code', '(', 'dag_id', ')', ':', 'session', '=', 'settings', '.', 'Session', '(', ')', 'DM', '=', 'models', '.', 'DagModel', 'dag', '=', 'session', '.', 'query', '(', 'DM', ')', '.', 'filter', '(', 'DM', '.', 'dag_id', '==', 'dag_id', ')', '.', 'first', '(', ')', 'session', '.', 'close', '(', ')', '# Check DAG exists.', 'if', 'dag', 'is', 'None', ':', 'error_message', '=', '""Dag id {} not found""', '.', 'format', '(', 'dag_id', ')', 'raise', 'DagNotFound', '(', 'error_message', ')', 'try', ':', 'with', 'wwwutils', '.', 'open_maybe_zipped', '(', 'dag', '.', 'fileloc', ',', ""'r'"", ')', 'as', 'f', ':', 'code', '=', 'f', '.', 'read', '(', ')', 'return', 'code', 'except', 'IOError', 'as', 'e', ':', 'error_message', '=', '""Error {} while reading Dag id {} Code""', '.', 'format', '(', 'str', '(', 'e', ')', ',', 'dag_id', ')', 'raise', 'AirflowException', '(', 'error_message', ')']",Return python code of a given dag_id.,"['Return', 'python', 'code', 'of', 'a', 'given', 'dag_id', '.']",python,test,"['return', 'python', 'code', 'of', 'a', 'given', 'dag_id', '.']",return python code of a given dag_id .,"['def', 'get_code', '(', 'dag_id', ')', ':', 'session', '=', 'settings', '.', 'session', '(', ')', 'dm', '=', 'models', '.', 'dagmodel', 'dag', '=', 'session', '.', 'query', '(', 'dm', ')', '.', 'filter', '(', 'dm', '.', 'dag_id', '==', 'dag_id', ')', '.', 'first', '(', ')', 'session', '.', 'close', '(', ')', '# check dag exists.', 'if', 'dag', 'is', 'none', ':', 'error_message', '=', '""dag id {} not found""', '.', 'format', '(', 'dag_id', ')', 'raise', 'dagnotfound', '(', 'error_message', ')', 'try', ':', 'with', 'wwwutils', '.', 'open_maybe_zipped', '(', 'dag', '.', 'fileloc', ',', ""'r'"", ')', 'as', 'f', ':', 'code', '=', 'f', '.', 'read', '(', ')', 'return', 'code', 'except', 'ioerror', 'as', 'e', ':', 'error_message', '=', '""error {} while reading dag id {} code""', '.', 'format', '(', 'str', '(', 'e', ')', ',', 'dag_id', ')', 'raise', 'airflowexception', '(', 'error_message', ')']","def get_code ( dag_id ) : session = settings . session ( ) dm = models . dagmodel dag = session . query ( dm ) . filter ( dm . dag_id == dag_id ) . first ( ) session . close ( ) # check dag exists. if dag is none : error_message = ""dag id {} not found"" . format ( dag_id ) raise dagnotfound ( error_message ) try : with wwwutils . open_maybe_zipped ( dag . fileloc , 'r' ) as f : code = f . read ( ) return code except ioerror as e : error_message = ""error {} while reading dag id {} code"" . format ( str ( e ) , dag_id ) raise airflowexception ( error_message )"
150,apache/airflow,airflow/contrib/hooks/gcp_function_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L76-L86,"def get_function(self, name):
        """"""
        Returns the Cloud Function with the given name.

        :param name: Name of the function.
        :type name: str
        :return: A Cloud Functions object representing the function.
        :rtype: dict
        """"""
        return self.get_conn().projects().locations().functions().get(
            name=name).execute(num_retries=self.num_retries)","['def', 'get_function', '(', 'self', ',', 'name', ')', ':', 'return', 'self', '.', 'get_conn', '(', ')', '.', 'projects', '(', ')', '.', 'locations', '(', ')', '.', 'functions', '(', ')', '.', 'get', '(', 'name', '=', 'name', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')']","Returns the Cloud Function with the given name.

        :param name: Name of the function.
        :type name: str
        :return: A Cloud Functions object representing the function.
        :rtype: dict","['Returns', 'the', 'Cloud', 'Function', 'with', 'the', 'given', 'name', '.']",python,test,"['returns', 'the', 'cloud', 'function', 'with', 'the', 'given', 'name', '.']",returns the cloud function with the given name .,"['def', 'get_function', '(', 'self', ',', 'name', ')', ':', 'return', 'self', '.', 'get_conn', '(', ')', '.', 'projects', '(', ')', '.', 'locations', '(', ')', '.', 'functions', '(', ')', '.', 'get', '(', 'name', '=', 'name', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')']","def get_function ( self , name ) : return self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . get ( name = name ) . execute ( num_retries = self . num_retries )"
151,apache/airflow,airflow/contrib/hooks/gcp_function_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L89-L107,"def create_new_function(self, location, body, project_id=None):
        """"""
        Creates a new function in Cloud Function in the location specified in the body.

        :param location: The location of the function.
        :type location: str
        :param body: The body required by the Cloud Functions insert API.
        :type body: dict
        :param project_id: Optional, Google Cloud Project project_id where the function belongs.
            If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().projects().locations().functions().create(
            location=self._full_location(project_id, location),
            body=body
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(operation_name=operation_name)","['def', 'create_new_function', '(', 'self', ',', 'location', ',', 'body', ',', 'project_id', '=', 'None', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'projects', '(', ')', '.', 'locations', '(', ')', '.', 'functions', '(', ')', '.', 'create', '(', 'location', '=', 'self', '.', '_full_location', '(', 'project_id', ',', 'location', ')', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'operation_name', '=', 'operation_name', ')']","Creates a new function in Cloud Function in the location specified in the body.

        :param location: The location of the function.
        :type location: str
        :param body: The body required by the Cloud Functions insert API.
        :type body: dict
        :param project_id: Optional, Google Cloud Project project_id where the function belongs.
            If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None","['Creates', 'a', 'new', 'function', 'in', 'Cloud', 'Function', 'in', 'the', 'location', 'specified', 'in', 'the', 'body', '.']",python,test,"['creates', 'a', 'new', 'function', 'in', 'cloud', 'function', 'in', 'the', 'location', 'specified', 'in', 'the', 'body', '.']",creates a new function in cloud function in the location specified in the body .,"['def', 'create_new_function', '(', 'self', ',', 'location', ',', 'body', ',', 'project_id', '=', 'none', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'projects', '(', ')', '.', 'locations', '(', ')', '.', 'functions', '(', ')', '.', 'create', '(', 'location', '=', 'self', '.', '_full_location', '(', 'project_id', ',', 'location', ')', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'operation_name', '=', 'operation_name', ')']","def create_new_function ( self , location , body , project_id = none ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . create ( location = self . _full_location ( project_id , location ) , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( operation_name = operation_name )"
152,apache/airflow,airflow/contrib/hooks/gcp_function_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L109-L127,"def update_function(self, name, body, update_mask):
        """"""
        Updates Cloud Functions according to the specified update mask.

        :param name: The name of the function.
        :type name: str
        :param body: The body required by the cloud function patch API.
        :type body: dict
        :param update_mask: The update mask - array of fields that should be patched.
        :type update_mask: [str]
        :return: None
        """"""
        response = self.get_conn().projects().locations().functions().patch(
            updateMask="","".join(update_mask),
            name=name,
            body=body
        ).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(operation_name=operation_name)","['def', 'update_function', '(', 'self', ',', 'name', ',', 'body', ',', 'update_mask', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'projects', '(', ')', '.', 'locations', '(', ')', '.', 'functions', '(', ')', '.', 'patch', '(', 'updateMask', '=', '"",""', '.', 'join', '(', 'update_mask', ')', ',', 'name', '=', 'name', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'operation_name', '=', 'operation_name', ')']","Updates Cloud Functions according to the specified update mask.

        :param name: The name of the function.
        :type name: str
        :param body: The body required by the cloud function patch API.
        :type body: dict
        :param update_mask: The update mask - array of fields that should be patched.
        :type update_mask: [str]
        :return: None","['Updates', 'Cloud', 'Functions', 'according', 'to', 'the', 'specified', 'update', 'mask', '.']",python,test,"['updates', 'cloud', 'functions', 'according', 'to', 'the', 'specified', 'update', 'mask', '.']",updates cloud functions according to the specified update mask .,"['def', 'update_function', '(', 'self', ',', 'name', ',', 'body', ',', 'update_mask', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'projects', '(', ')', '.', 'locations', '(', ')', '.', 'functions', '(', ')', '.', 'patch', '(', 'updatemask', '=', '"",""', '.', 'join', '(', 'update_mask', ')', ',', 'name', '=', 'name', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'operation_name', '=', 'operation_name', ')']","def update_function ( self , name , body , update_mask ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . patch ( updatemask = "","" . join ( update_mask ) , name = name , body = body ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( operation_name = operation_name )"
153,apache/airflow,airflow/contrib/hooks/gcp_function_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L130-L159,"def upload_function_zip(self, location, zip_path, project_id=None):
        """"""
        Uploads zip file with sources.

        :param location: The location where the function is created.
        :type location: str
        :param zip_path: The path of the valid .zip file to upload.
        :type zip_path: str
        :param project_id: Optional, Google Cloud Project project_id where the function belongs.
            If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: The upload URL that was returned by generateUploadUrl method.
        """"""
        response = self.get_conn().projects().locations().functions().generateUploadUrl(
            parent=self._full_location(project_id, location)
        ).execute(num_retries=self.num_retries)
        upload_url = response.get('uploadUrl')
        with open(zip_path, 'rb') as fp:
            requests.put(
                url=upload_url,
                data=fp,
                # Those two headers needs to be specified according to:
                # https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl
                # nopep8
                headers={
                    'Content-type': 'application/zip',
                    'x-goog-content-length-range': '0,104857600',
                }
            )
        return upload_url","['def', 'upload_function_zip', '(', 'self', ',', 'location', ',', 'zip_path', ',', 'project_id', '=', 'None', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'projects', '(', ')', '.', 'locations', '(', ')', '.', 'functions', '(', ')', '.', 'generateUploadUrl', '(', 'parent', '=', 'self', '.', '_full_location', '(', 'project_id', ',', 'location', ')', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'upload_url', '=', 'response', '.', 'get', '(', ""'uploadUrl'"", ')', 'with', 'open', '(', 'zip_path', ',', ""'rb'"", ')', 'as', 'fp', ':', 'requests', '.', 'put', '(', 'url', '=', 'upload_url', ',', 'data', '=', 'fp', ',', '# Those two headers needs to be specified according to:', '# https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateUploadUrl', '# nopep8', 'headers', '=', '{', ""'Content-type'"", ':', ""'application/zip'"", ',', ""'x-goog-content-length-range'"", ':', ""'0,104857600'"", ',', '}', ')', 'return', 'upload_url']","Uploads zip file with sources.

        :param location: The location where the function is created.
        :type location: str
        :param zip_path: The path of the valid .zip file to upload.
        :type zip_path: str
        :param project_id: Optional, Google Cloud Project project_id where the function belongs.
            If set to None or missing, the default project_id from the GCP connection is used.
        :type project_id: str
        :return: The upload URL that was returned by generateUploadUrl method.","['Uploads', 'zip', 'file', 'with', 'sources', '.']",python,test,"['uploads', 'zip', 'file', 'with', 'sources', '.']",uploads zip file with sources .,"['def', 'upload_function_zip', '(', 'self', ',', 'location', ',', 'zip_path', ',', 'project_id', '=', 'none', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'projects', '(', ')', '.', 'locations', '(', ')', '.', 'functions', '(', ')', '.', 'generateuploadurl', '(', 'parent', '=', 'self', '.', '_full_location', '(', 'project_id', ',', 'location', ')', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'upload_url', '=', 'response', '.', 'get', '(', ""'uploadurl'"", ')', 'with', 'open', '(', 'zip_path', ',', ""'rb'"", ')', 'as', 'fp', ':', 'requests', '.', 'put', '(', 'url', '=', 'upload_url', ',', 'data', '=', 'fp', ',', '# those two headers needs to be specified according to:', '# https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateuploadurl', '# nopep8', 'headers', '=', '{', ""'content-type'"", ':', ""'application/zip'"", ',', ""'x-goog-content-length-range'"", ':', ""'0,104857600'"", ',', '}', ')', 'return', 'upload_url']","def upload_function_zip ( self , location , zip_path , project_id = none ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . generateuploadurl ( parent = self . _full_location ( project_id , location ) ) . execute ( num_retries = self . num_retries ) upload_url = response . get ( 'uploadurl' ) with open ( zip_path , 'rb' ) as fp : requests . put ( url = upload_url , data = fp , # those two headers needs to be specified according to: # https://cloud.google.com/functions/docs/reference/rest/v1/projects.locations.functions/generateuploadurl # nopep8 headers = { 'content-type' : 'application/zip' , 'x-goog-content-length-range' : '0,104857600' , } ) return upload_url"
154,apache/airflow,airflow/contrib/hooks/gcp_function_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L161-L172,"def delete_function(self, name):
        """"""
        Deletes the specified Cloud Function.

        :param name: The name of the function.
        :type name: str
        :return: None
        """"""
        response = self.get_conn().projects().locations().functions().delete(
            name=name).execute(num_retries=self.num_retries)
        operation_name = response[""name""]
        self._wait_for_operation_to_complete(operation_name=operation_name)","['def', 'delete_function', '(', 'self', ',', 'name', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'projects', '(', ')', '.', 'locations', '(', ')', '.', 'functions', '(', ')', '.', 'delete', '(', 'name', '=', 'name', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'operation_name', '=', 'operation_name', ')']","Deletes the specified Cloud Function.

        :param name: The name of the function.
        :type name: str
        :return: None","['Deletes', 'the', 'specified', 'Cloud', 'Function', '.']",python,test,"['deletes', 'the', 'specified', 'cloud', 'function', '.']",deletes the specified cloud function .,"['def', 'delete_function', '(', 'self', ',', 'name', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'projects', '(', ')', '.', 'locations', '(', ')', '.', 'functions', '(', ')', '.', 'delete', '(', 'name', '=', 'name', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'operation_name', '=', 'response', '[', '""name""', ']', 'self', '.', '_wait_for_operation_to_complete', '(', 'operation_name', '=', 'operation_name', ')']","def delete_function ( self , name ) : response = self . get_conn ( ) . projects ( ) . locations ( ) . functions ( ) . delete ( name = name ) . execute ( num_retries = self . num_retries ) operation_name = response [ ""name"" ] self . _wait_for_operation_to_complete ( operation_name = operation_name )"
155,apache/airflow,airflow/contrib/hooks/gcp_function_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_function_hook.py#L174-L198,"def _wait_for_operation_to_complete(self, operation_name):
        """"""
        Waits for the named operation to complete - checks status of the
        asynchronous call.

        :param operation_name: The name of the operation.
        :type operation_name: str
        :return: The response returned by the operation.
        :rtype: dict
        :exception: AirflowException in case error is returned.
        """"""
        service = self.get_conn()
        while True:
            operation_response = service.operations().get(
                name=operation_name,
            ).execute(num_retries=self.num_retries)
            if operation_response.get(""done""):
                response = operation_response.get(""response"")
                error = operation_response.get(""error"")
                # Note, according to documentation always either response or error is
                # set when ""done"" == True
                if error:
                    raise AirflowException(str(error))
                return response
            time.sleep(TIME_TO_SLEEP_IN_SECONDS)","['def', '_wait_for_operation_to_complete', '(', 'self', ',', 'operation_name', ')', ':', 'service', '=', 'self', '.', 'get_conn', '(', ')', 'while', 'True', ':', 'operation_response', '=', 'service', '.', 'operations', '(', ')', '.', 'get', '(', 'name', '=', 'operation_name', ',', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'if', 'operation_response', '.', 'get', '(', '""done""', ')', ':', 'response', '=', 'operation_response', '.', 'get', '(', '""response""', ')', 'error', '=', 'operation_response', '.', 'get', '(', '""error""', ')', '# Note, according to documentation always either response or error is', '# set when ""done"" == True', 'if', 'error', ':', 'raise', 'AirflowException', '(', 'str', '(', 'error', ')', ')', 'return', 'response', 'time', '.', 'sleep', '(', 'TIME_TO_SLEEP_IN_SECONDS', ')']","Waits for the named operation to complete - checks status of the
        asynchronous call.

        :param operation_name: The name of the operation.
        :type operation_name: str
        :return: The response returned by the operation.
        :rtype: dict
        :exception: AirflowException in case error is returned.","['Waits', 'for', 'the', 'named', 'operation', 'to', 'complete', '-', 'checks', 'status', 'of', 'the', 'asynchronous', 'call', '.']",python,test,"['waits', 'for', 'the', 'named', 'operation', 'to', 'complete', '-', 'checks', 'status', 'of', 'the', 'asynchronous', 'call', '.']",waits for the named operation to complete - checks status of the asynchronous call .,"['def', '_wait_for_operation_to_complete', '(', 'self', ',', 'operation_name', ')', ':', 'service', '=', 'self', '.', 'get_conn', '(', ')', 'while', 'true', ':', 'operation_response', '=', 'service', '.', 'operations', '(', ')', '.', 'get', '(', 'name', '=', 'operation_name', ',', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'if', 'operation_response', '.', 'get', '(', '""done""', ')', ':', 'response', '=', 'operation_response', '.', 'get', '(', '""response""', ')', 'error', '=', 'operation_response', '.', 'get', '(', '""error""', ')', '# note, according to documentation always either response or error is', '# set when ""done"" == true', 'if', 'error', ':', 'raise', 'airflowexception', '(', 'str', '(', 'error', ')', ')', 'return', 'response', 'time', '.', 'sleep', '(', 'time_to_sleep_in_seconds', ')']","def _wait_for_operation_to_complete ( self , operation_name ) : service = self . get_conn ( ) while true : operation_response = service . operations ( ) . get ( name = operation_name , ) . execute ( num_retries = self . num_retries ) if operation_response . get ( ""done"" ) : response = operation_response . get ( ""response"" ) error = operation_response . get ( ""error"" ) # note, according to documentation always either response or error is # set when ""done"" == true if error : raise airflowexception ( str ( error ) ) return response time . sleep ( time_to_sleep_in_seconds )"
156,apache/airflow,airflow/contrib/hooks/gcp_pubsub_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_pubsub_hook.py#L60-L81,"def publish(self, project, topic, messages):
        """"""Publishes messages to a Pub/Sub topic.

        :param project: the GCP project ID in which to publish
        :type project: str
        :param topic: the Pub/Sub topic to which to publish; do not
            include the ``projects/{project}/topics/`` prefix.
        :type topic: str
        :param messages: messages to publish; if the data field in a
            message is set, it should already be base64 encoded.
        :type messages: list of PubSub messages; see
            http://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage
        """"""
        body = {'messages': messages}
        full_topic = _format_topic(project, topic)
        request = self.get_conn().projects().topics().publish(
            topic=full_topic, body=body)
        try:
            request.execute(num_retries=self.num_retries)
        except HttpError as e:
            raise PubSubException(
                'Error publishing to topic {}'.format(full_topic), e)","['def', 'publish', '(', 'self', ',', 'project', ',', 'topic', ',', 'messages', ')', ':', 'body', '=', '{', ""'messages'"", ':', 'messages', '}', 'full_topic', '=', '_format_topic', '(', 'project', ',', 'topic', ')', 'request', '=', 'self', '.', 'get_conn', '(', ')', '.', 'projects', '(', ')', '.', 'topics', '(', ')', '.', 'publish', '(', 'topic', '=', 'full_topic', ',', 'body', '=', 'body', ')', 'try', ':', 'request', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'except', 'HttpError', 'as', 'e', ':', 'raise', 'PubSubException', '(', ""'Error publishing to topic {}'"", '.', 'format', '(', 'full_topic', ')', ',', 'e', ')']","Publishes messages to a Pub/Sub topic.

        :param project: the GCP project ID in which to publish
        :type project: str
        :param topic: the Pub/Sub topic to which to publish; do not
            include the ``projects/{project}/topics/`` prefix.
        :type topic: str
        :param messages: messages to publish; if the data field in a
            message is set, it should already be base64 encoded.
        :type messages: list of PubSub messages; see
            http://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage","['Publishes', 'messages', 'to', 'a', 'Pub', '/', 'Sub', 'topic', '.']",python,test,"['publishes', 'messages', 'to', 'a', 'pub', '/', 'sub', 'topic', '.']",publishes messages to a pub / sub topic .,"['def', 'publish', '(', 'self', ',', 'project', ',', 'topic', ',', 'messages', ')', ':', 'body', '=', '{', ""'messages'"", ':', 'messages', '}', 'full_topic', '=', '_format_topic', '(', 'project', ',', 'topic', ')', 'request', '=', 'self', '.', 'get_conn', '(', ')', '.', 'projects', '(', ')', '.', 'topics', '(', ')', '.', 'publish', '(', 'topic', '=', 'full_topic', ',', 'body', '=', 'body', ')', 'try', ':', 'request', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'except', 'httperror', 'as', 'e', ':', 'raise', 'pubsubexception', '(', ""'error publishing to topic {}'"", '.', 'format', '(', 'full_topic', ')', ',', 'e', ')']","def publish ( self , project , topic , messages ) : body = { 'messages' : messages } full_topic = _format_topic ( project , topic ) request = self . get_conn ( ) . projects ( ) . topics ( ) . publish ( topic = full_topic , body = body ) try : request . execute ( num_retries = self . num_retries ) except httperror as e : raise pubsubexception ( 'error publishing to topic {}' . format ( full_topic ) , e )"
157,apache/airflow,airflow/contrib/hooks/gcp_pubsub_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_pubsub_hook.py#L83-L110,"def create_topic(self, project, topic, fail_if_exists=False):
        """"""Creates a Pub/Sub topic, if it does not already exist.

        :param project: the GCP project ID in which to create
            the topic
        :type project: str
        :param topic: the Pub/Sub topic name to create; do not
            include the ``projects/{project}/topics/`` prefix.
        :type topic: str
        :param fail_if_exists: if set, raise an exception if the topic
            already exists
        :type fail_if_exists: bool
        """"""
        service = self.get_conn()
        full_topic = _format_topic(project, topic)
        try:
            service.projects().topics().create(
                name=full_topic, body={}).execute(num_retries=self.num_retries)
        except HttpError as e:
            # Status code 409 indicates that the topic already exists.
            if str(e.resp['status']) == '409':
                message = 'Topic already exists: {}'.format(full_topic)
                self.log.warning(message)
                if fail_if_exists:
                    raise PubSubException(message)
            else:
                raise PubSubException(
                    'Error creating topic {}'.format(full_topic), e)","['def', 'create_topic', '(', 'self', ',', 'project', ',', 'topic', ',', 'fail_if_exists', '=', 'False', ')', ':', 'service', '=', 'self', '.', 'get_conn', '(', ')', 'full_topic', '=', '_format_topic', '(', 'project', ',', 'topic', ')', 'try', ':', 'service', '.', 'projects', '(', ')', '.', 'topics', '(', ')', '.', 'create', '(', 'name', '=', 'full_topic', ',', 'body', '=', '{', '}', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'except', 'HttpError', 'as', 'e', ':', '# Status code 409 indicates that the topic already exists.', 'if', 'str', '(', 'e', '.', 'resp', '[', ""'status'"", ']', ')', '==', ""'409'"", ':', 'message', '=', ""'Topic already exists: {}'"", '.', 'format', '(', 'full_topic', ')', 'self', '.', 'log', '.', 'warning', '(', 'message', ')', 'if', 'fail_if_exists', ':', 'raise', 'PubSubException', '(', 'message', ')', 'else', ':', 'raise', 'PubSubException', '(', ""'Error creating topic {}'"", '.', 'format', '(', 'full_topic', ')', ',', 'e', ')']","Creates a Pub/Sub topic, if it does not already exist.

        :param project: the GCP project ID in which to create
            the topic
        :type project: str
        :param topic: the Pub/Sub topic name to create; do not
            include the ``projects/{project}/topics/`` prefix.
        :type topic: str
        :param fail_if_exists: if set, raise an exception if the topic
            already exists
        :type fail_if_exists: bool","['Creates', 'a', 'Pub', '/', 'Sub', 'topic', 'if', 'it', 'does', 'not', 'already', 'exist', '.']",python,test,"['creates', 'a', 'pub', '/', 'sub', 'topic', 'if', 'it', 'does', 'not', 'already', 'exist', '.']",creates a pub / sub topic if it does not already exist .,"['def', 'create_topic', '(', 'self', ',', 'project', ',', 'topic', ',', 'fail_if_exists', '=', 'false', ')', ':', 'service', '=', 'self', '.', 'get_conn', '(', ')', 'full_topic', '=', '_format_topic', '(', 'project', ',', 'topic', ')', 'try', ':', 'service', '.', 'projects', '(', ')', '.', 'topics', '(', ')', '.', 'create', '(', 'name', '=', 'full_topic', ',', 'body', '=', '{', '}', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'except', 'httperror', 'as', 'e', ':', '# status code 409 indicates that the topic already exists.', 'if', 'str', '(', 'e', '.', 'resp', '[', ""'status'"", ']', ')', '==', ""'409'"", ':', 'message', '=', ""'topic already exists: {}'"", '.', 'format', '(', 'full_topic', ')', 'self', '.', 'log', '.', 'warning', '(', 'message', ')', 'if', 'fail_if_exists', ':', 'raise', 'pubsubexception', '(', 'message', ')', 'else', ':', 'raise', 'pubsubexception', '(', ""'error creating topic {}'"", '.', 'format', '(', 'full_topic', ')', ',', 'e', ')']","def create_topic ( self , project , topic , fail_if_exists = false ) : service = self . get_conn ( ) full_topic = _format_topic ( project , topic ) try : service . projects ( ) . topics ( ) . create ( name = full_topic , body = { } ) . execute ( num_retries = self . num_retries ) except httperror as e : # status code 409 indicates that the topic already exists. if str ( e . resp [ 'status' ] ) == '409' : message = 'topic already exists: {}' . format ( full_topic ) self . log . warning ( message ) if fail_if_exists : raise pubsubexception ( message ) else : raise pubsubexception ( 'error creating topic {}' . format ( full_topic ) , e )"
158,apache/airflow,airflow/contrib/hooks/gcp_pubsub_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_pubsub_hook.py#L112-L137,"def delete_topic(self, project, topic, fail_if_not_exists=False):
        """"""Deletes a Pub/Sub topic if it exists.

        :param project: the GCP project ID in which to delete the topic
        :type project: str
        :param topic: the Pub/Sub topic name to delete; do not
            include the ``projects/{project}/topics/`` prefix.
        :type topic: str
        :param fail_if_not_exists: if set, raise an exception if the topic
            does not exist
        :type fail_if_not_exists: bool
        """"""
        service = self.get_conn()
        full_topic = _format_topic(project, topic)
        try:
            service.projects().topics().delete(topic=full_topic).execute(num_retries=self.num_retries)
        except HttpError as e:
            # Status code 409 indicates that the topic was not found
            if str(e.resp['status']) == '404':
                message = 'Topic does not exist: {}'.format(full_topic)
                self.log.warning(message)
                if fail_if_not_exists:
                    raise PubSubException(message)
            else:
                raise PubSubException(
                    'Error deleting topic {}'.format(full_topic), e)","['def', 'delete_topic', '(', 'self', ',', 'project', ',', 'topic', ',', 'fail_if_not_exists', '=', 'False', ')', ':', 'service', '=', 'self', '.', 'get_conn', '(', ')', 'full_topic', '=', '_format_topic', '(', 'project', ',', 'topic', ')', 'try', ':', 'service', '.', 'projects', '(', ')', '.', 'topics', '(', ')', '.', 'delete', '(', 'topic', '=', 'full_topic', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'except', 'HttpError', 'as', 'e', ':', '# Status code 409 indicates that the topic was not found', 'if', 'str', '(', 'e', '.', 'resp', '[', ""'status'"", ']', ')', '==', ""'404'"", ':', 'message', '=', ""'Topic does not exist: {}'"", '.', 'format', '(', 'full_topic', ')', 'self', '.', 'log', '.', 'warning', '(', 'message', ')', 'if', 'fail_if_not_exists', ':', 'raise', 'PubSubException', '(', 'message', ')', 'else', ':', 'raise', 'PubSubException', '(', ""'Error deleting topic {}'"", '.', 'format', '(', 'full_topic', ')', ',', 'e', ')']","Deletes a Pub/Sub topic if it exists.

        :param project: the GCP project ID in which to delete the topic
        :type project: str
        :param topic: the Pub/Sub topic name to delete; do not
            include the ``projects/{project}/topics/`` prefix.
        :type topic: str
        :param fail_if_not_exists: if set, raise an exception if the topic
            does not exist
        :type fail_if_not_exists: bool","['Deletes', 'a', 'Pub', '/', 'Sub', 'topic', 'if', 'it', 'exists', '.']",python,test,"['deletes', 'a', 'pub', '/', 'sub', 'topic', 'if', 'it', 'exists', '.']",deletes a pub / sub topic if it exists .,"['def', 'delete_topic', '(', 'self', ',', 'project', ',', 'topic', ',', 'fail_if_not_exists', '=', 'false', ')', ':', 'service', '=', 'self', '.', 'get_conn', '(', ')', 'full_topic', '=', '_format_topic', '(', 'project', ',', 'topic', ')', 'try', ':', 'service', '.', 'projects', '(', ')', '.', 'topics', '(', ')', '.', 'delete', '(', 'topic', '=', 'full_topic', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'except', 'httperror', 'as', 'e', ':', '# status code 409 indicates that the topic was not found', 'if', 'str', '(', 'e', '.', 'resp', '[', ""'status'"", ']', ')', '==', ""'404'"", ':', 'message', '=', ""'topic does not exist: {}'"", '.', 'format', '(', 'full_topic', ')', 'self', '.', 'log', '.', 'warning', '(', 'message', ')', 'if', 'fail_if_not_exists', ':', 'raise', 'pubsubexception', '(', 'message', ')', 'else', ':', 'raise', 'pubsubexception', '(', ""'error deleting topic {}'"", '.', 'format', '(', 'full_topic', ')', ',', 'e', ')']","def delete_topic ( self , project , topic , fail_if_not_exists = false ) : service = self . get_conn ( ) full_topic = _format_topic ( project , topic ) try : service . projects ( ) . topics ( ) . delete ( topic = full_topic ) . execute ( num_retries = self . num_retries ) except httperror as e : # status code 409 indicates that the topic was not found if str ( e . resp [ 'status' ] ) == '404' : message = 'topic does not exist: {}' . format ( full_topic ) self . log . warning ( message ) if fail_if_not_exists : raise pubsubexception ( message ) else : raise pubsubexception ( 'error deleting topic {}' . format ( full_topic ) , e )"
159,apache/airflow,airflow/contrib/hooks/gcp_pubsub_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_pubsub_hook.py#L139-L194,"def create_subscription(self, topic_project, topic, subscription=None,
                            subscription_project=None, ack_deadline_secs=10,
                            fail_if_exists=False):
        """"""Creates a Pub/Sub subscription, if it does not already exist.

        :param topic_project: the GCP project ID of the topic that the
            subscription will be bound to.
        :type topic_project: str
        :param topic: the Pub/Sub topic name that the subscription will be bound
            to create; do not include the ``projects/{project}/subscriptions/``
            prefix.
        :type topic: str
        :param subscription: the Pub/Sub subscription name. If empty, a random
            name will be generated using the uuid module
        :type subscription: str
        :param subscription_project: the GCP project ID where the subscription
            will be created. If unspecified, ``topic_project`` will be used.
        :type subscription_project: str
        :param ack_deadline_secs: Number of seconds that a subscriber has to
            acknowledge each message pulled from the subscription
        :type ack_deadline_secs: int
        :param fail_if_exists: if set, raise an exception if the topic
            already exists
        :type fail_if_exists: bool
        :return: subscription name which will be the system-generated value if
            the ``subscription`` parameter is not supplied
        :rtype: str
        """"""
        service = self.get_conn()
        full_topic = _format_topic(topic_project, topic)
        if not subscription:
            subscription = 'sub-{}'.format(uuid4())
        if not subscription_project:
            subscription_project = topic_project
        full_subscription = _format_subscription(subscription_project,
                                                 subscription)
        body = {
            'topic': full_topic,
            'ackDeadlineSeconds': ack_deadline_secs
        }
        try:
            service.projects().subscriptions().create(
                name=full_subscription, body=body).execute(num_retries=self.num_retries)
        except HttpError as e:
            # Status code 409 indicates that the subscription already exists.
            if str(e.resp['status']) == '409':
                message = 'Subscription already exists: {}'.format(
                    full_subscription)
                self.log.warning(message)
                if fail_if_exists:
                    raise PubSubException(message)
            else:
                raise PubSubException(
                    'Error creating subscription {}'.format(full_subscription),
                    e)
        return subscription","['def', 'create_subscription', '(', 'self', ',', 'topic_project', ',', 'topic', ',', 'subscription', '=', 'None', ',', 'subscription_project', '=', 'None', ',', 'ack_deadline_secs', '=', '10', ',', 'fail_if_exists', '=', 'False', ')', ':', 'service', '=', 'self', '.', 'get_conn', '(', ')', 'full_topic', '=', '_format_topic', '(', 'topic_project', ',', 'topic', ')', 'if', 'not', 'subscription', ':', 'subscription', '=', ""'sub-{}'"", '.', 'format', '(', 'uuid4', '(', ')', ')', 'if', 'not', 'subscription_project', ':', 'subscription_project', '=', 'topic_project', 'full_subscription', '=', '_format_subscription', '(', 'subscription_project', ',', 'subscription', ')', 'body', '=', '{', ""'topic'"", ':', 'full_topic', ',', ""'ackDeadlineSeconds'"", ':', 'ack_deadline_secs', '}', 'try', ':', 'service', '.', 'projects', '(', ')', '.', 'subscriptions', '(', ')', '.', 'create', '(', 'name', '=', 'full_subscription', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'except', 'HttpError', 'as', 'e', ':', '# Status code 409 indicates that the subscription already exists.', 'if', 'str', '(', 'e', '.', 'resp', '[', ""'status'"", ']', ')', '==', ""'409'"", ':', 'message', '=', ""'Subscription already exists: {}'"", '.', 'format', '(', 'full_subscription', ')', 'self', '.', 'log', '.', 'warning', '(', 'message', ')', 'if', 'fail_if_exists', ':', 'raise', 'PubSubException', '(', 'message', ')', 'else', ':', 'raise', 'PubSubException', '(', ""'Error creating subscription {}'"", '.', 'format', '(', 'full_subscription', ')', ',', 'e', ')', 'return', 'subscription']","Creates a Pub/Sub subscription, if it does not already exist.

        :param topic_project: the GCP project ID of the topic that the
            subscription will be bound to.
        :type topic_project: str
        :param topic: the Pub/Sub topic name that the subscription will be bound
            to create; do not include the ``projects/{project}/subscriptions/``
            prefix.
        :type topic: str
        :param subscription: the Pub/Sub subscription name. If empty, a random
            name will be generated using the uuid module
        :type subscription: str
        :param subscription_project: the GCP project ID where the subscription
            will be created. If unspecified, ``topic_project`` will be used.
        :type subscription_project: str
        :param ack_deadline_secs: Number of seconds that a subscriber has to
            acknowledge each message pulled from the subscription
        :type ack_deadline_secs: int
        :param fail_if_exists: if set, raise an exception if the topic
            already exists
        :type fail_if_exists: bool
        :return: subscription name which will be the system-generated value if
            the ``subscription`` parameter is not supplied
        :rtype: str","['Creates', 'a', 'Pub', '/', 'Sub', 'subscription', 'if', 'it', 'does', 'not', 'already', 'exist', '.']",python,test,"['creates', 'a', 'pub', '/', 'sub', 'subscription', 'if', 'it', 'does', 'not', 'already', 'exist', '.']",creates a pub / sub subscription if it does not already exist .,"['def', 'create_subscription', '(', 'self', ',', 'topic_project', ',', 'topic', ',', 'subscription', '=', 'none', ',', 'subscription_project', '=', 'none', ',', 'ack_deadline_secs', '=', '10', ',', 'fail_if_exists', '=', 'false', ')', ':', 'service', '=', 'self', '.', 'get_conn', '(', ')', 'full_topic', '=', '_format_topic', '(', 'topic_project', ',', 'topic', ')', 'if', 'not', 'subscription', ':', 'subscription', '=', ""'sub-{}'"", '.', 'format', '(', 'uuid4', '(', ')', ')', 'if', 'not', 'subscription_project', ':', 'subscription_project', '=', 'topic_project', 'full_subscription', '=', '_format_subscription', '(', 'subscription_project', ',', 'subscription', ')', 'body', '=', '{', ""'topic'"", ':', 'full_topic', ',', ""'ackdeadlineseconds'"", ':', 'ack_deadline_secs', '}', 'try', ':', 'service', '.', 'projects', '(', ')', '.', 'subscriptions', '(', ')', '.', 'create', '(', 'name', '=', 'full_subscription', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'except', 'httperror', 'as', 'e', ':', '# status code 409 indicates that the subscription already exists.', 'if', 'str', '(', 'e', '.', 'resp', '[', ""'status'"", ']', ')', '==', ""'409'"", ':', 'message', '=', ""'subscription already exists: {}'"", '.', 'format', '(', 'full_subscription', ')', 'self', '.', 'log', '.', 'warning', '(', 'message', ')', 'if', 'fail_if_exists', ':', 'raise', 'pubsubexception', '(', 'message', ')', 'else', ':', 'raise', 'pubsubexception', '(', ""'error creating subscription {}'"", '.', 'format', '(', 'full_subscription', ')', ',', 'e', ')', 'return', 'subscription']","def create_subscription ( self , topic_project , topic , subscription = none , subscription_project = none , ack_deadline_secs = 10 , fail_if_exists = false ) : service = self . get_conn ( ) full_topic = _format_topic ( topic_project , topic ) if not subscription : subscription = 'sub-{}' . format ( uuid4 ( ) ) if not subscription_project : subscription_project = topic_project full_subscription = _format_subscription ( subscription_project , subscription ) body = { 'topic' : full_topic , 'ackdeadlineseconds' : ack_deadline_secs } try : service . projects ( ) . subscriptions ( ) . create ( name = full_subscription , body = body ) . execute ( num_retries = self . num_retries ) except httperror as e : # status code 409 indicates that the subscription already exists. if str ( e . resp [ 'status' ] ) == '409' : message = 'subscription already exists: {}' . format ( full_subscription ) self . log . warning ( message ) if fail_if_exists : raise pubsubexception ( message ) else : raise pubsubexception ( 'error creating subscription {}' . format ( full_subscription ) , e ) return subscription"
160,apache/airflow,airflow/contrib/hooks/gcp_pubsub_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_pubsub_hook.py#L196-L225,"def delete_subscription(self, project, subscription,
                            fail_if_not_exists=False):
        """"""Deletes a Pub/Sub subscription, if it exists.

        :param project: the GCP project ID where the subscription exists
        :type project: str
        :param subscription: the Pub/Sub subscription name to delete; do not
            include the ``projects/{project}/subscriptions/`` prefix.
        :type subscription: str
        :param fail_if_not_exists: if set, raise an exception if the topic
            does not exist
        :type fail_if_not_exists: bool
        """"""
        service = self.get_conn()
        full_subscription = _format_subscription(project, subscription)
        try:
            service.projects().subscriptions().delete(
                subscription=full_subscription).execute(num_retries=self.num_retries)
        except HttpError as e:
            # Status code 404 indicates that the subscription was not found
            if str(e.resp['status']) == '404':
                message = 'Subscription does not exist: {}'.format(
                    full_subscription)
                self.log.warning(message)
                if fail_if_not_exists:
                    raise PubSubException(message)
            else:
                raise PubSubException(
                    'Error deleting subscription {}'.format(full_subscription),
                    e)","['def', 'delete_subscription', '(', 'self', ',', 'project', ',', 'subscription', ',', 'fail_if_not_exists', '=', 'False', ')', ':', 'service', '=', 'self', '.', 'get_conn', '(', ')', 'full_subscription', '=', '_format_subscription', '(', 'project', ',', 'subscription', ')', 'try', ':', 'service', '.', 'projects', '(', ')', '.', 'subscriptions', '(', ')', '.', 'delete', '(', 'subscription', '=', 'full_subscription', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'except', 'HttpError', 'as', 'e', ':', '# Status code 404 indicates that the subscription was not found', 'if', 'str', '(', 'e', '.', 'resp', '[', ""'status'"", ']', ')', '==', ""'404'"", ':', 'message', '=', ""'Subscription does not exist: {}'"", '.', 'format', '(', 'full_subscription', ')', 'self', '.', 'log', '.', 'warning', '(', 'message', ')', 'if', 'fail_if_not_exists', ':', 'raise', 'PubSubException', '(', 'message', ')', 'else', ':', 'raise', 'PubSubException', '(', ""'Error deleting subscription {}'"", '.', 'format', '(', 'full_subscription', ')', ',', 'e', ')']","Deletes a Pub/Sub subscription, if it exists.

        :param project: the GCP project ID where the subscription exists
        :type project: str
        :param subscription: the Pub/Sub subscription name to delete; do not
            include the ``projects/{project}/subscriptions/`` prefix.
        :type subscription: str
        :param fail_if_not_exists: if set, raise an exception if the topic
            does not exist
        :type fail_if_not_exists: bool","['Deletes', 'a', 'Pub', '/', 'Sub', 'subscription', 'if', 'it', 'exists', '.']",python,test,"['deletes', 'a', 'pub', '/', 'sub', 'subscription', 'if', 'it', 'exists', '.']",deletes a pub / sub subscription if it exists .,"['def', 'delete_subscription', '(', 'self', ',', 'project', ',', 'subscription', ',', 'fail_if_not_exists', '=', 'false', ')', ':', 'service', '=', 'self', '.', 'get_conn', '(', ')', 'full_subscription', '=', '_format_subscription', '(', 'project', ',', 'subscription', ')', 'try', ':', 'service', '.', 'projects', '(', ')', '.', 'subscriptions', '(', ')', '.', 'delete', '(', 'subscription', '=', 'full_subscription', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'except', 'httperror', 'as', 'e', ':', '# status code 404 indicates that the subscription was not found', 'if', 'str', '(', 'e', '.', 'resp', '[', ""'status'"", ']', ')', '==', ""'404'"", ':', 'message', '=', ""'subscription does not exist: {}'"", '.', 'format', '(', 'full_subscription', ')', 'self', '.', 'log', '.', 'warning', '(', 'message', ')', 'if', 'fail_if_not_exists', ':', 'raise', 'pubsubexception', '(', 'message', ')', 'else', ':', 'raise', 'pubsubexception', '(', ""'error deleting subscription {}'"", '.', 'format', '(', 'full_subscription', ')', ',', 'e', ')']","def delete_subscription ( self , project , subscription , fail_if_not_exists = false ) : service = self . get_conn ( ) full_subscription = _format_subscription ( project , subscription ) try : service . projects ( ) . subscriptions ( ) . delete ( subscription = full_subscription ) . execute ( num_retries = self . num_retries ) except httperror as e : # status code 404 indicates that the subscription was not found if str ( e . resp [ 'status' ] ) == '404' : message = 'subscription does not exist: {}' . format ( full_subscription ) self . log . warning ( message ) if fail_if_not_exists : raise pubsubexception ( message ) else : raise pubsubexception ( 'error deleting subscription {}' . format ( full_subscription ) , e )"
161,apache/airflow,airflow/contrib/hooks/gcp_pubsub_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_pubsub_hook.py#L227-L261,"def pull(self, project, subscription, max_messages,
             return_immediately=False):
        """"""Pulls up to ``max_messages`` messages from Pub/Sub subscription.

        :param project: the GCP project ID where the subscription exists
        :type project: str
        :param subscription: the Pub/Sub subscription name to pull from; do not
            include the 'projects/{project}/topics/' prefix.
        :type subscription: str
        :param max_messages: The maximum number of messages to return from
            the Pub/Sub API.
        :type max_messages: int
        :param return_immediately: If set, the Pub/Sub API will immediately
            return if no messages are available. Otherwise, the request will
            block for an undisclosed, but bounded period of time
        :type return_immediately: bool
        :return: A list of Pub/Sub ReceivedMessage objects each containing
            an ``ackId`` property and a ``message`` property, which includes
            the base64-encoded message content. See
            https://cloud.google.com/pubsub/docs/reference/rest/v1/projects.subscriptions/pull#ReceivedMessage
        """"""
        service = self.get_conn()
        full_subscription = _format_subscription(project, subscription)
        body = {
            'maxMessages': max_messages,
            'returnImmediately': return_immediately
        }
        try:
            response = service.projects().subscriptions().pull(
                subscription=full_subscription, body=body).execute(num_retries=self.num_retries)
            return response.get('receivedMessages', [])
        except HttpError as e:
            raise PubSubException(
                'Error pulling messages from subscription {}'.format(
                    full_subscription), e)","['def', 'pull', '(', 'self', ',', 'project', ',', 'subscription', ',', 'max_messages', ',', 'return_immediately', '=', 'False', ')', ':', 'service', '=', 'self', '.', 'get_conn', '(', ')', 'full_subscription', '=', '_format_subscription', '(', 'project', ',', 'subscription', ')', 'body', '=', '{', ""'maxMessages'"", ':', 'max_messages', ',', ""'returnImmediately'"", ':', 'return_immediately', '}', 'try', ':', 'response', '=', 'service', '.', 'projects', '(', ')', '.', 'subscriptions', '(', ')', '.', 'pull', '(', 'subscription', '=', 'full_subscription', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'return', 'response', '.', 'get', '(', ""'receivedMessages'"", ',', '[', ']', ')', 'except', 'HttpError', 'as', 'e', ':', 'raise', 'PubSubException', '(', ""'Error pulling messages from subscription {}'"", '.', 'format', '(', 'full_subscription', ')', ',', 'e', ')']","Pulls up to ``max_messages`` messages from Pub/Sub subscription.

        :param project: the GCP project ID where the subscription exists
        :type project: str
        :param subscription: the Pub/Sub subscription name to pull from; do not
            include the 'projects/{project}/topics/' prefix.
        :type subscription: str
        :param max_messages: The maximum number of messages to return from
            the Pub/Sub API.
        :type max_messages: int
        :param return_immediately: If set, the Pub/Sub API will immediately
            return if no messages are available. Otherwise, the request will
            block for an undisclosed, but bounded period of time
        :type return_immediately: bool
        :return: A list of Pub/Sub ReceivedMessage objects each containing
            an ``ackId`` property and a ``message`` property, which includes
            the base64-encoded message content. See
            https://cloud.google.com/pubsub/docs/reference/rest/v1/projects.subscriptions/pull#ReceivedMessage","['Pulls', 'up', 'to', 'max_messages', 'messages', 'from', 'Pub', '/', 'Sub', 'subscription', '.']",python,test,"['pulls', 'up', 'to', 'max_messages', 'messages', 'from', 'pub', '/', 'sub', 'subscription', '.']",pulls up to max_messages messages from pub / sub subscription .,"['def', 'pull', '(', 'self', ',', 'project', ',', 'subscription', ',', 'max_messages', ',', 'return_immediately', '=', 'false', ')', ':', 'service', '=', 'self', '.', 'get_conn', '(', ')', 'full_subscription', '=', '_format_subscription', '(', 'project', ',', 'subscription', ')', 'body', '=', '{', ""'maxmessages'"", ':', 'max_messages', ',', ""'returnimmediately'"", ':', 'return_immediately', '}', 'try', ':', 'response', '=', 'service', '.', 'projects', '(', ')', '.', 'subscriptions', '(', ')', '.', 'pull', '(', 'subscription', '=', 'full_subscription', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'return', 'response', '.', 'get', '(', ""'receivedmessages'"", ',', '[', ']', ')', 'except', 'httperror', 'as', 'e', ':', 'raise', 'pubsubexception', '(', ""'error pulling messages from subscription {}'"", '.', 'format', '(', 'full_subscription', ')', ',', 'e', ')']","def pull ( self , project , subscription , max_messages , return_immediately = false ) : service = self . get_conn ( ) full_subscription = _format_subscription ( project , subscription ) body = { 'maxmessages' : max_messages , 'returnimmediately' : return_immediately } try : response = service . projects ( ) . subscriptions ( ) . pull ( subscription = full_subscription , body = body ) . execute ( num_retries = self . num_retries ) return response . get ( 'receivedmessages' , [ ] ) except httperror as e : raise pubsubexception ( 'error pulling messages from subscription {}' . format ( full_subscription ) , e )"
162,apache/airflow,airflow/contrib/hooks/gcp_pubsub_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_pubsub_hook.py#L263-L285,"def acknowledge(self, project, subscription, ack_ids):
        """"""Pulls up to ``max_messages`` messages from Pub/Sub subscription.

        :param project: the GCP project name or ID in which to create
            the topic
        :type project: str
        :param subscription: the Pub/Sub subscription name to delete; do not
            include the 'projects/{project}/topics/' prefix.
        :type subscription: str
        :param ack_ids: List of ReceivedMessage ackIds from a previous pull
            response
        :type ack_ids: list
        """"""
        service = self.get_conn()
        full_subscription = _format_subscription(project, subscription)
        try:
            service.projects().subscriptions().acknowledge(
                subscription=full_subscription, body={'ackIds': ack_ids}
            ).execute(num_retries=self.num_retries)
        except HttpError as e:
            raise PubSubException(
                'Error acknowledging {} messages pulled from subscription {}'
                .format(len(ack_ids), full_subscription), e)","['def', 'acknowledge', '(', 'self', ',', 'project', ',', 'subscription', ',', 'ack_ids', ')', ':', 'service', '=', 'self', '.', 'get_conn', '(', ')', 'full_subscription', '=', '_format_subscription', '(', 'project', ',', 'subscription', ')', 'try', ':', 'service', '.', 'projects', '(', ')', '.', 'subscriptions', '(', ')', '.', 'acknowledge', '(', 'subscription', '=', 'full_subscription', ',', 'body', '=', '{', ""'ackIds'"", ':', 'ack_ids', '}', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'except', 'HttpError', 'as', 'e', ':', 'raise', 'PubSubException', '(', ""'Error acknowledging {} messages pulled from subscription {}'"", '.', 'format', '(', 'len', '(', 'ack_ids', ')', ',', 'full_subscription', ')', ',', 'e', ')']","Pulls up to ``max_messages`` messages from Pub/Sub subscription.

        :param project: the GCP project name or ID in which to create
            the topic
        :type project: str
        :param subscription: the Pub/Sub subscription name to delete; do not
            include the 'projects/{project}/topics/' prefix.
        :type subscription: str
        :param ack_ids: List of ReceivedMessage ackIds from a previous pull
            response
        :type ack_ids: list","['Pulls', 'up', 'to', 'max_messages', 'messages', 'from', 'Pub', '/', 'Sub', 'subscription', '.']",python,test,"['pulls', 'up', 'to', 'max_messages', 'messages', 'from', 'pub', '/', 'sub', 'subscription', '.']",pulls up to max_messages messages from pub / sub subscription .,"['def', 'acknowledge', '(', 'self', ',', 'project', ',', 'subscription', ',', 'ack_ids', ')', ':', 'service', '=', 'self', '.', 'get_conn', '(', ')', 'full_subscription', '=', '_format_subscription', '(', 'project', ',', 'subscription', ')', 'try', ':', 'service', '.', 'projects', '(', ')', '.', 'subscriptions', '(', ')', '.', 'acknowledge', '(', 'subscription', '=', 'full_subscription', ',', 'body', '=', '{', ""'ackids'"", ':', 'ack_ids', '}', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'except', 'httperror', 'as', 'e', ':', 'raise', 'pubsubexception', '(', ""'error acknowledging {} messages pulled from subscription {}'"", '.', 'format', '(', 'len', '(', 'ack_ids', ')', ',', 'full_subscription', ')', ',', 'e', ')']","def acknowledge ( self , project , subscription , ack_ids ) : service = self . get_conn ( ) full_subscription = _format_subscription ( project , subscription ) try : service . projects ( ) . subscriptions ( ) . acknowledge ( subscription = full_subscription , body = { 'ackids' : ack_ids } ) . execute ( num_retries = self . num_retries ) except httperror as e : raise pubsubexception ( 'error acknowledging {} messages pulled from subscription {}' . format ( len ( ack_ids ) , full_subscription ) , e )"
163,apache/airflow,airflow/ti_deps/deps/base_ti_dep.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/ti_deps/deps/base_ti_dep.py#L78-L107,"def get_dep_statuses(self, ti, session, dep_context=None):
        """"""
        Wrapper around the private _get_dep_statuses method that contains some global
        checks for all dependencies.

        :param ti: the task instance to get the dependency status for
        :type ti: airflow.models.TaskInstance
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        :param dep_context: the context for which this dependency should be evaluated for
        :type dep_context: DepContext
        """"""
        # this avoids a circular dependency
        from airflow.ti_deps.dep_context import DepContext

        if dep_context is None:
            dep_context = DepContext()

        if self.IGNOREABLE and dep_context.ignore_all_deps:
            yield self._passing_status(
                reason=""Context specified all dependencies should be ignored."")
            return

        if self.IS_TASK_DEP and dep_context.ignore_task_deps:
            yield self._passing_status(
                reason=""Context specified all task dependencies should be ignored."")
            return

        for dep_status in self._get_dep_statuses(ti, session, dep_context):
            yield dep_status","['def', 'get_dep_statuses', '(', 'self', ',', 'ti', ',', 'session', ',', 'dep_context', '=', 'None', ')', ':', '# this avoids a circular dependency', 'from', 'airflow', '.', 'ti_deps', '.', 'dep_context', 'import', 'DepContext', 'if', 'dep_context', 'is', 'None', ':', 'dep_context', '=', 'DepContext', '(', ')', 'if', 'self', '.', 'IGNOREABLE', 'and', 'dep_context', '.', 'ignore_all_deps', ':', 'yield', 'self', '.', '_passing_status', '(', 'reason', '=', '""Context specified all dependencies should be ignored.""', ')', 'return', 'if', 'self', '.', 'IS_TASK_DEP', 'and', 'dep_context', '.', 'ignore_task_deps', ':', 'yield', 'self', '.', '_passing_status', '(', 'reason', '=', '""Context specified all task dependencies should be ignored.""', ')', 'return', 'for', 'dep_status', 'in', 'self', '.', '_get_dep_statuses', '(', 'ti', ',', 'session', ',', 'dep_context', ')', ':', 'yield', 'dep_status']","Wrapper around the private _get_dep_statuses method that contains some global
        checks for all dependencies.

        :param ti: the task instance to get the dependency status for
        :type ti: airflow.models.TaskInstance
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        :param dep_context: the context for which this dependency should be evaluated for
        :type dep_context: DepContext","['Wrapper', 'around', 'the', 'private', '_get_dep_statuses', 'method', 'that', 'contains', 'some', 'global', 'checks', 'for', 'all', 'dependencies', '.']",python,test,"['wrapper', 'around', 'the', 'private', '_get_dep_statuses', 'method', 'that', 'contains', 'some', 'global', 'checks', 'for', 'all', 'dependencies', '.']",wrapper around the private _get_dep_statuses method that contains some global checks for all dependencies .,"['def', 'get_dep_statuses', '(', 'self', ',', 'ti', ',', 'session', ',', 'dep_context', '=', 'none', ')', ':', '# this avoids a circular dependency', 'from', 'airflow', '.', 'ti_deps', '.', 'dep_context', 'import', 'depcontext', 'if', 'dep_context', 'is', 'none', ':', 'dep_context', '=', 'depcontext', '(', ')', 'if', 'self', '.', 'ignoreable', 'and', 'dep_context', '.', 'ignore_all_deps', ':', 'yield', 'self', '.', '_passing_status', '(', 'reason', '=', '""context specified all dependencies should be ignored.""', ')', 'return', 'if', 'self', '.', 'is_task_dep', 'and', 'dep_context', '.', 'ignore_task_deps', ':', 'yield', 'self', '.', '_passing_status', '(', 'reason', '=', '""context specified all task dependencies should be ignored.""', ')', 'return', 'for', 'dep_status', 'in', 'self', '.', '_get_dep_statuses', '(', 'ti', ',', 'session', ',', 'dep_context', ')', ':', 'yield', 'dep_status']","def get_dep_statuses ( self , ti , session , dep_context = none ) : # this avoids a circular dependency from airflow . ti_deps . dep_context import depcontext if dep_context is none : dep_context = depcontext ( ) if self . ignoreable and dep_context . ignore_all_deps : yield self . _passing_status ( reason = ""context specified all dependencies should be ignored."" ) return if self . is_task_dep and dep_context . ignore_task_deps : yield self . _passing_status ( reason = ""context specified all task dependencies should be ignored."" ) return for dep_status in self . _get_dep_statuses ( ti , session , dep_context ) : yield dep_status"
164,apache/airflow,airflow/ti_deps/deps/base_ti_dep.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/ti_deps/deps/base_ti_dep.py#L110-L125,"def is_met(self, ti, session, dep_context=None):
        """"""
        Returns whether or not this dependency is met for a given task instance. A
        dependency is considered met if all of the dependency statuses it reports are
        passing.

        :param ti: the task instance to see if this dependency is met for
        :type ti: airflow.models.TaskInstance
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        :param dep_context: The context this dependency is being checked under that stores
            state that can be used by this dependency.
        :type dep_context: BaseDepContext
        """"""
        return all(status.passed for status in
                   self.get_dep_statuses(ti, session, dep_context))","['def', 'is_met', '(', 'self', ',', 'ti', ',', 'session', ',', 'dep_context', '=', 'None', ')', ':', 'return', 'all', '(', 'status', '.', 'passed', 'for', 'status', 'in', 'self', '.', 'get_dep_statuses', '(', 'ti', ',', 'session', ',', 'dep_context', ')', ')']","Returns whether or not this dependency is met for a given task instance. A
        dependency is considered met if all of the dependency statuses it reports are
        passing.

        :param ti: the task instance to see if this dependency is met for
        :type ti: airflow.models.TaskInstance
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        :param dep_context: The context this dependency is being checked under that stores
            state that can be used by this dependency.
        :type dep_context: BaseDepContext","['Returns', 'whether', 'or', 'not', 'this', 'dependency', 'is', 'met', 'for', 'a', 'given', 'task', 'instance', '.', 'A', 'dependency', 'is', 'considered', 'met', 'if', 'all', 'of', 'the', 'dependency', 'statuses', 'it', 'reports', 'are', 'passing', '.']",python,test,"['returns', 'whether', 'or', 'not', 'this', 'dependency', 'is', 'met', 'for', 'a', 'given', 'task', 'instance', '.', 'a', 'dependency', 'is', 'considered', 'met', 'if', 'all', 'of', 'the', 'dependency', 'statuses', 'it', 'reports', 'are', 'passing', '.']",returns whether or not this dependency is met for a given task instance . a dependency is considered met if all of the dependency statuses it reports are passing .,"['def', 'is_met', '(', 'self', ',', 'ti', ',', 'session', ',', 'dep_context', '=', 'none', ')', ':', 'return', 'all', '(', 'status', '.', 'passed', 'for', 'status', 'in', 'self', '.', 'get_dep_statuses', '(', 'ti', ',', 'session', ',', 'dep_context', ')', ')']","def is_met ( self , ti , session , dep_context = none ) : return all ( status . passed for status in self . get_dep_statuses ( ti , session , dep_context ) )"
165,apache/airflow,airflow/ti_deps/deps/base_ti_dep.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/ti_deps/deps/base_ti_dep.py#L128-L142,"def get_failure_reasons(self, ti, session, dep_context=None):
        """"""
        Returns an iterable of strings that explain why this dependency wasn't met.

        :param ti: the task instance to see if this dependency is met for
        :type ti: airflow.models.TaskInstance
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        :param dep_context: The context this dependency is being checked under that stores
            state that can be used by this dependency.
        :type dep_context: BaseDepContext
        """"""
        for dep_status in self.get_dep_statuses(ti, session, dep_context):
            if not dep_status.passed:
                yield dep_status.reason","['def', 'get_failure_reasons', '(', 'self', ',', 'ti', ',', 'session', ',', 'dep_context', '=', 'None', ')', ':', 'for', 'dep_status', 'in', 'self', '.', 'get_dep_statuses', '(', 'ti', ',', 'session', ',', 'dep_context', ')', ':', 'if', 'not', 'dep_status', '.', 'passed', ':', 'yield', 'dep_status', '.', 'reason']","Returns an iterable of strings that explain why this dependency wasn't met.

        :param ti: the task instance to see if this dependency is met for
        :type ti: airflow.models.TaskInstance
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        :param dep_context: The context this dependency is being checked under that stores
            state that can be used by this dependency.
        :type dep_context: BaseDepContext","['Returns', 'an', 'iterable', 'of', 'strings', 'that', 'explain', 'why', 'this', 'dependency', 'wasn', 't', 'met', '.']",python,test,"['returns', 'an', 'iterable', 'of', 'strings', 'that', 'explain', 'why', 'this', 'dependency', 'wasn', 't', 'met', '.']",returns an iterable of strings that explain why this dependency wasn t met .,"['def', 'get_failure_reasons', '(', 'self', ',', 'ti', ',', 'session', ',', 'dep_context', '=', 'none', ')', ':', 'for', 'dep_status', 'in', 'self', '.', 'get_dep_statuses', '(', 'ti', ',', 'session', ',', 'dep_context', ')', ':', 'if', 'not', 'dep_status', '.', 'passed', ':', 'yield', 'dep_status', '.', 'reason']","def get_failure_reasons ( self , ti , session , dep_context = none ) : for dep_status in self . get_dep_statuses ( ti , session , dep_context ) : if not dep_status . passed : yield dep_status . reason"
166,apache/airflow,airflow/contrib/hooks/aws_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_hook.py#L28-L77,"def _parse_s3_config(config_file_name, config_format='boto', profile=None):
    """"""
    Parses a config file for s3 credentials. Can currently
    parse boto, s3cmd.conf and AWS SDK config formats

    :param config_file_name: path to the config file
    :type config_file_name: str
    :param config_format: config type. One of ""boto"", ""s3cmd"" or ""aws"".
        Defaults to ""boto""
    :type config_format: str
    :param profile: profile name in AWS type config file
    :type profile: str
    """"""
    config = configparser.ConfigParser()
    if config.read(config_file_name):  # pragma: no cover
        sections = config.sections()
    else:
        raise AirflowException(""Couldn't read {0}"".format(config_file_name))
    # Setting option names depending on file format
    if config_format is None:
        config_format = 'boto'
    conf_format = config_format.lower()
    if conf_format == 'boto':  # pragma: no cover
        if profile is not None and 'profile ' + profile in sections:
            cred_section = 'profile ' + profile
        else:
            cred_section = 'Credentials'
    elif conf_format == 'aws' and profile is not None:
        cred_section = profile
    else:
        cred_section = 'default'
    # Option names
    if conf_format in ('boto', 'aws'):  # pragma: no cover
        key_id_option = 'aws_access_key_id'
        secret_key_option = 'aws_secret_access_key'
        # security_token_option = 'aws_security_token'
    else:
        key_id_option = 'access_key'
        secret_key_option = 'secret_key'
    # Actual Parsing
    if cred_section not in sections:
        raise AirflowException(""This config file format is not recognized"")
    else:
        try:
            access_key = config.get(cred_section, key_id_option)
            secret_key = config.get(cred_section, secret_key_option)
        except Exception:
            logging.warning(""Option Error in parsing s3 config file"")
            raise
        return access_key, secret_key","['def', '_parse_s3_config', '(', 'config_file_name', ',', 'config_format', '=', ""'boto'"", ',', 'profile', '=', 'None', ')', ':', 'config', '=', 'configparser', '.', 'ConfigParser', '(', ')', 'if', 'config', '.', 'read', '(', 'config_file_name', ')', ':', '# pragma: no cover', 'sections', '=', 'config', '.', 'sections', '(', ')', 'else', ':', 'raise', 'AirflowException', '(', '""Couldn\'t read {0}""', '.', 'format', '(', 'config_file_name', ')', ')', '# Setting option names depending on file format', 'if', 'config_format', 'is', 'None', ':', 'config_format', '=', ""'boto'"", 'conf_format', '=', 'config_format', '.', 'lower', '(', ')', 'if', 'conf_format', '==', ""'boto'"", ':', '# pragma: no cover', 'if', 'profile', 'is', 'not', 'None', 'and', ""'profile '"", '+', 'profile', 'in', 'sections', ':', 'cred_section', '=', ""'profile '"", '+', 'profile', 'else', ':', 'cred_section', '=', ""'Credentials'"", 'elif', 'conf_format', '==', ""'aws'"", 'and', 'profile', 'is', 'not', 'None', ':', 'cred_section', '=', 'profile', 'else', ':', 'cred_section', '=', ""'default'"", '# Option names', 'if', 'conf_format', 'in', '(', ""'boto'"", ',', ""'aws'"", ')', ':', '# pragma: no cover', 'key_id_option', '=', ""'aws_access_key_id'"", 'secret_key_option', '=', ""'aws_secret_access_key'"", ""# security_token_option = 'aws_security_token'"", 'else', ':', 'key_id_option', '=', ""'access_key'"", 'secret_key_option', '=', ""'secret_key'"", '# Actual Parsing', 'if', 'cred_section', 'not', 'in', 'sections', ':', 'raise', 'AirflowException', '(', '""This config file format is not recognized""', ')', 'else', ':', 'try', ':', 'access_key', '=', 'config', '.', 'get', '(', 'cred_section', ',', 'key_id_option', ')', 'secret_key', '=', 'config', '.', 'get', '(', 'cred_section', ',', 'secret_key_option', ')', 'except', 'Exception', ':', 'logging', '.', 'warning', '(', '""Option Error in parsing s3 config file""', ')', 'raise', 'return', 'access_key', ',', 'secret_key']","Parses a config file for s3 credentials. Can currently
    parse boto, s3cmd.conf and AWS SDK config formats

    :param config_file_name: path to the config file
    :type config_file_name: str
    :param config_format: config type. One of ""boto"", ""s3cmd"" or ""aws"".
        Defaults to ""boto""
    :type config_format: str
    :param profile: profile name in AWS type config file
    :type profile: str","['Parses', 'a', 'config', 'file', 'for', 's3', 'credentials', '.', 'Can', 'currently', 'parse', 'boto', 's3cmd', '.', 'conf', 'and', 'AWS', 'SDK', 'config', 'formats']",python,test,"['parses', 'a', 'config', 'file', 'for', 's3', 'credentials', '.', 'can', 'currently', 'parse', 'boto', 's3cmd', '.', 'conf', 'and', 'aws', 'sdk', 'config', 'formats']",parses a config file for s3 credentials . can currently parse boto s3cmd . conf and aws sdk config formats,"['def', '_parse_s3_config', '(', 'config_file_name', ',', 'config_format', '=', ""'boto'"", ',', 'profile', '=', 'none', ')', ':', 'config', '=', 'configparser', '.', 'configparser', '(', ')', 'if', 'config', '.', 'read', '(', 'config_file_name', ')', ':', '# pragma: no cover', 'sections', '=', 'config', '.', 'sections', '(', ')', 'else', ':', 'raise', 'airflowexception', '(', '""couldn\'t read {0}""', '.', 'format', '(', 'config_file_name', ')', ')', '# setting option names depending on file format', 'if', 'config_format', 'is', 'none', ':', 'config_format', '=', ""'boto'"", 'conf_format', '=', 'config_format', '.', 'lower', '(', ')', 'if', 'conf_format', '==', ""'boto'"", ':', '# pragma: no cover', 'if', 'profile', 'is', 'not', 'none', 'and', ""'profile '"", '+', 'profile', 'in', 'sections', ':', 'cred_section', '=', ""'profile '"", '+', 'profile', 'else', ':', 'cred_section', '=', ""'credentials'"", 'elif', 'conf_format', '==', ""'aws'"", 'and', 'profile', 'is', 'not', 'none', ':', 'cred_section', '=', 'profile', 'else', ':', 'cred_section', '=', ""'default'"", '# option names', 'if', 'conf_format', 'in', '(', ""'boto'"", ',', ""'aws'"", ')', ':', '# pragma: no cover', 'key_id_option', '=', ""'aws_access_key_id'"", 'secret_key_option', '=', ""'aws_secret_access_key'"", ""# security_token_option = 'aws_security_token'"", 'else', ':', 'key_id_option', '=', ""'access_key'"", 'secret_key_option', '=', ""'secret_key'"", '# actual parsing', 'if', 'cred_section', 'not', 'in', 'sections', ':', 'raise', 'airflowexception', '(', '""this config file format is not recognized""', ')', 'else', ':', 'try', ':', 'access_key', '=', 'config', '.', 'get', '(', 'cred_section', ',', 'key_id_option', ')', 'secret_key', '=', 'config', '.', 'get', '(', 'cred_section', ',', 'secret_key_option', ')', 'except', 'exception', ':', 'logging', '.', 'warning', '(', '""option error in parsing s3 config file""', ')', 'raise', 'return', 'access_key', ',', 'secret_key']","def _parse_s3_config ( config_file_name , config_format = 'boto' , profile = none ) : config = configparser . configparser ( ) if config . read ( config_file_name ) : # pragma: no cover sections = config . sections ( ) else : raise airflowexception ( ""couldn't read {0}"" . format ( config_file_name ) ) # setting option names depending on file format if config_format is none : config_format = 'boto' conf_format = config_format . lower ( ) if conf_format == 'boto' : # pragma: no cover if profile is not none and 'profile ' + profile in sections : cred_section = 'profile ' + profile else : cred_section = 'credentials' elif conf_format == 'aws' and profile is not none : cred_section = profile else : cred_section = 'default' # option names if conf_format in ( 'boto' , 'aws' ) : # pragma: no cover key_id_option = 'aws_access_key_id' secret_key_option = 'aws_secret_access_key' # security_token_option = 'aws_security_token' else : key_id_option = 'access_key' secret_key_option = 'secret_key' # actual parsing if cred_section not in sections : raise airflowexception ( ""this config file format is not recognized"" ) else : try : access_key = config . get ( cred_section , key_id_option ) secret_key = config . get ( cred_section , secret_key_option ) except exception : logging . warning ( ""option error in parsing s3 config file"" ) raise return access_key , secret_key"
167,apache/airflow,airflow/contrib/hooks/aws_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_hook.py#L183-L192,"def get_credentials(self, region_name=None):
        """"""Get the underlying `botocore.Credentials` object.

        This contains the following authentication attributes: access_key, secret_key and token.
        """"""
        session, _ = self._get_credentials(region_name)
        # Credentials are refreshable, so accessing your access key and
        # secret key separately can lead to a race condition.
        # See https://stackoverflow.com/a/36291428/8283373
        return session.get_credentials().get_frozen_credentials()","['def', 'get_credentials', '(', 'self', ',', 'region_name', '=', 'None', ')', ':', 'session', ',', '_', '=', 'self', '.', '_get_credentials', '(', 'region_name', ')', '# Credentials are refreshable, so accessing your access key and', '# secret key separately can lead to a race condition.', '# See https://stackoverflow.com/a/36291428/8283373', 'return', 'session', '.', 'get_credentials', '(', ')', '.', 'get_frozen_credentials', '(', ')']","Get the underlying `botocore.Credentials` object.

        This contains the following authentication attributes: access_key, secret_key and token.","['Get', 'the', 'underlying', 'botocore', '.', 'Credentials', 'object', '.']",python,test,"['get', 'the', 'underlying', 'botocore', '.', 'credentials', 'object', '.']",get the underlying botocore . credentials object .,"['def', 'get_credentials', '(', 'self', ',', 'region_name', '=', 'none', ')', ':', 'session', ',', '_', '=', 'self', '.', '_get_credentials', '(', 'region_name', ')', '# credentials are refreshable, so accessing your access key and', '# secret key separately can lead to a race condition.', '# see https://stackoverflow.com/a/36291428/8283373', 'return', 'session', '.', 'get_credentials', '(', ')', '.', 'get_frozen_credentials', '(', ')']","def get_credentials ( self , region_name = none ) : session , _ = self . _get_credentials ( region_name ) # credentials are refreshable, so accessing your access key and # secret key separately can lead to a race condition. # see https://stackoverflow.com/a/36291428/8283373 return session . get_credentials ( ) . get_frozen_credentials ( )"
168,apache/airflow,airflow/contrib/hooks/aws_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_hook.py#L194-L205,"def expand_role(self, role):
        """"""
        If the IAM role is a role name, get the Amazon Resource Name (ARN) for the role.
        If IAM role is already an IAM role ARN, no change is made.

        :param role: IAM role name or ARN
        :return: IAM role ARN
        """"""
        if '/' in role:
            return role
        else:
            return self.get_client_type('iam').get_role(RoleName=role)['Role']['Arn']","['def', 'expand_role', '(', 'self', ',', 'role', ')', ':', 'if', ""'/'"", 'in', 'role', ':', 'return', 'role', 'else', ':', 'return', 'self', '.', 'get_client_type', '(', ""'iam'"", ')', '.', 'get_role', '(', 'RoleName', '=', 'role', ')', '[', ""'Role'"", ']', '[', ""'Arn'"", ']']","If the IAM role is a role name, get the Amazon Resource Name (ARN) for the role.
        If IAM role is already an IAM role ARN, no change is made.

        :param role: IAM role name or ARN
        :return: IAM role ARN","['If', 'the', 'IAM', 'role', 'is', 'a', 'role', 'name', 'get', 'the', 'Amazon', 'Resource', 'Name', '(', 'ARN', ')', 'for', 'the', 'role', '.', 'If', 'IAM', 'role', 'is', 'already', 'an', 'IAM', 'role', 'ARN', 'no', 'change', 'is', 'made', '.']",python,test,"['if', 'the', 'iam', 'role', 'is', 'a', 'role', 'name', 'get', 'the', 'amazon', 'resource', 'name', '(', 'arn', ')', 'for', 'the', 'role', '.', 'if', 'iam', 'role', 'is', 'already', 'an', 'iam', 'role', 'arn', 'no', 'change', 'is', 'made', '.']",if the iam role is a role name get the amazon resource name ( arn ) for the role . if iam role is already an iam role arn no change is made .,"['def', 'expand_role', '(', 'self', ',', 'role', ')', ':', 'if', ""'/'"", 'in', 'role', ':', 'return', 'role', 'else', ':', 'return', 'self', '.', 'get_client_type', '(', ""'iam'"", ')', '.', 'get_role', '(', 'rolename', '=', 'role', ')', '[', ""'role'"", ']', '[', ""'arn'"", ']']","def expand_role ( self , role ) : if '/' in role : return role else : return self . get_client_type ( 'iam' ) . get_role ( rolename = role ) [ 'role' ] [ 'arn' ]"
169,apache/airflow,airflow/contrib/hooks/vertica_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/vertica_hook.py#L35-L53,"def get_conn(self):
        """"""
        Returns verticaql connection object
        """"""
        conn = self.get_connection(self.vertica_conn_id)
        conn_config = {
            ""user"": conn.login,
            ""password"": conn.password or '',
            ""database"": conn.schema,
            ""host"": conn.host or 'localhost'
        }

        if not conn.port:
            conn_config[""port""] = 5433
        else:
            conn_config[""port""] = int(conn.port)

        conn = connect(**conn_config)
        return conn","['def', 'get_conn', '(', 'self', ')', ':', 'conn', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'vertica_conn_id', ')', 'conn_config', '=', '{', '""user""', ':', 'conn', '.', 'login', ',', '""password""', ':', 'conn', '.', 'password', 'or', ""''"", ',', '""database""', ':', 'conn', '.', 'schema', ',', '""host""', ':', 'conn', '.', 'host', 'or', ""'localhost'"", '}', 'if', 'not', 'conn', '.', 'port', ':', 'conn_config', '[', '""port""', ']', '=', '5433', 'else', ':', 'conn_config', '[', '""port""', ']', '=', 'int', '(', 'conn', '.', 'port', ')', 'conn', '=', 'connect', '(', '*', '*', 'conn_config', ')', 'return', 'conn']",Returns verticaql connection object,"['Returns', 'verticaql', 'connection', 'object']",python,test,"['returns', 'verticaql', 'connection', 'object']",returns verticaql connection object,"['def', 'get_conn', '(', 'self', ')', ':', 'conn', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'vertica_conn_id', ')', 'conn_config', '=', '{', '""user""', ':', 'conn', '.', 'login', ',', '""password""', ':', 'conn', '.', 'password', 'or', ""''"", ',', '""database""', ':', 'conn', '.', 'schema', ',', '""host""', ':', 'conn', '.', 'host', 'or', ""'localhost'"", '}', 'if', 'not', 'conn', '.', 'port', ':', 'conn_config', '[', '""port""', ']', '=', '5433', 'else', ':', 'conn_config', '[', '""port""', ']', '=', 'int', '(', 'conn', '.', 'port', ')', 'conn', '=', 'connect', '(', '*', '*', 'conn_config', ')', 'return', 'conn']","def get_conn ( self ) : conn = self . get_connection ( self . vertica_conn_id ) conn_config = { ""user"" : conn . login , ""password"" : conn . password or '' , ""database"" : conn . schema , ""host"" : conn . host or 'localhost' } if not conn . port : conn_config [ ""port"" ] = 5433 else : conn_config [ ""port"" ] = int ( conn . port ) conn = connect ( * * conn_config ) return conn"
170,apache/airflow,airflow/utils/log/logging_mixin.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/logging_mixin.py#L166-L184,"def set_context(logger, value):
    """"""
    Walks the tree of loggers and tries to set the context for each handler
    :param logger: logger
    :param value: value to set
    """"""
    _logger = logger
    while _logger:
        for handler in _logger.handlers:
            try:
                handler.set_context(value)
            except AttributeError:
                # Not all handlers need to have context passed in so we ignore
                # the error when handlers do not have set_context defined.
                pass
        if _logger.propagate is True:
            _logger = _logger.parent
        else:
            _logger = None","['def', 'set_context', '(', 'logger', ',', 'value', ')', ':', '_logger', '=', 'logger', 'while', '_logger', ':', 'for', 'handler', 'in', '_logger', '.', 'handlers', ':', 'try', ':', 'handler', '.', 'set_context', '(', 'value', ')', 'except', 'AttributeError', ':', '# Not all handlers need to have context passed in so we ignore', '# the error when handlers do not have set_context defined.', 'pass', 'if', '_logger', '.', 'propagate', 'is', 'True', ':', '_logger', '=', '_logger', '.', 'parent', 'else', ':', '_logger', '=', 'None']","Walks the tree of loggers and tries to set the context for each handler
    :param logger: logger
    :param value: value to set","['Walks', 'the', 'tree', 'of', 'loggers', 'and', 'tries', 'to', 'set', 'the', 'context', 'for', 'each', 'handler', ':', 'param', 'logger', ':', 'logger', ':', 'param', 'value', ':', 'value', 'to', 'set']",python,test,"['walks', 'the', 'tree', 'of', 'loggers', 'and', 'tries', 'to', 'set', 'the', 'context', 'for', 'each', 'handler', ':', 'param', 'logger', ':', 'logger', ':', 'param', 'value', ':', 'value', 'to', 'set']",walks the tree of loggers and tries to set the context for each handler : param logger : logger : param value : value to set,"['def', 'set_context', '(', 'logger', ',', 'value', ')', ':', '_logger', '=', 'logger', 'while', '_logger', ':', 'for', 'handler', 'in', '_logger', '.', 'handlers', ':', 'try', ':', 'handler', '.', 'set_context', '(', 'value', ')', 'except', 'attributeerror', ':', '# not all handlers need to have context passed in so we ignore', '# the error when handlers do not have set_context defined.', 'pass', 'if', '_logger', '.', 'propagate', 'is', 'true', ':', '_logger', '=', '_logger', '.', 'parent', 'else', ':', '_logger', '=', 'none']","def set_context ( logger , value ) : _logger = logger while _logger : for handler in _logger . handlers : try : handler . set_context ( value ) except attributeerror : # not all handlers need to have context passed in so we ignore # the error when handlers do not have set_context defined. pass if _logger . propagate is true : _logger = _logger . parent else : _logger = none"
171,apache/airflow,airflow/utils/log/logging_mixin.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/logging_mixin.py#L92-L102,"def write(self, message):
        """"""
        Do whatever it takes to actually log the specified logging record
        :param message: message to log
        """"""
        if not message.endswith(""\n""):
            self._buffer += message
        else:
            self._buffer += message
            self.logger.log(self.level, self._buffer.rstrip())
            self._buffer = str()","['def', 'write', '(', 'self', ',', 'message', ')', ':', 'if', 'not', 'message', '.', 'endswith', '(', '""\\n""', ')', ':', 'self', '.', '_buffer', '+=', 'message', 'else', ':', 'self', '.', '_buffer', '+=', 'message', 'self', '.', 'logger', '.', 'log', '(', 'self', '.', 'level', ',', 'self', '.', '_buffer', '.', 'rstrip', '(', ')', ')', 'self', '.', '_buffer', '=', 'str', '(', ')']","Do whatever it takes to actually log the specified logging record
        :param message: message to log","['Do', 'whatever', 'it', 'takes', 'to', 'actually', 'log', 'the', 'specified', 'logging', 'record', ':', 'param', 'message', ':', 'message', 'to', 'log']",python,test,"['do', 'whatever', 'it', 'takes', 'to', 'actually', 'log', 'the', 'specified', 'logging', 'record', ':', 'param', 'message', ':', 'message', 'to', 'log']",do whatever it takes to actually log the specified logging record : param message : message to log,"['def', 'write', '(', 'self', ',', 'message', ')', ':', 'if', 'not', 'message', '.', 'endswith', '(', '""\\n""', ')', ':', 'self', '.', '_buffer', '+=', 'message', 'else', ':', 'self', '.', '_buffer', '+=', 'message', 'self', '.', 'logger', '.', 'log', '(', 'self', '.', 'level', ',', 'self', '.', '_buffer', '.', 'rstrip', '(', ')', ')', 'self', '.', '_buffer', '=', 'str', '(', ')']","def write ( self , message ) : if not message . endswith ( ""\n"" ) : self . _buffer += message else : self . _buffer += message self . logger . log ( self . level , self . _buffer . rstrip ( ) ) self . _buffer = str ( )"
172,apache/airflow,airflow/utils/log/logging_mixin.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/logging_mixin.py#L104-L110,"def flush(self):
        """"""
        Ensure all logging output has been flushed
        """"""
        if len(self._buffer) > 0:
            self.logger.log(self.level, self._buffer)
            self._buffer = str()","['def', 'flush', '(', 'self', ')', ':', 'if', 'len', '(', 'self', '.', '_buffer', ')', '>', '0', ':', 'self', '.', 'logger', '.', 'log', '(', 'self', '.', 'level', ',', 'self', '.', '_buffer', ')', 'self', '.', '_buffer', '=', 'str', '(', ')']",Ensure all logging output has been flushed,"['Ensure', 'all', 'logging', 'output', 'has', 'been', 'flushed']",python,test,"['ensure', 'all', 'logging', 'output', 'has', 'been', 'flushed']",ensure all logging output has been flushed,"['def', 'flush', '(', 'self', ')', ':', 'if', 'len', '(', 'self', '.', '_buffer', ')', '>', '0', ':', 'self', '.', 'logger', '.', 'log', '(', 'self', '.', 'level', ',', 'self', '.', '_buffer', ')', 'self', '.', '_buffer', '=', 'str', '(', ')']","def flush ( self ) : if len ( self . _buffer ) > 0 : self . logger . log ( self . level , self . _buffer ) self . _buffer = str ( )"
173,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L275-L286,"def correct_maybe_zipped(fileloc):
    """"""
    If the path contains a folder with a .zip suffix, then
    the folder is treated as a zip archive and path to zip is returned.
    """"""

    _, archive, filename = re.search(
        r'((.*\.zip){})?(.*)'.format(re.escape(os.sep)), fileloc).groups()
    if archive and zipfile.is_zipfile(archive):
        return archive
    else:
        return fileloc","['def', 'correct_maybe_zipped', '(', 'fileloc', ')', ':', '_', ',', 'archive', ',', 'filename', '=', 're', '.', 'search', '(', ""r'((.*\\.zip){})?(.*)'"", '.', 'format', '(', 're', '.', 'escape', '(', 'os', '.', 'sep', ')', ')', ',', 'fileloc', ')', '.', 'groups', '(', ')', 'if', 'archive', 'and', 'zipfile', '.', 'is_zipfile', '(', 'archive', ')', ':', 'return', 'archive', 'else', ':', 'return', 'fileloc']","If the path contains a folder with a .zip suffix, then
    the folder is treated as a zip archive and path to zip is returned.","['If', 'the', 'path', 'contains', 'a', 'folder', 'with', 'a', '.', 'zip', 'suffix', 'then', 'the', 'folder', 'is', 'treated', 'as', 'a', 'zip', 'archive', 'and', 'path', 'to', 'zip', 'is', 'returned', '.']",python,test,"['if', 'the', 'path', 'contains', 'a', 'folder', 'with', 'a', '.', 'zip', 'suffix', 'then', 'the', 'folder', 'is', 'treated', 'as', 'a', 'zip', 'archive', 'and', 'path', 'to', 'zip', 'is', 'returned', '.']",if the path contains a folder with a . zip suffix then the folder is treated as a zip archive and path to zip is returned .,"['def', 'correct_maybe_zipped', '(', 'fileloc', ')', ':', '_', ',', 'archive', ',', 'filename', '=', 're', '.', 'search', '(', ""r'((.*\\.zip){})?(.*)'"", '.', 'format', '(', 're', '.', 'escape', '(', 'os', '.', 'sep', ')', ')', ',', 'fileloc', ')', '.', 'groups', '(', ')', 'if', 'archive', 'and', 'zipfile', '.', 'is_zipfile', '(', 'archive', ')', ':', 'return', 'archive', 'else', ':', 'return', 'fileloc']","def correct_maybe_zipped ( fileloc ) : _ , archive , filename = re . search ( r'((.*\.zip){})?(.*)' . format ( re . escape ( os . sep ) ) , fileloc ) . groups ( ) if archive and zipfile . is_zipfile ( archive ) : return archive else : return fileloc"
174,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L289-L365,"def list_py_file_paths(directory, safe_mode=True,
                       include_examples=None):
    """"""
    Traverse a directory and look for Python files.

    :param directory: the directory to traverse
    :type directory: unicode
    :param safe_mode: whether to use a heuristic to determine whether a file
        contains Airflow DAG definitions
    :return: a list of paths to Python files in the specified directory
    :rtype: list[unicode]
    """"""
    if include_examples is None:
        include_examples = conf.getboolean('core', 'LOAD_EXAMPLES')
    file_paths = []
    if directory is None:
        return []
    elif os.path.isfile(directory):
        return [directory]
    elif os.path.isdir(directory):
        patterns_by_dir = {}
        for root, dirs, files in os.walk(directory, followlinks=True):
            patterns = patterns_by_dir.get(root, [])
            ignore_file = os.path.join(root, '.airflowignore')
            if os.path.isfile(ignore_file):
                with open(ignore_file, 'r') as f:
                    # If we have new patterns create a copy so we don't change
                    # the previous list (which would affect other subdirs)
                    patterns += [re.compile(p) for p in f.read().split('\n') if p]

            # If we can ignore any subdirs entirely we should - fewer paths
            # to walk is better. We have to modify the ``dirs`` array in
            # place for this to affect os.walk
            dirs[:] = [
                d
                for d in dirs
                if not any(p.search(os.path.join(root, d)) for p in patterns)
            ]

            # We want patterns defined in a parent folder's .airflowignore to
            # apply to subdirs too
            for d in dirs:
                patterns_by_dir[os.path.join(root, d)] = patterns

            for f in files:
                try:
                    file_path = os.path.join(root, f)
                    if not os.path.isfile(file_path):
                        continue
                    mod_name, file_ext = os.path.splitext(
                        os.path.split(file_path)[-1])
                    if file_ext != '.py' and not zipfile.is_zipfile(file_path):
                        continue
                    if any([re.findall(p, file_path) for p in patterns]):
                        continue

                    # Heuristic that guesses whether a Python file contains an
                    # Airflow DAG definition.
                    might_contain_dag = True
                    if safe_mode and not zipfile.is_zipfile(file_path):
                        with open(file_path, 'rb') as fp:
                            content = fp.read()
                            might_contain_dag = all(
                                [s in content for s in (b'DAG', b'airflow')])

                    if not might_contain_dag:
                        continue

                    file_paths.append(file_path)
                except Exception:
                    log = LoggingMixin().log
                    log.exception(""Error while examining %s"", f)
    if include_examples:
        import airflow.example_dags
        example_dag_folder = airflow.example_dags.__path__[0]
        file_paths.extend(list_py_file_paths(example_dag_folder, safe_mode, False))
    return file_paths","['def', 'list_py_file_paths', '(', 'directory', ',', 'safe_mode', '=', 'True', ',', 'include_examples', '=', 'None', ')', ':', 'if', 'include_examples', 'is', 'None', ':', 'include_examples', '=', 'conf', '.', 'getboolean', '(', ""'core'"", ',', ""'LOAD_EXAMPLES'"", ')', 'file_paths', '=', '[', ']', 'if', 'directory', 'is', 'None', ':', 'return', '[', ']', 'elif', 'os', '.', 'path', '.', 'isfile', '(', 'directory', ')', ':', 'return', '[', 'directory', ']', 'elif', 'os', '.', 'path', '.', 'isdir', '(', 'directory', ')', ':', 'patterns_by_dir', '=', '{', '}', 'for', 'root', ',', 'dirs', ',', 'files', 'in', 'os', '.', 'walk', '(', 'directory', ',', 'followlinks', '=', 'True', ')', ':', 'patterns', '=', 'patterns_by_dir', '.', 'get', '(', 'root', ',', '[', ']', ')', 'ignore_file', '=', 'os', '.', 'path', '.', 'join', '(', 'root', ',', ""'.airflowignore'"", ')', 'if', 'os', '.', 'path', '.', 'isfile', '(', 'ignore_file', ')', ':', 'with', 'open', '(', 'ignore_file', ',', ""'r'"", ')', 'as', 'f', ':', ""# If we have new patterns create a copy so we don't change"", '# the previous list (which would affect other subdirs)', 'patterns', '+=', '[', 're', '.', 'compile', '(', 'p', ')', 'for', 'p', 'in', 'f', '.', 'read', '(', ')', '.', 'split', '(', ""'\\n'"", ')', 'if', 'p', ']', '# If we can ignore any subdirs entirely we should - fewer paths', '# to walk is better. We have to modify the ``dirs`` array in', '# place for this to affect os.walk', 'dirs', '[', ':', ']', '=', '[', 'd', 'for', 'd', 'in', 'dirs', 'if', 'not', 'any', '(', 'p', '.', 'search', '(', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'd', ')', ')', 'for', 'p', 'in', 'patterns', ')', ']', ""# We want patterns defined in a parent folder's .airflowignore to"", '# apply to subdirs too', 'for', 'd', 'in', 'dirs', ':', 'patterns_by_dir', '[', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'd', ')', ']', '=', 'patterns', 'for', 'f', 'in', 'files', ':', 'try', ':', 'file_path', '=', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'f', ')', 'if', 'not', 'os', '.', 'path', '.', 'isfile', '(', 'file_path', ')', ':', 'continue', 'mod_name', ',', 'file_ext', '=', 'os', '.', 'path', '.', 'splitext', '(', 'os', '.', 'path', '.', 'split', '(', 'file_path', ')', '[', '-', '1', ']', ')', 'if', 'file_ext', '!=', ""'.py'"", 'and', 'not', 'zipfile', '.', 'is_zipfile', '(', 'file_path', ')', ':', 'continue', 'if', 'any', '(', '[', 're', '.', 'findall', '(', 'p', ',', 'file_path', ')', 'for', 'p', 'in', 'patterns', ']', ')', ':', 'continue', '# Heuristic that guesses whether a Python file contains an', '# Airflow DAG definition.', 'might_contain_dag', '=', 'True', 'if', 'safe_mode', 'and', 'not', 'zipfile', '.', 'is_zipfile', '(', 'file_path', ')', ':', 'with', 'open', '(', 'file_path', ',', ""'rb'"", ')', 'as', 'fp', ':', 'content', '=', 'fp', '.', 'read', '(', ')', 'might_contain_dag', '=', 'all', '(', '[', 's', 'in', 'content', 'for', 's', 'in', '(', ""b'DAG'"", ',', ""b'airflow'"", ')', ']', ')', 'if', 'not', 'might_contain_dag', ':', 'continue', 'file_paths', '.', 'append', '(', 'file_path', ')', 'except', 'Exception', ':', 'log', '=', 'LoggingMixin', '(', ')', '.', 'log', 'log', '.', 'exception', '(', '""Error while examining %s""', ',', 'f', ')', 'if', 'include_examples', ':', 'import', 'airflow', '.', 'example_dags', 'example_dag_folder', '=', 'airflow', '.', 'example_dags', '.', '__path__', '[', '0', ']', 'file_paths', '.', 'extend', '(', 'list_py_file_paths', '(', 'example_dag_folder', ',', 'safe_mode', ',', 'False', ')', ')', 'return', 'file_paths']","Traverse a directory and look for Python files.

    :param directory: the directory to traverse
    :type directory: unicode
    :param safe_mode: whether to use a heuristic to determine whether a file
        contains Airflow DAG definitions
    :return: a list of paths to Python files in the specified directory
    :rtype: list[unicode]","['Traverse', 'a', 'directory', 'and', 'look', 'for', 'Python', 'files', '.']",python,test,"['traverse', 'a', 'directory', 'and', 'look', 'for', 'python', 'files', '.']",traverse a directory and look for python files .,"['def', 'list_py_file_paths', '(', 'directory', ',', 'safe_mode', '=', 'true', ',', 'include_examples', '=', 'none', ')', ':', 'if', 'include_examples', 'is', 'none', ':', 'include_examples', '=', 'conf', '.', 'getboolean', '(', ""'core'"", ',', ""'load_examples'"", ')', 'file_paths', '=', '[', ']', 'if', 'directory', 'is', 'none', ':', 'return', '[', ']', 'elif', 'os', '.', 'path', '.', 'isfile', '(', 'directory', ')', ':', 'return', '[', 'directory', ']', 'elif', 'os', '.', 'path', '.', 'isdir', '(', 'directory', ')', ':', 'patterns_by_dir', '=', '{', '}', 'for', 'root', ',', 'dirs', ',', 'files', 'in', 'os', '.', 'walk', '(', 'directory', ',', 'followlinks', '=', 'true', ')', ':', 'patterns', '=', 'patterns_by_dir', '.', 'get', '(', 'root', ',', '[', ']', ')', 'ignore_file', '=', 'os', '.', 'path', '.', 'join', '(', 'root', ',', ""'.airflowignore'"", ')', 'if', 'os', '.', 'path', '.', 'isfile', '(', 'ignore_file', ')', ':', 'with', 'open', '(', 'ignore_file', ',', ""'r'"", ')', 'as', 'f', ':', ""# if we have new patterns create a copy so we don't change"", '# the previous list (which would affect other subdirs)', 'patterns', '+=', '[', 're', '.', 'compile', '(', 'p', ')', 'for', 'p', 'in', 'f', '.', 'read', '(', ')', '.', 'split', '(', ""'\\n'"", ')', 'if', 'p', ']', '# if we can ignore any subdirs entirely we should - fewer paths', '# to walk is better. we have to modify the ``dirs`` array in', '# place for this to affect os.walk', 'dirs', '[', ':', ']', '=', '[', 'd', 'for', 'd', 'in', 'dirs', 'if', 'not', 'any', '(', 'p', '.', 'search', '(', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'd', ')', ')', 'for', 'p', 'in', 'patterns', ')', ']', ""# we want patterns defined in a parent folder's .airflowignore to"", '# apply to subdirs too', 'for', 'd', 'in', 'dirs', ':', 'patterns_by_dir', '[', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'd', ')', ']', '=', 'patterns', 'for', 'f', 'in', 'files', ':', 'try', ':', 'file_path', '=', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'f', ')', 'if', 'not', 'os', '.', 'path', '.', 'isfile', '(', 'file_path', ')', ':', 'continue', 'mod_name', ',', 'file_ext', '=', 'os', '.', 'path', '.', 'splitext', '(', 'os', '.', 'path', '.', 'split', '(', 'file_path', ')', '[', '-', '1', ']', ')', 'if', 'file_ext', '!=', ""'.py'"", 'and', 'not', 'zipfile', '.', 'is_zipfile', '(', 'file_path', ')', ':', 'continue', 'if', 'any', '(', '[', 're', '.', 'findall', '(', 'p', ',', 'file_path', ')', 'for', 'p', 'in', 'patterns', ']', ')', ':', 'continue', '# heuristic that guesses whether a python file contains an', '# airflow dag definition.', 'might_contain_dag', '=', 'true', 'if', 'safe_mode', 'and', 'not', 'zipfile', '.', 'is_zipfile', '(', 'file_path', ')', ':', 'with', 'open', '(', 'file_path', ',', ""'rb'"", ')', 'as', 'fp', ':', 'content', '=', 'fp', '.', 'read', '(', ')', 'might_contain_dag', '=', 'all', '(', '[', 's', 'in', 'content', 'for', 's', 'in', '(', ""b'dag'"", ',', ""b'airflow'"", ')', ']', ')', 'if', 'not', 'might_contain_dag', ':', 'continue', 'file_paths', '.', 'append', '(', 'file_path', ')', 'except', 'exception', ':', 'log', '=', 'loggingmixin', '(', ')', '.', 'log', 'log', '.', 'exception', '(', '""error while examining %s""', ',', 'f', ')', 'if', 'include_examples', ':', 'import', 'airflow', '.', 'example_dags', 'example_dag_folder', '=', 'airflow', '.', 'example_dags', '.', '__path__', '[', '0', ']', 'file_paths', '.', 'extend', '(', 'list_py_file_paths', '(', 'example_dag_folder', ',', 'safe_mode', ',', 'false', ')', ')', 'return', 'file_paths']","def list_py_file_paths ( directory , safe_mode = true , include_examples = none ) : if include_examples is none : include_examples = conf . getboolean ( 'core' , 'load_examples' ) file_paths = [ ] if directory is none : return [ ] elif os . path . isfile ( directory ) : return [ directory ] elif os . path . isdir ( directory ) : patterns_by_dir = { } for root , dirs , files in os . walk ( directory , followlinks = true ) : patterns = patterns_by_dir . get ( root , [ ] ) ignore_file = os . path . join ( root , '.airflowignore' ) if os . path . isfile ( ignore_file ) : with open ( ignore_file , 'r' ) as f : # if we have new patterns create a copy so we don't change # the previous list (which would affect other subdirs) patterns += [ re . compile ( p ) for p in f . read ( ) . split ( '\n' ) if p ] # if we can ignore any subdirs entirely we should - fewer paths # to walk is better. we have to modify the ``dirs`` array in # place for this to affect os.walk dirs [ : ] = [ d for d in dirs if not any ( p . search ( os . path . join ( root , d ) ) for p in patterns ) ] # we want patterns defined in a parent folder's .airflowignore to # apply to subdirs too for d in dirs : patterns_by_dir [ os . path . join ( root , d ) ] = patterns for f in files : try : file_path = os . path . join ( root , f ) if not os . path . isfile ( file_path ) : continue mod_name , file_ext = os . path . splitext ( os . path . split ( file_path ) [ - 1 ] ) if file_ext != '.py' and not zipfile . is_zipfile ( file_path ) : continue if any ( [ re . findall ( p , file_path ) for p in patterns ] ) : continue # heuristic that guesses whether a python file contains an # airflow dag definition. might_contain_dag = true if safe_mode and not zipfile . is_zipfile ( file_path ) : with open ( file_path , 'rb' ) as fp : content = fp . read ( ) might_contain_dag = all ( [ s in content for s in ( b'dag' , b'airflow' ) ] ) if not might_contain_dag : continue file_paths . append ( file_path ) except exception : log = loggingmixin ( ) . log log . exception ( ""error while examining %s"" , f ) if include_examples : import airflow . example_dags example_dag_folder = airflow . example_dags . __path__ [ 0 ] file_paths . extend ( list_py_file_paths ( example_dag_folder , safe_mode , false ) ) return file_paths"
175,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L213-L233,"def construct_task_instance(self, session=None, lock_for_update=False):
        """"""
        Construct a TaskInstance from the database based on the primary key

        :param session: DB session.
        :param lock_for_update: if True, indicates that the database should
            lock the TaskInstance (issuing a FOR UPDATE clause) until the
            session is committed.
        """"""
        TI = airflow.models.TaskInstance

        qry = session.query(TI).filter(
            TI.dag_id == self._dag_id,
            TI.task_id == self._task_id,
            TI.execution_date == self._execution_date)

        if lock_for_update:
            ti = qry.with_for_update().first()
        else:
            ti = qry.first()
        return ti","['def', 'construct_task_instance', '(', 'self', ',', 'session', '=', 'None', ',', 'lock_for_update', '=', 'False', ')', ':', 'TI', '=', 'airflow', '.', 'models', '.', 'TaskInstance', 'qry', '=', 'session', '.', 'query', '(', 'TI', ')', '.', 'filter', '(', 'TI', '.', 'dag_id', '==', 'self', '.', '_dag_id', ',', 'TI', '.', 'task_id', '==', 'self', '.', '_task_id', ',', 'TI', '.', 'execution_date', '==', 'self', '.', '_execution_date', ')', 'if', 'lock_for_update', ':', 'ti', '=', 'qry', '.', 'with_for_update', '(', ')', '.', 'first', '(', ')', 'else', ':', 'ti', '=', 'qry', '.', 'first', '(', ')', 'return', 'ti']","Construct a TaskInstance from the database based on the primary key

        :param session: DB session.
        :param lock_for_update: if True, indicates that the database should
            lock the TaskInstance (issuing a FOR UPDATE clause) until the
            session is committed.","['Construct', 'a', 'TaskInstance', 'from', 'the', 'database', 'based', 'on', 'the', 'primary', 'key']",python,test,"['construct', 'a', 'taskinstance', 'from', 'the', 'database', 'based', 'on', 'the', 'primary', 'key']",construct a taskinstance from the database based on the primary key,"['def', 'construct_task_instance', '(', 'self', ',', 'session', '=', 'none', ',', 'lock_for_update', '=', 'false', ')', ':', 'ti', '=', 'airflow', '.', 'models', '.', 'taskinstance', 'qry', '=', 'session', '.', 'query', '(', 'ti', ')', '.', 'filter', '(', 'ti', '.', 'dag_id', '==', 'self', '.', '_dag_id', ',', 'ti', '.', 'task_id', '==', 'self', '.', '_task_id', ',', 'ti', '.', 'execution_date', '==', 'self', '.', '_execution_date', ')', 'if', 'lock_for_update', ':', 'ti', '=', 'qry', '.', 'with_for_update', '(', ')', '.', 'first', '(', ')', 'else', ':', 'ti', '=', 'qry', '.', 'first', '(', ')', 'return', 'ti']","def construct_task_instance ( self , session = none , lock_for_update = false ) : ti = airflow . models . taskinstance qry = session . query ( ti ) . filter ( ti . dag_id == self . _dag_id , ti . task_id == self . _task_id , ti . execution_date == self . _execution_date ) if lock_for_update : ti = qry . with_for_update ( ) . first ( ) else : ti = qry . first ( ) return ti"
176,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L262-L272,"def get_dag(self, dag_id):
        """"""
        :param dag_id: DAG ID
        :type dag_id: unicode
        :return: if the given DAG ID exists in the bag, return the BaseDag
        corresponding to that ID. Otherwise, throw an Exception
        :rtype: airflow.utils.dag_processing.SimpleDag
        """"""
        if dag_id not in self.dag_id_to_simple_dag:
            raise AirflowException(""Unknown DAG ID {}"".format(dag_id))
        return self.dag_id_to_simple_dag[dag_id]","['def', 'get_dag', '(', 'self', ',', 'dag_id', ')', ':', 'if', 'dag_id', 'not', 'in', 'self', '.', 'dag_id_to_simple_dag', ':', 'raise', 'AirflowException', '(', '""Unknown DAG ID {}""', '.', 'format', '(', 'dag_id', ')', ')', 'return', 'self', '.', 'dag_id_to_simple_dag', '[', 'dag_id', ']']",":param dag_id: DAG ID
        :type dag_id: unicode
        :return: if the given DAG ID exists in the bag, return the BaseDag
        corresponding to that ID. Otherwise, throw an Exception
        :rtype: airflow.utils.dag_processing.SimpleDag","[':', 'param', 'dag_id', ':', 'DAG', 'ID', ':', 'type', 'dag_id', ':', 'unicode', ':', 'return', ':', 'if', 'the', 'given', 'DAG', 'ID', 'exists', 'in', 'the', 'bag', 'return', 'the', 'BaseDag', 'corresponding', 'to', 'that', 'ID', '.', 'Otherwise', 'throw', 'an', 'Exception', ':', 'rtype', ':', 'airflow', '.', 'utils', '.', 'dag_processing', '.', 'SimpleDag']",python,test,"[':', 'param', 'dag_id', ':', 'dag', 'id', ':', 'type', 'dag_id', ':', 'unicode', ':', 'return', ':', 'if', 'the', 'given', 'dag', 'id', 'exists', 'in', 'the', 'bag', 'return', 'the', 'basedag', 'corresponding', 'to', 'that', 'id', '.', 'otherwise', 'throw', 'an', 'exception', ':', 'rtype', ':', 'airflow', '.', 'utils', '.', 'dag_processing', '.', 'simpledag']",: param dag_id : dag id : type dag_id : unicode : return : if the given dag id exists in the bag return the basedag corresponding to that id . otherwise throw an exception : rtype : airflow . utils . dag_processing . simpledag,"['def', 'get_dag', '(', 'self', ',', 'dag_id', ')', ':', 'if', 'dag_id', 'not', 'in', 'self', '.', 'dag_id_to_simple_dag', ':', 'raise', 'airflowexception', '(', '""unknown dag id {}""', '.', 'format', '(', 'dag_id', ')', ')', 'return', 'self', '.', 'dag_id_to_simple_dag', '[', 'dag_id', ']']","def get_dag ( self , dag_id ) : if dag_id not in self . dag_id_to_simple_dag : raise airflowexception ( ""unknown dag id {}"" . format ( dag_id ) ) return self . dag_id_to_simple_dag [ dag_id ]"
177,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L512-L524,"def start(self):
        """"""
        Launch DagFileProcessorManager processor and start DAG parsing loop in manager.
        """"""
        self._process = self._launch_process(self._dag_directory,
                                             self._file_paths,
                                             self._max_runs,
                                             self._processor_factory,
                                             self._child_signal_conn,
                                             self._stat_queue,
                                             self._result_queue,
                                             self._async_mode)
        self.log.info(""Launched DagFileProcessorManager with pid: %s"", self._process.pid)","['def', 'start', '(', 'self', ')', ':', 'self', '.', '_process', '=', 'self', '.', '_launch_process', '(', 'self', '.', '_dag_directory', ',', 'self', '.', '_file_paths', ',', 'self', '.', '_max_runs', ',', 'self', '.', '_processor_factory', ',', 'self', '.', '_child_signal_conn', ',', 'self', '.', '_stat_queue', ',', 'self', '.', '_result_queue', ',', 'self', '.', '_async_mode', ')', 'self', '.', 'log', '.', 'info', '(', '""Launched DagFileProcessorManager with pid: %s""', ',', 'self', '.', '_process', '.', 'pid', ')']",Launch DagFileProcessorManager processor and start DAG parsing loop in manager.,"['Launch', 'DagFileProcessorManager', 'processor', 'and', 'start', 'DAG', 'parsing', 'loop', 'in', 'manager', '.']",python,test,"['launch', 'dagfileprocessormanager', 'processor', 'and', 'start', 'dag', 'parsing', 'loop', 'in', 'manager', '.']",launch dagfileprocessormanager processor and start dag parsing loop in manager .,"['def', 'start', '(', 'self', ')', ':', 'self', '.', '_process', '=', 'self', '.', '_launch_process', '(', 'self', '.', '_dag_directory', ',', 'self', '.', '_file_paths', ',', 'self', '.', '_max_runs', ',', 'self', '.', '_processor_factory', ',', 'self', '.', '_child_signal_conn', ',', 'self', '.', '_stat_queue', ',', 'self', '.', '_result_queue', ',', 'self', '.', '_async_mode', ')', 'self', '.', 'log', '.', 'info', '(', '""launched dagfileprocessormanager with pid: %s""', ',', 'self', '.', '_process', '.', 'pid', ')']","def start ( self ) : self . _process = self . _launch_process ( self . _dag_directory , self . _file_paths , self . _max_runs , self . _processor_factory , self . _child_signal_conn , self . _stat_queue , self . _result_queue , self . _async_mode ) self . log . info ( ""launched dagfileprocessormanager with pid: %s"" , self . _process . pid )"
178,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L580-L602,"def harvest_simple_dags(self):
        """"""
        Harvest DAG parsing results from result queue and sync metadata from stat queue.
        :return: List of parsing result in SimpleDag format.
        """"""
        # Metadata and results to be harvested can be inconsistent,
        # but it should not be a big problem.
        self._sync_metadata()
        # Heartbeating after syncing metadata so we do not restart manager
        # if it processed all files for max_run times and exit normally.
        self._heartbeat_manager()
        simple_dags = []
        # multiprocessing.Queue().qsize will not work on MacOS.
        if sys.platform == ""darwin"":
            qsize = self._result_count
        else:
            qsize = self._result_queue.qsize()
        for _ in range(qsize):
            simple_dags.append(self._result_queue.get())

        self._result_count = 0

        return simple_dags","['def', 'harvest_simple_dags', '(', 'self', ')', ':', '# Metadata and results to be harvested can be inconsistent,', '# but it should not be a big problem.', 'self', '.', '_sync_metadata', '(', ')', '# Heartbeating after syncing metadata so we do not restart manager', '# if it processed all files for max_run times and exit normally.', 'self', '.', '_heartbeat_manager', '(', ')', 'simple_dags', '=', '[', ']', '# multiprocessing.Queue().qsize will not work on MacOS.', 'if', 'sys', '.', 'platform', '==', '""darwin""', ':', 'qsize', '=', 'self', '.', '_result_count', 'else', ':', 'qsize', '=', 'self', '.', '_result_queue', '.', 'qsize', '(', ')', 'for', '_', 'in', 'range', '(', 'qsize', ')', ':', 'simple_dags', '.', 'append', '(', 'self', '.', '_result_queue', '.', 'get', '(', ')', ')', 'self', '.', '_result_count', '=', '0', 'return', 'simple_dags']","Harvest DAG parsing results from result queue and sync metadata from stat queue.
        :return: List of parsing result in SimpleDag format.","['Harvest', 'DAG', 'parsing', 'results', 'from', 'result', 'queue', 'and', 'sync', 'metadata', 'from', 'stat', 'queue', '.', ':', 'return', ':', 'List', 'of', 'parsing', 'result', 'in', 'SimpleDag', 'format', '.']",python,test,"['harvest', 'dag', 'parsing', 'results', 'from', 'result', 'queue', 'and', 'sync', 'metadata', 'from', 'stat', 'queue', '.', ':', 'return', ':', 'list', 'of', 'parsing', 'result', 'in', 'simpledag', 'format', '.']",harvest dag parsing results from result queue and sync metadata from stat queue . : return : list of parsing result in simpledag format .,"['def', 'harvest_simple_dags', '(', 'self', ')', ':', '# metadata and results to be harvested can be inconsistent,', '# but it should not be a big problem.', 'self', '.', '_sync_metadata', '(', ')', '# heartbeating after syncing metadata so we do not restart manager', '# if it processed all files for max_run times and exit normally.', 'self', '.', '_heartbeat_manager', '(', ')', 'simple_dags', '=', '[', ']', '# multiprocessing.queue().qsize will not work on macos.', 'if', 'sys', '.', 'platform', '==', '""darwin""', ':', 'qsize', '=', 'self', '.', '_result_count', 'else', ':', 'qsize', '=', 'self', '.', '_result_queue', '.', 'qsize', '(', ')', 'for', '_', 'in', 'range', '(', 'qsize', ')', ':', 'simple_dags', '.', 'append', '(', 'self', '.', '_result_queue', '.', 'get', '(', ')', ')', 'self', '.', '_result_count', '=', '0', 'return', 'simple_dags']","def harvest_simple_dags ( self ) : # metadata and results to be harvested can be inconsistent, # but it should not be a big problem. self . _sync_metadata ( ) # heartbeating after syncing metadata so we do not restart manager # if it processed all files for max_run times and exit normally. self . _heartbeat_manager ( ) simple_dags = [ ] # multiprocessing.queue().qsize will not work on macos. if sys . platform == ""darwin"" : qsize = self . _result_count else : qsize = self . _result_queue . qsize ( ) for _ in range ( qsize ) : simple_dags . append ( self . _result_queue . get ( ) ) self . _result_count = 0 return simple_dags"
179,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L604-L610,"def _heartbeat_manager(self):
        """"""
        Heartbeat DAG file processor and start it if it is not alive.
        :return:
        """"""
        if self._process and not self._process.is_alive() and not self.done:
            self.start()","['def', '_heartbeat_manager', '(', 'self', ')', ':', 'if', 'self', '.', '_process', 'and', 'not', 'self', '.', '_process', '.', 'is_alive', '(', ')', 'and', 'not', 'self', '.', 'done', ':', 'self', '.', 'start', '(', ')']","Heartbeat DAG file processor and start it if it is not alive.
        :return:","['Heartbeat', 'DAG', 'file', 'processor', 'and', 'start', 'it', 'if', 'it', 'is', 'not', 'alive', '.', ':', 'return', ':']",python,test,"['heartbeat', 'dag', 'file', 'processor', 'and', 'start', 'it', 'if', 'it', 'is', 'not', 'alive', '.', ':', 'return', ':']",heartbeat dag file processor and start it if it is not alive . : return :,"['def', '_heartbeat_manager', '(', 'self', ')', ':', 'if', 'self', '.', '_process', 'and', 'not', 'self', '.', '_process', '.', 'is_alive', '(', ')', 'and', 'not', 'self', '.', 'done', ':', 'self', '.', 'start', '(', ')']",def _heartbeat_manager ( self ) : if self . _process and not self . _process . is_alive ( ) and not self . done : self . start ( )
180,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L612-L623,"def _sync_metadata(self):
        """"""
        Sync metadata from stat queue and only keep the latest stat.
        :return:
        """"""
        while not self._stat_queue.empty():
            stat = self._stat_queue.get()
            self._file_paths = stat.file_paths
            self._all_pids = stat.all_pids
            self._done = stat.done
            self._all_files_processed = stat.all_files_processed
            self._result_count += stat.result_count","['def', '_sync_metadata', '(', 'self', ')', ':', 'while', 'not', 'self', '.', '_stat_queue', '.', 'empty', '(', ')', ':', 'stat', '=', 'self', '.', '_stat_queue', '.', 'get', '(', ')', 'self', '.', '_file_paths', '=', 'stat', '.', 'file_paths', 'self', '.', '_all_pids', '=', 'stat', '.', 'all_pids', 'self', '.', '_done', '=', 'stat', '.', 'done', 'self', '.', '_all_files_processed', '=', 'stat', '.', 'all_files_processed', 'self', '.', '_result_count', '+=', 'stat', '.', 'result_count']","Sync metadata from stat queue and only keep the latest stat.
        :return:","['Sync', 'metadata', 'from', 'stat', 'queue', 'and', 'only', 'keep', 'the', 'latest', 'stat', '.', ':', 'return', ':']",python,test,"['sync', 'metadata', 'from', 'stat', 'queue', 'and', 'only', 'keep', 'the', 'latest', 'stat', '.', ':', 'return', ':']",sync metadata from stat queue and only keep the latest stat . : return :,"['def', '_sync_metadata', '(', 'self', ')', ':', 'while', 'not', 'self', '.', '_stat_queue', '.', 'empty', '(', ')', ':', 'stat', '=', 'self', '.', '_stat_queue', '.', 'get', '(', ')', 'self', '.', '_file_paths', '=', 'stat', '.', 'file_paths', 'self', '.', '_all_pids', '=', 'stat', '.', 'all_pids', 'self', '.', '_done', '=', 'stat', '.', 'done', 'self', '.', '_all_files_processed', '=', 'stat', '.', 'all_files_processed', 'self', '.', '_result_count', '+=', 'stat', '.', 'result_count']",def _sync_metadata ( self ) : while not self . _stat_queue . empty ( ) : stat = self . _stat_queue . get ( ) self . _file_paths = stat . file_paths self . _all_pids = stat . all_pids self . _done = stat . done self . _all_files_processed = stat . all_files_processed self . _result_count += stat . result_count
181,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L637-L643,"def terminate(self):
        """"""
        Send termination signal to DAG parsing processor manager
        and expect it to terminate all DAG file processors.
        """"""
        self.log.info(""Sending termination message to manager."")
        self._child_signal_conn.send(DagParsingSignal.TERMINATE_MANAGER)","['def', 'terminate', '(', 'self', ')', ':', 'self', '.', 'log', '.', 'info', '(', '""Sending termination message to manager.""', ')', 'self', '.', '_child_signal_conn', '.', 'send', '(', 'DagParsingSignal', '.', 'TERMINATE_MANAGER', ')']","Send termination signal to DAG parsing processor manager
        and expect it to terminate all DAG file processors.","['Send', 'termination', 'signal', 'to', 'DAG', 'parsing', 'processor', 'manager', 'and', 'expect', 'it', 'to', 'terminate', 'all', 'DAG', 'file', 'processors', '.']",python,test,"['send', 'termination', 'signal', 'to', 'dag', 'parsing', 'processor', 'manager', 'and', 'expect', 'it', 'to', 'terminate', 'all', 'dag', 'file', 'processors', '.']",send termination signal to dag parsing processor manager and expect it to terminate all dag file processors .,"['def', 'terminate', '(', 'self', ')', ':', 'self', '.', 'log', '.', 'info', '(', '""sending termination message to manager.""', ')', 'self', '.', '_child_signal_conn', '.', 'send', '(', 'dagparsingsignal', '.', 'terminate_manager', ')']","def terminate ( self ) : self . log . info ( ""sending termination message to manager."" ) self . _child_signal_conn . send ( dagparsingsignal . terminate_manager )"
182,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L645-L679,"def end(self):
        """"""
        Terminate (and then kill) the manager process launched.
        :return:
        """"""
        if not self._process:
            self.log.warn('Ending without manager process.')
            return
        this_process = psutil.Process(os.getpid())
        try:
            manager_process = psutil.Process(self._process.pid)
        except psutil.NoSuchProcess:
            self.log.info(""Manager process not running."")
            return

        # First try SIGTERM
        if manager_process.is_running() \
                and manager_process.pid in [x.pid for x in this_process.children()]:
            self.log.info(""Terminating manager process: %s"", manager_process.pid)
            manager_process.terminate()
            # TODO: Remove magic number
            timeout = 5
            self.log.info(""Waiting up to %ss for manager process to exit..."", timeout)
            try:
                psutil.wait_procs({manager_process}, timeout)
            except psutil.TimeoutExpired:
                self.log.debug(""Ran out of time while waiting for ""
                               ""processes to exit"")

        # Then SIGKILL
        if manager_process.is_running() \
                and manager_process.pid in [x.pid for x in this_process.children()]:
            self.log.info(""Killing manager process: %s"", manager_process.pid)
            manager_process.kill()
            manager_process.wait()","['def', 'end', '(', 'self', ')', ':', 'if', 'not', 'self', '.', '_process', ':', 'self', '.', 'log', '.', 'warn', '(', ""'Ending without manager process.'"", ')', 'return', 'this_process', '=', 'psutil', '.', 'Process', '(', 'os', '.', 'getpid', '(', ')', ')', 'try', ':', 'manager_process', '=', 'psutil', '.', 'Process', '(', 'self', '.', '_process', '.', 'pid', ')', 'except', 'psutil', '.', 'NoSuchProcess', ':', 'self', '.', 'log', '.', 'info', '(', '""Manager process not running.""', ')', 'return', '# First try SIGTERM', 'if', 'manager_process', '.', 'is_running', '(', ')', 'and', 'manager_process', '.', 'pid', 'in', '[', 'x', '.', 'pid', 'for', 'x', 'in', 'this_process', '.', 'children', '(', ')', ']', ':', 'self', '.', 'log', '.', 'info', '(', '""Terminating manager process: %s""', ',', 'manager_process', '.', 'pid', ')', 'manager_process', '.', 'terminate', '(', ')', '# TODO: Remove magic number', 'timeout', '=', '5', 'self', '.', 'log', '.', 'info', '(', '""Waiting up to %ss for manager process to exit...""', ',', 'timeout', ')', 'try', ':', 'psutil', '.', 'wait_procs', '(', '{', 'manager_process', '}', ',', 'timeout', ')', 'except', 'psutil', '.', 'TimeoutExpired', ':', 'self', '.', 'log', '.', 'debug', '(', '""Ran out of time while waiting for ""', '""processes to exit""', ')', '# Then SIGKILL', 'if', 'manager_process', '.', 'is_running', '(', ')', 'and', 'manager_process', '.', 'pid', 'in', '[', 'x', '.', 'pid', 'for', 'x', 'in', 'this_process', '.', 'children', '(', ')', ']', ':', 'self', '.', 'log', '.', 'info', '(', '""Killing manager process: %s""', ',', 'manager_process', '.', 'pid', ')', 'manager_process', '.', 'kill', '(', ')', 'manager_process', '.', 'wait', '(', ')']","Terminate (and then kill) the manager process launched.
        :return:","['Terminate', '(', 'and', 'then', 'kill', ')', 'the', 'manager', 'process', 'launched', '.', ':', 'return', ':']",python,test,"['terminate', '(', 'and', 'then', 'kill', ')', 'the', 'manager', 'process', 'launched', '.', ':', 'return', ':']",terminate ( and then kill ) the manager process launched . : return :,"['def', 'end', '(', 'self', ')', ':', 'if', 'not', 'self', '.', '_process', ':', 'self', '.', 'log', '.', 'warn', '(', ""'ending without manager process.'"", ')', 'return', 'this_process', '=', 'psutil', '.', 'process', '(', 'os', '.', 'getpid', '(', ')', ')', 'try', ':', 'manager_process', '=', 'psutil', '.', 'process', '(', 'self', '.', '_process', '.', 'pid', ')', 'except', 'psutil', '.', 'nosuchprocess', ':', 'self', '.', 'log', '.', 'info', '(', '""manager process not running.""', ')', 'return', '# first try sigterm', 'if', 'manager_process', '.', 'is_running', '(', ')', 'and', 'manager_process', '.', 'pid', 'in', '[', 'x', '.', 'pid', 'for', 'x', 'in', 'this_process', '.', 'children', '(', ')', ']', ':', 'self', '.', 'log', '.', 'info', '(', '""terminating manager process: %s""', ',', 'manager_process', '.', 'pid', ')', 'manager_process', '.', 'terminate', '(', ')', '# todo: remove magic number', 'timeout', '=', '5', 'self', '.', 'log', '.', 'info', '(', '""waiting up to %ss for manager process to exit...""', ',', 'timeout', ')', 'try', ':', 'psutil', '.', 'wait_procs', '(', '{', 'manager_process', '}', ',', 'timeout', ')', 'except', 'psutil', '.', 'timeoutexpired', ':', 'self', '.', 'log', '.', 'debug', '(', '""ran out of time while waiting for ""', '""processes to exit""', ')', '# then sigkill', 'if', 'manager_process', '.', 'is_running', '(', ')', 'and', 'manager_process', '.', 'pid', 'in', '[', 'x', '.', 'pid', 'for', 'x', 'in', 'this_process', '.', 'children', '(', ')', ']', ':', 'self', '.', 'log', '.', 'info', '(', '""killing manager process: %s""', ',', 'manager_process', '.', 'pid', ')', 'manager_process', '.', 'kill', '(', ')', 'manager_process', '.', 'wait', '(', ')']","def end ( self ) : if not self . _process : self . log . warn ( 'ending without manager process.' ) return this_process = psutil . process ( os . getpid ( ) ) try : manager_process = psutil . process ( self . _process . pid ) except psutil . nosuchprocess : self . log . info ( ""manager process not running."" ) return # first try sigterm if manager_process . is_running ( ) and manager_process . pid in [ x . pid for x in this_process . children ( ) ] : self . log . info ( ""terminating manager process: %s"" , manager_process . pid ) manager_process . terminate ( ) # todo: remove magic number timeout = 5 self . log . info ( ""waiting up to %ss for manager process to exit..."" , timeout ) try : psutil . wait_procs ( { manager_process } , timeout ) except psutil . timeoutexpired : self . log . debug ( ""ran out of time while waiting for "" ""processes to exit"" ) # then sigkill if manager_process . is_running ( ) and manager_process . pid in [ x . pid for x in this_process . children ( ) ] : self . log . info ( ""killing manager process: %s"" , manager_process . pid ) manager_process . kill ( ) manager_process . wait ( )"
183,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L779-L787,"def _exit_gracefully(self, signum, frame):
        """"""
        Helper method to clean up DAG file processors to avoid leaving orphan processes.
        """"""
        self.log.info(""Exiting gracefully upon receiving signal %s"", signum)
        self.terminate()
        self.end()
        self.log.debug(""Finished terminating DAG processors."")
        sys.exit(os.EX_OK)","['def', '_exit_gracefully', '(', 'self', ',', 'signum', ',', 'frame', ')', ':', 'self', '.', 'log', '.', 'info', '(', '""Exiting gracefully upon receiving signal %s""', ',', 'signum', ')', 'self', '.', 'terminate', '(', ')', 'self', '.', 'end', '(', ')', 'self', '.', 'log', '.', 'debug', '(', '""Finished terminating DAG processors.""', ')', 'sys', '.', 'exit', '(', 'os', '.', 'EX_OK', ')']",Helper method to clean up DAG file processors to avoid leaving orphan processes.,"['Helper', 'method', 'to', 'clean', 'up', 'DAG', 'file', 'processors', 'to', 'avoid', 'leaving', 'orphan', 'processes', '.']",python,test,"['helper', 'method', 'to', 'clean', 'up', 'dag', 'file', 'processors', 'to', 'avoid', 'leaving', 'orphan', 'processes', '.']",helper method to clean up dag file processors to avoid leaving orphan processes .,"['def', '_exit_gracefully', '(', 'self', ',', 'signum', ',', 'frame', ')', ':', 'self', '.', 'log', '.', 'info', '(', '""exiting gracefully upon receiving signal %s""', ',', 'signum', ')', 'self', '.', 'terminate', '(', ')', 'self', '.', 'end', '(', ')', 'self', '.', 'log', '.', 'debug', '(', '""finished terminating dag processors.""', ')', 'sys', '.', 'exit', '(', 'os', '.', 'ex_ok', ')']","def _exit_gracefully ( self , signum , frame ) : self . log . info ( ""exiting gracefully upon receiving signal %s"" , signum ) self . terminate ( ) self . end ( ) self . log . debug ( ""finished terminating dag processors."" ) sys . exit ( os . ex_ok )"
184,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L789-L808,"def start(self):
        """"""
        Use multiple processes to parse and generate tasks for the
        DAGs in parallel. By processing them in separate processes,
        we can get parallelism and isolation from potentially harmful
        user code.
        """"""

        self.log.info(""Processing files using up to %s processes at a time "", self._parallelism)
        self.log.info(""Process each file at most once every %s seconds"", self._file_process_interval)
        self.log.info(
            ""Checking for new files in %s every %s seconds"", self._dag_directory, self.dag_dir_list_interval
        )

        if self._async_mode:
            self.log.debug(""Starting DagFileProcessorManager in async mode"")
            self.start_in_async()
        else:
            self.log.debug(""Starting DagFileProcessorManager in sync mode"")
            self.start_in_sync()","['def', 'start', '(', 'self', ')', ':', 'self', '.', 'log', '.', 'info', '(', '""Processing files using up to %s processes at a time ""', ',', 'self', '.', '_parallelism', ')', 'self', '.', 'log', '.', 'info', '(', '""Process each file at most once every %s seconds""', ',', 'self', '.', '_file_process_interval', ')', 'self', '.', 'log', '.', 'info', '(', '""Checking for new files in %s every %s seconds""', ',', 'self', '.', '_dag_directory', ',', 'self', '.', 'dag_dir_list_interval', ')', 'if', 'self', '.', '_async_mode', ':', 'self', '.', 'log', '.', 'debug', '(', '""Starting DagFileProcessorManager in async mode""', ')', 'self', '.', 'start_in_async', '(', ')', 'else', ':', 'self', '.', 'log', '.', 'debug', '(', '""Starting DagFileProcessorManager in sync mode""', ')', 'self', '.', 'start_in_sync', '(', ')']","Use multiple processes to parse and generate tasks for the
        DAGs in parallel. By processing them in separate processes,
        we can get parallelism and isolation from potentially harmful
        user code.","['Use', 'multiple', 'processes', 'to', 'parse', 'and', 'generate', 'tasks', 'for', 'the', 'DAGs', 'in', 'parallel', '.', 'By', 'processing', 'them', 'in', 'separate', 'processes', 'we', 'can', 'get', 'parallelism', 'and', 'isolation', 'from', 'potentially', 'harmful', 'user', 'code', '.']",python,test,"['use', 'multiple', 'processes', 'to', 'parse', 'and', 'generate', 'tasks', 'for', 'the', 'dags', 'in', 'parallel', '.', 'by', 'processing', 'them', 'in', 'separate', 'processes', 'we', 'can', 'get', 'parallelism', 'and', 'isolation', 'from', 'potentially', 'harmful', 'user', 'code', '.']",use multiple processes to parse and generate tasks for the dags in parallel . by processing them in separate processes we can get parallelism and isolation from potentially harmful user code .,"['def', 'start', '(', 'self', ')', ':', 'self', '.', 'log', '.', 'info', '(', '""processing files using up to %s processes at a time ""', ',', 'self', '.', '_parallelism', ')', 'self', '.', 'log', '.', 'info', '(', '""process each file at most once every %s seconds""', ',', 'self', '.', '_file_process_interval', ')', 'self', '.', 'log', '.', 'info', '(', '""checking for new files in %s every %s seconds""', ',', 'self', '.', '_dag_directory', ',', 'self', '.', 'dag_dir_list_interval', ')', 'if', 'self', '.', '_async_mode', ':', 'self', '.', 'log', '.', 'debug', '(', '""starting dagfileprocessormanager in async mode""', ')', 'self', '.', 'start_in_async', '(', ')', 'else', ':', 'self', '.', 'log', '.', 'debug', '(', '""starting dagfileprocessormanager in sync mode""', ')', 'self', '.', 'start_in_sync', '(', ')']","def start ( self ) : self . log . info ( ""processing files using up to %s processes at a time "" , self . _parallelism ) self . log . info ( ""process each file at most once every %s seconds"" , self . _file_process_interval ) self . log . info ( ""checking for new files in %s every %s seconds"" , self . _dag_directory , self . dag_dir_list_interval ) if self . _async_mode : self . log . debug ( ""starting dagfileprocessormanager in async mode"" ) self . start_in_async ( ) else : self . log . debug ( ""starting dagfileprocessormanager in sync mode"" ) self . start_in_sync ( )"
185,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L810-L854,"def start_in_async(self):
        """"""
        Parse DAG files repeatedly in a standalone loop.
        """"""
        while True:
            loop_start_time = time.time()

            if self._signal_conn.poll():
                agent_signal = self._signal_conn.recv()
                if agent_signal == DagParsingSignal.TERMINATE_MANAGER:
                    self.terminate()
                    break
                elif agent_signal == DagParsingSignal.END_MANAGER:
                    self.end()
                    sys.exit(os.EX_OK)

            self._refresh_dag_dir()

            simple_dags = self.heartbeat()
            for simple_dag in simple_dags:
                self._result_queue.put(simple_dag)

            self._print_stat()

            all_files_processed = all(self.get_last_finish_time(x) is not None
                                      for x in self.file_paths)
            max_runs_reached = self.max_runs_reached()

            dag_parsing_stat = DagParsingStat(self._file_paths,
                                              self.get_all_pids(),
                                              max_runs_reached,
                                              all_files_processed,
                                              len(simple_dags))
            self._stat_queue.put(dag_parsing_stat)

            if max_runs_reached:
                self.log.info(""Exiting dag parsing loop as all files ""
                              ""have been processed %s times"", self._max_runs)
                break

            loop_duration = time.time() - loop_start_time
            if loop_duration < 1:
                sleep_length = 1 - loop_duration
                self.log.debug(""Sleeping for %.2f seconds to prevent excessive logging"", sleep_length)
                time.sleep(sleep_length)","['def', 'start_in_async', '(', 'self', ')', ':', 'while', 'True', ':', 'loop_start_time', '=', 'time', '.', 'time', '(', ')', 'if', 'self', '.', '_signal_conn', '.', 'poll', '(', ')', ':', 'agent_signal', '=', 'self', '.', '_signal_conn', '.', 'recv', '(', ')', 'if', 'agent_signal', '==', 'DagParsingSignal', '.', 'TERMINATE_MANAGER', ':', 'self', '.', 'terminate', '(', ')', 'break', 'elif', 'agent_signal', '==', 'DagParsingSignal', '.', 'END_MANAGER', ':', 'self', '.', 'end', '(', ')', 'sys', '.', 'exit', '(', 'os', '.', 'EX_OK', ')', 'self', '.', '_refresh_dag_dir', '(', ')', 'simple_dags', '=', 'self', '.', 'heartbeat', '(', ')', 'for', 'simple_dag', 'in', 'simple_dags', ':', 'self', '.', '_result_queue', '.', 'put', '(', 'simple_dag', ')', 'self', '.', '_print_stat', '(', ')', 'all_files_processed', '=', 'all', '(', 'self', '.', 'get_last_finish_time', '(', 'x', ')', 'is', 'not', 'None', 'for', 'x', 'in', 'self', '.', 'file_paths', ')', 'max_runs_reached', '=', 'self', '.', 'max_runs_reached', '(', ')', 'dag_parsing_stat', '=', 'DagParsingStat', '(', 'self', '.', '_file_paths', ',', 'self', '.', 'get_all_pids', '(', ')', ',', 'max_runs_reached', ',', 'all_files_processed', ',', 'len', '(', 'simple_dags', ')', ')', 'self', '.', '_stat_queue', '.', 'put', '(', 'dag_parsing_stat', ')', 'if', 'max_runs_reached', ':', 'self', '.', 'log', '.', 'info', '(', '""Exiting dag parsing loop as all files ""', '""have been processed %s times""', ',', 'self', '.', '_max_runs', ')', 'break', 'loop_duration', '=', 'time', '.', 'time', '(', ')', '-', 'loop_start_time', 'if', 'loop_duration', '<', '1', ':', 'sleep_length', '=', '1', '-', 'loop_duration', 'self', '.', 'log', '.', 'debug', '(', '""Sleeping for %.2f seconds to prevent excessive logging""', ',', 'sleep_length', ')', 'time', '.', 'sleep', '(', 'sleep_length', ')']",Parse DAG files repeatedly in a standalone loop.,"['Parse', 'DAG', 'files', 'repeatedly', 'in', 'a', 'standalone', 'loop', '.']",python,test,"['parse', 'dag', 'files', 'repeatedly', 'in', 'a', 'standalone', 'loop', '.']",parse dag files repeatedly in a standalone loop .,"['def', 'start_in_async', '(', 'self', ')', ':', 'while', 'true', ':', 'loop_start_time', '=', 'time', '.', 'time', '(', ')', 'if', 'self', '.', '_signal_conn', '.', 'poll', '(', ')', ':', 'agent_signal', '=', 'self', '.', '_signal_conn', '.', 'recv', '(', ')', 'if', 'agent_signal', '==', 'dagparsingsignal', '.', 'terminate_manager', ':', 'self', '.', 'terminate', '(', ')', 'break', 'elif', 'agent_signal', '==', 'dagparsingsignal', '.', 'end_manager', ':', 'self', '.', 'end', '(', ')', 'sys', '.', 'exit', '(', 'os', '.', 'ex_ok', ')', 'self', '.', '_refresh_dag_dir', '(', ')', 'simple_dags', '=', 'self', '.', 'heartbeat', '(', ')', 'for', 'simple_dag', 'in', 'simple_dags', ':', 'self', '.', '_result_queue', '.', 'put', '(', 'simple_dag', ')', 'self', '.', '_print_stat', '(', ')', 'all_files_processed', '=', 'all', '(', 'self', '.', 'get_last_finish_time', '(', 'x', ')', 'is', 'not', 'none', 'for', 'x', 'in', 'self', '.', 'file_paths', ')', 'max_runs_reached', '=', 'self', '.', 'max_runs_reached', '(', ')', 'dag_parsing_stat', '=', 'dagparsingstat', '(', 'self', '.', '_file_paths', ',', 'self', '.', 'get_all_pids', '(', ')', ',', 'max_runs_reached', ',', 'all_files_processed', ',', 'len', '(', 'simple_dags', ')', ')', 'self', '.', '_stat_queue', '.', 'put', '(', 'dag_parsing_stat', ')', 'if', 'max_runs_reached', ':', 'self', '.', 'log', '.', 'info', '(', '""exiting dag parsing loop as all files ""', '""have been processed %s times""', ',', 'self', '.', '_max_runs', ')', 'break', 'loop_duration', '=', 'time', '.', 'time', '(', ')', '-', 'loop_start_time', 'if', 'loop_duration', '<', '1', ':', 'sleep_length', '=', '1', '-', 'loop_duration', 'self', '.', 'log', '.', 'debug', '(', '""sleeping for %.2f seconds to prevent excessive logging""', ',', 'sleep_length', ')', 'time', '.', 'sleep', '(', 'sleep_length', ')']","def start_in_async ( self ) : while true : loop_start_time = time . time ( ) if self . _signal_conn . poll ( ) : agent_signal = self . _signal_conn . recv ( ) if agent_signal == dagparsingsignal . terminate_manager : self . terminate ( ) break elif agent_signal == dagparsingsignal . end_manager : self . end ( ) sys . exit ( os . ex_ok ) self . _refresh_dag_dir ( ) simple_dags = self . heartbeat ( ) for simple_dag in simple_dags : self . _result_queue . put ( simple_dag ) self . _print_stat ( ) all_files_processed = all ( self . get_last_finish_time ( x ) is not none for x in self . file_paths ) max_runs_reached = self . max_runs_reached ( ) dag_parsing_stat = dagparsingstat ( self . _file_paths , self . get_all_pids ( ) , max_runs_reached , all_files_processed , len ( simple_dags ) ) self . _stat_queue . put ( dag_parsing_stat ) if max_runs_reached : self . log . info ( ""exiting dag parsing loop as all files "" ""have been processed %s times"" , self . _max_runs ) break loop_duration = time . time ( ) - loop_start_time if loop_duration < 1 : sleep_length = 1 - loop_duration self . log . debug ( ""sleeping for %.2f seconds to prevent excessive logging"" , sleep_length ) time . sleep ( sleep_length )"
186,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L856-L898,"def start_in_sync(self):
        """"""
        Parse DAG files in a loop controlled by DagParsingSignal.
        Actual DAG parsing loop will run once upon receiving one
        agent heartbeat message and will report done when finished the loop.
        """"""
        while True:
            agent_signal = self._signal_conn.recv()
            if agent_signal == DagParsingSignal.TERMINATE_MANAGER:
                self.terminate()
                break
            elif agent_signal == DagParsingSignal.END_MANAGER:
                self.end()
                sys.exit(os.EX_OK)
            elif agent_signal == DagParsingSignal.AGENT_HEARTBEAT:

                self._refresh_dag_dir()

                simple_dags = self.heartbeat()
                for simple_dag in simple_dags:
                    self._result_queue.put(simple_dag)

                self._print_stat()

                all_files_processed = all(self.get_last_finish_time(x) is not None
                                          for x in self.file_paths)
                max_runs_reached = self.max_runs_reached()

                dag_parsing_stat = DagParsingStat(self._file_paths,
                                                  self.get_all_pids(),
                                                  self.max_runs_reached(),
                                                  all_files_processed,
                                                  len(simple_dags))
                self._stat_queue.put(dag_parsing_stat)

                self.wait_until_finished()
                self._signal_conn.send(DagParsingSignal.MANAGER_DONE)

                if max_runs_reached:
                    self.log.info(""Exiting dag parsing loop as all files ""
                                  ""have been processed %s times"", self._max_runs)
                    self._signal_conn.send(DagParsingSignal.MANAGER_DONE)
                    break","['def', 'start_in_sync', '(', 'self', ')', ':', 'while', 'True', ':', 'agent_signal', '=', 'self', '.', '_signal_conn', '.', 'recv', '(', ')', 'if', 'agent_signal', '==', 'DagParsingSignal', '.', 'TERMINATE_MANAGER', ':', 'self', '.', 'terminate', '(', ')', 'break', 'elif', 'agent_signal', '==', 'DagParsingSignal', '.', 'END_MANAGER', ':', 'self', '.', 'end', '(', ')', 'sys', '.', 'exit', '(', 'os', '.', 'EX_OK', ')', 'elif', 'agent_signal', '==', 'DagParsingSignal', '.', 'AGENT_HEARTBEAT', ':', 'self', '.', '_refresh_dag_dir', '(', ')', 'simple_dags', '=', 'self', '.', 'heartbeat', '(', ')', 'for', 'simple_dag', 'in', 'simple_dags', ':', 'self', '.', '_result_queue', '.', 'put', '(', 'simple_dag', ')', 'self', '.', '_print_stat', '(', ')', 'all_files_processed', '=', 'all', '(', 'self', '.', 'get_last_finish_time', '(', 'x', ')', 'is', 'not', 'None', 'for', 'x', 'in', 'self', '.', 'file_paths', ')', 'max_runs_reached', '=', 'self', '.', 'max_runs_reached', '(', ')', 'dag_parsing_stat', '=', 'DagParsingStat', '(', 'self', '.', '_file_paths', ',', 'self', '.', 'get_all_pids', '(', ')', ',', 'self', '.', 'max_runs_reached', '(', ')', ',', 'all_files_processed', ',', 'len', '(', 'simple_dags', ')', ')', 'self', '.', '_stat_queue', '.', 'put', '(', 'dag_parsing_stat', ')', 'self', '.', 'wait_until_finished', '(', ')', 'self', '.', '_signal_conn', '.', 'send', '(', 'DagParsingSignal', '.', 'MANAGER_DONE', ')', 'if', 'max_runs_reached', ':', 'self', '.', 'log', '.', 'info', '(', '""Exiting dag parsing loop as all files ""', '""have been processed %s times""', ',', 'self', '.', '_max_runs', ')', 'self', '.', '_signal_conn', '.', 'send', '(', 'DagParsingSignal', '.', 'MANAGER_DONE', ')', 'break']","Parse DAG files in a loop controlled by DagParsingSignal.
        Actual DAG parsing loop will run once upon receiving one
        agent heartbeat message and will report done when finished the loop.","['Parse', 'DAG', 'files', 'in', 'a', 'loop', 'controlled', 'by', 'DagParsingSignal', '.', 'Actual', 'DAG', 'parsing', 'loop', 'will', 'run', 'once', 'upon', 'receiving', 'one', 'agent', 'heartbeat', 'message', 'and', 'will', 'report', 'done', 'when', 'finished', 'the', 'loop', '.']",python,test,"['parse', 'dag', 'files', 'in', 'a', 'loop', 'controlled', 'by', 'dagparsingsignal', '.', 'actual', 'dag', 'parsing', 'loop', 'will', 'run', 'once', 'upon', 'receiving', 'one', 'agent', 'heartbeat', 'message', 'and', 'will', 'report', 'done', 'when', 'finished', 'the', 'loop', '.']",parse dag files in a loop controlled by dagparsingsignal . actual dag parsing loop will run once upon receiving one agent heartbeat message and will report done when finished the loop .,"['def', 'start_in_sync', '(', 'self', ')', ':', 'while', 'true', ':', 'agent_signal', '=', 'self', '.', '_signal_conn', '.', 'recv', '(', ')', 'if', 'agent_signal', '==', 'dagparsingsignal', '.', 'terminate_manager', ':', 'self', '.', 'terminate', '(', ')', 'break', 'elif', 'agent_signal', '==', 'dagparsingsignal', '.', 'end_manager', ':', 'self', '.', 'end', '(', ')', 'sys', '.', 'exit', '(', 'os', '.', 'ex_ok', ')', 'elif', 'agent_signal', '==', 'dagparsingsignal', '.', 'agent_heartbeat', ':', 'self', '.', '_refresh_dag_dir', '(', ')', 'simple_dags', '=', 'self', '.', 'heartbeat', '(', ')', 'for', 'simple_dag', 'in', 'simple_dags', ':', 'self', '.', '_result_queue', '.', 'put', '(', 'simple_dag', ')', 'self', '.', '_print_stat', '(', ')', 'all_files_processed', '=', 'all', '(', 'self', '.', 'get_last_finish_time', '(', 'x', ')', 'is', 'not', 'none', 'for', 'x', 'in', 'self', '.', 'file_paths', ')', 'max_runs_reached', '=', 'self', '.', 'max_runs_reached', '(', ')', 'dag_parsing_stat', '=', 'dagparsingstat', '(', 'self', '.', '_file_paths', ',', 'self', '.', 'get_all_pids', '(', ')', ',', 'self', '.', 'max_runs_reached', '(', ')', ',', 'all_files_processed', ',', 'len', '(', 'simple_dags', ')', ')', 'self', '.', '_stat_queue', '.', 'put', '(', 'dag_parsing_stat', ')', 'self', '.', 'wait_until_finished', '(', ')', 'self', '.', '_signal_conn', '.', 'send', '(', 'dagparsingsignal', '.', 'manager_done', ')', 'if', 'max_runs_reached', ':', 'self', '.', 'log', '.', 'info', '(', '""exiting dag parsing loop as all files ""', '""have been processed %s times""', ',', 'self', '.', '_max_runs', ')', 'self', '.', '_signal_conn', '.', 'send', '(', 'dagparsingsignal', '.', 'manager_done', ')', 'break']","def start_in_sync ( self ) : while true : agent_signal = self . _signal_conn . recv ( ) if agent_signal == dagparsingsignal . terminate_manager : self . terminate ( ) break elif agent_signal == dagparsingsignal . end_manager : self . end ( ) sys . exit ( os . ex_ok ) elif agent_signal == dagparsingsignal . agent_heartbeat : self . _refresh_dag_dir ( ) simple_dags = self . heartbeat ( ) for simple_dag in simple_dags : self . _result_queue . put ( simple_dag ) self . _print_stat ( ) all_files_processed = all ( self . get_last_finish_time ( x ) is not none for x in self . file_paths ) max_runs_reached = self . max_runs_reached ( ) dag_parsing_stat = dagparsingstat ( self . _file_paths , self . get_all_pids ( ) , self . max_runs_reached ( ) , all_files_processed , len ( simple_dags ) ) self . _stat_queue . put ( dag_parsing_stat ) self . wait_until_finished ( ) self . _signal_conn . send ( dagparsingsignal . manager_done ) if max_runs_reached : self . log . info ( ""exiting dag parsing loop as all files "" ""have been processed %s times"" , self . _max_runs ) self . _signal_conn . send ( dagparsingsignal . manager_done ) break"
187,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L900-L918,"def _refresh_dag_dir(self):
        """"""
        Refresh file paths from dag dir if we haven't done it for too long.
        """"""
        elapsed_time_since_refresh = (timezone.utcnow() -
                                      self.last_dag_dir_refresh_time).total_seconds()
        if elapsed_time_since_refresh > self.dag_dir_list_interval:
            # Build up a list of Python files that could contain DAGs
            self.log.info(""Searching for files in %s"", self._dag_directory)
            self._file_paths = list_py_file_paths(self._dag_directory)
            self.last_dag_dir_refresh_time = timezone.utcnow()
            self.log.info(""There are %s files in %s"", len(self._file_paths), self._dag_directory)
            self.set_file_paths(self._file_paths)

            try:
                self.log.debug(""Removing old import errors"")
                self.clear_nonexistent_import_errors()
            except Exception:
                self.log.exception(""Error removing old import errors"")","['def', '_refresh_dag_dir', '(', 'self', ')', ':', 'elapsed_time_since_refresh', '=', '(', 'timezone', '.', 'utcnow', '(', ')', '-', 'self', '.', 'last_dag_dir_refresh_time', ')', '.', 'total_seconds', '(', ')', 'if', 'elapsed_time_since_refresh', '>', 'self', '.', 'dag_dir_list_interval', ':', '# Build up a list of Python files that could contain DAGs', 'self', '.', 'log', '.', 'info', '(', '""Searching for files in %s""', ',', 'self', '.', '_dag_directory', ')', 'self', '.', '_file_paths', '=', 'list_py_file_paths', '(', 'self', '.', '_dag_directory', ')', 'self', '.', 'last_dag_dir_refresh_time', '=', 'timezone', '.', 'utcnow', '(', ')', 'self', '.', 'log', '.', 'info', '(', '""There are %s files in %s""', ',', 'len', '(', 'self', '.', '_file_paths', ')', ',', 'self', '.', '_dag_directory', ')', 'self', '.', 'set_file_paths', '(', 'self', '.', '_file_paths', ')', 'try', ':', 'self', '.', 'log', '.', 'debug', '(', '""Removing old import errors""', ')', 'self', '.', 'clear_nonexistent_import_errors', '(', ')', 'except', 'Exception', ':', 'self', '.', 'log', '.', 'exception', '(', '""Error removing old import errors""', ')']",Refresh file paths from dag dir if we haven't done it for too long.,"['Refresh', 'file', 'paths', 'from', 'dag', 'dir', 'if', 'we', 'haven', 't', 'done', 'it', 'for', 'too', 'long', '.']",python,test,"['refresh', 'file', 'paths', 'from', 'dag', 'dir', 'if', 'we', 'haven', 't', 'done', 'it', 'for', 'too', 'long', '.']",refresh file paths from dag dir if we haven t done it for too long .,"['def', '_refresh_dag_dir', '(', 'self', ')', ':', 'elapsed_time_since_refresh', '=', '(', 'timezone', '.', 'utcnow', '(', ')', '-', 'self', '.', 'last_dag_dir_refresh_time', ')', '.', 'total_seconds', '(', ')', 'if', 'elapsed_time_since_refresh', '>', 'self', '.', 'dag_dir_list_interval', ':', '# build up a list of python files that could contain dags', 'self', '.', 'log', '.', 'info', '(', '""searching for files in %s""', ',', 'self', '.', '_dag_directory', ')', 'self', '.', '_file_paths', '=', 'list_py_file_paths', '(', 'self', '.', '_dag_directory', ')', 'self', '.', 'last_dag_dir_refresh_time', '=', 'timezone', '.', 'utcnow', '(', ')', 'self', '.', 'log', '.', 'info', '(', '""there are %s files in %s""', ',', 'len', '(', 'self', '.', '_file_paths', ')', ',', 'self', '.', '_dag_directory', ')', 'self', '.', 'set_file_paths', '(', 'self', '.', '_file_paths', ')', 'try', ':', 'self', '.', 'log', '.', 'debug', '(', '""removing old import errors""', ')', 'self', '.', 'clear_nonexistent_import_errors', '(', ')', 'except', 'exception', ':', 'self', '.', 'log', '.', 'exception', '(', '""error removing old import errors""', ')']","def _refresh_dag_dir ( self ) : elapsed_time_since_refresh = ( timezone . utcnow ( ) - self . last_dag_dir_refresh_time ) . total_seconds ( ) if elapsed_time_since_refresh > self . dag_dir_list_interval : # build up a list of python files that could contain dags self . log . info ( ""searching for files in %s"" , self . _dag_directory ) self . _file_paths = list_py_file_paths ( self . _dag_directory ) self . last_dag_dir_refresh_time = timezone . utcnow ( ) self . log . info ( ""there are %s files in %s"" , len ( self . _file_paths ) , self . _dag_directory ) self . set_file_paths ( self . _file_paths ) try : self . log . debug ( ""removing old import errors"" ) self . clear_nonexistent_import_errors ( ) except exception : self . log . exception ( ""error removing old import errors"" )"
188,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L920-L928,"def _print_stat(self):
        """"""
        Occasionally print out stats about how fast the files are getting processed
        """"""
        if ((timezone.utcnow() - self.last_stat_print_time).total_seconds() >
                self.print_stats_interval):
            if len(self._file_paths) > 0:
                self._log_file_processing_stats(self._file_paths)
            self.last_stat_print_time = timezone.utcnow()","['def', '_print_stat', '(', 'self', ')', ':', 'if', '(', '(', 'timezone', '.', 'utcnow', '(', ')', '-', 'self', '.', 'last_stat_print_time', ')', '.', 'total_seconds', '(', ')', '>', 'self', '.', 'print_stats_interval', ')', ':', 'if', 'len', '(', 'self', '.', '_file_paths', ')', '>', '0', ':', 'self', '.', '_log_file_processing_stats', '(', 'self', '.', '_file_paths', ')', 'self', '.', 'last_stat_print_time', '=', 'timezone', '.', 'utcnow', '(', ')']",Occasionally print out stats about how fast the files are getting processed,"['Occasionally', 'print', 'out', 'stats', 'about', 'how', 'fast', 'the', 'files', 'are', 'getting', 'processed']",python,test,"['occasionally', 'print', 'out', 'stats', 'about', 'how', 'fast', 'the', 'files', 'are', 'getting', 'processed']",occasionally print out stats about how fast the files are getting processed,"['def', '_print_stat', '(', 'self', ')', ':', 'if', '(', '(', 'timezone', '.', 'utcnow', '(', ')', '-', 'self', '.', 'last_stat_print_time', ')', '.', 'total_seconds', '(', ')', '>', 'self', '.', 'print_stats_interval', ')', ':', 'if', 'len', '(', 'self', '.', '_file_paths', ')', '>', '0', ':', 'self', '.', '_log_file_processing_stats', '(', 'self', '.', '_file_paths', ')', 'self', '.', 'last_stat_print_time', '=', 'timezone', '.', 'utcnow', '(', ')']",def _print_stat ( self ) : if ( ( timezone . utcnow ( ) - self . last_stat_print_time ) . total_seconds ( ) > self . print_stats_interval ) : if len ( self . _file_paths ) > 0 : self . _log_file_processing_stats ( self . _file_paths ) self . last_stat_print_time = timezone . utcnow ( )
189,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L931-L944,"def clear_nonexistent_import_errors(self, session):
        """"""
        Clears import errors for files that no longer exist.

        :param session: session for ORM operations
        :type session: sqlalchemy.orm.session.Session
        """"""
        query = session.query(errors.ImportError)
        if self._file_paths:
            query = query.filter(
                ~errors.ImportError.filename.in_(self._file_paths)
            )
        query.delete(synchronize_session='fetch')
        session.commit()","['def', 'clear_nonexistent_import_errors', '(', 'self', ',', 'session', ')', ':', 'query', '=', 'session', '.', 'query', '(', 'errors', '.', 'ImportError', ')', 'if', 'self', '.', '_file_paths', ':', 'query', '=', 'query', '.', 'filter', '(', '~', 'errors', '.', 'ImportError', '.', 'filename', '.', 'in_', '(', 'self', '.', '_file_paths', ')', ')', 'query', '.', 'delete', '(', 'synchronize_session', '=', ""'fetch'"", ')', 'session', '.', 'commit', '(', ')']","Clears import errors for files that no longer exist.

        :param session: session for ORM operations
        :type session: sqlalchemy.orm.session.Session","['Clears', 'import', 'errors', 'for', 'files', 'that', 'no', 'longer', 'exist', '.']",python,test,"['clears', 'import', 'errors', 'for', 'files', 'that', 'no', 'longer', 'exist', '.']",clears import errors for files that no longer exist .,"['def', 'clear_nonexistent_import_errors', '(', 'self', ',', 'session', ')', ':', 'query', '=', 'session', '.', 'query', '(', 'errors', '.', 'importerror', ')', 'if', 'self', '.', '_file_paths', ':', 'query', '=', 'query', '.', 'filter', '(', '~', 'errors', '.', 'importerror', '.', 'filename', '.', 'in_', '(', 'self', '.', '_file_paths', ')', ')', 'query', '.', 'delete', '(', 'synchronize_session', '=', ""'fetch'"", ')', 'session', '.', 'commit', '(', ')']","def clear_nonexistent_import_errors ( self , session ) : query = session . query ( errors . importerror ) if self . _file_paths : query = query . filter ( ~ errors . importerror . filename . in_ ( self . _file_paths ) ) query . delete ( synchronize_session = 'fetch' ) session . commit ( )"
190,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L946-L1020,"def _log_file_processing_stats(self, known_file_paths):
        """"""
        Print out stats about how files are getting processed.

        :param known_file_paths: a list of file paths that may contain Airflow
            DAG definitions
        :type known_file_paths: list[unicode]
        :return: None
        """"""

        # File Path: Path to the file containing the DAG definition
        # PID: PID associated with the process that's processing the file. May
        # be empty.
        # Runtime: If the process is currently running, how long it's been
        # running for in seconds.
        # Last Runtime: If the process ran before, how long did it take to
        # finish in seconds
        # Last Run: When the file finished processing in the previous run.
        headers = [""File Path"",
                   ""PID"",
                   ""Runtime"",
                   ""Last Runtime"",
                   ""Last Run""]

        rows = []
        for file_path in known_file_paths:
            last_runtime = self.get_last_runtime(file_path)
            file_name = os.path.basename(file_path)
            file_name = os.path.splitext(file_name)[0].replace(os.sep, '.')
            if last_runtime:
                Stats.gauge(
                    'dag_processing.last_runtime.{}'.format(file_name),
                    last_runtime
                )

            processor_pid = self.get_pid(file_path)
            processor_start_time = self.get_start_time(file_path)
            runtime = ((timezone.utcnow() - processor_start_time).total_seconds()
                       if processor_start_time else None)
            last_run = self.get_last_finish_time(file_path)
            if last_run:
                seconds_ago = (timezone.utcnow() - last_run).total_seconds()
                Stats.gauge(
                    'dag_processing.last_run.seconds_ago.{}'.format(file_name),
                    seconds_ago
                )

            rows.append((file_path,
                         processor_pid,
                         runtime,
                         last_runtime,
                         last_run))

        # Sort by longest last runtime. (Can't sort None values in python3)
        rows = sorted(rows, key=lambda x: x[3] or 0.0)

        formatted_rows = []
        for file_path, pid, runtime, last_runtime, last_run in rows:
            formatted_rows.append((file_path,
                                   pid,
                                   ""{:.2f}s"".format(runtime)
                                   if runtime else None,
                                   ""{:.2f}s"".format(last_runtime)
                                   if last_runtime else None,
                                   last_run.strftime(""%Y-%m-%dT%H:%M:%S"")
                                   if last_run else None))
        log_str = (""\n"" +
                   ""="" * 80 +
                   ""\n"" +
                   ""DAG File Processing Stats\n\n"" +
                   tabulate(formatted_rows, headers=headers) +
                   ""\n"" +
                   ""="" * 80)

        self.log.info(log_str)","['def', '_log_file_processing_stats', '(', 'self', ',', 'known_file_paths', ')', ':', '# File Path: Path to the file containing the DAG definition', ""# PID: PID associated with the process that's processing the file. May"", '# be empty.', ""# Runtime: If the process is currently running, how long it's been"", '# running for in seconds.', '# Last Runtime: If the process ran before, how long did it take to', '# finish in seconds', '# Last Run: When the file finished processing in the previous run.', 'headers', '=', '[', '""File Path""', ',', '""PID""', ',', '""Runtime""', ',', '""Last Runtime""', ',', '""Last Run""', ']', 'rows', '=', '[', ']', 'for', 'file_path', 'in', 'known_file_paths', ':', 'last_runtime', '=', 'self', '.', 'get_last_runtime', '(', 'file_path', ')', 'file_name', '=', 'os', '.', 'path', '.', 'basename', '(', 'file_path', ')', 'file_name', '=', 'os', '.', 'path', '.', 'splitext', '(', 'file_name', ')', '[', '0', ']', '.', 'replace', '(', 'os', '.', 'sep', ',', ""'.'"", ')', 'if', 'last_runtime', ':', 'Stats', '.', 'gauge', '(', ""'dag_processing.last_runtime.{}'"", '.', 'format', '(', 'file_name', ')', ',', 'last_runtime', ')', 'processor_pid', '=', 'self', '.', 'get_pid', '(', 'file_path', ')', 'processor_start_time', '=', 'self', '.', 'get_start_time', '(', 'file_path', ')', 'runtime', '=', '(', '(', 'timezone', '.', 'utcnow', '(', ')', '-', 'processor_start_time', ')', '.', 'total_seconds', '(', ')', 'if', 'processor_start_time', 'else', 'None', ')', 'last_run', '=', 'self', '.', 'get_last_finish_time', '(', 'file_path', ')', 'if', 'last_run', ':', 'seconds_ago', '=', '(', 'timezone', '.', 'utcnow', '(', ')', '-', 'last_run', ')', '.', 'total_seconds', '(', ')', 'Stats', '.', 'gauge', '(', ""'dag_processing.last_run.seconds_ago.{}'"", '.', 'format', '(', 'file_name', ')', ',', 'seconds_ago', ')', 'rows', '.', 'append', '(', '(', 'file_path', ',', 'processor_pid', ',', 'runtime', ',', 'last_runtime', ',', 'last_run', ')', ')', ""# Sort by longest last runtime. (Can't sort None values in python3)"", 'rows', '=', 'sorted', '(', 'rows', ',', 'key', '=', 'lambda', 'x', ':', 'x', '[', '3', ']', 'or', '0.0', ')', 'formatted_rows', '=', '[', ']', 'for', 'file_path', ',', 'pid', ',', 'runtime', ',', 'last_runtime', ',', 'last_run', 'in', 'rows', ':', 'formatted_rows', '.', 'append', '(', '(', 'file_path', ',', 'pid', ',', '""{:.2f}s""', '.', 'format', '(', 'runtime', ')', 'if', 'runtime', 'else', 'None', ',', '""{:.2f}s""', '.', 'format', '(', 'last_runtime', ')', 'if', 'last_runtime', 'else', 'None', ',', 'last_run', '.', 'strftime', '(', '""%Y-%m-%dT%H:%M:%S""', ')', 'if', 'last_run', 'else', 'None', ')', ')', 'log_str', '=', '(', '""\\n""', '+', '""=""', '*', '80', '+', '""\\n""', '+', '""DAG File Processing Stats\\n\\n""', '+', 'tabulate', '(', 'formatted_rows', ',', 'headers', '=', 'headers', ')', '+', '""\\n""', '+', '""=""', '*', '80', ')', 'self', '.', 'log', '.', 'info', '(', 'log_str', ')']","Print out stats about how files are getting processed.

        :param known_file_paths: a list of file paths that may contain Airflow
            DAG definitions
        :type known_file_paths: list[unicode]
        :return: None","['Print', 'out', 'stats', 'about', 'how', 'files', 'are', 'getting', 'processed', '.']",python,test,"['print', 'out', 'stats', 'about', 'how', 'files', 'are', 'getting', 'processed', '.']",print out stats about how files are getting processed .,"['def', '_log_file_processing_stats', '(', 'self', ',', 'known_file_paths', ')', ':', '# file path: path to the file containing the dag definition', ""# pid: pid associated with the process that's processing the file. may"", '# be empty.', ""# runtime: if the process is currently running, how long it's been"", '# running for in seconds.', '# last runtime: if the process ran before, how long did it take to', '# finish in seconds', '# last run: when the file finished processing in the previous run.', 'headers', '=', '[', '""file path""', ',', '""pid""', ',', '""runtime""', ',', '""last runtime""', ',', '""last run""', ']', 'rows', '=', '[', ']', 'for', 'file_path', 'in', 'known_file_paths', ':', 'last_runtime', '=', 'self', '.', 'get_last_runtime', '(', 'file_path', ')', 'file_name', '=', 'os', '.', 'path', '.', 'basename', '(', 'file_path', ')', 'file_name', '=', 'os', '.', 'path', '.', 'splitext', '(', 'file_name', ')', '[', '0', ']', '.', 'replace', '(', 'os', '.', 'sep', ',', ""'.'"", ')', 'if', 'last_runtime', ':', 'stats', '.', 'gauge', '(', ""'dag_processing.last_runtime.{}'"", '.', 'format', '(', 'file_name', ')', ',', 'last_runtime', ')', 'processor_pid', '=', 'self', '.', 'get_pid', '(', 'file_path', ')', 'processor_start_time', '=', 'self', '.', 'get_start_time', '(', 'file_path', ')', 'runtime', '=', '(', '(', 'timezone', '.', 'utcnow', '(', ')', '-', 'processor_start_time', ')', '.', 'total_seconds', '(', ')', 'if', 'processor_start_time', 'else', 'none', ')', 'last_run', '=', 'self', '.', 'get_last_finish_time', '(', 'file_path', ')', 'if', 'last_run', ':', 'seconds_ago', '=', '(', 'timezone', '.', 'utcnow', '(', ')', '-', 'last_run', ')', '.', 'total_seconds', '(', ')', 'stats', '.', 'gauge', '(', ""'dag_processing.last_run.seconds_ago.{}'"", '.', 'format', '(', 'file_name', ')', ',', 'seconds_ago', ')', 'rows', '.', 'append', '(', '(', 'file_path', ',', 'processor_pid', ',', 'runtime', ',', 'last_runtime', ',', 'last_run', ')', ')', ""# sort by longest last runtime. (can't sort none values in python3)"", 'rows', '=', 'sorted', '(', 'rows', ',', 'key', '=', 'lambda', 'x', ':', 'x', '[', '3', ']', 'or', '0.0', ')', 'formatted_rows', '=', '[', ']', 'for', 'file_path', ',', 'pid', ',', 'runtime', ',', 'last_runtime', ',', 'last_run', 'in', 'rows', ':', 'formatted_rows', '.', 'append', '(', '(', 'file_path', ',', 'pid', ',', '""{:.2f}s""', '.', 'format', '(', 'runtime', ')', 'if', 'runtime', 'else', 'none', ',', '""{:.2f}s""', '.', 'format', '(', 'last_runtime', ')', 'if', 'last_runtime', 'else', 'none', ',', 'last_run', '.', 'strftime', '(', '""%y-%m-%dt%h:%m:%s""', ')', 'if', 'last_run', 'else', 'none', ')', ')', 'log_str', '=', '(', '""\\n""', '+', '""=""', '*', '80', '+', '""\\n""', '+', '""dag file processing stats\\n\\n""', '+', 'tabulate', '(', 'formatted_rows', ',', 'headers', '=', 'headers', ')', '+', '""\\n""', '+', '""=""', '*', '80', ')', 'self', '.', 'log', '.', 'info', '(', 'log_str', ')']","def _log_file_processing_stats ( self , known_file_paths ) : # file path: path to the file containing the dag definition # pid: pid associated with the process that's processing the file. may # be empty. # runtime: if the process is currently running, how long it's been # running for in seconds. # last runtime: if the process ran before, how long did it take to # finish in seconds # last run: when the file finished processing in the previous run. headers = [ ""file path"" , ""pid"" , ""runtime"" , ""last runtime"" , ""last run"" ] rows = [ ] for file_path in known_file_paths : last_runtime = self . get_last_runtime ( file_path ) file_name = os . path . basename ( file_path ) file_name = os . path . splitext ( file_name ) [ 0 ] . replace ( os . sep , '.' ) if last_runtime : stats . gauge ( 'dag_processing.last_runtime.{}' . format ( file_name ) , last_runtime ) processor_pid = self . get_pid ( file_path ) processor_start_time = self . get_start_time ( file_path ) runtime = ( ( timezone . utcnow ( ) - processor_start_time ) . total_seconds ( ) if processor_start_time else none ) last_run = self . get_last_finish_time ( file_path ) if last_run : seconds_ago = ( timezone . utcnow ( ) - last_run ) . total_seconds ( ) stats . gauge ( 'dag_processing.last_run.seconds_ago.{}' . format ( file_name ) , seconds_ago ) rows . append ( ( file_path , processor_pid , runtime , last_runtime , last_run ) ) # sort by longest last runtime. (can't sort none values in python3) rows = sorted ( rows , key = lambda x : x [ 3 ] or 0.0 ) formatted_rows = [ ] for file_path , pid , runtime , last_runtime , last_run in rows : formatted_rows . append ( ( file_path , pid , ""{:.2f}s"" . format ( runtime ) if runtime else none , ""{:.2f}s"" . format ( last_runtime ) if last_runtime else none , last_run . strftime ( ""%y-%m-%dt%h:%m:%s"" ) if last_run else none ) ) log_str = ( ""\n"" + ""="" * 80 + ""\n"" + ""dag file processing stats\n\n"" + tabulate ( formatted_rows , headers = headers ) + ""\n"" + ""="" * 80 ) self . log . info ( log_str )"
191,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L1026-L1036,"def get_pid(self, file_path):
        """"""
        :param file_path: the path to the file that's being processed
        :type file_path: unicode
        :return: the PID of the process processing the given file or None if
            the specified file is not being processed
        :rtype: int
        """"""
        if file_path in self._processors:
            return self._processors[file_path].pid
        return None","['def', 'get_pid', '(', 'self', ',', 'file_path', ')', ':', 'if', 'file_path', 'in', 'self', '.', '_processors', ':', 'return', 'self', '.', '_processors', '[', 'file_path', ']', '.', 'pid', 'return', 'None']",":param file_path: the path to the file that's being processed
        :type file_path: unicode
        :return: the PID of the process processing the given file or None if
            the specified file is not being processed
        :rtype: int","[':', 'param', 'file_path', ':', 'the', 'path', 'to', 'the', 'file', 'that', 's', 'being', 'processed', ':', 'type', 'file_path', ':', 'unicode', ':', 'return', ':', 'the', 'PID', 'of', 'the', 'process', 'processing', 'the', 'given', 'file', 'or', 'None', 'if', 'the', 'specified', 'file', 'is', 'not', 'being', 'processed', ':', 'rtype', ':', 'int']",python,test,"[':', 'param', 'file_path', ':', 'the', 'path', 'to', 'the', 'file', 'that', 's', 'being', 'processed', ':', 'type', 'file_path', ':', 'unicode', ':', 'return', ':', 'the', 'pid', 'of', 'the', 'process', 'processing', 'the', 'given', 'file', 'or', 'none', 'if', 'the', 'specified', 'file', 'is', 'not', 'being', 'processed', ':', 'rtype', ':', 'int']",: param file_path : the path to the file that s being processed : type file_path : unicode : return : the pid of the process processing the given file or none if the specified file is not being processed : rtype : int,"['def', 'get_pid', '(', 'self', ',', 'file_path', ')', ':', 'if', 'file_path', 'in', 'self', '.', '_processors', ':', 'return', 'self', '.', '_processors', '[', 'file_path', ']', '.', 'pid', 'return', 'none']","def get_pid ( self , file_path ) : if file_path in self . _processors : return self . _processors [ file_path ] . pid return none"
192,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L1045-L1056,"def get_runtime(self, file_path):
        """"""
        :param file_path: the path to the file that's being processed
        :type file_path: unicode
        :return: the current runtime (in seconds) of the process that's
            processing the specified file or None if the file is not currently
            being processed
        """"""
        if file_path in self._processors:
            return (timezone.utcnow() - self._processors[file_path].start_time)\
                .total_seconds()
        return None","['def', 'get_runtime', '(', 'self', ',', 'file_path', ')', ':', 'if', 'file_path', 'in', 'self', '.', '_processors', ':', 'return', '(', 'timezone', '.', 'utcnow', '(', ')', '-', 'self', '.', '_processors', '[', 'file_path', ']', '.', 'start_time', ')', '.', 'total_seconds', '(', ')', 'return', 'None']",":param file_path: the path to the file that's being processed
        :type file_path: unicode
        :return: the current runtime (in seconds) of the process that's
            processing the specified file or None if the file is not currently
            being processed","[':', 'param', 'file_path', ':', 'the', 'path', 'to', 'the', 'file', 'that', 's', 'being', 'processed', ':', 'type', 'file_path', ':', 'unicode', ':', 'return', ':', 'the', 'current', 'runtime', '(', 'in', 'seconds', ')', 'of', 'the', 'process', 'that', 's', 'processing', 'the', 'specified', 'file', 'or', 'None', 'if', 'the', 'file', 'is', 'not', 'currently', 'being', 'processed']",python,test,"[':', 'param', 'file_path', ':', 'the', 'path', 'to', 'the', 'file', 'that', 's', 'being', 'processed', ':', 'type', 'file_path', ':', 'unicode', ':', 'return', ':', 'the', 'current', 'runtime', '(', 'in', 'seconds', ')', 'of', 'the', 'process', 'that', 's', 'processing', 'the', 'specified', 'file', 'or', 'none', 'if', 'the', 'file', 'is', 'not', 'currently', 'being', 'processed']",: param file_path : the path to the file that s being processed : type file_path : unicode : return : the current runtime ( in seconds ) of the process that s processing the specified file or none if the file is not currently being processed,"['def', 'get_runtime', '(', 'self', ',', 'file_path', ')', ':', 'if', 'file_path', 'in', 'self', '.', '_processors', ':', 'return', '(', 'timezone', '.', 'utcnow', '(', ')', '-', 'self', '.', '_processors', '[', 'file_path', ']', '.', 'start_time', ')', '.', 'total_seconds', '(', ')', 'return', 'none']","def get_runtime ( self , file_path ) : if file_path in self . _processors : return ( timezone . utcnow ( ) - self . _processors [ file_path ] . start_time ) . total_seconds ( ) return none"
193,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L1078-L1088,"def get_start_time(self, file_path):
        """"""
        :param file_path: the path to the file that's being processed
        :type file_path: unicode
        :return: the start time of the process that's processing the
            specified file or None if the file is not currently being processed
        :rtype: datetime
        """"""
        if file_path in self._processors:
            return self._processors[file_path].start_time
        return None","['def', 'get_start_time', '(', 'self', ',', 'file_path', ')', ':', 'if', 'file_path', 'in', 'self', '.', '_processors', ':', 'return', 'self', '.', '_processors', '[', 'file_path', ']', '.', 'start_time', 'return', 'None']",":param file_path: the path to the file that's being processed
        :type file_path: unicode
        :return: the start time of the process that's processing the
            specified file or None if the file is not currently being processed
        :rtype: datetime","[':', 'param', 'file_path', ':', 'the', 'path', 'to', 'the', 'file', 'that', 's', 'being', 'processed', ':', 'type', 'file_path', ':', 'unicode', ':', 'return', ':', 'the', 'start', 'time', 'of', 'the', 'process', 'that', 's', 'processing', 'the', 'specified', 'file', 'or', 'None', 'if', 'the', 'file', 'is', 'not', 'currently', 'being', 'processed', ':', 'rtype', ':', 'datetime']",python,test,"[':', 'param', 'file_path', ':', 'the', 'path', 'to', 'the', 'file', 'that', 's', 'being', 'processed', ':', 'type', 'file_path', ':', 'unicode', ':', 'return', ':', 'the', 'start', 'time', 'of', 'the', 'process', 'that', 's', 'processing', 'the', 'specified', 'file', 'or', 'none', 'if', 'the', 'file', 'is', 'not', 'currently', 'being', 'processed', ':', 'rtype', ':', 'datetime']",: param file_path : the path to the file that s being processed : type file_path : unicode : return : the start time of the process that s processing the specified file or none if the file is not currently being processed : rtype : datetime,"['def', 'get_start_time', '(', 'self', ',', 'file_path', ')', ':', 'if', 'file_path', 'in', 'self', '.', '_processors', ':', 'return', 'self', '.', '_processors', '[', 'file_path', ']', '.', 'start_time', 'return', 'none']","def get_start_time ( self , file_path ) : if file_path in self . _processors : return self . _processors [ file_path ] . start_time return none"
194,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L1090-L1109,"def set_file_paths(self, new_file_paths):
        """"""
        Update this with a new set of paths to DAG definition files.

        :param new_file_paths: list of paths to DAG definition files
        :type new_file_paths: list[unicode]
        :return: None
        """"""
        self._file_paths = new_file_paths
        self._file_path_queue = [x for x in self._file_path_queue
                                 if x in new_file_paths]
        # Stop processors that are working on deleted files
        filtered_processors = {}
        for file_path, processor in self._processors.items():
            if file_path in new_file_paths:
                filtered_processors[file_path] = processor
            else:
                self.log.warning(""Stopping processor for %s"", file_path)
                processor.terminate()
        self._processors = filtered_processors","['def', 'set_file_paths', '(', 'self', ',', 'new_file_paths', ')', ':', 'self', '.', '_file_paths', '=', 'new_file_paths', 'self', '.', '_file_path_queue', '=', '[', 'x', 'for', 'x', 'in', 'self', '.', '_file_path_queue', 'if', 'x', 'in', 'new_file_paths', ']', '# Stop processors that are working on deleted files', 'filtered_processors', '=', '{', '}', 'for', 'file_path', ',', 'processor', 'in', 'self', '.', '_processors', '.', 'items', '(', ')', ':', 'if', 'file_path', 'in', 'new_file_paths', ':', 'filtered_processors', '[', 'file_path', ']', '=', 'processor', 'else', ':', 'self', '.', 'log', '.', 'warning', '(', '""Stopping processor for %s""', ',', 'file_path', ')', 'processor', '.', 'terminate', '(', ')', 'self', '.', '_processors', '=', 'filtered_processors']","Update this with a new set of paths to DAG definition files.

        :param new_file_paths: list of paths to DAG definition files
        :type new_file_paths: list[unicode]
        :return: None","['Update', 'this', 'with', 'a', 'new', 'set', 'of', 'paths', 'to', 'DAG', 'definition', 'files', '.']",python,test,"['update', 'this', 'with', 'a', 'new', 'set', 'of', 'paths', 'to', 'dag', 'definition', 'files', '.']",update this with a new set of paths to dag definition files .,"['def', 'set_file_paths', '(', 'self', ',', 'new_file_paths', ')', ':', 'self', '.', '_file_paths', '=', 'new_file_paths', 'self', '.', '_file_path_queue', '=', '[', 'x', 'for', 'x', 'in', 'self', '.', '_file_path_queue', 'if', 'x', 'in', 'new_file_paths', ']', '# stop processors that are working on deleted files', 'filtered_processors', '=', '{', '}', 'for', 'file_path', ',', 'processor', 'in', 'self', '.', '_processors', '.', 'items', '(', ')', ':', 'if', 'file_path', 'in', 'new_file_paths', ':', 'filtered_processors', '[', 'file_path', ']', '=', 'processor', 'else', ':', 'self', '.', 'log', '.', 'warning', '(', '""stopping processor for %s""', ',', 'file_path', ')', 'processor', '.', 'terminate', '(', ')', 'self', '.', '_processors', '=', 'filtered_processors']","def set_file_paths ( self , new_file_paths ) : self . _file_paths = new_file_paths self . _file_path_queue = [ x for x in self . _file_path_queue if x in new_file_paths ] # stop processors that are working on deleted files filtered_processors = { } for file_path , processor in self . _processors . items ( ) : if file_path in new_file_paths : filtered_processors [ file_path ] = processor else : self . log . warning ( ""stopping processor for %s"" , file_path ) processor . terminate ( ) self . _processors = filtered_processors"
195,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L1118-L1124,"def wait_until_finished(self):
        """"""
        Sleeps until all the processors are done.
        """"""
        for file_path, processor in self._processors.items():
            while not processor.done:
                time.sleep(0.1)","['def', 'wait_until_finished', '(', 'self', ')', ':', 'for', 'file_path', ',', 'processor', 'in', 'self', '.', '_processors', '.', 'items', '(', ')', ':', 'while', 'not', 'processor', '.', 'done', ':', 'time', '.', 'sleep', '(', '0.1', ')']",Sleeps until all the processors are done.,"['Sleeps', 'until', 'all', 'the', 'processors', 'are', 'done', '.']",python,test,"['sleeps', 'until', 'all', 'the', 'processors', 'are', 'done', '.']",sleeps until all the processors are done .,"['def', 'wait_until_finished', '(', 'self', ')', ':', 'for', 'file_path', ',', 'processor', 'in', 'self', '.', '_processors', '.', 'items', '(', ')', ':', 'while', 'not', 'processor', '.', 'done', ':', 'time', '.', 'sleep', '(', '0.1', ')']","def wait_until_finished ( self ) : for file_path , processor in self . _processors . items ( ) : while not processor . done : time . sleep ( 0.1 )"
196,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L1126-L1227,"def heartbeat(self):
        """"""
        This should be periodically called by the manager loop. This method will
        kick off new processes to process DAG definition files and read the
        results from the finished processors.

        :return: a list of SimpleDags that were produced by processors that
            have finished since the last time this was called
        :rtype: list[airflow.utils.dag_processing.SimpleDag]
        """"""
        finished_processors = {}
        """""":type : dict[unicode, AbstractDagFileProcessor]""""""
        running_processors = {}
        """""":type : dict[unicode, AbstractDagFileProcessor]""""""

        for file_path, processor in self._processors.items():
            if processor.done:
                self.log.debug(""Processor for %s finished"", file_path)
                now = timezone.utcnow()
                finished_processors[file_path] = processor
                self._last_runtime[file_path] = (now -
                                                 processor.start_time).total_seconds()
                self._last_finish_time[file_path] = now
                self._run_count[file_path] += 1
            else:
                running_processors[file_path] = processor
        self._processors = running_processors

        self.log.debug(""%s/%s DAG parsing processes running"",
                       len(self._processors), self._parallelism)

        self.log.debug(""%s file paths queued for processing"",
                       len(self._file_path_queue))

        # Collect all the DAGs that were found in the processed files
        simple_dags = []
        for file_path, processor in finished_processors.items():
            if processor.result is None:
                self.log.warning(
                    ""Processor for %s exited with return code %s."",
                    processor.file_path, processor.exit_code
                )
            else:
                for simple_dag in processor.result:
                    simple_dags.append(simple_dag)

        # Generate more file paths to process if we processed all the files
        # already.
        if len(self._file_path_queue) == 0:
            # If the file path is already being processed, or if a file was
            # processed recently, wait until the next batch
            file_paths_in_progress = self._processors.keys()
            now = timezone.utcnow()
            file_paths_recently_processed = []
            for file_path in self._file_paths:
                last_finish_time = self.get_last_finish_time(file_path)
                if (last_finish_time is not None and
                    (now - last_finish_time).total_seconds() <
                        self._file_process_interval):
                    file_paths_recently_processed.append(file_path)

            files_paths_at_run_limit = [file_path
                                        for file_path, num_runs in self._run_count.items()
                                        if num_runs == self._max_runs]

            files_paths_to_queue = list(set(self._file_paths) -
                                        set(file_paths_in_progress) -
                                        set(file_paths_recently_processed) -
                                        set(files_paths_at_run_limit))

            for file_path, processor in self._processors.items():
                self.log.debug(
                    ""File path %s is still being processed (started: %s)"",
                    processor.file_path, processor.start_time.isoformat()
                )

            self.log.debug(
                ""Queuing the following files for processing:\n\t%s"",
                ""\n\t"".join(files_paths_to_queue)
            )

            self._file_path_queue.extend(files_paths_to_queue)

        zombies = self._find_zombies()

        # Start more processors if we have enough slots and files to process
        while (self._parallelism - len(self._processors) > 0 and
               len(self._file_path_queue) > 0):
            file_path = self._file_path_queue.pop(0)
            processor = self._processor_factory(file_path, zombies)

            processor.start()
            self.log.debug(
                ""Started a process (PID: %s) to generate tasks for %s"",
                processor.pid, file_path
            )
            self._processors[file_path] = processor

        # Update heartbeat count.
        self._run_count[self._heart_beat_key] += 1

        return simple_dags","['def', 'heartbeat', '(', 'self', ')', ':', 'finished_processors', '=', '{', '}', '"""""":type : dict[unicode, AbstractDagFileProcessor]""""""', 'running_processors', '=', '{', '}', '"""""":type : dict[unicode, AbstractDagFileProcessor]""""""', 'for', 'file_path', ',', 'processor', 'in', 'self', '.', '_processors', '.', 'items', '(', ')', ':', 'if', 'processor', '.', 'done', ':', 'self', '.', 'log', '.', 'debug', '(', '""Processor for %s finished""', ',', 'file_path', ')', 'now', '=', 'timezone', '.', 'utcnow', '(', ')', 'finished_processors', '[', 'file_path', ']', '=', 'processor', 'self', '.', '_last_runtime', '[', 'file_path', ']', '=', '(', 'now', '-', 'processor', '.', 'start_time', ')', '.', 'total_seconds', '(', ')', 'self', '.', '_last_finish_time', '[', 'file_path', ']', '=', 'now', 'self', '.', '_run_count', '[', 'file_path', ']', '+=', '1', 'else', ':', 'running_processors', '[', 'file_path', ']', '=', 'processor', 'self', '.', '_processors', '=', 'running_processors', 'self', '.', 'log', '.', 'debug', '(', '""%s/%s DAG parsing processes running""', ',', 'len', '(', 'self', '.', '_processors', ')', ',', 'self', '.', '_parallelism', ')', 'self', '.', 'log', '.', 'debug', '(', '""%s file paths queued for processing""', ',', 'len', '(', 'self', '.', '_file_path_queue', ')', ')', '# Collect all the DAGs that were found in the processed files', 'simple_dags', '=', '[', ']', 'for', 'file_path', ',', 'processor', 'in', 'finished_processors', '.', 'items', '(', ')', ':', 'if', 'processor', '.', 'result', 'is', 'None', ':', 'self', '.', 'log', '.', 'warning', '(', '""Processor for %s exited with return code %s.""', ',', 'processor', '.', 'file_path', ',', 'processor', '.', 'exit_code', ')', 'else', ':', 'for', 'simple_dag', 'in', 'processor', '.', 'result', ':', 'simple_dags', '.', 'append', '(', 'simple_dag', ')', '# Generate more file paths to process if we processed all the files', '# already.', 'if', 'len', '(', 'self', '.', '_file_path_queue', ')', '==', '0', ':', '# If the file path is already being processed, or if a file was', '# processed recently, wait until the next batch', 'file_paths_in_progress', '=', 'self', '.', '_processors', '.', 'keys', '(', ')', 'now', '=', 'timezone', '.', 'utcnow', '(', ')', 'file_paths_recently_processed', '=', '[', ']', 'for', 'file_path', 'in', 'self', '.', '_file_paths', ':', 'last_finish_time', '=', 'self', '.', 'get_last_finish_time', '(', 'file_path', ')', 'if', '(', 'last_finish_time', 'is', 'not', 'None', 'and', '(', 'now', '-', 'last_finish_time', ')', '.', 'total_seconds', '(', ')', '<', 'self', '.', '_file_process_interval', ')', ':', 'file_paths_recently_processed', '.', 'append', '(', 'file_path', ')', 'files_paths_at_run_limit', '=', '[', 'file_path', 'for', 'file_path', ',', 'num_runs', 'in', 'self', '.', '_run_count', '.', 'items', '(', ')', 'if', 'num_runs', '==', 'self', '.', '_max_runs', ']', 'files_paths_to_queue', '=', 'list', '(', 'set', '(', 'self', '.', '_file_paths', ')', '-', 'set', '(', 'file_paths_in_progress', ')', '-', 'set', '(', 'file_paths_recently_processed', ')', '-', 'set', '(', 'files_paths_at_run_limit', ')', ')', 'for', 'file_path', ',', 'processor', 'in', 'self', '.', '_processors', '.', 'items', '(', ')', ':', 'self', '.', 'log', '.', 'debug', '(', '""File path %s is still being processed (started: %s)""', ',', 'processor', '.', 'file_path', ',', 'processor', '.', 'start_time', '.', 'isoformat', '(', ')', ')', 'self', '.', 'log', '.', 'debug', '(', '""Queuing the following files for processing:\\n\\t%s""', ',', '""\\n\\t""', '.', 'join', '(', 'files_paths_to_queue', ')', ')', 'self', '.', '_file_path_queue', '.', 'extend', '(', 'files_paths_to_queue', ')', 'zombies', '=', 'self', '.', '_find_zombies', '(', ')', '# Start more processors if we have enough slots and files to process', 'while', '(', 'self', '.', '_parallelism', '-', 'len', '(', 'self', '.', '_processors', ')', '>', '0', 'and', 'len', '(', 'self', '.', '_file_path_queue', ')', '>', '0', ')', ':', 'file_path', '=', 'self', '.', '_file_path_queue', '.', 'pop', '(', '0', ')', 'processor', '=', 'self', '.', '_processor_factory', '(', 'file_path', ',', 'zombies', ')', 'processor', '.', 'start', '(', ')', 'self', '.', 'log', '.', 'debug', '(', '""Started a process (PID: %s) to generate tasks for %s""', ',', 'processor', '.', 'pid', ',', 'file_path', ')', 'self', '.', '_processors', '[', 'file_path', ']', '=', 'processor', '# Update heartbeat count.', 'self', '.', '_run_count', '[', 'self', '.', '_heart_beat_key', ']', '+=', '1', 'return', 'simple_dags']","This should be periodically called by the manager loop. This method will
        kick off new processes to process DAG definition files and read the
        results from the finished processors.

        :return: a list of SimpleDags that were produced by processors that
            have finished since the last time this was called
        :rtype: list[airflow.utils.dag_processing.SimpleDag]","['This', 'should', 'be', 'periodically', 'called', 'by', 'the', 'manager', 'loop', '.', 'This', 'method', 'will', 'kick', 'off', 'new', 'processes', 'to', 'process', 'DAG', 'definition', 'files', 'and', 'read', 'the', 'results', 'from', 'the', 'finished', 'processors', '.']",python,test,"['this', 'should', 'be', 'periodically', 'called', 'by', 'the', 'manager', 'loop', '.', 'this', 'method', 'will', 'kick', 'off', 'new', 'processes', 'to', 'process', 'dag', 'definition', 'files', 'and', 'read', 'the', 'results', 'from', 'the', 'finished', 'processors', '.']",this should be periodically called by the manager loop . this method will kick off new processes to process dag definition files and read the results from the finished processors .,"['def', 'heartbeat', '(', 'self', ')', ':', 'finished_processors', '=', '{', '}', '"""""":type : dict[unicode, abstractdagfileprocessor]""""""', 'running_processors', '=', '{', '}', '"""""":type : dict[unicode, abstractdagfileprocessor]""""""', 'for', 'file_path', ',', 'processor', 'in', 'self', '.', '_processors', '.', 'items', '(', ')', ':', 'if', 'processor', '.', 'done', ':', 'self', '.', 'log', '.', 'debug', '(', '""processor for %s finished""', ',', 'file_path', ')', 'now', '=', 'timezone', '.', 'utcnow', '(', ')', 'finished_processors', '[', 'file_path', ']', '=', 'processor', 'self', '.', '_last_runtime', '[', 'file_path', ']', '=', '(', 'now', '-', 'processor', '.', 'start_time', ')', '.', 'total_seconds', '(', ')', 'self', '.', '_last_finish_time', '[', 'file_path', ']', '=', 'now', 'self', '.', '_run_count', '[', 'file_path', ']', '+=', '1', 'else', ':', 'running_processors', '[', 'file_path', ']', '=', 'processor', 'self', '.', '_processors', '=', 'running_processors', 'self', '.', 'log', '.', 'debug', '(', '""%s/%s dag parsing processes running""', ',', 'len', '(', 'self', '.', '_processors', ')', ',', 'self', '.', '_parallelism', ')', 'self', '.', 'log', '.', 'debug', '(', '""%s file paths queued for processing""', ',', 'len', '(', 'self', '.', '_file_path_queue', ')', ')', '# collect all the dags that were found in the processed files', 'simple_dags', '=', '[', ']', 'for', 'file_path', ',', 'processor', 'in', 'finished_processors', '.', 'items', '(', ')', ':', 'if', 'processor', '.', 'result', 'is', 'none', ':', 'self', '.', 'log', '.', 'warning', '(', '""processor for %s exited with return code %s.""', ',', 'processor', '.', 'file_path', ',', 'processor', '.', 'exit_code', ')', 'else', ':', 'for', 'simple_dag', 'in', 'processor', '.', 'result', ':', 'simple_dags', '.', 'append', '(', 'simple_dag', ')', '# generate more file paths to process if we processed all the files', '# already.', 'if', 'len', '(', 'self', '.', '_file_path_queue', ')', '==', '0', ':', '# if the file path is already being processed, or if a file was', '# processed recently, wait until the next batch', 'file_paths_in_progress', '=', 'self', '.', '_processors', '.', 'keys', '(', ')', 'now', '=', 'timezone', '.', 'utcnow', '(', ')', 'file_paths_recently_processed', '=', '[', ']', 'for', 'file_path', 'in', 'self', '.', '_file_paths', ':', 'last_finish_time', '=', 'self', '.', 'get_last_finish_time', '(', 'file_path', ')', 'if', '(', 'last_finish_time', 'is', 'not', 'none', 'and', '(', 'now', '-', 'last_finish_time', ')', '.', 'total_seconds', '(', ')', '<', 'self', '.', '_file_process_interval', ')', ':', 'file_paths_recently_processed', '.', 'append', '(', 'file_path', ')', 'files_paths_at_run_limit', '=', '[', 'file_path', 'for', 'file_path', ',', 'num_runs', 'in', 'self', '.', '_run_count', '.', 'items', '(', ')', 'if', 'num_runs', '==', 'self', '.', '_max_runs', ']', 'files_paths_to_queue', '=', 'list', '(', 'set', '(', 'self', '.', '_file_paths', ')', '-', 'set', '(', 'file_paths_in_progress', ')', '-', 'set', '(', 'file_paths_recently_processed', ')', '-', 'set', '(', 'files_paths_at_run_limit', ')', ')', 'for', 'file_path', ',', 'processor', 'in', 'self', '.', '_processors', '.', 'items', '(', ')', ':', 'self', '.', 'log', '.', 'debug', '(', '""file path %s is still being processed (started: %s)""', ',', 'processor', '.', 'file_path', ',', 'processor', '.', 'start_time', '.', 'isoformat', '(', ')', ')', 'self', '.', 'log', '.', 'debug', '(', '""queuing the following files for processing:\\n\\t%s""', ',', '""\\n\\t""', '.', 'join', '(', 'files_paths_to_queue', ')', ')', 'self', '.', '_file_path_queue', '.', 'extend', '(', 'files_paths_to_queue', ')', 'zombies', '=', 'self', '.', '_find_zombies', '(', ')', '# start more processors if we have enough slots and files to process', 'while', '(', 'self', '.', '_parallelism', '-', 'len', '(', 'self', '.', '_processors', ')', '>', '0', 'and', 'len', '(', 'self', '.', '_file_path_queue', ')', '>', '0', ')', ':', 'file_path', '=', 'self', '.', '_file_path_queue', '.', 'pop', '(', '0', ')', 'processor', '=', 'self', '.', '_processor_factory', '(', 'file_path', ',', 'zombies', ')', 'processor', '.', 'start', '(', ')', 'self', '.', 'log', '.', 'debug', '(', '""started a process (pid: %s) to generate tasks for %s""', ',', 'processor', '.', 'pid', ',', 'file_path', ')', 'self', '.', '_processors', '[', 'file_path', ']', '=', 'processor', '# update heartbeat count.', 'self', '.', '_run_count', '[', 'self', '.', '_heart_beat_key', ']', '+=', '1', 'return', 'simple_dags']","def heartbeat ( self ) : finished_processors = { } """""":type : dict[unicode, abstractdagfileprocessor]"""""" running_processors = { } """""":type : dict[unicode, abstractdagfileprocessor]"""""" for file_path , processor in self . _processors . items ( ) : if processor . done : self . log . debug ( ""processor for %s finished"" , file_path ) now = timezone . utcnow ( ) finished_processors [ file_path ] = processor self . _last_runtime [ file_path ] = ( now - processor . start_time ) . total_seconds ( ) self . _last_finish_time [ file_path ] = now self . _run_count [ file_path ] += 1 else : running_processors [ file_path ] = processor self . _processors = running_processors self . log . debug ( ""%s/%s dag parsing processes running"" , len ( self . _processors ) , self . _parallelism ) self . log . debug ( ""%s file paths queued for processing"" , len ( self . _file_path_queue ) ) # collect all the dags that were found in the processed files simple_dags = [ ] for file_path , processor in finished_processors . items ( ) : if processor . result is none : self . log . warning ( ""processor for %s exited with return code %s."" , processor . file_path , processor . exit_code ) else : for simple_dag in processor . result : simple_dags . append ( simple_dag ) # generate more file paths to process if we processed all the files # already. if len ( self . _file_path_queue ) == 0 : # if the file path is already being processed, or if a file was # processed recently, wait until the next batch file_paths_in_progress = self . _processors . keys ( ) now = timezone . utcnow ( ) file_paths_recently_processed = [ ] for file_path in self . _file_paths : last_finish_time = self . get_last_finish_time ( file_path ) if ( last_finish_time is not none and ( now - last_finish_time ) . total_seconds ( ) < self . _file_process_interval ) : file_paths_recently_processed . append ( file_path ) files_paths_at_run_limit = [ file_path for file_path , num_runs in self . _run_count . items ( ) if num_runs == self . _max_runs ] files_paths_to_queue = list ( set ( self . _file_paths ) - set ( file_paths_in_progress ) - set ( file_paths_recently_processed ) - set ( files_paths_at_run_limit ) ) for file_path , processor in self . _processors . items ( ) : self . log . debug ( ""file path %s is still being processed (started: %s)"" , processor . file_path , processor . start_time . isoformat ( ) ) self . log . debug ( ""queuing the following files for processing:\n\t%s"" , ""\n\t"" . join ( files_paths_to_queue ) ) self . _file_path_queue . extend ( files_paths_to_queue ) zombies = self . _find_zombies ( ) # start more processors if we have enough slots and files to process while ( self . _parallelism - len ( self . _processors ) > 0 and len ( self . _file_path_queue ) > 0 ) : file_path = self . _file_path_queue . pop ( 0 ) processor = self . _processor_factory ( file_path , zombies ) processor . start ( ) self . log . debug ( ""started a process (pid: %s) to generate tasks for %s"" , processor . pid , file_path ) self . _processors [ file_path ] = processor # update heartbeat count. self . _run_count [ self . _heart_beat_key ] += 1 return simple_dags"
197,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L1230-L1262,"def _find_zombies(self, session):
        """"""
        Find zombie task instances, which are tasks haven't heartbeated for too long.
        :return: Zombie task instances in SimpleTaskInstance format.
        """"""
        now = timezone.utcnow()
        zombies = []
        if (now - self._last_zombie_query_time).total_seconds() \
                > self._zombie_query_interval:
            # to avoid circular imports
            from airflow.jobs import LocalTaskJob as LJ
            self.log.info(""Finding 'running' jobs without a recent heartbeat"")
            TI = airflow.models.TaskInstance
            limit_dttm = timezone.utcnow() - timedelta(
                seconds=self._zombie_threshold_secs)
            self.log.info(""Failing jobs without heartbeat after %s"", limit_dttm)

            tis = (
                session.query(TI)
                .join(LJ, TI.job_id == LJ.id)
                .filter(TI.state == State.RUNNING)
                .filter(
                    or_(
                        LJ.state != State.RUNNING,
                        LJ.latest_heartbeat < limit_dttm,
                    )
                ).all()
            )
            self._last_zombie_query_time = timezone.utcnow()
            for ti in tis:
                zombies.append(SimpleTaskInstance(ti))

        return zombies","['def', '_find_zombies', '(', 'self', ',', 'session', ')', ':', 'now', '=', 'timezone', '.', 'utcnow', '(', ')', 'zombies', '=', '[', ']', 'if', '(', 'now', '-', 'self', '.', '_last_zombie_query_time', ')', '.', 'total_seconds', '(', ')', '>', 'self', '.', '_zombie_query_interval', ':', '# to avoid circular imports', 'from', 'airflow', '.', 'jobs', 'import', 'LocalTaskJob', 'as', 'LJ', 'self', '.', 'log', '.', 'info', '(', '""Finding \'running\' jobs without a recent heartbeat""', ')', 'TI', '=', 'airflow', '.', 'models', '.', 'TaskInstance', 'limit_dttm', '=', 'timezone', '.', 'utcnow', '(', ')', '-', 'timedelta', '(', 'seconds', '=', 'self', '.', '_zombie_threshold_secs', ')', 'self', '.', 'log', '.', 'info', '(', '""Failing jobs without heartbeat after %s""', ',', 'limit_dttm', ')', 'tis', '=', '(', 'session', '.', 'query', '(', 'TI', ')', '.', 'join', '(', 'LJ', ',', 'TI', '.', 'job_id', '==', 'LJ', '.', 'id', ')', '.', 'filter', '(', 'TI', '.', 'state', '==', 'State', '.', 'RUNNING', ')', '.', 'filter', '(', 'or_', '(', 'LJ', '.', 'state', '!=', 'State', '.', 'RUNNING', ',', 'LJ', '.', 'latest_heartbeat', '<', 'limit_dttm', ',', ')', ')', '.', 'all', '(', ')', ')', 'self', '.', '_last_zombie_query_time', '=', 'timezone', '.', 'utcnow', '(', ')', 'for', 'ti', 'in', 'tis', ':', 'zombies', '.', 'append', '(', 'SimpleTaskInstance', '(', 'ti', ')', ')', 'return', 'zombies']","Find zombie task instances, which are tasks haven't heartbeated for too long.
        :return: Zombie task instances in SimpleTaskInstance format.","['Find', 'zombie', 'task', 'instances', 'which', 'are', 'tasks', 'haven', 't', 'heartbeated', 'for', 'too', 'long', '.', ':', 'return', ':', 'Zombie', 'task', 'instances', 'in', 'SimpleTaskInstance', 'format', '.']",python,test,"['find', 'zombie', 'task', 'instances', 'which', 'are', 'tasks', 'haven', 't', 'heartbeated', 'for', 'too', 'long', '.', ':', 'return', ':', 'zombie', 'task', 'instances', 'in', 'simpletaskinstance', 'format', '.']",find zombie task instances which are tasks haven t heartbeated for too long . : return : zombie task instances in simpletaskinstance format .,"['def', '_find_zombies', '(', 'self', ',', 'session', ')', ':', 'now', '=', 'timezone', '.', 'utcnow', '(', ')', 'zombies', '=', '[', ']', 'if', '(', 'now', '-', 'self', '.', '_last_zombie_query_time', ')', '.', 'total_seconds', '(', ')', '>', 'self', '.', '_zombie_query_interval', ':', '# to avoid circular imports', 'from', 'airflow', '.', 'jobs', 'import', 'localtaskjob', 'as', 'lj', 'self', '.', 'log', '.', 'info', '(', '""finding \'running\' jobs without a recent heartbeat""', ')', 'ti', '=', 'airflow', '.', 'models', '.', 'taskinstance', 'limit_dttm', '=', 'timezone', '.', 'utcnow', '(', ')', '-', 'timedelta', '(', 'seconds', '=', 'self', '.', '_zombie_threshold_secs', ')', 'self', '.', 'log', '.', 'info', '(', '""failing jobs without heartbeat after %s""', ',', 'limit_dttm', ')', 'tis', '=', '(', 'session', '.', 'query', '(', 'ti', ')', '.', 'join', '(', 'lj', ',', 'ti', '.', 'job_id', '==', 'lj', '.', 'id', ')', '.', 'filter', '(', 'ti', '.', 'state', '==', 'state', '.', 'running', ')', '.', 'filter', '(', 'or_', '(', 'lj', '.', 'state', '!=', 'state', '.', 'running', ',', 'lj', '.', 'latest_heartbeat', '<', 'limit_dttm', ',', ')', ')', '.', 'all', '(', ')', ')', 'self', '.', '_last_zombie_query_time', '=', 'timezone', '.', 'utcnow', '(', ')', 'for', 'ti', 'in', 'tis', ':', 'zombies', '.', 'append', '(', 'simpletaskinstance', '(', 'ti', ')', ')', 'return', 'zombies']","def _find_zombies ( self , session ) : now = timezone . utcnow ( ) zombies = [ ] if ( now - self . _last_zombie_query_time ) . total_seconds ( ) > self . _zombie_query_interval : # to avoid circular imports from airflow . jobs import localtaskjob as lj self . log . info ( ""finding 'running' jobs without a recent heartbeat"" ) ti = airflow . models . taskinstance limit_dttm = timezone . utcnow ( ) - timedelta ( seconds = self . _zombie_threshold_secs ) self . log . info ( ""failing jobs without heartbeat after %s"" , limit_dttm ) tis = ( session . query ( ti ) . join ( lj , ti . job_id == lj . id ) . filter ( ti . state == state . running ) . filter ( or_ ( lj . state != state . running , lj . latest_heartbeat < limit_dttm , ) ) . all ( ) ) self . _last_zombie_query_time = timezone . utcnow ( ) for ti in tis : zombies . append ( simpletaskinstance ( ti ) ) return zombies"
198,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L1264-L1275,"def max_runs_reached(self):
        """"""
        :return: whether all file paths have been processed max_runs times
        """"""
        if self._max_runs == -1:  # Unlimited runs.
            return False
        for file_path in self._file_paths:
            if self._run_count[file_path] < self._max_runs:
                return False
        if self._run_count[self._heart_beat_key] < self._max_runs:
            return False
        return True","['def', 'max_runs_reached', '(', 'self', ')', ':', 'if', 'self', '.', '_max_runs', '==', '-', '1', ':', '# Unlimited runs.', 'return', 'False', 'for', 'file_path', 'in', 'self', '.', '_file_paths', ':', 'if', 'self', '.', '_run_count', '[', 'file_path', ']', '<', 'self', '.', '_max_runs', ':', 'return', 'False', 'if', 'self', '.', '_run_count', '[', 'self', '.', '_heart_beat_key', ']', '<', 'self', '.', '_max_runs', ':', 'return', 'False', 'return', 'True']",:return: whether all file paths have been processed max_runs times,"[':', 'return', ':', 'whether', 'all', 'file', 'paths', 'have', 'been', 'processed', 'max_runs', 'times']",python,test,"[':', 'return', ':', 'whether', 'all', 'file', 'paths', 'have', 'been', 'processed', 'max_runs', 'times']",: return : whether all file paths have been processed max_runs times,"['def', 'max_runs_reached', '(', 'self', ')', ':', 'if', 'self', '.', '_max_runs', '==', '-', '1', ':', '# unlimited runs.', 'return', 'false', 'for', 'file_path', 'in', 'self', '.', '_file_paths', ':', 'if', 'self', '.', '_run_count', '[', 'file_path', ']', '<', 'self', '.', '_max_runs', ':', 'return', 'false', 'if', 'self', '.', '_run_count', '[', 'self', '.', '_heart_beat_key', ']', '<', 'self', '.', '_max_runs', ':', 'return', 'false', 'return', 'true']",def max_runs_reached ( self ) : if self . _max_runs == - 1 : # unlimited runs. return false for file_path in self . _file_paths : if self . _run_count [ file_path ] < self . _max_runs : return false if self . _run_count [ self . _heart_beat_key ] < self . _max_runs : return false return true
199,apache/airflow,airflow/utils/dag_processing.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/dag_processing.py#L1285-L1320,"def end(self):
        """"""
        Kill all child processes on exit since we don't want to leave
        them as orphaned.
        """"""
        pids_to_kill = self.get_all_pids()
        if len(pids_to_kill) > 0:
            # First try SIGTERM
            this_process = psutil.Process(os.getpid())
            # Only check child processes to ensure that we don't have a case
            # where we kill the wrong process because a child process died
            # but the PID got reused.
            child_processes = [x for x in this_process.children(recursive=True)
                               if x.is_running() and x.pid in pids_to_kill]
            for child in child_processes:
                self.log.info(""Terminating child PID: %s"", child.pid)
                child.terminate()
            # TODO: Remove magic number
            timeout = 5
            self.log.info(""Waiting up to %s seconds for processes to exit..."", timeout)
            try:
                psutil.wait_procs(
                    child_processes, timeout=timeout,
                    callback=lambda x: self.log.info('Terminated PID %s', x.pid))
            except psutil.TimeoutExpired:
                self.log.debug(""Ran out of time while waiting for processes to exit"")

            # Then SIGKILL
            child_processes = [x for x in this_process.children(recursive=True)
                               if x.is_running() and x.pid in pids_to_kill]
            if len(child_processes) > 0:
                self.log.info(""SIGKILL processes that did not terminate gracefully"")
                for child in child_processes:
                    self.log.info(""Killing child PID: %s"", child.pid)
                    child.kill()
                    child.wait()","['def', 'end', '(', 'self', ')', ':', 'pids_to_kill', '=', 'self', '.', 'get_all_pids', '(', ')', 'if', 'len', '(', 'pids_to_kill', ')', '>', '0', ':', '# First try SIGTERM', 'this_process', '=', 'psutil', '.', 'Process', '(', 'os', '.', 'getpid', '(', ')', ')', ""# Only check child processes to ensure that we don't have a case"", '# where we kill the wrong process because a child process died', '# but the PID got reused.', 'child_processes', '=', '[', 'x', 'for', 'x', 'in', 'this_process', '.', 'children', '(', 'recursive', '=', 'True', ')', 'if', 'x', '.', 'is_running', '(', ')', 'and', 'x', '.', 'pid', 'in', 'pids_to_kill', ']', 'for', 'child', 'in', 'child_processes', ':', 'self', '.', 'log', '.', 'info', '(', '""Terminating child PID: %s""', ',', 'child', '.', 'pid', ')', 'child', '.', 'terminate', '(', ')', '# TODO: Remove magic number', 'timeout', '=', '5', 'self', '.', 'log', '.', 'info', '(', '""Waiting up to %s seconds for processes to exit...""', ',', 'timeout', ')', 'try', ':', 'psutil', '.', 'wait_procs', '(', 'child_processes', ',', 'timeout', '=', 'timeout', ',', 'callback', '=', 'lambda', 'x', ':', 'self', '.', 'log', '.', 'info', '(', ""'Terminated PID %s'"", ',', 'x', '.', 'pid', ')', ')', 'except', 'psutil', '.', 'TimeoutExpired', ':', 'self', '.', 'log', '.', 'debug', '(', '""Ran out of time while waiting for processes to exit""', ')', '# Then SIGKILL', 'child_processes', '=', '[', 'x', 'for', 'x', 'in', 'this_process', '.', 'children', '(', 'recursive', '=', 'True', ')', 'if', 'x', '.', 'is_running', '(', ')', 'and', 'x', '.', 'pid', 'in', 'pids_to_kill', ']', 'if', 'len', '(', 'child_processes', ')', '>', '0', ':', 'self', '.', 'log', '.', 'info', '(', '""SIGKILL processes that did not terminate gracefully""', ')', 'for', 'child', 'in', 'child_processes', ':', 'self', '.', 'log', '.', 'info', '(', '""Killing child PID: %s""', ',', 'child', '.', 'pid', ')', 'child', '.', 'kill', '(', ')', 'child', '.', 'wait', '(', ')']","Kill all child processes on exit since we don't want to leave
        them as orphaned.","['Kill', 'all', 'child', 'processes', 'on', 'exit', 'since', 'we', 'don', 't', 'want', 'to', 'leave', 'them', 'as', 'orphaned', '.']",python,test,"['kill', 'all', 'child', 'processes', 'on', 'exit', 'since', 'we', 'don', 't', 'want', 'to', 'leave', 'them', 'as', 'orphaned', '.']",kill all child processes on exit since we don t want to leave them as orphaned .,"['def', 'end', '(', 'self', ')', ':', 'pids_to_kill', '=', 'self', '.', 'get_all_pids', '(', ')', 'if', 'len', '(', 'pids_to_kill', ')', '>', '0', ':', '# first try sigterm', 'this_process', '=', 'psutil', '.', 'process', '(', 'os', '.', 'getpid', '(', ')', ')', ""# only check child processes to ensure that we don't have a case"", '# where we kill the wrong process because a child process died', '# but the pid got reused.', 'child_processes', '=', '[', 'x', 'for', 'x', 'in', 'this_process', '.', 'children', '(', 'recursive', '=', 'true', ')', 'if', 'x', '.', 'is_running', '(', ')', 'and', 'x', '.', 'pid', 'in', 'pids_to_kill', ']', 'for', 'child', 'in', 'child_processes', ':', 'self', '.', 'log', '.', 'info', '(', '""terminating child pid: %s""', ',', 'child', '.', 'pid', ')', 'child', '.', 'terminate', '(', ')', '# todo: remove magic number', 'timeout', '=', '5', 'self', '.', 'log', '.', 'info', '(', '""waiting up to %s seconds for processes to exit...""', ',', 'timeout', ')', 'try', ':', 'psutil', '.', 'wait_procs', '(', 'child_processes', ',', 'timeout', '=', 'timeout', ',', 'callback', '=', 'lambda', 'x', ':', 'self', '.', 'log', '.', 'info', '(', ""'terminated pid %s'"", ',', 'x', '.', 'pid', ')', ')', 'except', 'psutil', '.', 'timeoutexpired', ':', 'self', '.', 'log', '.', 'debug', '(', '""ran out of time while waiting for processes to exit""', ')', '# then sigkill', 'child_processes', '=', '[', 'x', 'for', 'x', 'in', 'this_process', '.', 'children', '(', 'recursive', '=', 'true', ')', 'if', 'x', '.', 'is_running', '(', ')', 'and', 'x', '.', 'pid', 'in', 'pids_to_kill', ']', 'if', 'len', '(', 'child_processes', ')', '>', '0', ':', 'self', '.', 'log', '.', 'info', '(', '""sigkill processes that did not terminate gracefully""', ')', 'for', 'child', 'in', 'child_processes', ':', 'self', '.', 'log', '.', 'info', '(', '""killing child pid: %s""', ',', 'child', '.', 'pid', ')', 'child', '.', 'kill', '(', ')', 'child', '.', 'wait', '(', ')']","def end ( self ) : pids_to_kill = self . get_all_pids ( ) if len ( pids_to_kill ) > 0 : # first try sigterm this_process = psutil . process ( os . getpid ( ) ) # only check child processes to ensure that we don't have a case # where we kill the wrong process because a child process died # but the pid got reused. child_processes = [ x for x in this_process . children ( recursive = true ) if x . is_running ( ) and x . pid in pids_to_kill ] for child in child_processes : self . log . info ( ""terminating child pid: %s"" , child . pid ) child . terminate ( ) # todo: remove magic number timeout = 5 self . log . info ( ""waiting up to %s seconds for processes to exit..."" , timeout ) try : psutil . wait_procs ( child_processes , timeout = timeout , callback = lambda x : self . log . info ( 'terminated pid %s' , x . pid ) ) except psutil . timeoutexpired : self . log . debug ( ""ran out of time while waiting for processes to exit"" ) # then sigkill child_processes = [ x for x in this_process . children ( recursive = true ) if x . is_running ( ) and x . pid in pids_to_kill ] if len ( child_processes ) > 0 : self . log . info ( ""sigkill processes that did not terminate gracefully"" ) for child in child_processes : self . log . info ( ""killing child pid: %s"" , child . pid ) child . kill ( ) child . wait ( )"
200,apache/airflow,airflow/contrib/hooks/ssh_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/ssh_hook.py#L144-L185,"def get_conn(self):
        """"""
        Opens a ssh connection to the remote host.

        :rtype: paramiko.client.SSHClient
        """"""

        self.log.debug('Creating SSH client for conn_id: %s', self.ssh_conn_id)
        client = paramiko.SSHClient()
        if not self.allow_host_key_change:
            self.log.warning('Remote Identification Change is not verified. '
                             'This wont protect against Man-In-The-Middle attacks')
            client.load_system_host_keys()
        if self.no_host_key_check:
            self.log.warning('No Host Key Verification. This wont protect '
                             'against Man-In-The-Middle attacks')
            # Default is RejectPolicy
            client.set_missing_host_key_policy(paramiko.AutoAddPolicy())

        if self.password and self.password.strip():
            client.connect(hostname=self.remote_host,
                           username=self.username,
                           password=self.password,
                           key_filename=self.key_file,
                           timeout=self.timeout,
                           compress=self.compress,
                           port=self.port,
                           sock=self.host_proxy)
        else:
            client.connect(hostname=self.remote_host,
                           username=self.username,
                           key_filename=self.key_file,
                           timeout=self.timeout,
                           compress=self.compress,
                           port=self.port,
                           sock=self.host_proxy)

        if self.keepalive_interval:
            client.get_transport().set_keepalive(self.keepalive_interval)

        self.client = client
        return client","['def', 'get_conn', '(', 'self', ')', ':', 'self', '.', 'log', '.', 'debug', '(', ""'Creating SSH client for conn_id: %s'"", ',', 'self', '.', 'ssh_conn_id', ')', 'client', '=', 'paramiko', '.', 'SSHClient', '(', ')', 'if', 'not', 'self', '.', 'allow_host_key_change', ':', 'self', '.', 'log', '.', 'warning', '(', ""'Remote Identification Change is not verified. '"", ""'This wont protect against Man-In-The-Middle attacks'"", ')', 'client', '.', 'load_system_host_keys', '(', ')', 'if', 'self', '.', 'no_host_key_check', ':', 'self', '.', 'log', '.', 'warning', '(', ""'No Host Key Verification. This wont protect '"", ""'against Man-In-The-Middle attacks'"", ')', '# Default is RejectPolicy', 'client', '.', 'set_missing_host_key_policy', '(', 'paramiko', '.', 'AutoAddPolicy', '(', ')', ')', 'if', 'self', '.', 'password', 'and', 'self', '.', 'password', '.', 'strip', '(', ')', ':', 'client', '.', 'connect', '(', 'hostname', '=', 'self', '.', 'remote_host', ',', 'username', '=', 'self', '.', 'username', ',', 'password', '=', 'self', '.', 'password', ',', 'key_filename', '=', 'self', '.', 'key_file', ',', 'timeout', '=', 'self', '.', 'timeout', ',', 'compress', '=', 'self', '.', 'compress', ',', 'port', '=', 'self', '.', 'port', ',', 'sock', '=', 'self', '.', 'host_proxy', ')', 'else', ':', 'client', '.', 'connect', '(', 'hostname', '=', 'self', '.', 'remote_host', ',', 'username', '=', 'self', '.', 'username', ',', 'key_filename', '=', 'self', '.', 'key_file', ',', 'timeout', '=', 'self', '.', 'timeout', ',', 'compress', '=', 'self', '.', 'compress', ',', 'port', '=', 'self', '.', 'port', ',', 'sock', '=', 'self', '.', 'host_proxy', ')', 'if', 'self', '.', 'keepalive_interval', ':', 'client', '.', 'get_transport', '(', ')', '.', 'set_keepalive', '(', 'self', '.', 'keepalive_interval', ')', 'self', '.', 'client', '=', 'client', 'return', 'client']","Opens a ssh connection to the remote host.

        :rtype: paramiko.client.SSHClient","['Opens', 'a', 'ssh', 'connection', 'to', 'the', 'remote', 'host', '.']",python,test,"['opens', 'a', 'ssh', 'connection', 'to', 'the', 'remote', 'host', '.']",opens a ssh connection to the remote host .,"['def', 'get_conn', '(', 'self', ')', ':', 'self', '.', 'log', '.', 'debug', '(', ""'creating ssh client for conn_id: %s'"", ',', 'self', '.', 'ssh_conn_id', ')', 'client', '=', 'paramiko', '.', 'sshclient', '(', ')', 'if', 'not', 'self', '.', 'allow_host_key_change', ':', 'self', '.', 'log', '.', 'warning', '(', ""'remote identification change is not verified. '"", ""'this wont protect against man-in-the-middle attacks'"", ')', 'client', '.', 'load_system_host_keys', '(', ')', 'if', 'self', '.', 'no_host_key_check', ':', 'self', '.', 'log', '.', 'warning', '(', ""'no host key verification. this wont protect '"", ""'against man-in-the-middle attacks'"", ')', '# default is rejectpolicy', 'client', '.', 'set_missing_host_key_policy', '(', 'paramiko', '.', 'autoaddpolicy', '(', ')', ')', 'if', 'self', '.', 'password', 'and', 'self', '.', 'password', '.', 'strip', '(', ')', ':', 'client', '.', 'connect', '(', 'hostname', '=', 'self', '.', 'remote_host', ',', 'username', '=', 'self', '.', 'username', ',', 'password', '=', 'self', '.', 'password', ',', 'key_filename', '=', 'self', '.', 'key_file', ',', 'timeout', '=', 'self', '.', 'timeout', ',', 'compress', '=', 'self', '.', 'compress', ',', 'port', '=', 'self', '.', 'port', ',', 'sock', '=', 'self', '.', 'host_proxy', ')', 'else', ':', 'client', '.', 'connect', '(', 'hostname', '=', 'self', '.', 'remote_host', ',', 'username', '=', 'self', '.', 'username', ',', 'key_filename', '=', 'self', '.', 'key_file', ',', 'timeout', '=', 'self', '.', 'timeout', ',', 'compress', '=', 'self', '.', 'compress', ',', 'port', '=', 'self', '.', 'port', ',', 'sock', '=', 'self', '.', 'host_proxy', ')', 'if', 'self', '.', 'keepalive_interval', ':', 'client', '.', 'get_transport', '(', ')', '.', 'set_keepalive', '(', 'self', '.', 'keepalive_interval', ')', 'self', '.', 'client', '=', 'client', 'return', 'client']","def get_conn ( self ) : self . log . debug ( 'creating ssh client for conn_id: %s' , self . ssh_conn_id ) client = paramiko . sshclient ( ) if not self . allow_host_key_change : self . log . warning ( 'remote identification change is not verified. ' 'this wont protect against man-in-the-middle attacks' ) client . load_system_host_keys ( ) if self . no_host_key_check : self . log . warning ( 'no host key verification. this wont protect ' 'against man-in-the-middle attacks' ) # default is rejectpolicy client . set_missing_host_key_policy ( paramiko . autoaddpolicy ( ) ) if self . password and self . password . strip ( ) : client . connect ( hostname = self . remote_host , username = self . username , password = self . password , key_filename = self . key_file , timeout = self . timeout , compress = self . compress , port = self . port , sock = self . host_proxy ) else : client . connect ( hostname = self . remote_host , username = self . username , key_filename = self . key_file , timeout = self . timeout , compress = self . compress , port = self . port , sock = self . host_proxy ) if self . keepalive_interval : client . get_transport ( ) . set_keepalive ( self . keepalive_interval ) self . client = client return client"
201,apache/airflow,airflow/contrib/hooks/ssh_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/ssh_hook.py#L199-L239,"def get_tunnel(self, remote_port, remote_host=""localhost"", local_port=None):
        """"""
        Creates a tunnel between two hosts. Like ssh -L <LOCAL_PORT>:host:<REMOTE_PORT>.

        :param remote_port: The remote port to create a tunnel to
        :type remote_port: int
        :param remote_host: The remote host to create a tunnel to (default localhost)
        :type remote_host: str
        :param local_port:  The local port to attach the tunnel to
        :type local_port: int

        :return: sshtunnel.SSHTunnelForwarder object
        """"""

        if local_port:
            local_bind_address = ('localhost', local_port)
        else:
            local_bind_address = ('localhost',)

        if self.password and self.password.strip():
            client = SSHTunnelForwarder(self.remote_host,
                                        ssh_port=self.port,
                                        ssh_username=self.username,
                                        ssh_password=self.password,
                                        ssh_pkey=self.key_file,
                                        ssh_proxy=self.host_proxy,
                                        local_bind_address=local_bind_address,
                                        remote_bind_address=(remote_host, remote_port),
                                        logger=self.log)
        else:
            client = SSHTunnelForwarder(self.remote_host,
                                        ssh_port=self.port,
                                        ssh_username=self.username,
                                        ssh_pkey=self.key_file,
                                        ssh_proxy=self.host_proxy,
                                        local_bind_address=local_bind_address,
                                        remote_bind_address=(remote_host, remote_port),
                                        host_pkey_directories=[],
                                        logger=self.log)

        return client","['def', 'get_tunnel', '(', 'self', ',', 'remote_port', ',', 'remote_host', '=', '""localhost""', ',', 'local_port', '=', 'None', ')', ':', 'if', 'local_port', ':', 'local_bind_address', '=', '(', ""'localhost'"", ',', 'local_port', ')', 'else', ':', 'local_bind_address', '=', '(', ""'localhost'"", ',', ')', 'if', 'self', '.', 'password', 'and', 'self', '.', 'password', '.', 'strip', '(', ')', ':', 'client', '=', 'SSHTunnelForwarder', '(', 'self', '.', 'remote_host', ',', 'ssh_port', '=', 'self', '.', 'port', ',', 'ssh_username', '=', 'self', '.', 'username', ',', 'ssh_password', '=', 'self', '.', 'password', ',', 'ssh_pkey', '=', 'self', '.', 'key_file', ',', 'ssh_proxy', '=', 'self', '.', 'host_proxy', ',', 'local_bind_address', '=', 'local_bind_address', ',', 'remote_bind_address', '=', '(', 'remote_host', ',', 'remote_port', ')', ',', 'logger', '=', 'self', '.', 'log', ')', 'else', ':', 'client', '=', 'SSHTunnelForwarder', '(', 'self', '.', 'remote_host', ',', 'ssh_port', '=', 'self', '.', 'port', ',', 'ssh_username', '=', 'self', '.', 'username', ',', 'ssh_pkey', '=', 'self', '.', 'key_file', ',', 'ssh_proxy', '=', 'self', '.', 'host_proxy', ',', 'local_bind_address', '=', 'local_bind_address', ',', 'remote_bind_address', '=', '(', 'remote_host', ',', 'remote_port', ')', ',', 'host_pkey_directories', '=', '[', ']', ',', 'logger', '=', 'self', '.', 'log', ')', 'return', 'client']","Creates a tunnel between two hosts. Like ssh -L <LOCAL_PORT>:host:<REMOTE_PORT>.

        :param remote_port: The remote port to create a tunnel to
        :type remote_port: int
        :param remote_host: The remote host to create a tunnel to (default localhost)
        :type remote_host: str
        :param local_port:  The local port to attach the tunnel to
        :type local_port: int

        :return: sshtunnel.SSHTunnelForwarder object","['Creates', 'a', 'tunnel', 'between', 'two', 'hosts', '.', 'Like', 'ssh', '-', 'L', '<LOCAL_PORT', '>', ':', 'host', ':', '<REMOTE_PORT', '>', '.']",python,test,"['creates', 'a', 'tunnel', 'between', 'two', 'hosts', '.', 'like', 'ssh', '-', 'l', '<local_port', '>', ':', 'host', ':', '<remote_port', '>', '.']",creates a tunnel between two hosts . like ssh - l <local_port > : host : <remote_port > .,"['def', 'get_tunnel', '(', 'self', ',', 'remote_port', ',', 'remote_host', '=', '""localhost""', ',', 'local_port', '=', 'none', ')', ':', 'if', 'local_port', ':', 'local_bind_address', '=', '(', ""'localhost'"", ',', 'local_port', ')', 'else', ':', 'local_bind_address', '=', '(', ""'localhost'"", ',', ')', 'if', 'self', '.', 'password', 'and', 'self', '.', 'password', '.', 'strip', '(', ')', ':', 'client', '=', 'sshtunnelforwarder', '(', 'self', '.', 'remote_host', ',', 'ssh_port', '=', 'self', '.', 'port', ',', 'ssh_username', '=', 'self', '.', 'username', ',', 'ssh_password', '=', 'self', '.', 'password', ',', 'ssh_pkey', '=', 'self', '.', 'key_file', ',', 'ssh_proxy', '=', 'self', '.', 'host_proxy', ',', 'local_bind_address', '=', 'local_bind_address', ',', 'remote_bind_address', '=', '(', 'remote_host', ',', 'remote_port', ')', ',', 'logger', '=', 'self', '.', 'log', ')', 'else', ':', 'client', '=', 'sshtunnelforwarder', '(', 'self', '.', 'remote_host', ',', 'ssh_port', '=', 'self', '.', 'port', ',', 'ssh_username', '=', 'self', '.', 'username', ',', 'ssh_pkey', '=', 'self', '.', 'key_file', ',', 'ssh_proxy', '=', 'self', '.', 'host_proxy', ',', 'local_bind_address', '=', 'local_bind_address', ',', 'remote_bind_address', '=', '(', 'remote_host', ',', 'remote_port', ')', ',', 'host_pkey_directories', '=', '[', ']', ',', 'logger', '=', 'self', '.', 'log', ')', 'return', 'client']","def get_tunnel ( self , remote_port , remote_host = ""localhost"" , local_port = none ) : if local_port : local_bind_address = ( 'localhost' , local_port ) else : local_bind_address = ( 'localhost' , ) if self . password and self . password . strip ( ) : client = sshtunnelforwarder ( self . remote_host , ssh_port = self . port , ssh_username = self . username , ssh_password = self . password , ssh_pkey = self . key_file , ssh_proxy = self . host_proxy , local_bind_address = local_bind_address , remote_bind_address = ( remote_host , remote_port ) , logger = self . log ) else : client = sshtunnelforwarder ( self . remote_host , ssh_port = self . port , ssh_username = self . username , ssh_pkey = self . key_file , ssh_proxy = self . host_proxy , local_bind_address = local_bind_address , remote_bind_address = ( remote_host , remote_port ) , host_pkey_directories = [ ] , logger = self . log ) return client"
202,apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L119-L132,"def create_transfer_job(self, body):
        """"""
        Creates a transfer job that runs periodically.

        :param body: (Required) A request body, as described in
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body
        :type body: dict
        :return: transfer job.
            See:
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob
        :rtype: dict
        """"""
        body = self._inject_project_id(body, BODY, PROJECT_ID)
        return self.get_conn().transferJobs().create(body=body).execute(num_retries=self.num_retries)","['def', 'create_transfer_job', '(', 'self', ',', 'body', ')', ':', 'body', '=', 'self', '.', '_inject_project_id', '(', 'body', ',', 'BODY', ',', 'PROJECT_ID', ')', 'return', 'self', '.', 'get_conn', '(', ')', '.', 'transferJobs', '(', ')', '.', 'create', '(', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')']","Creates a transfer job that runs periodically.

        :param body: (Required) A request body, as described in
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body
        :type body: dict
        :return: transfer job.
            See:
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob
        :rtype: dict","['Creates', 'a', 'transfer', 'job', 'that', 'runs', 'periodically', '.']",python,test,"['creates', 'a', 'transfer', 'job', 'that', 'runs', 'periodically', '.']",creates a transfer job that runs periodically .,"['def', 'create_transfer_job', '(', 'self', ',', 'body', ')', ':', 'body', '=', 'self', '.', '_inject_project_id', '(', 'body', ',', 'body', ',', 'project_id', ')', 'return', 'self', '.', 'get_conn', '(', ')', '.', 'transferjobs', '(', ')', '.', 'create', '(', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')']","def create_transfer_job ( self , body ) : body = self . _inject_project_id ( body , body , project_id ) return self . get_conn ( ) . transferjobs ( ) . create ( body = body ) . execute ( num_retries = self . num_retries )"
203,apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L136-L155,"def get_transfer_job(self, job_name, project_id=None):
        """"""
        Gets the latest state of a long-running operation in Google Storage
        Transfer Service.

        :param job_name: (Required) Name of the job to be fetched
        :type job_name: str
        :param project_id: (Optional) the ID of the project that owns the Transfer
            Job. If set to None or missing, the default project_id from the GCP
            connection is used.
        :type project_id: str
        :return: Transfer Job
        :rtype: dict
        """"""
        return (
            self.get_conn()
            .transferJobs()
            .get(jobName=job_name, projectId=project_id)
            .execute(num_retries=self.num_retries)
        )","['def', 'get_transfer_job', '(', 'self', ',', 'job_name', ',', 'project_id', '=', 'None', ')', ':', 'return', '(', 'self', '.', 'get_conn', '(', ')', '.', 'transferJobs', '(', ')', '.', 'get', '(', 'jobName', '=', 'job_name', ',', 'projectId', '=', 'project_id', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')']","Gets the latest state of a long-running operation in Google Storage
        Transfer Service.

        :param job_name: (Required) Name of the job to be fetched
        :type job_name: str
        :param project_id: (Optional) the ID of the project that owns the Transfer
            Job. If set to None or missing, the default project_id from the GCP
            connection is used.
        :type project_id: str
        :return: Transfer Job
        :rtype: dict","['Gets', 'the', 'latest', 'state', 'of', 'a', 'long', '-', 'running', 'operation', 'in', 'Google', 'Storage', 'Transfer', 'Service', '.']",python,test,"['gets', 'the', 'latest', 'state', 'of', 'a', 'long', '-', 'running', 'operation', 'in', 'google', 'storage', 'transfer', 'service', '.']",gets the latest state of a long - running operation in google storage transfer service .,"['def', 'get_transfer_job', '(', 'self', ',', 'job_name', ',', 'project_id', '=', 'none', ')', ':', 'return', '(', 'self', '.', 'get_conn', '(', ')', '.', 'transferjobs', '(', ')', '.', 'get', '(', 'jobname', '=', 'job_name', ',', 'projectid', '=', 'project_id', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')']","def get_transfer_job ( self , job_name , project_id = none ) : return ( self . get_conn ( ) . transferjobs ( ) . get ( jobname = job_name , projectid = project_id ) . execute ( num_retries = self . num_retries ) )"
204,apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L157-L179,"def list_transfer_job(self, filter):
        """"""
        Lists long-running operations in Google Storage Transfer
        Service that match the specified filter.

        :param filter: (Required) A request filter, as described in
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/list#body.QUERY_PARAMETERS.filter
        :type filter: dict
        :return: List of Transfer Jobs
        :rtype: list[dict]
        """"""
        conn = self.get_conn()
        filter = self._inject_project_id(filter, FILTER, FILTER_PROJECT_ID)
        request = conn.transferJobs().list(filter=json.dumps(filter))
        jobs = []

        while request is not None:
            response = request.execute(num_retries=self.num_retries)
            jobs.extend(response[TRANSFER_JOBS])

            request = conn.transferJobs().list_next(previous_request=request, previous_response=response)

        return jobs","['def', 'list_transfer_job', '(', 'self', ',', 'filter', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'filter', '=', 'self', '.', '_inject_project_id', '(', 'filter', ',', 'FILTER', ',', 'FILTER_PROJECT_ID', ')', 'request', '=', 'conn', '.', 'transferJobs', '(', ')', '.', 'list', '(', 'filter', '=', 'json', '.', 'dumps', '(', 'filter', ')', ')', 'jobs', '=', '[', ']', 'while', 'request', 'is', 'not', 'None', ':', 'response', '=', 'request', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'jobs', '.', 'extend', '(', 'response', '[', 'TRANSFER_JOBS', ']', ')', 'request', '=', 'conn', '.', 'transferJobs', '(', ')', '.', 'list_next', '(', 'previous_request', '=', 'request', ',', 'previous_response', '=', 'response', ')', 'return', 'jobs']","Lists long-running operations in Google Storage Transfer
        Service that match the specified filter.

        :param filter: (Required) A request filter, as described in
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/list#body.QUERY_PARAMETERS.filter
        :type filter: dict
        :return: List of Transfer Jobs
        :rtype: list[dict]","['Lists', 'long', '-', 'running', 'operations', 'in', 'Google', 'Storage', 'Transfer', 'Service', 'that', 'match', 'the', 'specified', 'filter', '.']",python,test,"['lists', 'long', '-', 'running', 'operations', 'in', 'google', 'storage', 'transfer', 'service', 'that', 'match', 'the', 'specified', 'filter', '.']",lists long - running operations in google storage transfer service that match the specified filter .,"['def', 'list_transfer_job', '(', 'self', ',', 'filter', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'filter', '=', 'self', '.', '_inject_project_id', '(', 'filter', ',', 'filter', ',', 'filter_project_id', ')', 'request', '=', 'conn', '.', 'transferjobs', '(', ')', '.', 'list', '(', 'filter', '=', 'json', '.', 'dumps', '(', 'filter', ')', ')', 'jobs', '=', '[', ']', 'while', 'request', 'is', 'not', 'none', ':', 'response', '=', 'request', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'jobs', '.', 'extend', '(', 'response', '[', 'transfer_jobs', ']', ')', 'request', '=', 'conn', '.', 'transferjobs', '(', ')', '.', 'list_next', '(', 'previous_request', '=', 'request', ',', 'previous_response', '=', 'response', ')', 'return', 'jobs']","def list_transfer_job ( self , filter ) : conn = self . get_conn ( ) filter = self . _inject_project_id ( filter , filter , filter_project_id ) request = conn . transferjobs ( ) . list ( filter = json . dumps ( filter ) ) jobs = [ ] while request is not none : response = request . execute ( num_retries = self . num_retries ) jobs . extend ( response [ transfer_jobs ] ) request = conn . transferjobs ( ) . list_next ( previous_request = request , previous_response = response ) return jobs"
205,apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L182-L200,"def update_transfer_job(self, job_name, body):
        """"""
        Updates a transfer job that runs periodically.

        :param job_name: (Required) Name of the job to be updated
        :type job_name: str
        :param body: A request body, as described in
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body
        :type body: dict
        :return: If successful, TransferJob.
        :rtype: dict
        """"""
        body = self._inject_project_id(body, BODY, PROJECT_ID)
        return (
            self.get_conn()
            .transferJobs()
            .patch(jobName=job_name, body=body)
            .execute(num_retries=self.num_retries)
        )","['def', 'update_transfer_job', '(', 'self', ',', 'job_name', ',', 'body', ')', ':', 'body', '=', 'self', '.', '_inject_project_id', '(', 'body', ',', 'BODY', ',', 'PROJECT_ID', ')', 'return', '(', 'self', '.', 'get_conn', '(', ')', '.', 'transferJobs', '(', ')', '.', 'patch', '(', 'jobName', '=', 'job_name', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')']","Updates a transfer job that runs periodically.

        :param job_name: (Required) Name of the job to be updated
        :type job_name: str
        :param body: A request body, as described in
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body
        :type body: dict
        :return: If successful, TransferJob.
        :rtype: dict","['Updates', 'a', 'transfer', 'job', 'that', 'runs', 'periodically', '.']",python,test,"['updates', 'a', 'transfer', 'job', 'that', 'runs', 'periodically', '.']",updates a transfer job that runs periodically .,"['def', 'update_transfer_job', '(', 'self', ',', 'job_name', ',', 'body', ')', ':', 'body', '=', 'self', '.', '_inject_project_id', '(', 'body', ',', 'body', ',', 'project_id', ')', 'return', '(', 'self', '.', 'get_conn', '(', ')', '.', 'transferjobs', '(', ')', '.', 'patch', '(', 'jobname', '=', 'job_name', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')']","def update_transfer_job ( self , job_name , body ) : body = self . _inject_project_id ( body , body , project_id ) return ( self . get_conn ( ) . transferjobs ( ) . patch ( jobname = job_name , body = body ) . execute ( num_retries = self . num_retries ) )"
206,apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L204-L232,"def delete_transfer_job(self, job_name, project_id):
        """"""
        Deletes a transfer job. This is a soft delete. After a transfer job is
        deleted, the job and all the transfer executions are subject to garbage
        collection. Transfer jobs become eligible for garbage collection
        30 days after soft delete.

        :param job_name: (Required) Name of the job to be deleted
        :type job_name: str
        :param project_id: (Optional) the ID of the project that owns the Transfer
            Job. If set to None or missing, the default project_id from the GCP
            connection is used.
        :type project_id: str
        :rtype: None
        """"""

        return (
            self.get_conn()
            .transferJobs()
            .patch(
                jobName=job_name,
                body={
                    PROJECT_ID: project_id,
                    TRANSFER_JOB: {STATUS1: GcpTransferJobsStatus.DELETED},
                    TRANSFER_JOB_FIELD_MASK: STATUS1,
                },
            )
            .execute(num_retries=self.num_retries)
        )","['def', 'delete_transfer_job', '(', 'self', ',', 'job_name', ',', 'project_id', ')', ':', 'return', '(', 'self', '.', 'get_conn', '(', ')', '.', 'transferJobs', '(', ')', '.', 'patch', '(', 'jobName', '=', 'job_name', ',', 'body', '=', '{', 'PROJECT_ID', ':', 'project_id', ',', 'TRANSFER_JOB', ':', '{', 'STATUS1', ':', 'GcpTransferJobsStatus', '.', 'DELETED', '}', ',', 'TRANSFER_JOB_FIELD_MASK', ':', 'STATUS1', ',', '}', ',', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')']","Deletes a transfer job. This is a soft delete. After a transfer job is
        deleted, the job and all the transfer executions are subject to garbage
        collection. Transfer jobs become eligible for garbage collection
        30 days after soft delete.

        :param job_name: (Required) Name of the job to be deleted
        :type job_name: str
        :param project_id: (Optional) the ID of the project that owns the Transfer
            Job. If set to None or missing, the default project_id from the GCP
            connection is used.
        :type project_id: str
        :rtype: None","['Deletes', 'a', 'transfer', 'job', '.', 'This', 'is', 'a', 'soft', 'delete', '.', 'After', 'a', 'transfer', 'job', 'is', 'deleted', 'the', 'job', 'and', 'all', 'the', 'transfer', 'executions', 'are', 'subject', 'to', 'garbage', 'collection', '.', 'Transfer', 'jobs', 'become', 'eligible', 'for', 'garbage', 'collection', '30', 'days', 'after', 'soft', 'delete', '.']",python,test,"['deletes', 'a', 'transfer', 'job', '.', 'this', 'is', 'a', 'soft', 'delete', '.', 'after', 'a', 'transfer', 'job', 'is', 'deleted', 'the', 'job', 'and', 'all', 'the', 'transfer', 'executions', 'are', 'subject', 'to', 'garbage', 'collection', '.', 'transfer', 'jobs', 'become', 'eligible', 'for', 'garbage', 'collection', '30', 'days', 'after', 'soft', 'delete', '.']",deletes a transfer job . this is a soft delete . after a transfer job is deleted the job and all the transfer executions are subject to garbage collection . transfer jobs become eligible for garbage collection 30 days after soft delete .,"['def', 'delete_transfer_job', '(', 'self', ',', 'job_name', ',', 'project_id', ')', ':', 'return', '(', 'self', '.', 'get_conn', '(', ')', '.', 'transferjobs', '(', ')', '.', 'patch', '(', 'jobname', '=', 'job_name', ',', 'body', '=', '{', 'project_id', ':', 'project_id', ',', 'transfer_job', ':', '{', 'status1', ':', 'gcptransferjobsstatus', '.', 'deleted', '}', ',', 'transfer_job_field_mask', ':', 'status1', ',', '}', ',', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')']","def delete_transfer_job ( self , job_name , project_id ) : return ( self . get_conn ( ) . transferjobs ( ) . patch ( jobname = job_name , body = { project_id : project_id , transfer_job : { status1 : gcptransferjobsstatus . deleted } , transfer_job_field_mask : status1 , } , ) . execute ( num_retries = self . num_retries ) )"
207,apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L235-L243,"def cancel_transfer_operation(self, operation_name):
        """"""
        Cancels an transfer operation in Google Storage Transfer Service.

        :param operation_name: Name of the transfer operation.
        :type operation_name: str
        :rtype: None
        """"""
        self.get_conn().transferOperations().cancel(name=operation_name).execute(num_retries=self.num_retries)","['def', 'cancel_transfer_operation', '(', 'self', ',', 'operation_name', ')', ':', 'self', '.', 'get_conn', '(', ')', '.', 'transferOperations', '(', ')', '.', 'cancel', '(', 'name', '=', 'operation_name', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')']","Cancels an transfer operation in Google Storage Transfer Service.

        :param operation_name: Name of the transfer operation.
        :type operation_name: str
        :rtype: None","['Cancels', 'an', 'transfer', 'operation', 'in', 'Google', 'Storage', 'Transfer', 'Service', '.']",python,test,"['cancels', 'an', 'transfer', 'operation', 'in', 'google', 'storage', 'transfer', 'service', '.']",cancels an transfer operation in google storage transfer service .,"['def', 'cancel_transfer_operation', '(', 'self', ',', 'operation_name', ')', ':', 'self', '.', 'get_conn', '(', ')', '.', 'transferoperations', '(', ')', '.', 'cancel', '(', 'name', '=', 'operation_name', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')']","def cancel_transfer_operation ( self , operation_name ) : self . get_conn ( ) . transferoperations ( ) . cancel ( name = operation_name ) . execute ( num_retries = self . num_retries )"
208,apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L246-L262,"def get_transfer_operation(self, operation_name):
        """"""
        Gets an transfer operation in Google Storage Transfer Service.

        :param operation_name: (Required) Name of the transfer operation.
        :type operation_name: str
        :return: transfer operation
            See:
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/Operation
        :rtype: dict
        """"""
        return (
            self.get_conn()
            .transferOperations()
            .get(name=operation_name)
            .execute(num_retries=self.num_retries)
        )","['def', 'get_transfer_operation', '(', 'self', ',', 'operation_name', ')', ':', 'return', '(', 'self', '.', 'get_conn', '(', ')', '.', 'transferOperations', '(', ')', '.', 'get', '(', 'name', '=', 'operation_name', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')']","Gets an transfer operation in Google Storage Transfer Service.

        :param operation_name: (Required) Name of the transfer operation.
        :type operation_name: str
        :return: transfer operation
            See:
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/Operation
        :rtype: dict","['Gets', 'an', 'transfer', 'operation', 'in', 'Google', 'Storage', 'Transfer', 'Service', '.']",python,test,"['gets', 'an', 'transfer', 'operation', 'in', 'google', 'storage', 'transfer', 'service', '.']",gets an transfer operation in google storage transfer service .,"['def', 'get_transfer_operation', '(', 'self', ',', 'operation_name', ')', ':', 'return', '(', 'self', '.', 'get_conn', '(', ')', '.', 'transferoperations', '(', ')', '.', 'get', '(', 'name', '=', 'operation_name', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')']","def get_transfer_operation ( self , operation_name ) : return ( self . get_conn ( ) . transferoperations ( ) . get ( name = operation_name ) . execute ( num_retries = self . num_retries ) )"
209,apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L265-L298,"def list_transfer_operations(self, filter):
        """"""
        Gets an transfer operation in Google Storage Transfer Service.

        :param filter: (Required) A request filter, as described in
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/list#body.QUERY_PARAMETERS.filter
            With one additional improvement:

            * project_id is optional if you have a project id defined
              in the connection
              See: :ref:`howto/connection:gcp`

        :type filter: dict
        :return: transfer operation
        :rtype: list[dict]
        """"""
        conn = self.get_conn()

        filter = self._inject_project_id(filter, FILTER, FILTER_PROJECT_ID)

        operations = []

        request = conn.transferOperations().list(name=TRANSFER_OPERATIONS, filter=json.dumps(filter))

        while request is not None:
            response = request.execute(num_retries=self.num_retries)
            if OPERATIONS in response:
                operations.extend(response[OPERATIONS])

            request = conn.transferOperations().list_next(
                previous_request=request, previous_response=response
            )

        return operations","['def', 'list_transfer_operations', '(', 'self', ',', 'filter', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'filter', '=', 'self', '.', '_inject_project_id', '(', 'filter', ',', 'FILTER', ',', 'FILTER_PROJECT_ID', ')', 'operations', '=', '[', ']', 'request', '=', 'conn', '.', 'transferOperations', '(', ')', '.', 'list', '(', 'name', '=', 'TRANSFER_OPERATIONS', ',', 'filter', '=', 'json', '.', 'dumps', '(', 'filter', ')', ')', 'while', 'request', 'is', 'not', 'None', ':', 'response', '=', 'request', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'if', 'OPERATIONS', 'in', 'response', ':', 'operations', '.', 'extend', '(', 'response', '[', 'OPERATIONS', ']', ')', 'request', '=', 'conn', '.', 'transferOperations', '(', ')', '.', 'list_next', '(', 'previous_request', '=', 'request', ',', 'previous_response', '=', 'response', ')', 'return', 'operations']","Gets an transfer operation in Google Storage Transfer Service.

        :param filter: (Required) A request filter, as described in
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/list#body.QUERY_PARAMETERS.filter
            With one additional improvement:

            * project_id is optional if you have a project id defined
              in the connection
              See: :ref:`howto/connection:gcp`

        :type filter: dict
        :return: transfer operation
        :rtype: list[dict]","['Gets', 'an', 'transfer', 'operation', 'in', 'Google', 'Storage', 'Transfer', 'Service', '.']",python,test,"['gets', 'an', 'transfer', 'operation', 'in', 'google', 'storage', 'transfer', 'service', '.']",gets an transfer operation in google storage transfer service .,"['def', 'list_transfer_operations', '(', 'self', ',', 'filter', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'filter', '=', 'self', '.', '_inject_project_id', '(', 'filter', ',', 'filter', ',', 'filter_project_id', ')', 'operations', '=', '[', ']', 'request', '=', 'conn', '.', 'transferoperations', '(', ')', '.', 'list', '(', 'name', '=', 'transfer_operations', ',', 'filter', '=', 'json', '.', 'dumps', '(', 'filter', ')', ')', 'while', 'request', 'is', 'not', 'none', ':', 'response', '=', 'request', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'if', 'operations', 'in', 'response', ':', 'operations', '.', 'extend', '(', 'response', '[', 'operations', ']', ')', 'request', '=', 'conn', '.', 'transferoperations', '(', ')', '.', 'list_next', '(', 'previous_request', '=', 'request', ',', 'previous_response', '=', 'response', ')', 'return', 'operations']","def list_transfer_operations ( self , filter ) : conn = self . get_conn ( ) filter = self . _inject_project_id ( filter , filter , filter_project_id ) operations = [ ] request = conn . transferoperations ( ) . list ( name = transfer_operations , filter = json . dumps ( filter ) ) while request is not none : response = request . execute ( num_retries = self . num_retries ) if operations in response : operations . extend ( response [ operations ] ) request = conn . transferoperations ( ) . list_next ( previous_request = request , previous_response = response ) return operations"
210,apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L301-L309,"def pause_transfer_operation(self, operation_name):
        """"""
        Pauses an transfer operation in Google Storage Transfer Service.

        :param operation_name: (Required) Name of the transfer operation.
        :type operation_name: str
        :rtype: None
        """"""
        self.get_conn().transferOperations().pause(name=operation_name).execute(num_retries=self.num_retries)","['def', 'pause_transfer_operation', '(', 'self', ',', 'operation_name', ')', ':', 'self', '.', 'get_conn', '(', ')', '.', 'transferOperations', '(', ')', '.', 'pause', '(', 'name', '=', 'operation_name', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')']","Pauses an transfer operation in Google Storage Transfer Service.

        :param operation_name: (Required) Name of the transfer operation.
        :type operation_name: str
        :rtype: None","['Pauses', 'an', 'transfer', 'operation', 'in', 'Google', 'Storage', 'Transfer', 'Service', '.']",python,test,"['pauses', 'an', 'transfer', 'operation', 'in', 'google', 'storage', 'transfer', 'service', '.']",pauses an transfer operation in google storage transfer service .,"['def', 'pause_transfer_operation', '(', 'self', ',', 'operation_name', ')', ':', 'self', '.', 'get_conn', '(', ')', '.', 'transferoperations', '(', ')', '.', 'pause', '(', 'name', '=', 'operation_name', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')']","def pause_transfer_operation ( self , operation_name ) : self . get_conn ( ) . transferoperations ( ) . pause ( name = operation_name ) . execute ( num_retries = self . num_retries )"
211,apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L312-L320,"def resume_transfer_operation(self, operation_name):
        """"""
        Resumes an transfer operation in Google Storage Transfer Service.

        :param operation_name: (Required) Name of the transfer operation.
        :type operation_name: str
        :rtype: None
        """"""
        self.get_conn().transferOperations().resume(name=operation_name).execute(num_retries=self.num_retries)","['def', 'resume_transfer_operation', '(', 'self', ',', 'operation_name', ')', ':', 'self', '.', 'get_conn', '(', ')', '.', 'transferOperations', '(', ')', '.', 'resume', '(', 'name', '=', 'operation_name', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')']","Resumes an transfer operation in Google Storage Transfer Service.

        :param operation_name: (Required) Name of the transfer operation.
        :type operation_name: str
        :rtype: None","['Resumes', 'an', 'transfer', 'operation', 'in', 'Google', 'Storage', 'Transfer', 'Service', '.']",python,test,"['resumes', 'an', 'transfer', 'operation', 'in', 'google', 'storage', 'transfer', 'service', '.']",resumes an transfer operation in google storage transfer service .,"['def', 'resume_transfer_operation', '(', 'self', ',', 'operation_name', ')', ':', 'self', '.', 'get_conn', '(', ')', '.', 'transferoperations', '(', ')', '.', 'resume', '(', 'name', '=', 'operation_name', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')']","def resume_transfer_operation ( self , operation_name ) : self . get_conn ( ) . transferoperations ( ) . resume ( name = operation_name ) . execute ( num_retries = self . num_retries )"
212,apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L323-L348,"def wait_for_transfer_job(self, job, expected_statuses=(GcpTransferOperationStatus.SUCCESS,), timeout=60):
        """"""
        Waits until the job reaches the expected state.

        :param job: Transfer job
            See:
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob
        :type job: dict
        :param expected_statuses: State that is expected
            See:
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status
        :type expected_statuses: set[str]
        :param timeout:
        :type timeout: time in which the operation must end in seconds
        :rtype: None
        """"""
        while timeout > 0:
            operations = self.list_transfer_operations(
                filter={FILTER_PROJECT_ID: job[PROJECT_ID], FILTER_JOB_NAMES: [job[NAME]]}
            )

            if GCPTransferServiceHook.operations_contain_expected_statuses(operations, expected_statuses):
                return
            time.sleep(TIME_TO_SLEEP_IN_SECONDS)
            timeout -= TIME_TO_SLEEP_IN_SECONDS
        raise AirflowException(""Timeout. The operation could not be completed within the allotted time."")","['def', 'wait_for_transfer_job', '(', 'self', ',', 'job', ',', 'expected_statuses', '=', '(', 'GcpTransferOperationStatus', '.', 'SUCCESS', ',', ')', ',', 'timeout', '=', '60', ')', ':', 'while', 'timeout', '>', '0', ':', 'operations', '=', 'self', '.', 'list_transfer_operations', '(', 'filter', '=', '{', 'FILTER_PROJECT_ID', ':', 'job', '[', 'PROJECT_ID', ']', ',', 'FILTER_JOB_NAMES', ':', '[', 'job', '[', 'NAME', ']', ']', '}', ')', 'if', 'GCPTransferServiceHook', '.', 'operations_contain_expected_statuses', '(', 'operations', ',', 'expected_statuses', ')', ':', 'return', 'time', '.', 'sleep', '(', 'TIME_TO_SLEEP_IN_SECONDS', ')', 'timeout', '-=', 'TIME_TO_SLEEP_IN_SECONDS', 'raise', 'AirflowException', '(', '""Timeout. The operation could not be completed within the allotted time.""', ')']","Waits until the job reaches the expected state.

        :param job: Transfer job
            See:
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob
        :type job: dict
        :param expected_statuses: State that is expected
            See:
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status
        :type expected_statuses: set[str]
        :param timeout:
        :type timeout: time in which the operation must end in seconds
        :rtype: None","['Waits', 'until', 'the', 'job', 'reaches', 'the', 'expected', 'state', '.']",python,test,"['waits', 'until', 'the', 'job', 'reaches', 'the', 'expected', 'state', '.']",waits until the job reaches the expected state .,"['def', 'wait_for_transfer_job', '(', 'self', ',', 'job', ',', 'expected_statuses', '=', '(', 'gcptransferoperationstatus', '.', 'success', ',', ')', ',', 'timeout', '=', '60', ')', ':', 'while', 'timeout', '>', '0', ':', 'operations', '=', 'self', '.', 'list_transfer_operations', '(', 'filter', '=', '{', 'filter_project_id', ':', 'job', '[', 'project_id', ']', ',', 'filter_job_names', ':', '[', 'job', '[', 'name', ']', ']', '}', ')', 'if', 'gcptransferservicehook', '.', 'operations_contain_expected_statuses', '(', 'operations', ',', 'expected_statuses', ')', ':', 'return', 'time', '.', 'sleep', '(', 'time_to_sleep_in_seconds', ')', 'timeout', '-=', 'time_to_sleep_in_seconds', 'raise', 'airflowexception', '(', '""timeout. the operation could not be completed within the allotted time.""', ')']","def wait_for_transfer_job ( self , job , expected_statuses = ( gcptransferoperationstatus . success , ) , timeout = 60 ) : while timeout > 0 : operations = self . list_transfer_operations ( filter = { filter_project_id : job [ project_id ] , filter_job_names : [ job [ name ] ] } ) if gcptransferservicehook . operations_contain_expected_statuses ( operations , expected_statuses ) : return time . sleep ( time_to_sleep_in_seconds ) timeout -= time_to_sleep_in_seconds raise airflowexception ( ""timeout. the operation could not be completed within the allotted time."" )"
213,apache/airflow,airflow/contrib/hooks/gcp_transfer_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_transfer_hook.py#L361-L397,"def operations_contain_expected_statuses(operations, expected_statuses):
        """"""
        Checks whether the operation list has an operation with the
        expected status, then returns true
        If it encounters operations in FAILED or ABORTED state
        throw :class:`airflow.exceptions.AirflowException`.

        :param operations: (Required) List of transfer operations to check.
        :type operations: list[dict]
        :param expected_statuses: (Required) status that is expected
            See:
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status
        :type expected_statuses: set[str]
        :return: If there is an operation with the expected state
            in the operation list, returns true,
        :raises: airflow.exceptions.AirflowException If it encounters operations
            with a state in the list,
        :rtype: bool
        """"""
        expected_statuses = (
            {expected_statuses} if isinstance(expected_statuses, six.string_types) else set(expected_statuses)
        )
        if len(operations) == 0:
            return False

        current_statuses = {operation[METADATA][STATUS] for operation in operations}

        if len(current_statuses - set(expected_statuses)) != len(current_statuses):
            return True

        if len(NEGATIVE_STATUSES - current_statuses) != len(NEGATIVE_STATUSES):
            raise AirflowException(
                'An unexpected operation status was encountered. Expected: {}'.format(
                    "", "".join(expected_statuses)
                )
            )
        return False","['def', 'operations_contain_expected_statuses', '(', 'operations', ',', 'expected_statuses', ')', ':', 'expected_statuses', '=', '(', '{', 'expected_statuses', '}', 'if', 'isinstance', '(', 'expected_statuses', ',', 'six', '.', 'string_types', ')', 'else', 'set', '(', 'expected_statuses', ')', ')', 'if', 'len', '(', 'operations', ')', '==', '0', ':', 'return', 'False', 'current_statuses', '=', '{', 'operation', '[', 'METADATA', ']', '[', 'STATUS', ']', 'for', 'operation', 'in', 'operations', '}', 'if', 'len', '(', 'current_statuses', '-', 'set', '(', 'expected_statuses', ')', ')', '!=', 'len', '(', 'current_statuses', ')', ':', 'return', 'True', 'if', 'len', '(', 'NEGATIVE_STATUSES', '-', 'current_statuses', ')', '!=', 'len', '(', 'NEGATIVE_STATUSES', ')', ':', 'raise', 'AirflowException', '(', ""'An unexpected operation status was encountered. Expected: {}'"", '.', 'format', '(', '"", ""', '.', 'join', '(', 'expected_statuses', ')', ')', ')', 'return', 'False']","Checks whether the operation list has an operation with the
        expected status, then returns true
        If it encounters operations in FAILED or ABORTED state
        throw :class:`airflow.exceptions.AirflowException`.

        :param operations: (Required) List of transfer operations to check.
        :type operations: list[dict]
        :param expected_statuses: (Required) status that is expected
            See:
            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status
        :type expected_statuses: set[str]
        :return: If there is an operation with the expected state
            in the operation list, returns true,
        :raises: airflow.exceptions.AirflowException If it encounters operations
            with a state in the list,
        :rtype: bool","['Checks', 'whether', 'the', 'operation', 'list', 'has', 'an', 'operation', 'with', 'the', 'expected', 'status', 'then', 'returns', 'true', 'If', 'it', 'encounters', 'operations', 'in', 'FAILED', 'or', 'ABORTED', 'state', 'throw', ':', 'class', ':', 'airflow', '.', 'exceptions', '.', 'AirflowException', '.']",python,test,"['checks', 'whether', 'the', 'operation', 'list', 'has', 'an', 'operation', 'with', 'the', 'expected', 'status', 'then', 'returns', 'true', 'if', 'it', 'encounters', 'operations', 'in', 'failed', 'or', 'aborted', 'state', 'throw', ':', 'class', ':', 'airflow', '.', 'exceptions', '.', 'airflowexception', '.']",checks whether the operation list has an operation with the expected status then returns true if it encounters operations in failed or aborted state throw : class : airflow . exceptions . airflowexception .,"['def', 'operations_contain_expected_statuses', '(', 'operations', ',', 'expected_statuses', ')', ':', 'expected_statuses', '=', '(', '{', 'expected_statuses', '}', 'if', 'isinstance', '(', 'expected_statuses', ',', 'six', '.', 'string_types', ')', 'else', 'set', '(', 'expected_statuses', ')', ')', 'if', 'len', '(', 'operations', ')', '==', '0', ':', 'return', 'false', 'current_statuses', '=', '{', 'operation', '[', 'metadata', ']', '[', 'status', ']', 'for', 'operation', 'in', 'operations', '}', 'if', 'len', '(', 'current_statuses', '-', 'set', '(', 'expected_statuses', ')', ')', '!=', 'len', '(', 'current_statuses', ')', ':', 'return', 'true', 'if', 'len', '(', 'negative_statuses', '-', 'current_statuses', ')', '!=', 'len', '(', 'negative_statuses', ')', ':', 'raise', 'airflowexception', '(', ""'an unexpected operation status was encountered. expected: {}'"", '.', 'format', '(', '"", ""', '.', 'join', '(', 'expected_statuses', ')', ')', ')', 'return', 'false']","def operations_contain_expected_statuses ( operations , expected_statuses ) : expected_statuses = ( { expected_statuses } if isinstance ( expected_statuses , six . string_types ) else set ( expected_statuses ) ) if len ( operations ) == 0 : return false current_statuses = { operation [ metadata ] [ status ] for operation in operations } if len ( current_statuses - set ( expected_statuses ) ) != len ( current_statuses ) : return true if len ( negative_statuses - current_statuses ) != len ( negative_statuses ) : raise airflowexception ( 'an unexpected operation status was encountered. expected: {}' . format ( "", "" . join ( expected_statuses ) ) ) return false"
214,apache/airflow,airflow/models/taskreschedule.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskreschedule.py#L67-L85,"def find_for_task_instance(task_instance, session):
        """"""
        Returns all task reschedules for the task instance and try number,
        in ascending order.

        :param task_instance: the task instance to find task reschedules for
        :type task_instance: airflow.models.TaskInstance
        """"""
        TR = TaskReschedule
        return (
            session
            .query(TR)
            .filter(TR.dag_id == task_instance.dag_id,
                    TR.task_id == task_instance.task_id,
                    TR.execution_date == task_instance.execution_date,
                    TR.try_number == task_instance.try_number)
            .order_by(asc(TR.id))
            .all()
        )","['def', 'find_for_task_instance', '(', 'task_instance', ',', 'session', ')', ':', 'TR', '=', 'TaskReschedule', 'return', '(', 'session', '.', 'query', '(', 'TR', ')', '.', 'filter', '(', 'TR', '.', 'dag_id', '==', 'task_instance', '.', 'dag_id', ',', 'TR', '.', 'task_id', '==', 'task_instance', '.', 'task_id', ',', 'TR', '.', 'execution_date', '==', 'task_instance', '.', 'execution_date', ',', 'TR', '.', 'try_number', '==', 'task_instance', '.', 'try_number', ')', '.', 'order_by', '(', 'asc', '(', 'TR', '.', 'id', ')', ')', '.', 'all', '(', ')', ')']","Returns all task reschedules for the task instance and try number,
        in ascending order.

        :param task_instance: the task instance to find task reschedules for
        :type task_instance: airflow.models.TaskInstance","['Returns', 'all', 'task', 'reschedules', 'for', 'the', 'task', 'instance', 'and', 'try', 'number', 'in', 'ascending', 'order', '.']",python,test,"['returns', 'all', 'task', 'reschedules', 'for', 'the', 'task', 'instance', 'and', 'try', 'number', 'in', 'ascending', 'order', '.']",returns all task reschedules for the task instance and try number in ascending order .,"['def', 'find_for_task_instance', '(', 'task_instance', ',', 'session', ')', ':', 'tr', '=', 'taskreschedule', 'return', '(', 'session', '.', 'query', '(', 'tr', ')', '.', 'filter', '(', 'tr', '.', 'dag_id', '==', 'task_instance', '.', 'dag_id', ',', 'tr', '.', 'task_id', '==', 'task_instance', '.', 'task_id', ',', 'tr', '.', 'execution_date', '==', 'task_instance', '.', 'execution_date', ',', 'tr', '.', 'try_number', '==', 'task_instance', '.', 'try_number', ')', '.', 'order_by', '(', 'asc', '(', 'tr', '.', 'id', ')', ')', '.', 'all', '(', ')', ')']","def find_for_task_instance ( task_instance , session ) : tr = taskreschedule return ( session . query ( tr ) . filter ( tr . dag_id == task_instance . dag_id , tr . task_id == task_instance . task_id , tr . execution_date == task_instance . execution_date , tr . try_number == task_instance . try_number ) . order_by ( asc ( tr . id ) ) . all ( ) )"
215,apache/airflow,airflow/contrib/executors/kubernetes_executor.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/executors/kubernetes_executor.py#L458-L470,"def _strip_unsafe_kubernetes_special_chars(string):
        """"""
        Kubernetes only supports lowercase alphanumeric characters and ""-"" and ""."" in
        the pod name
        However, there are special rules about how ""-"" and ""."" can be used so let's
        only keep
        alphanumeric chars  see here for detail:
        https://kubernetes.io/docs/concepts/overview/working-with-objects/names/

        :param string: The requested Pod name
        :return: ``str`` Pod name stripped of any unsafe characters
        """"""
        return ''.join(ch.lower() for ind, ch in enumerate(string) if ch.isalnum())","['def', '_strip_unsafe_kubernetes_special_chars', '(', 'string', ')', ':', 'return', ""''"", '.', 'join', '(', 'ch', '.', 'lower', '(', ')', 'for', 'ind', ',', 'ch', 'in', 'enumerate', '(', 'string', ')', 'if', 'ch', '.', 'isalnum', '(', ')', ')']","Kubernetes only supports lowercase alphanumeric characters and ""-"" and ""."" in
        the pod name
        However, there are special rules about how ""-"" and ""."" can be used so let's
        only keep
        alphanumeric chars  see here for detail:
        https://kubernetes.io/docs/concepts/overview/working-with-objects/names/

        :param string: The requested Pod name
        :return: ``str`` Pod name stripped of any unsafe characters","['Kubernetes', 'only', 'supports', 'lowercase', 'alphanumeric', 'characters', 'and', '-', 'and', '.', 'in', 'the', 'pod', 'name', 'However', 'there', 'are', 'special', 'rules', 'about', 'how', '-', 'and', '.', 'can', 'be', 'used', 'so', 'let', 's', 'only', 'keep', 'alphanumeric', 'chars', 'see', 'here', 'for', 'detail', ':', 'https', ':', '//', 'kubernetes', '.', 'io', '/', 'docs', '/', 'concepts', '/', 'overview', '/', 'working', '-', 'with', '-', 'objects', '/', 'names', '/']",python,test,"['kubernetes', 'only', 'supports', 'lowercase', 'alphanumeric', 'characters', 'and', '-', 'and', '.', 'in', 'the', 'pod', 'name', 'however', 'there', 'are', 'special', 'rules', 'about', 'how', '-', 'and', '.', 'can', 'be', 'used', 'so', 'let', 's', 'only', 'keep', 'alphanumeric', 'chars', 'see', 'here', 'for', 'detail', ':', 'https', ':', '//', 'kubernetes', '.', 'io', '/', 'docs', '/', 'concepts', '/', 'overview', '/', 'working', '-', 'with', '-', 'objects', '/', 'names', '/']",kubernetes only supports lowercase alphanumeric characters and - and . in the pod name however there are special rules about how - and . can be used so let s only keep alphanumeric chars see here for detail : https : // kubernetes . io / docs / concepts / overview / working - with - objects / names /,"['def', '_strip_unsafe_kubernetes_special_chars', '(', 'string', ')', ':', 'return', ""''"", '.', 'join', '(', 'ch', '.', 'lower', '(', ')', 'for', 'ind', ',', 'ch', 'in', 'enumerate', '(', 'string', ')', 'if', 'ch', '.', 'isalnum', '(', ')', ')']","def _strip_unsafe_kubernetes_special_chars ( string ) : return '' . join ( ch . lower ( ) for ind , ch in enumerate ( string ) if ch . isalnum ( ) )"
216,apache/airflow,airflow/contrib/executors/kubernetes_executor.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/executors/kubernetes_executor.py#L473-L490,"def _make_safe_pod_id(safe_dag_id, safe_task_id, safe_uuid):
        """"""
        Kubernetes pod names must be <= 253 chars and must pass the following regex for
        validation
        ""^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$""

        :param safe_dag_id: a dag_id with only alphanumeric characters
        :param safe_task_id: a task_id with only alphanumeric characters
        :param random_uuid: a uuid
        :return: ``str`` valid Pod name of appropriate length
        """"""
        MAX_POD_ID_LEN = 253

        safe_key = safe_dag_id + safe_task_id

        safe_pod_id = safe_key[:MAX_POD_ID_LEN - len(safe_uuid) - 1] + ""-"" + safe_uuid

        return safe_pod_id","['def', '_make_safe_pod_id', '(', 'safe_dag_id', ',', 'safe_task_id', ',', 'safe_uuid', ')', ':', 'MAX_POD_ID_LEN', '=', '253', 'safe_key', '=', 'safe_dag_id', '+', 'safe_task_id', 'safe_pod_id', '=', 'safe_key', '[', ':', 'MAX_POD_ID_LEN', '-', 'len', '(', 'safe_uuid', ')', '-', '1', ']', '+', '""-""', '+', 'safe_uuid', 'return', 'safe_pod_id']","Kubernetes pod names must be <= 253 chars and must pass the following regex for
        validation
        ""^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$""

        :param safe_dag_id: a dag_id with only alphanumeric characters
        :param safe_task_id: a task_id with only alphanumeric characters
        :param random_uuid: a uuid
        :return: ``str`` valid Pod name of appropriate length","['Kubernetes', 'pod', 'names', 'must', 'be', '<', '=', '253', 'chars', 'and', 'must', 'pass', 'the', 'following', 'regex', 'for', 'validation', '^', '[', 'a', '-', 'z0', '-', '9', ']', '(', '[', '-', 'a', '-', 'z0', '-', '9', ']', '*', '[', 'a', '-', 'z0', '-', '9', ']', ')', '?', '(', '\\\\', '.', '[', 'a', '-', 'z0', '-', '9', ']', '(', '[', '-', 'a', '-', 'z0', '-', '9', ']', '*', '[', 'a', '-', 'z0', '-', '9', ']', ')', '?', ')', '*', '$']",python,test,"['kubernetes', 'pod', 'names', 'must', 'be', '<', '=', '253', 'chars', 'and', 'must', 'pass', 'the', 'following', 'regex', 'for', 'validation', '^', '[', 'a', '-', 'z0', '-', '9', ']', '(', '[', '-', 'a', '-', 'z0', '-', '9', ']', '*', '[', 'a', '-', 'z0', '-', '9', ']', ')', '?', '(', '\\\\', '.', '[', 'a', '-', 'z0', '-', '9', ']', '(', '[', '-', 'a', '-', 'z0', '-', '9', ']', '*', '[', 'a', '-', 'z0', '-', '9', ']', ')', '?', ')', '*', '$']",kubernetes pod names must be < = 253 chars and must pass the following regex for validation ^ [ a - z0 - 9 ] ( [ - a - z0 - 9 ] * [ a - z0 - 9 ] ) ? ( \\ . [ a - z0 - 9 ] ( [ - a - z0 - 9 ] * [ a - z0 - 9 ] ) ? ) * $,"['def', '_make_safe_pod_id', '(', 'safe_dag_id', ',', 'safe_task_id', ',', 'safe_uuid', ')', ':', 'max_pod_id_len', '=', '253', 'safe_key', '=', 'safe_dag_id', '+', 'safe_task_id', 'safe_pod_id', '=', 'safe_key', '[', ':', 'max_pod_id_len', '-', 'len', '(', 'safe_uuid', ')', '-', '1', ']', '+', '""-""', '+', 'safe_uuid', 'return', 'safe_pod_id']","def _make_safe_pod_id ( safe_dag_id , safe_task_id , safe_uuid ) : max_pod_id_len = 253 safe_key = safe_dag_id + safe_task_id safe_pod_id = safe_key [ : max_pod_id_len - len ( safe_uuid ) - 1 ] + ""-"" + safe_uuid return safe_pod_id"
217,apache/airflow,airflow/contrib/executors/kubernetes_executor.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/executors/kubernetes_executor.py#L493-L511,"def _make_safe_label_value(string):
        """"""
        Valid label values must be 63 characters or less and must be empty or begin and
        end with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (_),
        dots (.), and alphanumerics between.

        If the label value is then greater than 63 chars once made safe, or differs in any
        way from the original value sent to this function, then we need to truncate to
        53chars, and append it with a unique hash.
        """"""
        MAX_LABEL_LEN = 63

        safe_label = re.sub(r'^[^a-z0-9A-Z]*|[^a-zA-Z0-9_\-\.]|[^a-z0-9A-Z]*$', '', string)

        if len(safe_label) > MAX_LABEL_LEN or string != safe_label:
            safe_hash = hashlib.md5(string.encode()).hexdigest()[:9]
            safe_label = safe_label[:MAX_LABEL_LEN - len(safe_hash) - 1] + ""-"" + safe_hash

        return safe_label","['def', '_make_safe_label_value', '(', 'string', ')', ':', 'MAX_LABEL_LEN', '=', '63', 'safe_label', '=', 're', '.', 'sub', '(', ""r'^[^a-z0-9A-Z]*|[^a-zA-Z0-9_\\-\\.]|[^a-z0-9A-Z]*$'"", ',', ""''"", ',', 'string', ')', 'if', 'len', '(', 'safe_label', ')', '>', 'MAX_LABEL_LEN', 'or', 'string', '!=', 'safe_label', ':', 'safe_hash', '=', 'hashlib', '.', 'md5', '(', 'string', '.', 'encode', '(', ')', ')', '.', 'hexdigest', '(', ')', '[', ':', '9', ']', 'safe_label', '=', 'safe_label', '[', ':', 'MAX_LABEL_LEN', '-', 'len', '(', 'safe_hash', ')', '-', '1', ']', '+', '""-""', '+', 'safe_hash', 'return', 'safe_label']","Valid label values must be 63 characters or less and must be empty or begin and
        end with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (_),
        dots (.), and alphanumerics between.

        If the label value is then greater than 63 chars once made safe, or differs in any
        way from the original value sent to this function, then we need to truncate to
        53chars, and append it with a unique hash.","['Valid', 'label', 'values', 'must', 'be', '63', 'characters', 'or', 'less', 'and', 'must', 'be', 'empty', 'or', 'begin', 'and', 'end', 'with', 'an', 'alphanumeric', 'character', '(', '[', 'a', '-', 'z0', '-', '9A', '-', 'Z', ']', ')', 'with', 'dashes', '(', '-', ')', 'underscores', '(', '_', ')', 'dots', '(', '.', ')', 'and', 'alphanumerics', 'between', '.']",python,test,"['valid', 'label', 'values', 'must', 'be', '63', 'characters', 'or', 'less', 'and', 'must', 'be', 'empty', 'or', 'begin', 'and', 'end', 'with', 'an', 'alphanumeric', 'character', '(', '[', 'a', '-', 'z0', '-', '9a', '-', 'z', ']', ')', 'with', 'dashes', '(', '-', ')', 'underscores', '(', '_', ')', 'dots', '(', '.', ')', 'and', 'alphanumerics', 'between', '.']",valid label values must be 63 characters or less and must be empty or begin and end with an alphanumeric character ( [ a - z0 - 9a - z ] ) with dashes ( - ) underscores ( _ ) dots ( . ) and alphanumerics between .,"['def', '_make_safe_label_value', '(', 'string', ')', ':', 'max_label_len', '=', '63', 'safe_label', '=', 're', '.', 'sub', '(', ""r'^[^a-z0-9a-z]*|[^a-za-z0-9_\\-\\.]|[^a-z0-9a-z]*$'"", ',', ""''"", ',', 'string', ')', 'if', 'len', '(', 'safe_label', ')', '>', 'max_label_len', 'or', 'string', '!=', 'safe_label', ':', 'safe_hash', '=', 'hashlib', '.', 'md5', '(', 'string', '.', 'encode', '(', ')', ')', '.', 'hexdigest', '(', ')', '[', ':', '9', ']', 'safe_label', '=', 'safe_label', '[', ':', 'max_label_len', '-', 'len', '(', 'safe_hash', ')', '-', '1', ']', '+', '""-""', '+', 'safe_hash', 'return', 'safe_label']","def _make_safe_label_value ( string ) : max_label_len = 63 safe_label = re . sub ( r'^[^a-z0-9a-z]*|[^a-za-z0-9_\-\.]|[^a-z0-9a-z]*$' , '' , string ) if len ( safe_label ) > max_label_len or string != safe_label : safe_hash = hashlib . md5 ( string . encode ( ) ) . hexdigest ( ) [ : 9 ] safe_label = safe_label [ : max_label_len - len ( safe_hash ) - 1 ] + ""-"" + safe_hash return safe_label"
218,apache/airflow,airflow/contrib/executors/kubernetes_executor.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/executors/kubernetes_executor.py#L606-L653,"def clear_not_launched_queued_tasks(self, session=None):
        """"""
        If the airflow scheduler restarts with pending ""Queued"" tasks, the tasks may or
        may not
        have been launched Thus, on starting up the scheduler let's check every
        ""Queued"" task to
        see if it has been launched (ie: if there is a corresponding pod on kubernetes)

        If it has been launched then do nothing, otherwise reset the state to ""None"" so
        the task
        will be rescheduled

        This will not be necessary in a future version of airflow in which there is
        proper support
        for State.LAUNCHED
        """"""
        queued_tasks = session\
            .query(TaskInstance)\
            .filter(TaskInstance.state == State.QUEUED).all()
        self.log.info(
            'When executor started up, found %s queued task instances',
            len(queued_tasks)
        )

        for task in queued_tasks:
            dict_string = (
                ""dag_id={},task_id={},execution_date={},airflow-worker={}"".format(
                    AirflowKubernetesScheduler._make_safe_label_value(task.dag_id),
                    AirflowKubernetesScheduler._make_safe_label_value(task.task_id),
                    AirflowKubernetesScheduler._datetime_to_label_safe_datestring(
                        task.execution_date
                    ),
                    self.worker_uuid
                )
            )
            kwargs = dict(label_selector=dict_string)
            pod_list = self.kube_client.list_namespaced_pod(
                self.kube_config.kube_namespace, **kwargs)
            if len(pod_list.items) == 0:
                self.log.info(
                    'TaskInstance: %s found in queued state but was not launched, '
                    'rescheduling', task
                )
                session.query(TaskInstance).filter(
                    TaskInstance.dag_id == task.dag_id,
                    TaskInstance.task_id == task.task_id,
                    TaskInstance.execution_date == task.execution_date
                ).update({TaskInstance.state: State.NONE})","['def', 'clear_not_launched_queued_tasks', '(', 'self', ',', 'session', '=', 'None', ')', ':', 'queued_tasks', '=', 'session', '.', 'query', '(', 'TaskInstance', ')', '.', 'filter', '(', 'TaskInstance', '.', 'state', '==', 'State', '.', 'QUEUED', ')', '.', 'all', '(', ')', 'self', '.', 'log', '.', 'info', '(', ""'When executor started up, found %s queued task instances'"", ',', 'len', '(', 'queued_tasks', ')', ')', 'for', 'task', 'in', 'queued_tasks', ':', 'dict_string', '=', '(', '""dag_id={},task_id={},execution_date={},airflow-worker={}""', '.', 'format', '(', 'AirflowKubernetesScheduler', '.', '_make_safe_label_value', '(', 'task', '.', 'dag_id', ')', ',', 'AirflowKubernetesScheduler', '.', '_make_safe_label_value', '(', 'task', '.', 'task_id', ')', ',', 'AirflowKubernetesScheduler', '.', '_datetime_to_label_safe_datestring', '(', 'task', '.', 'execution_date', ')', ',', 'self', '.', 'worker_uuid', ')', ')', 'kwargs', '=', 'dict', '(', 'label_selector', '=', 'dict_string', ')', 'pod_list', '=', 'self', '.', 'kube_client', '.', 'list_namespaced_pod', '(', 'self', '.', 'kube_config', '.', 'kube_namespace', ',', '*', '*', 'kwargs', ')', 'if', 'len', '(', 'pod_list', '.', 'items', ')', '==', '0', ':', 'self', '.', 'log', '.', 'info', '(', ""'TaskInstance: %s found in queued state but was not launched, '"", ""'rescheduling'"", ',', 'task', ')', 'session', '.', 'query', '(', 'TaskInstance', ')', '.', 'filter', '(', 'TaskInstance', '.', 'dag_id', '==', 'task', '.', 'dag_id', ',', 'TaskInstance', '.', 'task_id', '==', 'task', '.', 'task_id', ',', 'TaskInstance', '.', 'execution_date', '==', 'task', '.', 'execution_date', ')', '.', 'update', '(', '{', 'TaskInstance', '.', 'state', ':', 'State', '.', 'NONE', '}', ')']","If the airflow scheduler restarts with pending ""Queued"" tasks, the tasks may or
        may not
        have been launched Thus, on starting up the scheduler let's check every
        ""Queued"" task to
        see if it has been launched (ie: if there is a corresponding pod on kubernetes)

        If it has been launched then do nothing, otherwise reset the state to ""None"" so
        the task
        will be rescheduled

        This will not be necessary in a future version of airflow in which there is
        proper support
        for State.LAUNCHED","['If', 'the', 'airflow', 'scheduler', 'restarts', 'with', 'pending', 'Queued', 'tasks', 'the', 'tasks', 'may', 'or', 'may', 'not', 'have', 'been', 'launched', 'Thus', 'on', 'starting', 'up', 'the', 'scheduler', 'let', 's', 'check', 'every', 'Queued', 'task', 'to', 'see', 'if', 'it', 'has', 'been', 'launched', '(', 'ie', ':', 'if', 'there', 'is', 'a', 'corresponding', 'pod', 'on', 'kubernetes', ')']",python,test,"['if', 'the', 'airflow', 'scheduler', 'restarts', 'with', 'pending', 'queued', 'tasks', 'the', 'tasks', 'may', 'or', 'may', 'not', 'have', 'been', 'launched', 'thus', 'on', 'starting', 'up', 'the', 'scheduler', 'let', 's', 'check', 'every', 'queued', 'task', 'to', 'see', 'if', 'it', 'has', 'been', 'launched', '(', 'ie', ':', 'if', 'there', 'is', 'a', 'corresponding', 'pod', 'on', 'kubernetes', ')']",if the airflow scheduler restarts with pending queued tasks the tasks may or may not have been launched thus on starting up the scheduler let s check every queued task to see if it has been launched ( ie : if there is a corresponding pod on kubernetes ),"['def', 'clear_not_launched_queued_tasks', '(', 'self', ',', 'session', '=', 'none', ')', ':', 'queued_tasks', '=', 'session', '.', 'query', '(', 'taskinstance', ')', '.', 'filter', '(', 'taskinstance', '.', 'state', '==', 'state', '.', 'queued', ')', '.', 'all', '(', ')', 'self', '.', 'log', '.', 'info', '(', ""'when executor started up, found %s queued task instances'"", ',', 'len', '(', 'queued_tasks', ')', ')', 'for', 'task', 'in', 'queued_tasks', ':', 'dict_string', '=', '(', '""dag_id={},task_id={},execution_date={},airflow-worker={}""', '.', 'format', '(', 'airflowkubernetesscheduler', '.', '_make_safe_label_value', '(', 'task', '.', 'dag_id', ')', ',', 'airflowkubernetesscheduler', '.', '_make_safe_label_value', '(', 'task', '.', 'task_id', ')', ',', 'airflowkubernetesscheduler', '.', '_datetime_to_label_safe_datestring', '(', 'task', '.', 'execution_date', ')', ',', 'self', '.', 'worker_uuid', ')', ')', 'kwargs', '=', 'dict', '(', 'label_selector', '=', 'dict_string', ')', 'pod_list', '=', 'self', '.', 'kube_client', '.', 'list_namespaced_pod', '(', 'self', '.', 'kube_config', '.', 'kube_namespace', ',', '*', '*', 'kwargs', ')', 'if', 'len', '(', 'pod_list', '.', 'items', ')', '==', '0', ':', 'self', '.', 'log', '.', 'info', '(', ""'taskinstance: %s found in queued state but was not launched, '"", ""'rescheduling'"", ',', 'task', ')', 'session', '.', 'query', '(', 'taskinstance', ')', '.', 'filter', '(', 'taskinstance', '.', 'dag_id', '==', 'task', '.', 'dag_id', ',', 'taskinstance', '.', 'task_id', '==', 'task', '.', 'task_id', ',', 'taskinstance', '.', 'execution_date', '==', 'task', '.', 'execution_date', ')', '.', 'update', '(', '{', 'taskinstance', '.', 'state', ':', 'state', '.', 'none', '}', ')']","def clear_not_launched_queued_tasks ( self , session = none ) : queued_tasks = session . query ( taskinstance ) . filter ( taskinstance . state == state . queued ) . all ( ) self . log . info ( 'when executor started up, found %s queued task instances' , len ( queued_tasks ) ) for task in queued_tasks : dict_string = ( ""dag_id={},task_id={},execution_date={},airflow-worker={}"" . format ( airflowkubernetesscheduler . _make_safe_label_value ( task . dag_id ) , airflowkubernetesscheduler . _make_safe_label_value ( task . task_id ) , airflowkubernetesscheduler . _datetime_to_label_safe_datestring ( task . execution_date ) , self . worker_uuid ) ) kwargs = dict ( label_selector = dict_string ) pod_list = self . kube_client . list_namespaced_pod ( self . kube_config . kube_namespace , * * kwargs ) if len ( pod_list . items ) == 0 : self . log . info ( 'taskinstance: %s found in queued state but was not launched, ' 'rescheduling' , task ) session . query ( taskinstance ) . filter ( taskinstance . dag_id == task . dag_id , taskinstance . task_id == task . task_id , taskinstance . execution_date == task . execution_date ) . update ( { taskinstance . state : state . none } )"
219,apache/airflow,airflow/models/pool.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/pool.py#L60-L69,"def open_slots(self, session):
        """"""
        Returns the number of slots open at the moment
        """"""
        from airflow.models.taskinstance import \
            TaskInstance as TI  # Avoid circular import

        used_slots = session.query(func.count()).filter(TI.pool == self.pool).filter(
            TI.state.in_([State.RUNNING, State.QUEUED])).scalar()
        return self.slots - used_slots","['def', 'open_slots', '(', 'self', ',', 'session', ')', ':', 'from', 'airflow', '.', 'models', '.', 'taskinstance', 'import', 'TaskInstance', 'as', 'TI', '# Avoid circular import', 'used_slots', '=', 'session', '.', 'query', '(', 'func', '.', 'count', '(', ')', ')', '.', 'filter', '(', 'TI', '.', 'pool', '==', 'self', '.', 'pool', ')', '.', 'filter', '(', 'TI', '.', 'state', '.', 'in_', '(', '[', 'State', '.', 'RUNNING', ',', 'State', '.', 'QUEUED', ']', ')', ')', '.', 'scalar', '(', ')', 'return', 'self', '.', 'slots', '-', 'used_slots']",Returns the number of slots open at the moment,"['Returns', 'the', 'number', 'of', 'slots', 'open', 'at', 'the', 'moment']",python,test,"['returns', 'the', 'number', 'of', 'slots', 'open', 'at', 'the', 'moment']",returns the number of slots open at the moment,"['def', 'open_slots', '(', 'self', ',', 'session', ')', ':', 'from', 'airflow', '.', 'models', '.', 'taskinstance', 'import', 'taskinstance', 'as', 'ti', '# avoid circular import', 'used_slots', '=', 'session', '.', 'query', '(', 'func', '.', 'count', '(', ')', ')', '.', 'filter', '(', 'ti', '.', 'pool', '==', 'self', '.', 'pool', ')', '.', 'filter', '(', 'ti', '.', 'state', '.', 'in_', '(', '[', 'state', '.', 'running', ',', 'state', '.', 'queued', ']', ')', ')', '.', 'scalar', '(', ')', 'return', 'self', '.', 'slots', '-', 'used_slots']","def open_slots ( self , session ) : from airflow . models . taskinstance import taskinstance as ti # avoid circular import used_slots = session . query ( func . count ( ) ) . filter ( ti . pool == self . pool ) . filter ( ti . state . in_ ( [ state . running , state . queued ] ) ) . scalar ( ) return self . slots - used_slots"
220,apache/airflow,airflow/configuration.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/configuration.py#L59-L72,"def expand_env_var(env_var):
    """"""
    Expands (potentially nested) env vars by repeatedly applying
    `expandvars` and `expanduser` until interpolation stops having
    any effect.
    """"""
    if not env_var:
        return env_var
    while True:
        interpolated = os.path.expanduser(os.path.expandvars(str(env_var)))
        if interpolated == env_var:
            return interpolated
        else:
            env_var = interpolated","['def', 'expand_env_var', '(', 'env_var', ')', ':', 'if', 'not', 'env_var', ':', 'return', 'env_var', 'while', 'True', ':', 'interpolated', '=', 'os', '.', 'path', '.', 'expanduser', '(', 'os', '.', 'path', '.', 'expandvars', '(', 'str', '(', 'env_var', ')', ')', ')', 'if', 'interpolated', '==', 'env_var', ':', 'return', 'interpolated', 'else', ':', 'env_var', '=', 'interpolated']","Expands (potentially nested) env vars by repeatedly applying
    `expandvars` and `expanduser` until interpolation stops having
    any effect.","['Expands', '(', 'potentially', 'nested', ')', 'env', 'vars', 'by', 'repeatedly', 'applying', 'expandvars', 'and', 'expanduser', 'until', 'interpolation', 'stops', 'having', 'any', 'effect', '.']",python,test,"['expands', '(', 'potentially', 'nested', ')', 'env', 'vars', 'by', 'repeatedly', 'applying', 'expandvars', 'and', 'expanduser', 'until', 'interpolation', 'stops', 'having', 'any', 'effect', '.']",expands ( potentially nested ) env vars by repeatedly applying expandvars and expanduser until interpolation stops having any effect .,"['def', 'expand_env_var', '(', 'env_var', ')', ':', 'if', 'not', 'env_var', ':', 'return', 'env_var', 'while', 'true', ':', 'interpolated', '=', 'os', '.', 'path', '.', 'expanduser', '(', 'os', '.', 'path', '.', 'expandvars', '(', 'str', '(', 'env_var', ')', ')', ')', 'if', 'interpolated', '==', 'env_var', ':', 'return', 'interpolated', 'else', ':', 'env_var', '=', 'interpolated']",def expand_env_var ( env_var ) : if not env_var : return env_var while true : interpolated = os . path . expanduser ( os . path . expandvars ( str ( env_var ) ) ) if interpolated == env_var : return interpolated else : env_var = interpolated
221,apache/airflow,airflow/configuration.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/configuration.py#L75-L93,"def run_command(command):
    """"""
    Runs command and returns stdout
    """"""
    process = subprocess.Popen(
        shlex.split(command),
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        close_fds=True)
    output, stderr = [stream.decode(sys.getdefaultencoding(), 'ignore')
                      for stream in process.communicate()]

    if process.returncode != 0:
        raise AirflowConfigException(
            ""Cannot execute {}. Error code is: {}. Output: {}, Stderr: {}""
            .format(command, process.returncode, output, stderr)
        )

    return output","['def', 'run_command', '(', 'command', ')', ':', 'process', '=', 'subprocess', '.', 'Popen', '(', 'shlex', '.', 'split', '(', 'command', ')', ',', 'stdout', '=', 'subprocess', '.', 'PIPE', ',', 'stderr', '=', 'subprocess', '.', 'PIPE', ',', 'close_fds', '=', 'True', ')', 'output', ',', 'stderr', '=', '[', 'stream', '.', 'decode', '(', 'sys', '.', 'getdefaultencoding', '(', ')', ',', ""'ignore'"", ')', 'for', 'stream', 'in', 'process', '.', 'communicate', '(', ')', ']', 'if', 'process', '.', 'returncode', '!=', '0', ':', 'raise', 'AirflowConfigException', '(', '""Cannot execute {}. Error code is: {}. Output: {}, Stderr: {}""', '.', 'format', '(', 'command', ',', 'process', '.', 'returncode', ',', 'output', ',', 'stderr', ')', ')', 'return', 'output']",Runs command and returns stdout,"['Runs', 'command', 'and', 'returns', 'stdout']",python,test,"['runs', 'command', 'and', 'returns', 'stdout']",runs command and returns stdout,"['def', 'run_command', '(', 'command', ')', ':', 'process', '=', 'subprocess', '.', 'popen', '(', 'shlex', '.', 'split', '(', 'command', ')', ',', 'stdout', '=', 'subprocess', '.', 'pipe', ',', 'stderr', '=', 'subprocess', '.', 'pipe', ',', 'close_fds', '=', 'true', ')', 'output', ',', 'stderr', '=', '[', 'stream', '.', 'decode', '(', 'sys', '.', 'getdefaultencoding', '(', ')', ',', ""'ignore'"", ')', 'for', 'stream', 'in', 'process', '.', 'communicate', '(', ')', ']', 'if', 'process', '.', 'returncode', '!=', '0', ':', 'raise', 'airflowconfigexception', '(', '""cannot execute {}. error code is: {}. output: {}, stderr: {}""', '.', 'format', '(', 'command', ',', 'process', '.', 'returncode', ',', 'output', ',', 'stderr', ')', ')', 'return', 'output']","def run_command ( command ) : process = subprocess . popen ( shlex . split ( command ) , stdout = subprocess . pipe , stderr = subprocess . pipe , close_fds = true ) output , stderr = [ stream . decode ( sys . getdefaultencoding ( ) , 'ignore' ) for stream in process . communicate ( ) ] if process . returncode != 0 : raise airflowconfigexception ( ""cannot execute {}. error code is: {}. output: {}, stderr: {}"" . format ( command , process . returncode , output , stderr ) ) return output"
222,apache/airflow,airflow/configuration.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/configuration.py#L484-L491,"def parameterized_config(template):
    """"""
    Generates a configuration from the provided template + variables defined in
    current scope
    :param template: a config content templated with {{variables}}
    """"""
    all_vars = {k: v for d in [globals(), locals()] for k, v in d.items()}
    return template.format(**all_vars)","['def', 'parameterized_config', '(', 'template', ')', ':', 'all_vars', '=', '{', 'k', ':', 'v', 'for', 'd', 'in', '[', 'globals', '(', ')', ',', 'locals', '(', ')', ']', 'for', 'k', ',', 'v', 'in', 'd', '.', 'items', '(', ')', '}', 'return', 'template', '.', 'format', '(', '*', '*', 'all_vars', ')']","Generates a configuration from the provided template + variables defined in
    current scope
    :param template: a config content templated with {{variables}}","['Generates', 'a', 'configuration', 'from', 'the', 'provided', 'template', '+', 'variables', 'defined', 'in', 'current', 'scope', ':', 'param', 'template', ':', 'a', 'config', 'content', 'templated', 'with', '{{', 'variables', '}}']",python,test,"['generates', 'a', 'configuration', 'from', 'the', 'provided', 'template', '+', 'variables', 'defined', 'in', 'current', 'scope', ':', 'param', 'template', ':', 'a', 'config', 'content', 'templated', 'with', '{{', 'variables', '}}']",generates a configuration from the provided template + variables defined in current scope : param template : a config content templated with {{ variables }},"['def', 'parameterized_config', '(', 'template', ')', ':', 'all_vars', '=', '{', 'k', ':', 'v', 'for', 'd', 'in', '[', 'globals', '(', ')', ',', 'locals', '(', ')', ']', 'for', 'k', ',', 'v', 'in', 'd', '.', 'items', '(', ')', '}', 'return', 'template', '.', 'format', '(', '*', '*', 'all_vars', ')']","def parameterized_config ( template ) : all_vars = { k : v for d in [ globals ( ) , locals ( ) ] for k , v in d . items ( ) } return template . format ( * * all_vars )"
223,apache/airflow,airflow/configuration.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/configuration.py#L297-L307,"def remove_option(self, section, option, remove_default=True):
        """"""
        Remove an option if it exists in config from a file or
        default config. If both of config have the same option, this removes
        the option in both configs unless remove_default=False.
        """"""
        if super().has_option(section, option):
            super().remove_option(section, option)

        if self.airflow_defaults.has_option(section, option) and remove_default:
            self.airflow_defaults.remove_option(section, option)","['def', 'remove_option', '(', 'self', ',', 'section', ',', 'option', ',', 'remove_default', '=', 'True', ')', ':', 'if', 'super', '(', ')', '.', 'has_option', '(', 'section', ',', 'option', ')', ':', 'super', '(', ')', '.', 'remove_option', '(', 'section', ',', 'option', ')', 'if', 'self', '.', 'airflow_defaults', '.', 'has_option', '(', 'section', ',', 'option', ')', 'and', 'remove_default', ':', 'self', '.', 'airflow_defaults', '.', 'remove_option', '(', 'section', ',', 'option', ')']","Remove an option if it exists in config from a file or
        default config. If both of config have the same option, this removes
        the option in both configs unless remove_default=False.","['Remove', 'an', 'option', 'if', 'it', 'exists', 'in', 'config', 'from', 'a', 'file', 'or', 'default', 'config', '.', 'If', 'both', 'of', 'config', 'have', 'the', 'same', 'option', 'this', 'removes', 'the', 'option', 'in', 'both', 'configs', 'unless', 'remove_default', '=', 'False', '.']",python,test,"['remove', 'an', 'option', 'if', 'it', 'exists', 'in', 'config', 'from', 'a', 'file', 'or', 'default', 'config', '.', 'if', 'both', 'of', 'config', 'have', 'the', 'same', 'option', 'this', 'removes', 'the', 'option', 'in', 'both', 'configs', 'unless', 'remove_default', '=', 'false', '.']",remove an option if it exists in config from a file or default config . if both of config have the same option this removes the option in both configs unless remove_default = false .,"['def', 'remove_option', '(', 'self', ',', 'section', ',', 'option', ',', 'remove_default', '=', 'true', ')', ':', 'if', 'super', '(', ')', '.', 'has_option', '(', 'section', ',', 'option', ')', ':', 'super', '(', ')', '.', 'remove_option', '(', 'section', ',', 'option', ')', 'if', 'self', '.', 'airflow_defaults', '.', 'has_option', '(', 'section', ',', 'option', ')', 'and', 'remove_default', ':', 'self', '.', 'airflow_defaults', '.', 'remove_option', '(', 'section', ',', 'option', ')']","def remove_option ( self , section , option , remove_default = true ) : if super ( ) . has_option ( section , option ) : super ( ) . remove_option ( section , option ) if self . airflow_defaults . has_option ( section , option ) and remove_default : self . airflow_defaults . remove_option ( section , option )"
224,apache/airflow,airflow/configuration.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/configuration.py#L309-L344,"def getsection(self, section):
        """"""
        Returns the section as a dict. Values are converted to int, float, bool
        as required.

        :param section: section from the config
        :rtype: dict
        """"""
        if (section not in self._sections and
                section not in self.airflow_defaults._sections):
            return None

        _section = copy.deepcopy(self.airflow_defaults._sections[section])

        if section in self._sections:
            _section.update(copy.deepcopy(self._sections[section]))

        section_prefix = 'AIRFLOW__{S}__'.format(S=section.upper())
        for env_var in sorted(os.environ.keys()):
            if env_var.startswith(section_prefix):
                key = env_var.replace(section_prefix, '').lower()
                _section[key] = self._get_env_var_option(section, key)

        for key, val in iteritems(_section):
            try:
                val = int(val)
            except ValueError:
                try:
                    val = float(val)
                except ValueError:
                    if val.lower() in ('t', 'true'):
                        val = True
                    elif val.lower() in ('f', 'false'):
                        val = False
            _section[key] = val
        return _section","['def', 'getsection', '(', 'self', ',', 'section', ')', ':', 'if', '(', 'section', 'not', 'in', 'self', '.', '_sections', 'and', 'section', 'not', 'in', 'self', '.', 'airflow_defaults', '.', '_sections', ')', ':', 'return', 'None', '_section', '=', 'copy', '.', 'deepcopy', '(', 'self', '.', 'airflow_defaults', '.', '_sections', '[', 'section', ']', ')', 'if', 'section', 'in', 'self', '.', '_sections', ':', '_section', '.', 'update', '(', 'copy', '.', 'deepcopy', '(', 'self', '.', '_sections', '[', 'section', ']', ')', ')', 'section_prefix', '=', ""'AIRFLOW__{S}__'"", '.', 'format', '(', 'S', '=', 'section', '.', 'upper', '(', ')', ')', 'for', 'env_var', 'in', 'sorted', '(', 'os', '.', 'environ', '.', 'keys', '(', ')', ')', ':', 'if', 'env_var', '.', 'startswith', '(', 'section_prefix', ')', ':', 'key', '=', 'env_var', '.', 'replace', '(', 'section_prefix', ',', ""''"", ')', '.', 'lower', '(', ')', '_section', '[', 'key', ']', '=', 'self', '.', '_get_env_var_option', '(', 'section', ',', 'key', ')', 'for', 'key', ',', 'val', 'in', 'iteritems', '(', '_section', ')', ':', 'try', ':', 'val', '=', 'int', '(', 'val', ')', 'except', 'ValueError', ':', 'try', ':', 'val', '=', 'float', '(', 'val', ')', 'except', 'ValueError', ':', 'if', 'val', '.', 'lower', '(', ')', 'in', '(', ""'t'"", ',', ""'true'"", ')', ':', 'val', '=', 'True', 'elif', 'val', '.', 'lower', '(', ')', 'in', '(', ""'f'"", ',', ""'false'"", ')', ':', 'val', '=', 'False', '_section', '[', 'key', ']', '=', 'val', 'return', '_section']","Returns the section as a dict. Values are converted to int, float, bool
        as required.

        :param section: section from the config
        :rtype: dict","['Returns', 'the', 'section', 'as', 'a', 'dict', '.', 'Values', 'are', 'converted', 'to', 'int', 'float', 'bool', 'as', 'required', '.']",python,test,"['returns', 'the', 'section', 'as', 'a', 'dict', '.', 'values', 'are', 'converted', 'to', 'int', 'float', 'bool', 'as', 'required', '.']",returns the section as a dict . values are converted to int float bool as required .,"['def', 'getsection', '(', 'self', ',', 'section', ')', ':', 'if', '(', 'section', 'not', 'in', 'self', '.', '_sections', 'and', 'section', 'not', 'in', 'self', '.', 'airflow_defaults', '.', '_sections', ')', ':', 'return', 'none', '_section', '=', 'copy', '.', 'deepcopy', '(', 'self', '.', 'airflow_defaults', '.', '_sections', '[', 'section', ']', ')', 'if', 'section', 'in', 'self', '.', '_sections', ':', '_section', '.', 'update', '(', 'copy', '.', 'deepcopy', '(', 'self', '.', '_sections', '[', 'section', ']', ')', ')', 'section_prefix', '=', ""'airflow__{s}__'"", '.', 'format', '(', 's', '=', 'section', '.', 'upper', '(', ')', ')', 'for', 'env_var', 'in', 'sorted', '(', 'os', '.', 'environ', '.', 'keys', '(', ')', ')', ':', 'if', 'env_var', '.', 'startswith', '(', 'section_prefix', ')', ':', 'key', '=', 'env_var', '.', 'replace', '(', 'section_prefix', ',', ""''"", ')', '.', 'lower', '(', ')', '_section', '[', 'key', ']', '=', 'self', '.', '_get_env_var_option', '(', 'section', ',', 'key', ')', 'for', 'key', ',', 'val', 'in', 'iteritems', '(', '_section', ')', ':', 'try', ':', 'val', '=', 'int', '(', 'val', ')', 'except', 'valueerror', ':', 'try', ':', 'val', '=', 'float', '(', 'val', ')', 'except', 'valueerror', ':', 'if', 'val', '.', 'lower', '(', ')', 'in', '(', ""'t'"", ',', ""'true'"", ')', ':', 'val', '=', 'true', 'elif', 'val', '.', 'lower', '(', ')', 'in', '(', ""'f'"", ',', ""'false'"", ')', ':', 'val', '=', 'false', '_section', '[', 'key', ']', '=', 'val', 'return', '_section']","def getsection ( self , section ) : if ( section not in self . _sections and section not in self . airflow_defaults . _sections ) : return none _section = copy . deepcopy ( self . airflow_defaults . _sections [ section ] ) if section in self . _sections : _section . update ( copy . deepcopy ( self . _sections [ section ] ) ) section_prefix = 'airflow__{s}__' . format ( s = section . upper ( ) ) for env_var in sorted ( os . environ . keys ( ) ) : if env_var . startswith ( section_prefix ) : key = env_var . replace ( section_prefix , '' ) . lower ( ) _section [ key ] = self . _get_env_var_option ( section , key ) for key , val in iteritems ( _section ) : try : val = int ( val ) except valueerror : try : val = float ( val ) except valueerror : if val . lower ( ) in ( 't' , 'true' ) : val = true elif val . lower ( ) in ( 'f' , 'false' ) : val = false _section [ key ] = val return _section"
225,apache/airflow,airflow/configuration.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/configuration.py#L346-L405,"def as_dict(
            self, display_source=False, display_sensitive=False, raw=False):
        """"""
        Returns the current configuration as an OrderedDict of OrderedDicts.
        :param display_source: If False, the option value is returned. If True,
            a tuple of (option_value, source) is returned. Source is either
            'airflow.cfg', 'default', 'env var', or 'cmd'.
        :type display_source: bool
        :param display_sensitive: If True, the values of options set by env
            vars and bash commands will be displayed. If False, those options
            are shown as '< hidden >'
        :type display_sensitive: bool
        :param raw: Should the values be output as interpolated values, or the
            ""raw"" form that can be fed back in to ConfigParser
        :type raw: bool
        """"""
        cfg = {}
        configs = [
            ('default', self.airflow_defaults),
            ('airflow.cfg', self),
        ]

        for (source_name, config) in configs:
            for section in config.sections():
                sect = cfg.setdefault(section, OrderedDict())
                for (k, val) in config.items(section=section, raw=raw):
                    if display_source:
                        val = (val, source_name)
                    sect[k] = val

        # add env vars and overwrite because they have priority
        for ev in [ev for ev in os.environ if ev.startswith('AIRFLOW__')]:
            try:
                _, section, key = ev.split('__')
                opt = self._get_env_var_option(section, key)
            except ValueError:
                continue
            if not display_sensitive and ev != 'AIRFLOW__CORE__UNIT_TEST_MODE':
                opt = '< hidden >'
            elif raw:
                opt = opt.replace('%', '%%')
            if display_source:
                opt = (opt, 'env var')
            cfg.setdefault(section.lower(), OrderedDict()).update(
                {key.lower(): opt})

        # add bash commands
        for (section, key) in self.as_command_stdout:
            opt = self._get_cmd_option(section, key)
            if opt:
                if not display_sensitive:
                    opt = '< hidden >'
                if display_source:
                    opt = (opt, 'cmd')
                elif raw:
                    opt = opt.replace('%', '%%')
                cfg.setdefault(section, OrderedDict()).update({key: opt})
                del cfg[section][key + '_cmd']

        return cfg","['def', 'as_dict', '(', 'self', ',', 'display_source', '=', 'False', ',', 'display_sensitive', '=', 'False', ',', 'raw', '=', 'False', ')', ':', 'cfg', '=', '{', '}', 'configs', '=', '[', '(', ""'default'"", ',', 'self', '.', 'airflow_defaults', ')', ',', '(', ""'airflow.cfg'"", ',', 'self', ')', ',', ']', 'for', '(', 'source_name', ',', 'config', ')', 'in', 'configs', ':', 'for', 'section', 'in', 'config', '.', 'sections', '(', ')', ':', 'sect', '=', 'cfg', '.', 'setdefault', '(', 'section', ',', 'OrderedDict', '(', ')', ')', 'for', '(', 'k', ',', 'val', ')', 'in', 'config', '.', 'items', '(', 'section', '=', 'section', ',', 'raw', '=', 'raw', ')', ':', 'if', 'display_source', ':', 'val', '=', '(', 'val', ',', 'source_name', ')', 'sect', '[', 'k', ']', '=', 'val', '# add env vars and overwrite because they have priority', 'for', 'ev', 'in', '[', 'ev', 'for', 'ev', 'in', 'os', '.', 'environ', 'if', 'ev', '.', 'startswith', '(', ""'AIRFLOW__'"", ')', ']', ':', 'try', ':', '_', ',', 'section', ',', 'key', '=', 'ev', '.', 'split', '(', ""'__'"", ')', 'opt', '=', 'self', '.', '_get_env_var_option', '(', 'section', ',', 'key', ')', 'except', 'ValueError', ':', 'continue', 'if', 'not', 'display_sensitive', 'and', 'ev', '!=', ""'AIRFLOW__CORE__UNIT_TEST_MODE'"", ':', 'opt', '=', ""'< hidden >'"", 'elif', 'raw', ':', 'opt', '=', 'opt', '.', 'replace', '(', ""'%'"", ',', ""'%%'"", ')', 'if', 'display_source', ':', 'opt', '=', '(', 'opt', ',', ""'env var'"", ')', 'cfg', '.', 'setdefault', '(', 'section', '.', 'lower', '(', ')', ',', 'OrderedDict', '(', ')', ')', '.', 'update', '(', '{', 'key', '.', 'lower', '(', ')', ':', 'opt', '}', ')', '# add bash commands', 'for', '(', 'section', ',', 'key', ')', 'in', 'self', '.', 'as_command_stdout', ':', 'opt', '=', 'self', '.', '_get_cmd_option', '(', 'section', ',', 'key', ')', 'if', 'opt', ':', 'if', 'not', 'display_sensitive', ':', 'opt', '=', ""'< hidden >'"", 'if', 'display_source', ':', 'opt', '=', '(', 'opt', ',', ""'cmd'"", ')', 'elif', 'raw', ':', 'opt', '=', 'opt', '.', 'replace', '(', ""'%'"", ',', ""'%%'"", ')', 'cfg', '.', 'setdefault', '(', 'section', ',', 'OrderedDict', '(', ')', ')', '.', 'update', '(', '{', 'key', ':', 'opt', '}', ')', 'del', 'cfg', '[', 'section', ']', '[', 'key', '+', ""'_cmd'"", ']', 'return', 'cfg']","Returns the current configuration as an OrderedDict of OrderedDicts.
        :param display_source: If False, the option value is returned. If True,
            a tuple of (option_value, source) is returned. Source is either
            'airflow.cfg', 'default', 'env var', or 'cmd'.
        :type display_source: bool
        :param display_sensitive: If True, the values of options set by env
            vars and bash commands will be displayed. If False, those options
            are shown as '< hidden >'
        :type display_sensitive: bool
        :param raw: Should the values be output as interpolated values, or the
            ""raw"" form that can be fed back in to ConfigParser
        :type raw: bool","['Returns', 'the', 'current', 'configuration', 'as', 'an', 'OrderedDict', 'of', 'OrderedDicts', '.', ':', 'param', 'display_source', ':', 'If', 'False', 'the', 'option', 'value', 'is', 'returned', '.', 'If', 'True', 'a', 'tuple', 'of', '(', 'option_value', 'source', ')', 'is', 'returned', '.', 'Source', 'is', 'either', 'airflow', '.', 'cfg', 'default', 'env', 'var', 'or', 'cmd', '.', ':', 'type', 'display_source', ':', 'bool', ':', 'param', 'display_sensitive', ':', 'If', 'True', 'the', 'values', 'of', 'options', 'set', 'by', 'env', 'vars', 'and', 'bash', 'commands', 'will', 'be', 'displayed', '.', 'If', 'False', 'those', 'options', 'are', 'shown', 'as', '<', 'hidden', '>', ':', 'type', 'display_sensitive', ':', 'bool', ':', 'param', 'raw', ':', 'Should', 'the', 'values', 'be', 'output', 'as', 'interpolated', 'values', 'or', 'the', 'raw', 'form', 'that', 'can', 'be', 'fed', 'back', 'in', 'to', 'ConfigParser', ':', 'type', 'raw', ':', 'bool']",python,test,"['returns', 'the', 'current', 'configuration', 'as', 'an', 'ordereddict', 'of', 'ordereddicts', '.', ':', 'param', 'display_source', ':', 'if', 'false', 'the', 'option', 'value', 'is', 'returned', '.', 'if', 'true', 'a', 'tuple', 'of', '(', 'option_value', 'source', ')', 'is', 'returned', '.', 'source', 'is', 'either', 'airflow', '.', 'cfg', 'default', 'env', 'var', 'or', 'cmd', '.', ':', 'type', 'display_source', ':', 'bool', ':', 'param', 'display_sensitive', ':', 'if', 'true', 'the', 'values', 'of', 'options', 'set', 'by', 'env', 'vars', 'and', 'bash', 'commands', 'will', 'be', 'displayed', '.', 'if', 'false', 'those', 'options', 'are', 'shown', 'as', '<', 'hidden', '>', ':', 'type', 'display_sensitive', ':', 'bool', ':', 'param', 'raw', ':', 'should', 'the', 'values', 'be', 'output', 'as', 'interpolated', 'values', 'or', 'the', 'raw', 'form', 'that', 'can', 'be', 'fed', 'back', 'in', 'to', 'configparser', ':', 'type', 'raw', ':', 'bool']",returns the current configuration as an ordereddict of ordereddicts . : param display_source : if false the option value is returned . if true a tuple of ( option_value source ) is returned . source is either airflow . cfg default env var or cmd . : type display_source : bool : param display_sensitive : if true the values of options set by env vars and bash commands will be displayed . if false those options are shown as < hidden > : type display_sensitive : bool : param raw : should the values be output as interpolated values or the raw form that can be fed back in to configparser : type raw : bool,"['def', 'as_dict', '(', 'self', ',', 'display_source', '=', 'false', ',', 'display_sensitive', '=', 'false', ',', 'raw', '=', 'false', ')', ':', 'cfg', '=', '{', '}', 'configs', '=', '[', '(', ""'default'"", ',', 'self', '.', 'airflow_defaults', ')', ',', '(', ""'airflow.cfg'"", ',', 'self', ')', ',', ']', 'for', '(', 'source_name', ',', 'config', ')', 'in', 'configs', ':', 'for', 'section', 'in', 'config', '.', 'sections', '(', ')', ':', 'sect', '=', 'cfg', '.', 'setdefault', '(', 'section', ',', 'ordereddict', '(', ')', ')', 'for', '(', 'k', ',', 'val', ')', 'in', 'config', '.', 'items', '(', 'section', '=', 'section', ',', 'raw', '=', 'raw', ')', ':', 'if', 'display_source', ':', 'val', '=', '(', 'val', ',', 'source_name', ')', 'sect', '[', 'k', ']', '=', 'val', '# add env vars and overwrite because they have priority', 'for', 'ev', 'in', '[', 'ev', 'for', 'ev', 'in', 'os', '.', 'environ', 'if', 'ev', '.', 'startswith', '(', ""'airflow__'"", ')', ']', ':', 'try', ':', '_', ',', 'section', ',', 'key', '=', 'ev', '.', 'split', '(', ""'__'"", ')', 'opt', '=', 'self', '.', '_get_env_var_option', '(', 'section', ',', 'key', ')', 'except', 'valueerror', ':', 'continue', 'if', 'not', 'display_sensitive', 'and', 'ev', '!=', ""'airflow__core__unit_test_mode'"", ':', 'opt', '=', ""'< hidden >'"", 'elif', 'raw', ':', 'opt', '=', 'opt', '.', 'replace', '(', ""'%'"", ',', ""'%%'"", ')', 'if', 'display_source', ':', 'opt', '=', '(', 'opt', ',', ""'env var'"", ')', 'cfg', '.', 'setdefault', '(', 'section', '.', 'lower', '(', ')', ',', 'ordereddict', '(', ')', ')', '.', 'update', '(', '{', 'key', '.', 'lower', '(', ')', ':', 'opt', '}', ')', '# add bash commands', 'for', '(', 'section', ',', 'key', ')', 'in', 'self', '.', 'as_command_stdout', ':', 'opt', '=', 'self', '.', '_get_cmd_option', '(', 'section', ',', 'key', ')', 'if', 'opt', ':', 'if', 'not', 'display_sensitive', ':', 'opt', '=', ""'< hidden >'"", 'if', 'display_source', ':', 'opt', '=', '(', 'opt', ',', ""'cmd'"", ')', 'elif', 'raw', ':', 'opt', '=', 'opt', '.', 'replace', '(', ""'%'"", ',', ""'%%'"", ')', 'cfg', '.', 'setdefault', '(', 'section', ',', 'ordereddict', '(', ')', ')', '.', 'update', '(', '{', 'key', ':', 'opt', '}', ')', 'del', 'cfg', '[', 'section', ']', '[', 'key', '+', ""'_cmd'"", ']', 'return', 'cfg']","def as_dict ( self , display_source = false , display_sensitive = false , raw = false ) : cfg = { } configs = [ ( 'default' , self . airflow_defaults ) , ( 'airflow.cfg' , self ) , ] for ( source_name , config ) in configs : for section in config . sections ( ) : sect = cfg . setdefault ( section , ordereddict ( ) ) for ( k , val ) in config . items ( section = section , raw = raw ) : if display_source : val = ( val , source_name ) sect [ k ] = val # add env vars and overwrite because they have priority for ev in [ ev for ev in os . environ if ev . startswith ( 'airflow__' ) ] : try : _ , section , key = ev . split ( '__' ) opt = self . _get_env_var_option ( section , key ) except valueerror : continue if not display_sensitive and ev != 'airflow__core__unit_test_mode' : opt = '< hidden >' elif raw : opt = opt . replace ( '%' , '%%' ) if display_source : opt = ( opt , 'env var' ) cfg . setdefault ( section . lower ( ) , ordereddict ( ) ) . update ( { key . lower ( ) : opt } ) # add bash commands for ( section , key ) in self . as_command_stdout : opt = self . _get_cmd_option ( section , key ) if opt : if not display_sensitive : opt = '< hidden >' if display_source : opt = ( opt , 'cmd' ) elif raw : opt = opt . replace ( '%' , '%%' ) cfg . setdefault ( section , ordereddict ( ) ) . update ( { key : opt } ) del cfg [ section ] [ key + '_cmd' ] return cfg"
226,apache/airflow,airflow/contrib/hooks/datastore_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L62-L81,"def allocate_ids(self, partial_keys):
        """"""
        Allocate IDs for incomplete keys.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/allocateIds

        :param partial_keys: a list of partial keys.
        :type partial_keys: list
        :return: a list of full keys.
        :rtype: list
        """"""
        conn = self.get_conn()

        resp = (conn
                .projects()
                .allocateIds(projectId=self.project_id, body={'keys': partial_keys})
                .execute(num_retries=self.num_retries))

        return resp['keys']","['def', 'allocate_ids', '(', 'self', ',', 'partial_keys', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'resp', '=', '(', 'conn', '.', 'projects', '(', ')', '.', 'allocateIds', '(', 'projectId', '=', 'self', '.', 'project_id', ',', 'body', '=', '{', ""'keys'"", ':', 'partial_keys', '}', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')', 'return', 'resp', '[', ""'keys'"", ']']","Allocate IDs for incomplete keys.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/allocateIds

        :param partial_keys: a list of partial keys.
        :type partial_keys: list
        :return: a list of full keys.
        :rtype: list","['Allocate', 'IDs', 'for', 'incomplete', 'keys', '.']",python,test,"['allocate', 'ids', 'for', 'incomplete', 'keys', '.']",allocate ids for incomplete keys .,"['def', 'allocate_ids', '(', 'self', ',', 'partial_keys', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'resp', '=', '(', 'conn', '.', 'projects', '(', ')', '.', 'allocateids', '(', 'projectid', '=', 'self', '.', 'project_id', ',', 'body', '=', '{', ""'keys'"", ':', 'partial_keys', '}', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')', 'return', 'resp', '[', ""'keys'"", ']']","def allocate_ids ( self , partial_keys ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . allocateids ( projectid = self . project_id , body = { 'keys' : partial_keys } ) . execute ( num_retries = self . num_retries ) ) return resp [ 'keys' ]"
227,apache/airflow,airflow/contrib/hooks/datastore_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L83-L100,"def begin_transaction(self):
        """"""
        Begins a new transaction.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/beginTransaction

        :return: a transaction handle.
        :rtype: str
        """"""
        conn = self.get_conn()

        resp = (conn
                .projects()
                .beginTransaction(projectId=self.project_id, body={})
                .execute(num_retries=self.num_retries))

        return resp['transaction']","['def', 'begin_transaction', '(', 'self', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'resp', '=', '(', 'conn', '.', 'projects', '(', ')', '.', 'beginTransaction', '(', 'projectId', '=', 'self', '.', 'project_id', ',', 'body', '=', '{', '}', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')', 'return', 'resp', '[', ""'transaction'"", ']']","Begins a new transaction.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/beginTransaction

        :return: a transaction handle.
        :rtype: str","['Begins', 'a', 'new', 'transaction', '.']",python,test,"['begins', 'a', 'new', 'transaction', '.']",begins a new transaction .,"['def', 'begin_transaction', '(', 'self', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'resp', '=', '(', 'conn', '.', 'projects', '(', ')', '.', 'begintransaction', '(', 'projectid', '=', 'self', '.', 'project_id', ',', 'body', '=', '{', '}', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')', 'return', 'resp', '[', ""'transaction'"", ']']","def begin_transaction ( self ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . begintransaction ( projectid = self . project_id , body = { } ) . execute ( num_retries = self . num_retries ) ) return resp [ 'transaction' ]"
228,apache/airflow,airflow/contrib/hooks/datastore_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L102-L121,"def commit(self, body):
        """"""
        Commit a transaction, optionally creating, deleting or modifying some entities.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/commit

        :param body: the body of the commit request.
        :type body: dict
        :return: the response body of the commit request.
        :rtype: dict
        """"""
        conn = self.get_conn()

        resp = (conn
                .projects()
                .commit(projectId=self.project_id, body=body)
                .execute(num_retries=self.num_retries))

        return resp","['def', 'commit', '(', 'self', ',', 'body', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'resp', '=', '(', 'conn', '.', 'projects', '(', ')', '.', 'commit', '(', 'projectId', '=', 'self', '.', 'project_id', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')', 'return', 'resp']","Commit a transaction, optionally creating, deleting or modifying some entities.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/commit

        :param body: the body of the commit request.
        :type body: dict
        :return: the response body of the commit request.
        :rtype: dict","['Commit', 'a', 'transaction', 'optionally', 'creating', 'deleting', 'or', 'modifying', 'some', 'entities', '.']",python,test,"['commit', 'a', 'transaction', 'optionally', 'creating', 'deleting', 'or', 'modifying', 'some', 'entities', '.']",commit a transaction optionally creating deleting or modifying some entities .,"['def', 'commit', '(', 'self', ',', 'body', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'resp', '=', '(', 'conn', '.', 'projects', '(', ')', '.', 'commit', '(', 'projectid', '=', 'self', '.', 'project_id', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')', 'return', 'resp']","def commit ( self , body ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . commit ( projectid = self . project_id , body = body ) . execute ( num_retries = self . num_retries ) ) return resp"
229,apache/airflow,airflow/contrib/hooks/datastore_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L123-L152,"def lookup(self, keys, read_consistency=None, transaction=None):
        """"""
        Lookup some entities by key.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/lookup

        :param keys: the keys to lookup.
        :type keys: list
        :param read_consistency: the read consistency to use. default, strong or eventual.
                                 Cannot be used with a transaction.
        :type read_consistency: str
        :param transaction: the transaction to use, if any.
        :type transaction: str
        :return: the response body of the lookup request.
        :rtype: dict
        """"""
        conn = self.get_conn()

        body = {'keys': keys}
        if read_consistency:
            body['readConsistency'] = read_consistency
        if transaction:
            body['transaction'] = transaction
        resp = (conn
                .projects()
                .lookup(projectId=self.project_id, body=body)
                .execute(num_retries=self.num_retries))

        return resp","['def', 'lookup', '(', 'self', ',', 'keys', ',', 'read_consistency', '=', 'None', ',', 'transaction', '=', 'None', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'body', '=', '{', ""'keys'"", ':', 'keys', '}', 'if', 'read_consistency', ':', 'body', '[', ""'readConsistency'"", ']', '=', 'read_consistency', 'if', 'transaction', ':', 'body', '[', ""'transaction'"", ']', '=', 'transaction', 'resp', '=', '(', 'conn', '.', 'projects', '(', ')', '.', 'lookup', '(', 'projectId', '=', 'self', '.', 'project_id', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')', 'return', 'resp']","Lookup some entities by key.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/lookup

        :param keys: the keys to lookup.
        :type keys: list
        :param read_consistency: the read consistency to use. default, strong or eventual.
                                 Cannot be used with a transaction.
        :type read_consistency: str
        :param transaction: the transaction to use, if any.
        :type transaction: str
        :return: the response body of the lookup request.
        :rtype: dict","['Lookup', 'some', 'entities', 'by', 'key', '.']",python,test,"['lookup', 'some', 'entities', 'by', 'key', '.']",lookup some entities by key .,"['def', 'lookup', '(', 'self', ',', 'keys', ',', 'read_consistency', '=', 'none', ',', 'transaction', '=', 'none', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'body', '=', '{', ""'keys'"", ':', 'keys', '}', 'if', 'read_consistency', ':', 'body', '[', ""'readconsistency'"", ']', '=', 'read_consistency', 'if', 'transaction', ':', 'body', '[', ""'transaction'"", ']', '=', 'transaction', 'resp', '=', '(', 'conn', '.', 'projects', '(', ')', '.', 'lookup', '(', 'projectid', '=', 'self', '.', 'project_id', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')', 'return', 'resp']","def lookup ( self , keys , read_consistency = none , transaction = none ) : conn = self . get_conn ( ) body = { 'keys' : keys } if read_consistency : body [ 'readconsistency' ] = read_consistency if transaction : body [ 'transaction' ] = transaction resp = ( conn . projects ( ) . lookup ( projectid = self . project_id , body = body ) . execute ( num_retries = self . num_retries ) ) return resp"
230,apache/airflow,airflow/contrib/hooks/datastore_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L154-L168,"def rollback(self, transaction):
        """"""
        Roll back a transaction.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/rollback

        :param transaction: the transaction to roll back.
        :type transaction: str
        """"""
        conn = self.get_conn()

        conn.projects().rollback(
            projectId=self.project_id, body={'transaction': transaction}
        ).execute(num_retries=self.num_retries)","['def', 'rollback', '(', 'self', ',', 'transaction', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'conn', '.', 'projects', '(', ')', '.', 'rollback', '(', 'projectId', '=', 'self', '.', 'project_id', ',', 'body', '=', '{', ""'transaction'"", ':', 'transaction', '}', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')']","Roll back a transaction.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/rollback

        :param transaction: the transaction to roll back.
        :type transaction: str","['Roll', 'back', 'a', 'transaction', '.']",python,test,"['roll', 'back', 'a', 'transaction', '.']",roll back a transaction .,"['def', 'rollback', '(', 'self', ',', 'transaction', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'conn', '.', 'projects', '(', ')', '.', 'rollback', '(', 'projectid', '=', 'self', '.', 'project_id', ',', 'body', '=', '{', ""'transaction'"", ':', 'transaction', '}', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')']","def rollback ( self , transaction ) : conn = self . get_conn ( ) conn . projects ( ) . rollback ( projectid = self . project_id , body = { 'transaction' : transaction } ) . execute ( num_retries = self . num_retries )"
231,apache/airflow,airflow/contrib/hooks/datastore_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L170-L189,"def run_query(self, body):
        """"""
        Run a query for entities.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/runQuery

        :param body: the body of the query request.
        :type body: dict
        :return: the batch of query results.
        :rtype: dict
        """"""
        conn = self.get_conn()

        resp = (conn
                .projects()
                .runQuery(projectId=self.project_id, body=body)
                .execute(num_retries=self.num_retries))

        return resp['batch']","['def', 'run_query', '(', 'self', ',', 'body', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'resp', '=', '(', 'conn', '.', 'projects', '(', ')', '.', 'runQuery', '(', 'projectId', '=', 'self', '.', 'project_id', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')', 'return', 'resp', '[', ""'batch'"", ']']","Run a query for entities.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/runQuery

        :param body: the body of the query request.
        :type body: dict
        :return: the batch of query results.
        :rtype: dict","['Run', 'a', 'query', 'for', 'entities', '.']",python,test,"['run', 'a', 'query', 'for', 'entities', '.']",run a query for entities .,"['def', 'run_query', '(', 'self', ',', 'body', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'resp', '=', '(', 'conn', '.', 'projects', '(', ')', '.', 'runquery', '(', 'projectid', '=', 'self', '.', 'project_id', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')', 'return', 'resp', '[', ""'batch'"", ']']","def run_query ( self , body ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . runquery ( projectid = self . project_id , body = body ) . execute ( num_retries = self . num_retries ) ) return resp [ 'batch' ]"
232,apache/airflow,airflow/contrib/hooks/datastore_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L191-L211,"def get_operation(self, name):
        """"""
        Gets the latest state of a long-running operation.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/get

        :param name: the name of the operation resource.
        :type name: str
        :return: a resource operation instance.
        :rtype: dict
        """"""
        conn = self.get_conn()

        resp = (conn
                .projects()
                .operations()
                .get(name=name)
                .execute(num_retries=self.num_retries))

        return resp","['def', 'get_operation', '(', 'self', ',', 'name', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'resp', '=', '(', 'conn', '.', 'projects', '(', ')', '.', 'operations', '(', ')', '.', 'get', '(', 'name', '=', 'name', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')', 'return', 'resp']","Gets the latest state of a long-running operation.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/get

        :param name: the name of the operation resource.
        :type name: str
        :return: a resource operation instance.
        :rtype: dict","['Gets', 'the', 'latest', 'state', 'of', 'a', 'long', '-', 'running', 'operation', '.']",python,test,"['gets', 'the', 'latest', 'state', 'of', 'a', 'long', '-', 'running', 'operation', '.']",gets the latest state of a long - running operation .,"['def', 'get_operation', '(', 'self', ',', 'name', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'resp', '=', '(', 'conn', '.', 'projects', '(', ')', '.', 'operations', '(', ')', '.', 'get', '(', 'name', '=', 'name', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')', 'return', 'resp']","def get_operation ( self , name ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . operations ( ) . get ( name = name ) . execute ( num_retries = self . num_retries ) ) return resp"
233,apache/airflow,airflow/contrib/hooks/datastore_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L213-L233,"def delete_operation(self, name):
        """"""
        Deletes the long-running operation.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/delete

        :param name: the name of the operation resource.
        :type name: str
        :return: none if successful.
        :rtype: dict
        """"""
        conn = self.get_conn()

        resp = (conn
                .projects()
                .operations()
                .delete(name=name)
                .execute(num_retries=self.num_retries))

        return resp","['def', 'delete_operation', '(', 'self', ',', 'name', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'resp', '=', '(', 'conn', '.', 'projects', '(', ')', '.', 'operations', '(', ')', '.', 'delete', '(', 'name', '=', 'name', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')', 'return', 'resp']","Deletes the long-running operation.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/delete

        :param name: the name of the operation resource.
        :type name: str
        :return: none if successful.
        :rtype: dict","['Deletes', 'the', 'long', '-', 'running', 'operation', '.']",python,test,"['deletes', 'the', 'long', '-', 'running', 'operation', '.']",deletes the long - running operation .,"['def', 'delete_operation', '(', 'self', ',', 'name', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'resp', '=', '(', 'conn', '.', 'projects', '(', ')', '.', 'operations', '(', ')', '.', 'delete', '(', 'name', '=', 'name', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')', 'return', 'resp']","def delete_operation ( self , name ) : conn = self . get_conn ( ) resp = ( conn . projects ( ) . operations ( ) . delete ( name = name ) . execute ( num_retries = self . num_retries ) ) return resp"
234,apache/airflow,airflow/contrib/hooks/datastore_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L235-L255,"def poll_operation_until_done(self, name, polling_interval_in_seconds):
        """"""
        Poll backup operation state until it's completed.

        :param name: the name of the operation resource
        :type name: str
        :param polling_interval_in_seconds: The number of seconds to wait before calling another request.
        :type polling_interval_in_seconds: int
        :return: a resource operation instance.
        :rtype: dict
        """"""
        while True:
            result = self.get_operation(name)

            state = result['metadata']['common']['state']
            if state == 'PROCESSING':
                self.log.info('Operation is processing. Re-polling state in {} seconds'
                              .format(polling_interval_in_seconds))
                time.sleep(polling_interval_in_seconds)
            else:
                return result","['def', 'poll_operation_until_done', '(', 'self', ',', 'name', ',', 'polling_interval_in_seconds', ')', ':', 'while', 'True', ':', 'result', '=', 'self', '.', 'get_operation', '(', 'name', ')', 'state', '=', 'result', '[', ""'metadata'"", ']', '[', ""'common'"", ']', '[', ""'state'"", ']', 'if', 'state', '==', ""'PROCESSING'"", ':', 'self', '.', 'log', '.', 'info', '(', ""'Operation is processing. Re-polling state in {} seconds'"", '.', 'format', '(', 'polling_interval_in_seconds', ')', ')', 'time', '.', 'sleep', '(', 'polling_interval_in_seconds', ')', 'else', ':', 'return', 'result']","Poll backup operation state until it's completed.

        :param name: the name of the operation resource
        :type name: str
        :param polling_interval_in_seconds: The number of seconds to wait before calling another request.
        :type polling_interval_in_seconds: int
        :return: a resource operation instance.
        :rtype: dict","['Poll', 'backup', 'operation', 'state', 'until', 'it', 's', 'completed', '.']",python,test,"['poll', 'backup', 'operation', 'state', 'until', 'it', 's', 'completed', '.']",poll backup operation state until it s completed .,"['def', 'poll_operation_until_done', '(', 'self', ',', 'name', ',', 'polling_interval_in_seconds', ')', ':', 'while', 'true', ':', 'result', '=', 'self', '.', 'get_operation', '(', 'name', ')', 'state', '=', 'result', '[', ""'metadata'"", ']', '[', ""'common'"", ']', '[', ""'state'"", ']', 'if', 'state', '==', ""'processing'"", ':', 'self', '.', 'log', '.', 'info', '(', ""'operation is processing. re-polling state in {} seconds'"", '.', 'format', '(', 'polling_interval_in_seconds', ')', ')', 'time', '.', 'sleep', '(', 'polling_interval_in_seconds', ')', 'else', ':', 'return', 'result']","def poll_operation_until_done ( self , name , polling_interval_in_seconds ) : while true : result = self . get_operation ( name ) state = result [ 'metadata' ] [ 'common' ] [ 'state' ] if state == 'processing' : self . log . info ( 'operation is processing. re-polling state in {} seconds' . format ( polling_interval_in_seconds ) ) time . sleep ( polling_interval_in_seconds ) else : return result"
235,apache/airflow,airflow/contrib/hooks/datastore_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L257-L295,"def export_to_storage_bucket(self, bucket, namespace=None, entity_filter=None, labels=None):
        """"""
        Export entities from Cloud Datastore to Cloud Storage for backup.

        .. note::
            Keep in mind that this requests the Admin API not the Data API.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/export

        :param bucket: The name of the Cloud Storage bucket.
        :type bucket: str
        :param namespace: The Cloud Storage namespace path.
        :type namespace: str
        :param entity_filter: Description of what data from the project is included in the export.
        :type entity_filter: dict
        :param labels: Client-assigned labels.
        :type labels: dict of str
        :return: a resource operation instance.
        :rtype: dict
        """"""
        admin_conn = self.get_conn()

        output_uri_prefix = 'gs://' + '/'.join(filter(None, [bucket, namespace]))
        if not entity_filter:
            entity_filter = {}
        if not labels:
            labels = {}
        body = {
            'outputUrlPrefix': output_uri_prefix,
            'entityFilter': entity_filter,
            'labels': labels,
        }
        resp = (admin_conn
                .projects()
                .export(projectId=self.project_id, body=body)
                .execute(num_retries=self.num_retries))

        return resp","['def', 'export_to_storage_bucket', '(', 'self', ',', 'bucket', ',', 'namespace', '=', 'None', ',', 'entity_filter', '=', 'None', ',', 'labels', '=', 'None', ')', ':', 'admin_conn', '=', 'self', '.', 'get_conn', '(', ')', 'output_uri_prefix', '=', ""'gs://'"", '+', ""'/'"", '.', 'join', '(', 'filter', '(', 'None', ',', '[', 'bucket', ',', 'namespace', ']', ')', ')', 'if', 'not', 'entity_filter', ':', 'entity_filter', '=', '{', '}', 'if', 'not', 'labels', ':', 'labels', '=', '{', '}', 'body', '=', '{', ""'outputUrlPrefix'"", ':', 'output_uri_prefix', ',', ""'entityFilter'"", ':', 'entity_filter', ',', ""'labels'"", ':', 'labels', ',', '}', 'resp', '=', '(', 'admin_conn', '.', 'projects', '(', ')', '.', 'export', '(', 'projectId', '=', 'self', '.', 'project_id', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')', 'return', 'resp']","Export entities from Cloud Datastore to Cloud Storage for backup.

        .. note::
            Keep in mind that this requests the Admin API not the Data API.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/export

        :param bucket: The name of the Cloud Storage bucket.
        :type bucket: str
        :param namespace: The Cloud Storage namespace path.
        :type namespace: str
        :param entity_filter: Description of what data from the project is included in the export.
        :type entity_filter: dict
        :param labels: Client-assigned labels.
        :type labels: dict of str
        :return: a resource operation instance.
        :rtype: dict","['Export', 'entities', 'from', 'Cloud', 'Datastore', 'to', 'Cloud', 'Storage', 'for', 'backup', '.']",python,test,"['export', 'entities', 'from', 'cloud', 'datastore', 'to', 'cloud', 'storage', 'for', 'backup', '.']",export entities from cloud datastore to cloud storage for backup .,"['def', 'export_to_storage_bucket', '(', 'self', ',', 'bucket', ',', 'namespace', '=', 'none', ',', 'entity_filter', '=', 'none', ',', 'labels', '=', 'none', ')', ':', 'admin_conn', '=', 'self', '.', 'get_conn', '(', ')', 'output_uri_prefix', '=', ""'gs://'"", '+', ""'/'"", '.', 'join', '(', 'filter', '(', 'none', ',', '[', 'bucket', ',', 'namespace', ']', ')', ')', 'if', 'not', 'entity_filter', ':', 'entity_filter', '=', '{', '}', 'if', 'not', 'labels', ':', 'labels', '=', '{', '}', 'body', '=', '{', ""'outputurlprefix'"", ':', 'output_uri_prefix', ',', ""'entityfilter'"", ':', 'entity_filter', ',', ""'labels'"", ':', 'labels', ',', '}', 'resp', '=', '(', 'admin_conn', '.', 'projects', '(', ')', '.', 'export', '(', 'projectid', '=', 'self', '.', 'project_id', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')', 'return', 'resp']","def export_to_storage_bucket ( self , bucket , namespace = none , entity_filter = none , labels = none ) : admin_conn = self . get_conn ( ) output_uri_prefix = 'gs://' + '/' . join ( filter ( none , [ bucket , namespace ] ) ) if not entity_filter : entity_filter = { } if not labels : labels = { } body = { 'outputurlprefix' : output_uri_prefix , 'entityfilter' : entity_filter , 'labels' : labels , } resp = ( admin_conn . projects ( ) . export ( projectid = self . project_id , body = body ) . execute ( num_retries = self . num_retries ) ) return resp"
236,apache/airflow,airflow/contrib/hooks/datastore_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datastore_hook.py#L297-L337,"def import_from_storage_bucket(self, bucket, file, namespace=None, entity_filter=None, labels=None):
        """"""
        Import a backup from Cloud Storage to Cloud Datastore.

        .. note::
            Keep in mind that this requests the Admin API not the Data API.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/import

        :param bucket: The name of the Cloud Storage bucket.
        :type bucket: str
        :param file: the metadata file written by the projects.export operation.
        :type file: str
        :param namespace: The Cloud Storage namespace path.
        :type namespace: str
        :param entity_filter: specify which kinds/namespaces are to be imported.
        :type entity_filter: dict
        :param labels: Client-assigned labels.
        :type labels: dict of str
        :return: a resource operation instance.
        :rtype: dict
        """"""
        admin_conn = self.get_conn()

        input_url = 'gs://' + '/'.join(filter(None, [bucket, namespace, file]))
        if not entity_filter:
            entity_filter = {}
        if not labels:
            labels = {}
        body = {
            'inputUrl': input_url,
            'entityFilter': entity_filter,
            'labels': labels,
        }
        resp = (admin_conn
                .projects()
                .import_(projectId=self.project_id, body=body)
                .execute(num_retries=self.num_retries))

        return resp","['def', 'import_from_storage_bucket', '(', 'self', ',', 'bucket', ',', 'file', ',', 'namespace', '=', 'None', ',', 'entity_filter', '=', 'None', ',', 'labels', '=', 'None', ')', ':', 'admin_conn', '=', 'self', '.', 'get_conn', '(', ')', 'input_url', '=', ""'gs://'"", '+', ""'/'"", '.', 'join', '(', 'filter', '(', 'None', ',', '[', 'bucket', ',', 'namespace', ',', 'file', ']', ')', ')', 'if', 'not', 'entity_filter', ':', 'entity_filter', '=', '{', '}', 'if', 'not', 'labels', ':', 'labels', '=', '{', '}', 'body', '=', '{', ""'inputUrl'"", ':', 'input_url', ',', ""'entityFilter'"", ':', 'entity_filter', ',', ""'labels'"", ':', 'labels', ',', '}', 'resp', '=', '(', 'admin_conn', '.', 'projects', '(', ')', '.', 'import_', '(', 'projectId', '=', 'self', '.', 'project_id', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')', 'return', 'resp']","Import a backup from Cloud Storage to Cloud Datastore.

        .. note::
            Keep in mind that this requests the Admin API not the Data API.

        .. seealso::
            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/import

        :param bucket: The name of the Cloud Storage bucket.
        :type bucket: str
        :param file: the metadata file written by the projects.export operation.
        :type file: str
        :param namespace: The Cloud Storage namespace path.
        :type namespace: str
        :param entity_filter: specify which kinds/namespaces are to be imported.
        :type entity_filter: dict
        :param labels: Client-assigned labels.
        :type labels: dict of str
        :return: a resource operation instance.
        :rtype: dict","['Import', 'a', 'backup', 'from', 'Cloud', 'Storage', 'to', 'Cloud', 'Datastore', '.']",python,test,"['import', 'a', 'backup', 'from', 'cloud', 'storage', 'to', 'cloud', 'datastore', '.']",import a backup from cloud storage to cloud datastore .,"['def', 'import_from_storage_bucket', '(', 'self', ',', 'bucket', ',', 'file', ',', 'namespace', '=', 'none', ',', 'entity_filter', '=', 'none', ',', 'labels', '=', 'none', ')', ':', 'admin_conn', '=', 'self', '.', 'get_conn', '(', ')', 'input_url', '=', ""'gs://'"", '+', ""'/'"", '.', 'join', '(', 'filter', '(', 'none', ',', '[', 'bucket', ',', 'namespace', ',', 'file', ']', ')', ')', 'if', 'not', 'entity_filter', ':', 'entity_filter', '=', '{', '}', 'if', 'not', 'labels', ':', 'labels', '=', '{', '}', 'body', '=', '{', ""'inputurl'"", ':', 'input_url', ',', ""'entityfilter'"", ':', 'entity_filter', ',', ""'labels'"", ':', 'labels', ',', '}', 'resp', '=', '(', 'admin_conn', '.', 'projects', '(', ')', '.', 'import_', '(', 'projectid', '=', 'self', '.', 'project_id', ',', 'body', '=', 'body', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', ')', 'return', 'resp']","def import_from_storage_bucket ( self , bucket , file , namespace = none , entity_filter = none , labels = none ) : admin_conn = self . get_conn ( ) input_url = 'gs://' + '/' . join ( filter ( none , [ bucket , namespace , file ] ) ) if not entity_filter : entity_filter = { } if not labels : labels = { } body = { 'inputurl' : input_url , 'entityfilter' : entity_filter , 'labels' : labels , } resp = ( admin_conn . projects ( ) . import_ ( projectid = self . project_id , body = body ) . execute ( num_retries = self . num_retries ) ) return resp"
237,apache/airflow,airflow/contrib/hooks/aws_sns_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_sns_hook.py#L40-L60,"def publish_to_target(self, target_arn, message):
        """"""
        Publish a message to a topic or an endpoint.

        :param target_arn: either a TopicArn or an EndpointArn
        :type target_arn: str
        :param message: the default message you want to send
        :param message: str
        """"""

        conn = self.get_conn()

        messages = {
            'default': message
        }

        return conn.publish(
            TargetArn=target_arn,
            Message=json.dumps(messages),
            MessageStructure='json'
        )","['def', 'publish_to_target', '(', 'self', ',', 'target_arn', ',', 'message', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'messages', '=', '{', ""'default'"", ':', 'message', '}', 'return', 'conn', '.', 'publish', '(', 'TargetArn', '=', 'target_arn', ',', 'Message', '=', 'json', '.', 'dumps', '(', 'messages', ')', ',', 'MessageStructure', '=', ""'json'"", ')']","Publish a message to a topic or an endpoint.

        :param target_arn: either a TopicArn or an EndpointArn
        :type target_arn: str
        :param message: the default message you want to send
        :param message: str","['Publish', 'a', 'message', 'to', 'a', 'topic', 'or', 'an', 'endpoint', '.']",python,test,"['publish', 'a', 'message', 'to', 'a', 'topic', 'or', 'an', 'endpoint', '.']",publish a message to a topic or an endpoint .,"['def', 'publish_to_target', '(', 'self', ',', 'target_arn', ',', 'message', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'messages', '=', '{', ""'default'"", ':', 'message', '}', 'return', 'conn', '.', 'publish', '(', 'targetarn', '=', 'target_arn', ',', 'message', '=', 'json', '.', 'dumps', '(', 'messages', ')', ',', 'messagestructure', '=', ""'json'"", ')']","def publish_to_target ( self , target_arn , message ) : conn = self . get_conn ( ) messages = { 'default' : message } return conn . publish ( targetarn = target_arn , message = json . dumps ( messages ) , messagestructure = 'json' )"
238,apache/airflow,airflow/utils/net.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/net.py#L25-L45,"def get_hostname():
    """"""
    Fetch the hostname using the callable from the config or using
    `socket.getfqdn` as a fallback.
    """"""
    # First we attempt to fetch the callable path from the config.
    try:
        callable_path = conf.get('core', 'hostname_callable')
    except AirflowConfigException:
        callable_path = None

    # Then we handle the case when the config is missing or empty. This is the
    # default behavior.
    if not callable_path:
        return socket.getfqdn()

    # Since we have a callable path, we try to import and run it next.
    module_path, attr_name = callable_path.split(':')
    module = importlib.import_module(module_path)
    callable = getattr(module, attr_name)
    return callable()","['def', 'get_hostname', '(', ')', ':', '# First we attempt to fetch the callable path from the config.', 'try', ':', 'callable_path', '=', 'conf', '.', 'get', '(', ""'core'"", ',', ""'hostname_callable'"", ')', 'except', 'AirflowConfigException', ':', 'callable_path', '=', 'None', '# Then we handle the case when the config is missing or empty. This is the', '# default behavior.', 'if', 'not', 'callable_path', ':', 'return', 'socket', '.', 'getfqdn', '(', ')', '# Since we have a callable path, we try to import and run it next.', 'module_path', ',', 'attr_name', '=', 'callable_path', '.', 'split', '(', ""':'"", ')', 'module', '=', 'importlib', '.', 'import_module', '(', 'module_path', ')', 'callable', '=', 'getattr', '(', 'module', ',', 'attr_name', ')', 'return', 'callable', '(', ')']","Fetch the hostname using the callable from the config or using
    `socket.getfqdn` as a fallback.","['Fetch', 'the', 'hostname', 'using', 'the', 'callable', 'from', 'the', 'config', 'or', 'using', 'socket', '.', 'getfqdn', 'as', 'a', 'fallback', '.']",python,test,"['fetch', 'the', 'hostname', 'using', 'the', 'callable', 'from', 'the', 'config', 'or', 'using', 'socket', '.', 'getfqdn', 'as', 'a', 'fallback', '.']",fetch the hostname using the callable from the config or using socket . getfqdn as a fallback .,"['def', 'get_hostname', '(', ')', ':', '# first we attempt to fetch the callable path from the config.', 'try', ':', 'callable_path', '=', 'conf', '.', 'get', '(', ""'core'"", ',', ""'hostname_callable'"", ')', 'except', 'airflowconfigexception', ':', 'callable_path', '=', 'none', '# then we handle the case when the config is missing or empty. this is the', '# default behavior.', 'if', 'not', 'callable_path', ':', 'return', 'socket', '.', 'getfqdn', '(', ')', '# since we have a callable path, we try to import and run it next.', 'module_path', ',', 'attr_name', '=', 'callable_path', '.', 'split', '(', ""':'"", ')', 'module', '=', 'importlib', '.', 'import_module', '(', 'module_path', ')', 'callable', '=', 'getattr', '(', 'module', ',', 'attr_name', ')', 'return', 'callable', '(', ')']","def get_hostname ( ) : # first we attempt to fetch the callable path from the config. try : callable_path = conf . get ( 'core' , 'hostname_callable' ) except airflowconfigexception : callable_path = none # then we handle the case when the config is missing or empty. this is the # default behavior. if not callable_path : return socket . getfqdn ( ) # since we have a callable path, we try to import and run it next. module_path , attr_name = callable_path . split ( ':' ) module = importlib . import_module ( module_path ) callable = getattr ( module , attr_name ) return callable ( )"
239,apache/airflow,airflow/contrib/hooks/gcp_natural_language_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_natural_language_hook.py#L44-L53,"def get_conn(self):
        """"""
        Retrieves connection to Cloud Natural Language service.

        :return: Cloud Natural Language service object
        :rtype: google.cloud.language_v1.LanguageServiceClient
        """"""
        if not self._conn:
            self._conn = LanguageServiceClient(credentials=self._get_credentials())
        return self._conn","['def', 'get_conn', '(', 'self', ')', ':', 'if', 'not', 'self', '.', '_conn', ':', 'self', '.', '_conn', '=', 'LanguageServiceClient', '(', 'credentials', '=', 'self', '.', '_get_credentials', '(', ')', ')', 'return', 'self', '.', '_conn']","Retrieves connection to Cloud Natural Language service.

        :return: Cloud Natural Language service object
        :rtype: google.cloud.language_v1.LanguageServiceClient","['Retrieves', 'connection', 'to', 'Cloud', 'Natural', 'Language', 'service', '.']",python,test,"['retrieves', 'connection', 'to', 'cloud', 'natural', 'language', 'service', '.']",retrieves connection to cloud natural language service .,"['def', 'get_conn', '(', 'self', ')', ':', 'if', 'not', 'self', '.', '_conn', ':', 'self', '.', '_conn', '=', 'languageserviceclient', '(', 'credentials', '=', 'self', '.', '_get_credentials', '(', ')', ')', 'return', 'self', '.', '_conn']",def get_conn ( self ) : if not self . _conn : self . _conn = languageserviceclient ( credentials = self . _get_credentials ( ) ) return self . _conn
240,apache/airflow,airflow/contrib/hooks/gcp_natural_language_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_natural_language_hook.py#L56-L80,"def analyze_entities(self, document, encoding_type=None, retry=None, timeout=None, metadata=None):
        """"""
        Finds named entities in the text along with entity types,
        salience, mentions for each entity, and other properties.

        :param document: Input document.
            If a dict is provided, it must be of the same form as the protobuf message Document
        :type document: dict or class google.cloud.language_v1.types.Document
        :param encoding_type: The encoding type used by the API to calculate offsets.
        :type encoding_type: google.cloud.language_v1.types.EncodingType
        :param retry: A retry object used to retry requests. If None is specified, requests will not be
            retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if
            retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        :param metadata: Additional metadata that is provided to the method.
        :type metadata: sequence[tuple[str, str]]]
        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse
        """"""
        client = self.get_conn()

        return client.analyze_entities(
            document=document, encoding_type=encoding_type, retry=retry, timeout=timeout, metadata=metadata
        )","['def', 'analyze_entities', '(', 'self', ',', 'document', ',', 'encoding_type', '=', 'None', ',', 'retry', '=', 'None', ',', 'timeout', '=', 'None', ',', 'metadata', '=', 'None', ')', ':', 'client', '=', 'self', '.', 'get_conn', '(', ')', 'return', 'client', '.', 'analyze_entities', '(', 'document', '=', 'document', ',', 'encoding_type', '=', 'encoding_type', ',', 'retry', '=', 'retry', ',', 'timeout', '=', 'timeout', ',', 'metadata', '=', 'metadata', ')']","Finds named entities in the text along with entity types,
        salience, mentions for each entity, and other properties.

        :param document: Input document.
            If a dict is provided, it must be of the same form as the protobuf message Document
        :type document: dict or class google.cloud.language_v1.types.Document
        :param encoding_type: The encoding type used by the API to calculate offsets.
        :type encoding_type: google.cloud.language_v1.types.EncodingType
        :param retry: A retry object used to retry requests. If None is specified, requests will not be
            retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if
            retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        :param metadata: Additional metadata that is provided to the method.
        :type metadata: sequence[tuple[str, str]]]
        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse","['Finds', 'named', 'entities', 'in', 'the', 'text', 'along', 'with', 'entity', 'types', 'salience', 'mentions', 'for', 'each', 'entity', 'and', 'other', 'properties', '.']",python,test,"['finds', 'named', 'entities', 'in', 'the', 'text', 'along', 'with', 'entity', 'types', 'salience', 'mentions', 'for', 'each', 'entity', 'and', 'other', 'properties', '.']",finds named entities in the text along with entity types salience mentions for each entity and other properties .,"['def', 'analyze_entities', '(', 'self', ',', 'document', ',', 'encoding_type', '=', 'none', ',', 'retry', '=', 'none', ',', 'timeout', '=', 'none', ',', 'metadata', '=', 'none', ')', ':', 'client', '=', 'self', '.', 'get_conn', '(', ')', 'return', 'client', '.', 'analyze_entities', '(', 'document', '=', 'document', ',', 'encoding_type', '=', 'encoding_type', ',', 'retry', '=', 'retry', ',', 'timeout', '=', 'timeout', ',', 'metadata', '=', 'metadata', ')']","def analyze_entities ( self , document , encoding_type = none , retry = none , timeout = none , metadata = none ) : client = self . get_conn ( ) return client . analyze_entities ( document = document , encoding_type = encoding_type , retry = retry , timeout = timeout , metadata = metadata )"
241,apache/airflow,airflow/contrib/hooks/gcp_natural_language_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_natural_language_hook.py#L163-L195,"def annotate_text(self, document, features, encoding_type=None, retry=None, timeout=None, metadata=None):
        """"""
        A convenience method that provides all the features that analyzeSentiment,
        analyzeEntities, and analyzeSyntax provide in one call.

        :param document: Input document.
            If a dict is provided, it must be of the same form as the protobuf message Document
        :type document: dict or google.cloud.language_v1.types.Document
        :param features: The enabled features.
            If a dict is provided, it must be of the same form as the protobuf message Features
        :type features: dict or google.cloud.language_v1.enums.Features
        :param encoding_type: The encoding type used by the API to calculate offsets.
        :type encoding_type: google.cloud.language_v1.types.EncodingType
        :param retry: A retry object used to retry requests. If None is specified, requests will not be
            retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if
            retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        :param metadata: Additional metadata that is provided to the method.
        :type metadata: sequence[tuple[str, str]]]
        :rtype: google.cloud.language_v1.types.AnnotateTextResponse
        """"""
        client = self.get_conn()

        return client.annotate_text(
            document=document,
            features=features,
            encoding_type=encoding_type,
            retry=retry,
            timeout=timeout,
            metadata=metadata,
        )","['def', 'annotate_text', '(', 'self', ',', 'document', ',', 'features', ',', 'encoding_type', '=', 'None', ',', 'retry', '=', 'None', ',', 'timeout', '=', 'None', ',', 'metadata', '=', 'None', ')', ':', 'client', '=', 'self', '.', 'get_conn', '(', ')', 'return', 'client', '.', 'annotate_text', '(', 'document', '=', 'document', ',', 'features', '=', 'features', ',', 'encoding_type', '=', 'encoding_type', ',', 'retry', '=', 'retry', ',', 'timeout', '=', 'timeout', ',', 'metadata', '=', 'metadata', ',', ')']","A convenience method that provides all the features that analyzeSentiment,
        analyzeEntities, and analyzeSyntax provide in one call.

        :param document: Input document.
            If a dict is provided, it must be of the same form as the protobuf message Document
        :type document: dict or google.cloud.language_v1.types.Document
        :param features: The enabled features.
            If a dict is provided, it must be of the same form as the protobuf message Features
        :type features: dict or google.cloud.language_v1.enums.Features
        :param encoding_type: The encoding type used by the API to calculate offsets.
        :type encoding_type: google.cloud.language_v1.types.EncodingType
        :param retry: A retry object used to retry requests. If None is specified, requests will not be
            retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if
            retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        :param metadata: Additional metadata that is provided to the method.
        :type metadata: sequence[tuple[str, str]]]
        :rtype: google.cloud.language_v1.types.AnnotateTextResponse","['A', 'convenience', 'method', 'that', 'provides', 'all', 'the', 'features', 'that', 'analyzeSentiment', 'analyzeEntities', 'and', 'analyzeSyntax', 'provide', 'in', 'one', 'call', '.']",python,test,"['a', 'convenience', 'method', 'that', 'provides', 'all', 'the', 'features', 'that', 'analyzesentiment', 'analyzeentities', 'and', 'analyzesyntax', 'provide', 'in', 'one', 'call', '.']",a convenience method that provides all the features that analyzesentiment analyzeentities and analyzesyntax provide in one call .,"['def', 'annotate_text', '(', 'self', ',', 'document', ',', 'features', ',', 'encoding_type', '=', 'none', ',', 'retry', '=', 'none', ',', 'timeout', '=', 'none', ',', 'metadata', '=', 'none', ')', ':', 'client', '=', 'self', '.', 'get_conn', '(', ')', 'return', 'client', '.', 'annotate_text', '(', 'document', '=', 'document', ',', 'features', '=', 'features', ',', 'encoding_type', '=', 'encoding_type', ',', 'retry', '=', 'retry', ',', 'timeout', '=', 'timeout', ',', 'metadata', '=', 'metadata', ',', ')']","def annotate_text ( self , document , features , encoding_type = none , retry = none , timeout = none , metadata = none ) : client = self . get_conn ( ) return client . annotate_text ( document = document , features = features , encoding_type = encoding_type , retry = retry , timeout = timeout , metadata = metadata , )"
242,apache/airflow,airflow/contrib/hooks/gcp_natural_language_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_natural_language_hook.py#L198-L217,"def classify_text(self, document, retry=None, timeout=None, metadata=None):
        """"""
        Classifies a document into categories.

        :param document: Input document.
            If a dict is provided, it must be of the same form as the protobuf message Document
        :type document: dict or class google.cloud.language_v1.types.Document
        :param retry: A retry object used to retry requests. If None is specified, requests will not be
            retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if
            retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        :param metadata: Additional metadata that is provided to the method.
        :type metadata: sequence[tuple[str, str]]]
        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse
        """"""
        client = self.get_conn()

        return client.classify_text(document=document, retry=retry, timeout=timeout, metadata=metadata)","['def', 'classify_text', '(', 'self', ',', 'document', ',', 'retry', '=', 'None', ',', 'timeout', '=', 'None', ',', 'metadata', '=', 'None', ')', ':', 'client', '=', 'self', '.', 'get_conn', '(', ')', 'return', 'client', '.', 'classify_text', '(', 'document', '=', 'document', ',', 'retry', '=', 'retry', ',', 'timeout', '=', 'timeout', ',', 'metadata', '=', 'metadata', ')']","Classifies a document into categories.

        :param document: Input document.
            If a dict is provided, it must be of the same form as the protobuf message Document
        :type document: dict or class google.cloud.language_v1.types.Document
        :param retry: A retry object used to retry requests. If None is specified, requests will not be
            retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if
            retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        :param metadata: Additional metadata that is provided to the method.
        :type metadata: sequence[tuple[str, str]]]
        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse","['Classifies', 'a', 'document', 'into', 'categories', '.']",python,test,"['classifies', 'a', 'document', 'into', 'categories', '.']",classifies a document into categories .,"['def', 'classify_text', '(', 'self', ',', 'document', ',', 'retry', '=', 'none', ',', 'timeout', '=', 'none', ',', 'metadata', '=', 'none', ')', ':', 'client', '=', 'self', '.', 'get_conn', '(', ')', 'return', 'client', '.', 'classify_text', '(', 'document', '=', 'document', ',', 'retry', '=', 'retry', ',', 'timeout', '=', 'timeout', ',', 'metadata', '=', 'metadata', ')']","def classify_text ( self , document , retry = none , timeout = none , metadata = none ) : client = self . get_conn ( ) return client . classify_text ( document = document , retry = retry , timeout = timeout , metadata = metadata )"
243,apache/airflow,airflow/api/common/experimental/get_task.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/api/common/experimental/get_task.py#L24-L40,"def get_task(dag_id, task_id):
    """"""Return the task object identified by the given dag_id and task_id.""""""
    dagbag = DagBag()

    # Check DAG exists.
    if dag_id not in dagbag.dags:
        error_message = ""Dag id {} not found"".format(dag_id)
        raise DagNotFound(error_message)

    # Get DAG object and check Task Exists
    dag = dagbag.get_dag(dag_id)
    if not dag.has_task(task_id):
        error_message = 'Task {} not found in dag {}'.format(task_id, dag_id)
        raise TaskNotFound(error_message)

    # Return the task.
    return dag.get_task(task_id)","['def', 'get_task', '(', 'dag_id', ',', 'task_id', ')', ':', 'dagbag', '=', 'DagBag', '(', ')', '# Check DAG exists.', 'if', 'dag_id', 'not', 'in', 'dagbag', '.', 'dags', ':', 'error_message', '=', '""Dag id {} not found""', '.', 'format', '(', 'dag_id', ')', 'raise', 'DagNotFound', '(', 'error_message', ')', '# Get DAG object and check Task Exists', 'dag', '=', 'dagbag', '.', 'get_dag', '(', 'dag_id', ')', 'if', 'not', 'dag', '.', 'has_task', '(', 'task_id', ')', ':', 'error_message', '=', ""'Task {} not found in dag {}'"", '.', 'format', '(', 'task_id', ',', 'dag_id', ')', 'raise', 'TaskNotFound', '(', 'error_message', ')', '# Return the task.', 'return', 'dag', '.', 'get_task', '(', 'task_id', ')']",Return the task object identified by the given dag_id and task_id.,"['Return', 'the', 'task', 'object', 'identified', 'by', 'the', 'given', 'dag_id', 'and', 'task_id', '.']",python,test,"['return', 'the', 'task', 'object', 'identified', 'by', 'the', 'given', 'dag_id', 'and', 'task_id', '.']",return the task object identified by the given dag_id and task_id .,"['def', 'get_task', '(', 'dag_id', ',', 'task_id', ')', ':', 'dagbag', '=', 'dagbag', '(', ')', '# check dag exists.', 'if', 'dag_id', 'not', 'in', 'dagbag', '.', 'dags', ':', 'error_message', '=', '""dag id {} not found""', '.', 'format', '(', 'dag_id', ')', 'raise', 'dagnotfound', '(', 'error_message', ')', '# get dag object and check task exists', 'dag', '=', 'dagbag', '.', 'get_dag', '(', 'dag_id', ')', 'if', 'not', 'dag', '.', 'has_task', '(', 'task_id', ')', ':', 'error_message', '=', ""'task {} not found in dag {}'"", '.', 'format', '(', 'task_id', ',', 'dag_id', ')', 'raise', 'tasknotfound', '(', 'error_message', ')', '# return the task.', 'return', 'dag', '.', 'get_task', '(', 'task_id', ')']","def get_task ( dag_id , task_id ) : dagbag = dagbag ( ) # check dag exists. if dag_id not in dagbag . dags : error_message = ""dag id {} not found"" . format ( dag_id ) raise dagnotfound ( error_message ) # get dag object and check task exists dag = dagbag . get_dag ( dag_id ) if not dag . has_task ( task_id ) : error_message = 'task {} not found in dag {}' . format ( task_id , dag_id ) raise tasknotfound ( error_message ) # return the task. return dag . get_task ( task_id )"
244,apache/airflow,docs/exts/docroles.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/docs/exts/docroles.py#L27-L55,"def get_template_field(env, fullname):
    """"""
    Gets template fields for specific operator class.

    :param fullname: Full path to operator class.
        For example: ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetCreateOperator``
    :return: List of template field
    :rtype: list[str]
    """"""
    modname, classname = fullname.rsplit(""."", 1)

    try:
        with mock(env.config.autodoc_mock_imports):
            mod = import_module(modname)
    except ImportError:
        raise RoleException(""Error loading %s module."" % (modname, ))

    clazz = getattr(mod, classname)
    if not clazz:
        raise RoleException(""Error finding %s class in %s module."" % (classname, modname))

    template_fields = getattr(clazz, ""template_fields"")

    if not template_fields:
        raise RoleException(
            ""Could not find the template fields for %s class in %s module."" % (classname, modname)
        )

    return list(template_fields)","['def', 'get_template_field', '(', 'env', ',', 'fullname', ')', ':', 'modname', ',', 'classname', '=', 'fullname', '.', 'rsplit', '(', '"".""', ',', '1', ')', 'try', ':', 'with', 'mock', '(', 'env', '.', 'config', '.', 'autodoc_mock_imports', ')', ':', 'mod', '=', 'import_module', '(', 'modname', ')', 'except', 'ImportError', ':', 'raise', 'RoleException', '(', '""Error loading %s module.""', '%', '(', 'modname', ',', ')', ')', 'clazz', '=', 'getattr', '(', 'mod', ',', 'classname', ')', 'if', 'not', 'clazz', ':', 'raise', 'RoleException', '(', '""Error finding %s class in %s module.""', '%', '(', 'classname', ',', 'modname', ')', ')', 'template_fields', '=', 'getattr', '(', 'clazz', ',', '""template_fields""', ')', 'if', 'not', 'template_fields', ':', 'raise', 'RoleException', '(', '""Could not find the template fields for %s class in %s module.""', '%', '(', 'classname', ',', 'modname', ')', ')', 'return', 'list', '(', 'template_fields', ')']","Gets template fields for specific operator class.

    :param fullname: Full path to operator class.
        For example: ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetCreateOperator``
    :return: List of template field
    :rtype: list[str]","['Gets', 'template', 'fields', 'for', 'specific', 'operator', 'class', '.']",python,test,"['gets', 'template', 'fields', 'for', 'specific', 'operator', 'class', '.']",gets template fields for specific operator class .,"['def', 'get_template_field', '(', 'env', ',', 'fullname', ')', ':', 'modname', ',', 'classname', '=', 'fullname', '.', 'rsplit', '(', '"".""', ',', '1', ')', 'try', ':', 'with', 'mock', '(', 'env', '.', 'config', '.', 'autodoc_mock_imports', ')', ':', 'mod', '=', 'import_module', '(', 'modname', ')', 'except', 'importerror', ':', 'raise', 'roleexception', '(', '""error loading %s module.""', '%', '(', 'modname', ',', ')', ')', 'clazz', '=', 'getattr', '(', 'mod', ',', 'classname', ')', 'if', 'not', 'clazz', ':', 'raise', 'roleexception', '(', '""error finding %s class in %s module.""', '%', '(', 'classname', ',', 'modname', ')', ')', 'template_fields', '=', 'getattr', '(', 'clazz', ',', '""template_fields""', ')', 'if', 'not', 'template_fields', ':', 'raise', 'roleexception', '(', '""could not find the template fields for %s class in %s module.""', '%', '(', 'classname', ',', 'modname', ')', ')', 'return', 'list', '(', 'template_fields', ')']","def get_template_field ( env , fullname ) : modname , classname = fullname . rsplit ( ""."" , 1 ) try : with mock ( env . config . autodoc_mock_imports ) : mod = import_module ( modname ) except importerror : raise roleexception ( ""error loading %s module."" % ( modname , ) ) clazz = getattr ( mod , classname ) if not clazz : raise roleexception ( ""error finding %s class in %s module."" % ( classname , modname ) ) template_fields = getattr ( clazz , ""template_fields"" ) if not template_fields : raise roleexception ( ""could not find the template fields for %s class in %s module."" % ( classname , modname ) ) return list ( template_fields )"
245,apache/airflow,docs/exts/docroles.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/docs/exts/docroles.py#L58-L88,"def template_field_role(app, typ, rawtext, text, lineno, inliner, options={}, content=[]):
    """"""
    A role that allows you to include a list of template fields in the middle of the text. This is especially
    useful when writing guides describing how to use the operator.
    The result is a list of fields where each field is shorted in the literal block.

    Sample usage::

    :template-fields:`airflow.contrib.operators.gcp_natural_language_operator.CloudLanguageAnalyzeSentimentOperator`

    For further information look at:

    * [http://docutils.sourceforge.net/docs/howto/rst-roles.html](Creating reStructuredText Interpreted
      Text Roles)
    """"""
    text = utils.unescape(text)

    try:
        template_fields = get_template_field(app.env, text)
    except RoleException as e:
        msg = inliner.reporter.error(""invalid class name %s \n%s"" % (text, e, ), line=lineno)
        prb = inliner.problematic(rawtext, rawtext, msg)
        return [prb], [msg]

    node = nodes.inline(rawtext=rawtext)
    for i, field in enumerate(template_fields):
        if i != 0:
            node += nodes.Text("", "")
        node += nodes.literal(field, """", nodes.Text(field))

    return [node], []","['def', 'template_field_role', '(', 'app', ',', 'typ', ',', 'rawtext', ',', 'text', ',', 'lineno', ',', 'inliner', ',', 'options', '=', '{', '}', ',', 'content', '=', '[', ']', ')', ':', 'text', '=', 'utils', '.', 'unescape', '(', 'text', ')', 'try', ':', 'template_fields', '=', 'get_template_field', '(', 'app', '.', 'env', ',', 'text', ')', 'except', 'RoleException', 'as', 'e', ':', 'msg', '=', 'inliner', '.', 'reporter', '.', 'error', '(', '""invalid class name %s \\n%s""', '%', '(', 'text', ',', 'e', ',', ')', ',', 'line', '=', 'lineno', ')', 'prb', '=', 'inliner', '.', 'problematic', '(', 'rawtext', ',', 'rawtext', ',', 'msg', ')', 'return', '[', 'prb', ']', ',', '[', 'msg', ']', 'node', '=', 'nodes', '.', 'inline', '(', 'rawtext', '=', 'rawtext', ')', 'for', 'i', ',', 'field', 'in', 'enumerate', '(', 'template_fields', ')', ':', 'if', 'i', '!=', '0', ':', 'node', '+=', 'nodes', '.', 'Text', '(', '"", ""', ')', 'node', '+=', 'nodes', '.', 'literal', '(', 'field', ',', '""""', ',', 'nodes', '.', 'Text', '(', 'field', ')', ')', 'return', '[', 'node', ']', ',', '[', ']']","A role that allows you to include a list of template fields in the middle of the text. This is especially
    useful when writing guides describing how to use the operator.
    The result is a list of fields where each field is shorted in the literal block.

    Sample usage::

    :template-fields:`airflow.contrib.operators.gcp_natural_language_operator.CloudLanguageAnalyzeSentimentOperator`

    For further information look at:

    * [http://docutils.sourceforge.net/docs/howto/rst-roles.html](Creating reStructuredText Interpreted
      Text Roles)","['A', 'role', 'that', 'allows', 'you', 'to', 'include', 'a', 'list', 'of', 'template', 'fields', 'in', 'the', 'middle', 'of', 'the', 'text', '.', 'This', 'is', 'especially', 'useful', 'when', 'writing', 'guides', 'describing', 'how', 'to', 'use', 'the', 'operator', '.', 'The', 'result', 'is', 'a', 'list', 'of', 'fields', 'where', 'each', 'field', 'is', 'shorted', 'in', 'the', 'literal', 'block', '.']",python,test,"['a', 'role', 'that', 'allows', 'you', 'to', 'include', 'a', 'list', 'of', 'template', 'fields', 'in', 'the', 'middle', 'of', 'the', 'text', '.', 'this', 'is', 'especially', 'useful', 'when', 'writing', 'guides', 'describing', 'how', 'to', 'use', 'the', 'operator', '.', 'the', 'result', 'is', 'a', 'list', 'of', 'fields', 'where', 'each', 'field', 'is', 'shorted', 'in', 'the', 'literal', 'block', '.']",a role that allows you to include a list of template fields in the middle of the text . this is especially useful when writing guides describing how to use the operator . the result is a list of fields where each field is shorted in the literal block .,"['def', 'template_field_role', '(', 'app', ',', 'typ', ',', 'rawtext', ',', 'text', ',', 'lineno', ',', 'inliner', ',', 'options', '=', '{', '}', ',', 'content', '=', '[', ']', ')', ':', 'text', '=', 'utils', '.', 'unescape', '(', 'text', ')', 'try', ':', 'template_fields', '=', 'get_template_field', '(', 'app', '.', 'env', ',', 'text', ')', 'except', 'roleexception', 'as', 'e', ':', 'msg', '=', 'inliner', '.', 'reporter', '.', 'error', '(', '""invalid class name %s \\n%s""', '%', '(', 'text', ',', 'e', ',', ')', ',', 'line', '=', 'lineno', ')', 'prb', '=', 'inliner', '.', 'problematic', '(', 'rawtext', ',', 'rawtext', ',', 'msg', ')', 'return', '[', 'prb', ']', ',', '[', 'msg', ']', 'node', '=', 'nodes', '.', 'inline', '(', 'rawtext', '=', 'rawtext', ')', 'for', 'i', ',', 'field', 'in', 'enumerate', '(', 'template_fields', ')', ':', 'if', 'i', '!=', '0', ':', 'node', '+=', 'nodes', '.', 'text', '(', '"", ""', ')', 'node', '+=', 'nodes', '.', 'literal', '(', 'field', ',', '""""', ',', 'nodes', '.', 'text', '(', 'field', ')', ')', 'return', '[', 'node', ']', ',', '[', ']']","def template_field_role ( app , typ , rawtext , text , lineno , inliner , options = { } , content = [ ] ) : text = utils . unescape ( text ) try : template_fields = get_template_field ( app . env , text ) except roleexception as e : msg = inliner . reporter . error ( ""invalid class name %s \n%s"" % ( text , e , ) , line = lineno ) prb = inliner . problematic ( rawtext , rawtext , msg ) return [ prb ] , [ msg ] node = nodes . inline ( rawtext = rawtext ) for i , field in enumerate ( template_fields ) : if i != 0 : node += nodes . text ( "", "" ) node += nodes . literal ( field , """" , nodes . text ( field ) ) return [ node ] , [ ]"
246,apache/airflow,airflow/settings.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/settings.py#L166-L177,"def dispose_orm():
    """""" Properly close pooled database connections """"""
    log.debug(""Disposing DB connection pool (PID %s)"", os.getpid())
    global engine
    global Session

    if Session:
        Session.remove()
        Session = None
    if engine:
        engine.dispose()
        engine = None","['def', 'dispose_orm', '(', ')', ':', 'log', '.', 'debug', '(', '""Disposing DB connection pool (PID %s)""', ',', 'os', '.', 'getpid', '(', ')', ')', 'global', 'engine', 'global', 'Session', 'if', 'Session', ':', 'Session', '.', 'remove', '(', ')', 'Session', '=', 'None', 'if', 'engine', ':', 'engine', '.', 'dispose', '(', ')', 'engine', '=', 'None']",Properly close pooled database connections,"['Properly', 'close', 'pooled', 'database', 'connections']",python,test,"['properly', 'close', 'pooled', 'database', 'connections']",properly close pooled database connections,"['def', 'dispose_orm', '(', ')', ':', 'log', '.', 'debug', '(', '""disposing db connection pool (pid %s)""', ',', 'os', '.', 'getpid', '(', ')', ')', 'global', 'engine', 'global', 'session', 'if', 'session', ':', 'session', '.', 'remove', '(', ')', 'session', '=', 'none', 'if', 'engine', ':', 'engine', '.', 'dispose', '(', ')', 'engine', '=', 'none']","def dispose_orm ( ) : log . debug ( ""disposing db connection pool (pid %s)"" , os . getpid ( ) ) global engine global session if session : session . remove ( ) session = none if engine : engine . dispose ( ) engine = none"
247,apache/airflow,airflow/settings.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/settings.py#L220-L235,"def prepare_classpath():
    """"""
    Ensures that certain subfolders of AIRFLOW_HOME are on the classpath
    """"""

    if DAGS_FOLDER not in sys.path:
        sys.path.append(DAGS_FOLDER)

    # Add ./config/ for loading custom log parsers etc, or
    # airflow_local_settings etc.
    config_path = os.path.join(AIRFLOW_HOME, 'config')
    if config_path not in sys.path:
        sys.path.append(config_path)

    if PLUGINS_FOLDER not in sys.path:
        sys.path.append(PLUGINS_FOLDER)","['def', 'prepare_classpath', '(', ')', ':', 'if', 'DAGS_FOLDER', 'not', 'in', 'sys', '.', 'path', ':', 'sys', '.', 'path', '.', 'append', '(', 'DAGS_FOLDER', ')', '# Add ./config/ for loading custom log parsers etc, or', '# airflow_local_settings etc.', 'config_path', '=', 'os', '.', 'path', '.', 'join', '(', 'AIRFLOW_HOME', ',', ""'config'"", ')', 'if', 'config_path', 'not', 'in', 'sys', '.', 'path', ':', 'sys', '.', 'path', '.', 'append', '(', 'config_path', ')', 'if', 'PLUGINS_FOLDER', 'not', 'in', 'sys', '.', 'path', ':', 'sys', '.', 'path', '.', 'append', '(', 'PLUGINS_FOLDER', ')']",Ensures that certain subfolders of AIRFLOW_HOME are on the classpath,"['Ensures', 'that', 'certain', 'subfolders', 'of', 'AIRFLOW_HOME', 'are', 'on', 'the', 'classpath']",python,test,"['ensures', 'that', 'certain', 'subfolders', 'of', 'airflow_home', 'are', 'on', 'the', 'classpath']",ensures that certain subfolders of airflow_home are on the classpath,"['def', 'prepare_classpath', '(', ')', ':', 'if', 'dags_folder', 'not', 'in', 'sys', '.', 'path', ':', 'sys', '.', 'path', '.', 'append', '(', 'dags_folder', ')', '# add ./config/ for loading custom log parsers etc, or', '# airflow_local_settings etc.', 'config_path', '=', 'os', '.', 'path', '.', 'join', '(', 'airflow_home', ',', ""'config'"", ')', 'if', 'config_path', 'not', 'in', 'sys', '.', 'path', ':', 'sys', '.', 'path', '.', 'append', '(', 'config_path', ')', 'if', 'plugins_folder', 'not', 'in', 'sys', '.', 'path', ':', 'sys', '.', 'path', '.', 'append', '(', 'plugins_folder', ')']","def prepare_classpath ( ) : if dags_folder not in sys . path : sys . path . append ( dags_folder ) # add ./config/ for loading custom log parsers etc, or # airflow_local_settings etc. config_path = os . path . join ( airflow_home , 'config' ) if config_path not in sys . path : sys . path . append ( config_path ) if plugins_folder not in sys . path : sys . path . append ( plugins_folder )"
248,apache/airflow,airflow/contrib/sensors/celery_queue_sensor.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/celery_queue_sensor.py#L49-L62,"def _check_task_id(self, context):
        """"""
        Gets the returned Celery result from the Airflow task
        ID provided to the sensor, and returns True if the
        celery result has been finished execution.

        :param context: Airflow's execution context
        :type context: dict
        :return: True if task has been executed, otherwise False
        :rtype: bool
        """"""
        ti = context['ti']
        celery_result = ti.xcom_pull(task_ids=self.target_task_id)
        return celery_result.ready()","['def', '_check_task_id', '(', 'self', ',', 'context', ')', ':', 'ti', '=', 'context', '[', ""'ti'"", ']', 'celery_result', '=', 'ti', '.', 'xcom_pull', '(', 'task_ids', '=', 'self', '.', 'target_task_id', ')', 'return', 'celery_result', '.', 'ready', '(', ')']","Gets the returned Celery result from the Airflow task
        ID provided to the sensor, and returns True if the
        celery result has been finished execution.

        :param context: Airflow's execution context
        :type context: dict
        :return: True if task has been executed, otherwise False
        :rtype: bool","['Gets', 'the', 'returned', 'Celery', 'result', 'from', 'the', 'Airflow', 'task', 'ID', 'provided', 'to', 'the', 'sensor', 'and', 'returns', 'True', 'if', 'the', 'celery', 'result', 'has', 'been', 'finished', 'execution', '.']",python,test,"['gets', 'the', 'returned', 'celery', 'result', 'from', 'the', 'airflow', 'task', 'id', 'provided', 'to', 'the', 'sensor', 'and', 'returns', 'true', 'if', 'the', 'celery', 'result', 'has', 'been', 'finished', 'execution', '.']",gets the returned celery result from the airflow task id provided to the sensor and returns true if the celery result has been finished execution .,"['def', '_check_task_id', '(', 'self', ',', 'context', ')', ':', 'ti', '=', 'context', '[', ""'ti'"", ']', 'celery_result', '=', 'ti', '.', 'xcom_pull', '(', 'task_ids', '=', 'self', '.', 'target_task_id', ')', 'return', 'celery_result', '.', 'ready', '(', ')']","def _check_task_id ( self , context ) : ti = context [ 'ti' ] celery_result = ti . xcom_pull ( task_ids = self . target_task_id ) return celery_result . ready ( )"
249,apache/airflow,airflow/security/kerberos.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/security/kerberos.py#L100-L110,"def detect_conf_var():
    """"""Return true if the ticket cache contains ""conf"" information as is found
    in ticket caches of Kerberos 1.8.1 or later. This is incompatible with the
    Sun Java Krb5LoginModule in Java6, so we need to take an action to work
    around it.
    """"""
    ticket_cache = configuration.conf.get('kerberos', 'ccache')

    with open(ticket_cache, 'rb') as f:
        # Note: this file is binary, so we check against a bytearray.
        return b'X-CACHECONF:' in f.read()","['def', 'detect_conf_var', '(', ')', ':', 'ticket_cache', '=', 'configuration', '.', 'conf', '.', 'get', '(', ""'kerberos'"", ',', ""'ccache'"", ')', 'with', 'open', '(', 'ticket_cache', ',', ""'rb'"", ')', 'as', 'f', ':', '# Note: this file is binary, so we check against a bytearray.', 'return', ""b'X-CACHECONF:'"", 'in', 'f', '.', 'read', '(', ')']","Return true if the ticket cache contains ""conf"" information as is found
    in ticket caches of Kerberos 1.8.1 or later. This is incompatible with the
    Sun Java Krb5LoginModule in Java6, so we need to take an action to work
    around it.","['Return', 'true', 'if', 'the', 'ticket', 'cache', 'contains', 'conf', 'information', 'as', 'is', 'found', 'in', 'ticket', 'caches', 'of', 'Kerberos', '1', '.', '8', '.', '1', 'or', 'later', '.', 'This', 'is', 'incompatible', 'with', 'the', 'Sun', 'Java', 'Krb5LoginModule', 'in', 'Java6', 'so', 'we', 'need', 'to', 'take', 'an', 'action', 'to', 'work', 'around', 'it', '.']",python,test,"['return', 'true', 'if', 'the', 'ticket', 'cache', 'contains', 'conf', 'information', 'as', 'is', 'found', 'in', 'ticket', 'caches', 'of', 'kerberos', '1', '.', '8', '.', '1', 'or', 'later', '.', 'this', 'is', 'incompatible', 'with', 'the', 'sun', 'java', 'krb5loginmodule', 'in', 'java6', 'so', 'we', 'need', 'to', 'take', 'an', 'action', 'to', 'work', 'around', 'it', '.']",return true if the ticket cache contains conf information as is found in ticket caches of kerberos 1 . 8 . 1 or later . this is incompatible with the sun java krb5loginmodule in java6 so we need to take an action to work around it .,"['def', 'detect_conf_var', '(', ')', ':', 'ticket_cache', '=', 'configuration', '.', 'conf', '.', 'get', '(', ""'kerberos'"", ',', ""'ccache'"", ')', 'with', 'open', '(', 'ticket_cache', ',', ""'rb'"", ')', 'as', 'f', ':', '# note: this file is binary, so we check against a bytearray.', 'return', ""b'x-cacheconf:'"", 'in', 'f', '.', 'read', '(', ')']","def detect_conf_var ( ) : ticket_cache = configuration . conf . get ( 'kerberos' , 'ccache' ) with open ( ticket_cache , 'rb' ) as f : # note: this file is binary, so we check against a bytearray. return b'x-cacheconf:' in f . read ( )"
250,apache/airflow,airflow/utils/helpers.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/helpers.py#L60-L72,"def alchemy_to_dict(obj):
    """"""
    Transforms a SQLAlchemy model instance into a dictionary
    """"""
    if not obj:
        return None
    d = {}
    for c in obj.__table__.columns:
        value = getattr(obj, c.name)
        if type(value) == datetime:
            value = value.isoformat()
        d[c.name] = value
    return d","['def', 'alchemy_to_dict', '(', 'obj', ')', ':', 'if', 'not', 'obj', ':', 'return', 'None', 'd', '=', '{', '}', 'for', 'c', 'in', 'obj', '.', '__table__', '.', 'columns', ':', 'value', '=', 'getattr', '(', 'obj', ',', 'c', '.', 'name', ')', 'if', 'type', '(', 'value', ')', '==', 'datetime', ':', 'value', '=', 'value', '.', 'isoformat', '(', ')', 'd', '[', 'c', '.', 'name', ']', '=', 'value', 'return', 'd']",Transforms a SQLAlchemy model instance into a dictionary,"['Transforms', 'a', 'SQLAlchemy', 'model', 'instance', 'into', 'a', 'dictionary']",python,test,"['transforms', 'a', 'sqlalchemy', 'model', 'instance', 'into', 'a', 'dictionary']",transforms a sqlalchemy model instance into a dictionary,"['def', 'alchemy_to_dict', '(', 'obj', ')', ':', 'if', 'not', 'obj', ':', 'return', 'none', 'd', '=', '{', '}', 'for', 'c', 'in', 'obj', '.', '__table__', '.', 'columns', ':', 'value', '=', 'getattr', '(', 'obj', ',', 'c', '.', 'name', ')', 'if', 'type', '(', 'value', ')', '==', 'datetime', ':', 'value', '=', 'value', '.', 'isoformat', '(', ')', 'd', '[', 'c', '.', 'name', ']', '=', 'value', 'return', 'd']","def alchemy_to_dict ( obj ) : if not obj : return none d = { } for c in obj . __table__ . columns : value = getattr ( obj , c . name ) if type ( value ) == datetime : value = value . isoformat ( ) d [ c . name ] = value return d"
251,apache/airflow,airflow/utils/helpers.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/helpers.py#L121-L128,"def chunks(items, chunk_size):
    """"""
    Yield successive chunks of a given size from a list of items
    """"""
    if chunk_size <= 0:
        raise ValueError('Chunk size must be a positive integer')
    for i in range(0, len(items), chunk_size):
        yield items[i:i + chunk_size]","['def', 'chunks', '(', 'items', ',', 'chunk_size', ')', ':', 'if', 'chunk_size', '<=', '0', ':', 'raise', 'ValueError', '(', ""'Chunk size must be a positive integer'"", ')', 'for', 'i', 'in', 'range', '(', '0', ',', 'len', '(', 'items', ')', ',', 'chunk_size', ')', ':', 'yield', 'items', '[', 'i', ':', 'i', '+', 'chunk_size', ']']",Yield successive chunks of a given size from a list of items,"['Yield', 'successive', 'chunks', 'of', 'a', 'given', 'size', 'from', 'a', 'list', 'of', 'items']",python,test,"['yield', 'successive', 'chunks', 'of', 'a', 'given', 'size', 'from', 'a', 'list', 'of', 'items']",yield successive chunks of a given size from a list of items,"['def', 'chunks', '(', 'items', ',', 'chunk_size', ')', ':', 'if', 'chunk_size', '<=', '0', ':', 'raise', 'valueerror', '(', ""'chunk size must be a positive integer'"", ')', 'for', 'i', 'in', 'range', '(', '0', ',', 'len', '(', 'items', ')', ',', 'chunk_size', ')', ':', 'yield', 'items', '[', 'i', ':', 'i', '+', 'chunk_size', ']']","def chunks ( items , chunk_size ) : if chunk_size <= 0 : raise valueerror ( 'chunk size must be a positive integer' ) for i in range ( 0 , len ( items ) , chunk_size ) : yield items [ i : i + chunk_size ]"
252,apache/airflow,airflow/utils/helpers.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/helpers.py#L131-L140,"def reduce_in_chunks(fn, iterable, initializer, chunk_size=0):
    """"""
    Reduce the given list of items by splitting it into chunks
    of the given size and passing each chunk through the reducer
    """"""
    if len(iterable) == 0:
        return initializer
    if chunk_size == 0:
        chunk_size = len(iterable)
    return reduce(fn, chunks(iterable, chunk_size), initializer)","['def', 'reduce_in_chunks', '(', 'fn', ',', 'iterable', ',', 'initializer', ',', 'chunk_size', '=', '0', ')', ':', 'if', 'len', '(', 'iterable', ')', '==', '0', ':', 'return', 'initializer', 'if', 'chunk_size', '==', '0', ':', 'chunk_size', '=', 'len', '(', 'iterable', ')', 'return', 'reduce', '(', 'fn', ',', 'chunks', '(', 'iterable', ',', 'chunk_size', ')', ',', 'initializer', ')']","Reduce the given list of items by splitting it into chunks
    of the given size and passing each chunk through the reducer","['Reduce', 'the', 'given', 'list', 'of', 'items', 'by', 'splitting', 'it', 'into', 'chunks', 'of', 'the', 'given', 'size', 'and', 'passing', 'each', 'chunk', 'through', 'the', 'reducer']",python,test,"['reduce', 'the', 'given', 'list', 'of', 'items', 'by', 'splitting', 'it', 'into', 'chunks', 'of', 'the', 'given', 'size', 'and', 'passing', 'each', 'chunk', 'through', 'the', 'reducer']",reduce the given list of items by splitting it into chunks of the given size and passing each chunk through the reducer,"['def', 'reduce_in_chunks', '(', 'fn', ',', 'iterable', ',', 'initializer', ',', 'chunk_size', '=', '0', ')', ':', 'if', 'len', '(', 'iterable', ')', '==', '0', ':', 'return', 'initializer', 'if', 'chunk_size', '==', '0', ':', 'chunk_size', '=', 'len', '(', 'iterable', ')', 'return', 'reduce', '(', 'fn', ',', 'chunks', '(', 'iterable', ',', 'chunk_size', ')', ',', 'initializer', ')']","def reduce_in_chunks ( fn , iterable , initializer , chunk_size = 0 ) : if len ( iterable ) == 0 : return initializer if chunk_size == 0 : chunk_size = len ( iterable ) return reduce ( fn , chunks ( iterable , chunk_size ) , initializer )"
253,apache/airflow,airflow/utils/helpers.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/helpers.py#L153-L166,"def chain(*tasks):
    """"""
    Given a number of tasks, builds a dependency chain.

    chain(task_1, task_2, task_3, task_4)

    is equivalent to

    task_1.set_downstream(task_2)
    task_2.set_downstream(task_3)
    task_3.set_downstream(task_4)
    """"""
    for up_task, down_task in zip(tasks[:-1], tasks[1:]):
        up_task.set_downstream(down_task)","['def', 'chain', '(', '*', 'tasks', ')', ':', 'for', 'up_task', ',', 'down_task', 'in', 'zip', '(', 'tasks', '[', ':', '-', '1', ']', ',', 'tasks', '[', '1', ':', ']', ')', ':', 'up_task', '.', 'set_downstream', '(', 'down_task', ')']","Given a number of tasks, builds a dependency chain.

    chain(task_1, task_2, task_3, task_4)

    is equivalent to

    task_1.set_downstream(task_2)
    task_2.set_downstream(task_3)
    task_3.set_downstream(task_4)","['Given', 'a', 'number', 'of', 'tasks', 'builds', 'a', 'dependency', 'chain', '.']",python,test,"['given', 'a', 'number', 'of', 'tasks', 'builds', 'a', 'dependency', 'chain', '.']",given a number of tasks builds a dependency chain .,"['def', 'chain', '(', '*', 'tasks', ')', ':', 'for', 'up_task', ',', 'down_task', 'in', 'zip', '(', 'tasks', '[', ':', '-', '1', ']', ',', 'tasks', '[', '1', ':', ']', ')', ':', 'up_task', '.', 'set_downstream', '(', 'down_task', ')']","def chain ( * tasks ) : for up_task , down_task in zip ( tasks [ : - 1 ] , tasks [ 1 : ] ) : up_task . set_downstream ( down_task )"
254,apache/airflow,airflow/utils/helpers.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/helpers.py#L200-L240,"def pprinttable(rows):
    """"""Returns a pretty ascii table from tuples

    If namedtuple are used, the table will have headers
    """"""
    if not rows:
        return
    if hasattr(rows[0], '_fields'):  # if namedtuple
        headers = rows[0]._fields
    else:
        headers = [""col{}"".format(i) for i in range(len(rows[0]))]
    lens = [len(s) for s in headers]

    for row in rows:
        for i in range(len(rows[0])):
            slenght = len(""{}"".format(row[i]))
            if slenght > lens[i]:
                lens[i] = slenght
    formats = []
    hformats = []
    for i in range(len(rows[0])):
        if isinstance(rows[0][i], int):
            formats.append(""%%%dd"" % lens[i])
        else:
            formats.append(""%%-%ds"" % lens[i])
        hformats.append(""%%-%ds"" % lens[i])
    pattern = "" | "".join(formats)
    hpattern = "" | "".join(hformats)
    separator = ""-+-"".join(['-' * n for n in lens])
    s = """"
    s += separator + '\n'
    s += (hpattern % tuple(headers)) + '\n'
    s += separator + '\n'

    def f(t):
        return ""{}"".format(t) if isinstance(t, basestring) else t

    for line in rows:
        s += pattern % tuple(f(t) for t in line) + '\n'
    s += separator + '\n'
    return s","['def', 'pprinttable', '(', 'rows', ')', ':', 'if', 'not', 'rows', ':', 'return', 'if', 'hasattr', '(', 'rows', '[', '0', ']', ',', ""'_fields'"", ')', ':', '# if namedtuple', 'headers', '=', 'rows', '[', '0', ']', '.', '_fields', 'else', ':', 'headers', '=', '[', '""col{}""', '.', 'format', '(', 'i', ')', 'for', 'i', 'in', 'range', '(', 'len', '(', 'rows', '[', '0', ']', ')', ')', ']', 'lens', '=', '[', 'len', '(', 's', ')', 'for', 's', 'in', 'headers', ']', 'for', 'row', 'in', 'rows', ':', 'for', 'i', 'in', 'range', '(', 'len', '(', 'rows', '[', '0', ']', ')', ')', ':', 'slenght', '=', 'len', '(', '""{}""', '.', 'format', '(', 'row', '[', 'i', ']', ')', ')', 'if', 'slenght', '>', 'lens', '[', 'i', ']', ':', 'lens', '[', 'i', ']', '=', 'slenght', 'formats', '=', '[', ']', 'hformats', '=', '[', ']', 'for', 'i', 'in', 'range', '(', 'len', '(', 'rows', '[', '0', ']', ')', ')', ':', 'if', 'isinstance', '(', 'rows', '[', '0', ']', '[', 'i', ']', ',', 'int', ')', ':', 'formats', '.', 'append', '(', '""%%%dd""', '%', 'lens', '[', 'i', ']', ')', 'else', ':', 'formats', '.', 'append', '(', '""%%-%ds""', '%', 'lens', '[', 'i', ']', ')', 'hformats', '.', 'append', '(', '""%%-%ds""', '%', 'lens', '[', 'i', ']', ')', 'pattern', '=', '"" | ""', '.', 'join', '(', 'formats', ')', 'hpattern', '=', '"" | ""', '.', 'join', '(', 'hformats', ')', 'separator', '=', '""-+-""', '.', 'join', '(', '[', ""'-'"", '*', 'n', 'for', 'n', 'in', 'lens', ']', ')', 's', '=', '""""', 's', '+=', 'separator', '+', ""'\\n'"", 's', '+=', '(', 'hpattern', '%', 'tuple', '(', 'headers', ')', ')', '+', ""'\\n'"", 's', '+=', 'separator', '+', ""'\\n'"", 'def', 'f', '(', 't', ')', ':', 'return', '""{}""', '.', 'format', '(', 't', ')', 'if', 'isinstance', '(', 't', ',', 'basestring', ')', 'else', 't', 'for', 'line', 'in', 'rows', ':', 's', '+=', 'pattern', '%', 'tuple', '(', 'f', '(', 't', ')', 'for', 't', 'in', 'line', ')', '+', ""'\\n'"", 's', '+=', 'separator', '+', ""'\\n'"", 'return', 's']","Returns a pretty ascii table from tuples

    If namedtuple are used, the table will have headers","['Returns', 'a', 'pretty', 'ascii', 'table', 'from', 'tuples']",python,test,"['returns', 'a', 'pretty', 'ascii', 'table', 'from', 'tuples']",returns a pretty ascii table from tuples,"['def', 'pprinttable', '(', 'rows', ')', ':', 'if', 'not', 'rows', ':', 'return', 'if', 'hasattr', '(', 'rows', '[', '0', ']', ',', ""'_fields'"", ')', ':', '# if namedtuple', 'headers', '=', 'rows', '[', '0', ']', '.', '_fields', 'else', ':', 'headers', '=', '[', '""col{}""', '.', 'format', '(', 'i', ')', 'for', 'i', 'in', 'range', '(', 'len', '(', 'rows', '[', '0', ']', ')', ')', ']', 'lens', '=', '[', 'len', '(', 's', ')', 'for', 's', 'in', 'headers', ']', 'for', 'row', 'in', 'rows', ':', 'for', 'i', 'in', 'range', '(', 'len', '(', 'rows', '[', '0', ']', ')', ')', ':', 'slenght', '=', 'len', '(', '""{}""', '.', 'format', '(', 'row', '[', 'i', ']', ')', ')', 'if', 'slenght', '>', 'lens', '[', 'i', ']', ':', 'lens', '[', 'i', ']', '=', 'slenght', 'formats', '=', '[', ']', 'hformats', '=', '[', ']', 'for', 'i', 'in', 'range', '(', 'len', '(', 'rows', '[', '0', ']', ')', ')', ':', 'if', 'isinstance', '(', 'rows', '[', '0', ']', '[', 'i', ']', ',', 'int', ')', ':', 'formats', '.', 'append', '(', '""%%%dd""', '%', 'lens', '[', 'i', ']', ')', 'else', ':', 'formats', '.', 'append', '(', '""%%-%ds""', '%', 'lens', '[', 'i', ']', ')', 'hformats', '.', 'append', '(', '""%%-%ds""', '%', 'lens', '[', 'i', ']', ')', 'pattern', '=', '"" | ""', '.', 'join', '(', 'formats', ')', 'hpattern', '=', '"" | ""', '.', 'join', '(', 'hformats', ')', 'separator', '=', '""-+-""', '.', 'join', '(', '[', ""'-'"", '*', 'n', 'for', 'n', 'in', 'lens', ']', ')', 's', '=', '""""', 's', '+=', 'separator', '+', ""'\\n'"", 's', '+=', '(', 'hpattern', '%', 'tuple', '(', 'headers', ')', ')', '+', ""'\\n'"", 's', '+=', 'separator', '+', ""'\\n'"", 'def', 'f', '(', 't', ')', ':', 'return', '""{}""', '.', 'format', '(', 't', ')', 'if', 'isinstance', '(', 't', ',', 'basestring', ')', 'else', 't', 'for', 'line', 'in', 'rows', ':', 's', '+=', 'pattern', '%', 'tuple', '(', 'f', '(', 't', ')', 'for', 't', 'in', 'line', ')', '+', ""'\\n'"", 's', '+=', 'separator', '+', ""'\\n'"", 'return', 's']","def pprinttable ( rows ) : if not rows : return if hasattr ( rows [ 0 ] , '_fields' ) : # if namedtuple headers = rows [ 0 ] . _fields else : headers = [ ""col{}"" . format ( i ) for i in range ( len ( rows [ 0 ] ) ) ] lens = [ len ( s ) for s in headers ] for row in rows : for i in range ( len ( rows [ 0 ] ) ) : slenght = len ( ""{}"" . format ( row [ i ] ) ) if slenght > lens [ i ] : lens [ i ] = slenght formats = [ ] hformats = [ ] for i in range ( len ( rows [ 0 ] ) ) : if isinstance ( rows [ 0 ] [ i ] , int ) : formats . append ( ""%%%dd"" % lens [ i ] ) else : formats . append ( ""%%-%ds"" % lens [ i ] ) hformats . append ( ""%%-%ds"" % lens [ i ] ) pattern = "" | "" . join ( formats ) hpattern = "" | "" . join ( hformats ) separator = ""-+-"" . join ( [ '-' * n for n in lens ] ) s = """" s += separator + '\n' s += ( hpattern % tuple ( headers ) ) + '\n' s += separator + '\n' def f ( t ) : return ""{}"" . format ( t ) if isinstance ( t , basestring ) else t for line in rows : s += pattern % tuple ( f ( t ) for t in line ) + '\n' s += separator + '\n' return s"
255,apache/airflow,airflow/utils/helpers.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/helpers.py#L243-L289,"def reap_process_group(pid, log, sig=signal.SIGTERM,
                       timeout=DEFAULT_TIME_TO_WAIT_AFTER_SIGTERM):
    """"""
    Tries really hard to terminate all children (including grandchildren). Will send
    sig (SIGTERM) to the process group of pid. If any process is alive after timeout
    a SIGKILL will be send.

    :param log: log handler
    :param pid: pid to kill
    :param sig: signal type
    :param timeout: how much time a process has to terminate
    """"""

    def on_terminate(p):
        log.info(""Process %s (%s) terminated with exit code %s"", p, p.pid, p.returncode)

    if pid == os.getpid():
        raise RuntimeError(""I refuse to kill myself"")

    parent = psutil.Process(pid)

    children = parent.children(recursive=True)
    children.append(parent)

    try:
        pg = os.getpgid(pid)
    except OSError as err:
        # Skip if not such process - we experience a race and it just terminated
        if err.errno == errno.ESRCH:
            return
        raise

    log.info(""Sending %s to GPID %s"", sig, pg)
    os.killpg(os.getpgid(pid), sig)

    gone, alive = psutil.wait_procs(children, timeout=timeout, callback=on_terminate)

    if alive:
        for p in alive:
            log.warn(""process %s (%s) did not respond to SIGTERM. Trying SIGKILL"", p, pid)

        os.killpg(os.getpgid(pid), signal.SIGKILL)

        gone, alive = psutil.wait_procs(alive, timeout=timeout, callback=on_terminate)
        if alive:
            for p in alive:
                log.error(""Process %s (%s) could not be killed. Giving up."", p, p.pid)","['def', 'reap_process_group', '(', 'pid', ',', 'log', ',', 'sig', '=', 'signal', '.', 'SIGTERM', ',', 'timeout', '=', 'DEFAULT_TIME_TO_WAIT_AFTER_SIGTERM', ')', ':', 'def', 'on_terminate', '(', 'p', ')', ':', 'log', '.', 'info', '(', '""Process %s (%s) terminated with exit code %s""', ',', 'p', ',', 'p', '.', 'pid', ',', 'p', '.', 'returncode', ')', 'if', 'pid', '==', 'os', '.', 'getpid', '(', ')', ':', 'raise', 'RuntimeError', '(', '""I refuse to kill myself""', ')', 'parent', '=', 'psutil', '.', 'Process', '(', 'pid', ')', 'children', '=', 'parent', '.', 'children', '(', 'recursive', '=', 'True', ')', 'children', '.', 'append', '(', 'parent', ')', 'try', ':', 'pg', '=', 'os', '.', 'getpgid', '(', 'pid', ')', 'except', 'OSError', 'as', 'err', ':', '# Skip if not such process - we experience a race and it just terminated', 'if', 'err', '.', 'errno', '==', 'errno', '.', 'ESRCH', ':', 'return', 'raise', 'log', '.', 'info', '(', '""Sending %s to GPID %s""', ',', 'sig', ',', 'pg', ')', 'os', '.', 'killpg', '(', 'os', '.', 'getpgid', '(', 'pid', ')', ',', 'sig', ')', 'gone', ',', 'alive', '=', 'psutil', '.', 'wait_procs', '(', 'children', ',', 'timeout', '=', 'timeout', ',', 'callback', '=', 'on_terminate', ')', 'if', 'alive', ':', 'for', 'p', 'in', 'alive', ':', 'log', '.', 'warn', '(', '""process %s (%s) did not respond to SIGTERM. Trying SIGKILL""', ',', 'p', ',', 'pid', ')', 'os', '.', 'killpg', '(', 'os', '.', 'getpgid', '(', 'pid', ')', ',', 'signal', '.', 'SIGKILL', ')', 'gone', ',', 'alive', '=', 'psutil', '.', 'wait_procs', '(', 'alive', ',', 'timeout', '=', 'timeout', ',', 'callback', '=', 'on_terminate', ')', 'if', 'alive', ':', 'for', 'p', 'in', 'alive', ':', 'log', '.', 'error', '(', '""Process %s (%s) could not be killed. Giving up.""', ',', 'p', ',', 'p', '.', 'pid', ')']","Tries really hard to terminate all children (including grandchildren). Will send
    sig (SIGTERM) to the process group of pid. If any process is alive after timeout
    a SIGKILL will be send.

    :param log: log handler
    :param pid: pid to kill
    :param sig: signal type
    :param timeout: how much time a process has to terminate","['Tries', 'really', 'hard', 'to', 'terminate', 'all', 'children', '(', 'including', 'grandchildren', ')', '.', 'Will', 'send', 'sig', '(', 'SIGTERM', ')', 'to', 'the', 'process', 'group', 'of', 'pid', '.', 'If', 'any', 'process', 'is', 'alive', 'after', 'timeout', 'a', 'SIGKILL', 'will', 'be', 'send', '.']",python,test,"['tries', 'really', 'hard', 'to', 'terminate', 'all', 'children', '(', 'including', 'grandchildren', ')', '.', 'will', 'send', 'sig', '(', 'sigterm', ')', 'to', 'the', 'process', 'group', 'of', 'pid', '.', 'if', 'any', 'process', 'is', 'alive', 'after', 'timeout', 'a', 'sigkill', 'will', 'be', 'send', '.']",tries really hard to terminate all children ( including grandchildren ) . will send sig ( sigterm ) to the process group of pid . if any process is alive after timeout a sigkill will be send .,"['def', 'reap_process_group', '(', 'pid', ',', 'log', ',', 'sig', '=', 'signal', '.', 'sigterm', ',', 'timeout', '=', 'default_time_to_wait_after_sigterm', ')', ':', 'def', 'on_terminate', '(', 'p', ')', ':', 'log', '.', 'info', '(', '""process %s (%s) terminated with exit code %s""', ',', 'p', ',', 'p', '.', 'pid', ',', 'p', '.', 'returncode', ')', 'if', 'pid', '==', 'os', '.', 'getpid', '(', ')', ':', 'raise', 'runtimeerror', '(', '""i refuse to kill myself""', ')', 'parent', '=', 'psutil', '.', 'process', '(', 'pid', ')', 'children', '=', 'parent', '.', 'children', '(', 'recursive', '=', 'true', ')', 'children', '.', 'append', '(', 'parent', ')', 'try', ':', 'pg', '=', 'os', '.', 'getpgid', '(', 'pid', ')', 'except', 'oserror', 'as', 'err', ':', '# skip if not such process - we experience a race and it just terminated', 'if', 'err', '.', 'errno', '==', 'errno', '.', 'esrch', ':', 'return', 'raise', 'log', '.', 'info', '(', '""sending %s to gpid %s""', ',', 'sig', ',', 'pg', ')', 'os', '.', 'killpg', '(', 'os', '.', 'getpgid', '(', 'pid', ')', ',', 'sig', ')', 'gone', ',', 'alive', '=', 'psutil', '.', 'wait_procs', '(', 'children', ',', 'timeout', '=', 'timeout', ',', 'callback', '=', 'on_terminate', ')', 'if', 'alive', ':', 'for', 'p', 'in', 'alive', ':', 'log', '.', 'warn', '(', '""process %s (%s) did not respond to sigterm. trying sigkill""', ',', 'p', ',', 'pid', ')', 'os', '.', 'killpg', '(', 'os', '.', 'getpgid', '(', 'pid', ')', ',', 'signal', '.', 'sigkill', ')', 'gone', ',', 'alive', '=', 'psutil', '.', 'wait_procs', '(', 'alive', ',', 'timeout', '=', 'timeout', ',', 'callback', '=', 'on_terminate', ')', 'if', 'alive', ':', 'for', 'p', 'in', 'alive', ':', 'log', '.', 'error', '(', '""process %s (%s) could not be killed. giving up.""', ',', 'p', ',', 'p', '.', 'pid', ')']","def reap_process_group ( pid , log , sig = signal . sigterm , timeout = default_time_to_wait_after_sigterm ) : def on_terminate ( p ) : log . info ( ""process %s (%s) terminated with exit code %s"" , p , p . pid , p . returncode ) if pid == os . getpid ( ) : raise runtimeerror ( ""i refuse to kill myself"" ) parent = psutil . process ( pid ) children = parent . children ( recursive = true ) children . append ( parent ) try : pg = os . getpgid ( pid ) except oserror as err : # skip if not such process - we experience a race and it just terminated if err . errno == errno . esrch : return raise log . info ( ""sending %s to gpid %s"" , sig , pg ) os . killpg ( os . getpgid ( pid ) , sig ) gone , alive = psutil . wait_procs ( children , timeout = timeout , callback = on_terminate ) if alive : for p in alive : log . warn ( ""process %s (%s) did not respond to sigterm. trying sigkill"" , p , pid ) os . killpg ( os . getpgid ( pid ) , signal . sigkill ) gone , alive = psutil . wait_procs ( alive , timeout = timeout , callback = on_terminate ) if alive : for p in alive : log . error ( ""process %s (%s) could not be killed. giving up."" , p , p . pid )"
256,apache/airflow,airflow/utils/helpers.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/helpers.py#L299-L318,"def render_log_filename(ti, try_number, filename_template):
    """"""
    Given task instance, try_number, filename_template, return the rendered log
    filename

    :param ti: task instance
    :param try_number: try_number of the task
    :param filename_template: filename template, which can be jinja template or
        python string template
    """"""
    filename_template, filename_jinja_template = parse_template_string(filename_template)
    if filename_jinja_template:
        jinja_context = ti.get_template_context()
        jinja_context['try_number'] = try_number
        return filename_jinja_template.render(**jinja_context)

    return filename_template.format(dag_id=ti.dag_id,
                                    task_id=ti.task_id,
                                    execution_date=ti.execution_date.isoformat(),
                                    try_number=try_number)","['def', 'render_log_filename', '(', 'ti', ',', 'try_number', ',', 'filename_template', ')', ':', 'filename_template', ',', 'filename_jinja_template', '=', 'parse_template_string', '(', 'filename_template', ')', 'if', 'filename_jinja_template', ':', 'jinja_context', '=', 'ti', '.', 'get_template_context', '(', ')', 'jinja_context', '[', ""'try_number'"", ']', '=', 'try_number', 'return', 'filename_jinja_template', '.', 'render', '(', '*', '*', 'jinja_context', ')', 'return', 'filename_template', '.', 'format', '(', 'dag_id', '=', 'ti', '.', 'dag_id', ',', 'task_id', '=', 'ti', '.', 'task_id', ',', 'execution_date', '=', 'ti', '.', 'execution_date', '.', 'isoformat', '(', ')', ',', 'try_number', '=', 'try_number', ')']","Given task instance, try_number, filename_template, return the rendered log
    filename

    :param ti: task instance
    :param try_number: try_number of the task
    :param filename_template: filename template, which can be jinja template or
        python string template","['Given', 'task', 'instance', 'try_number', 'filename_template', 'return', 'the', 'rendered', 'log', 'filename']",python,test,"['given', 'task', 'instance', 'try_number', 'filename_template', 'return', 'the', 'rendered', 'log', 'filename']",given task instance try_number filename_template return the rendered log filename,"['def', 'render_log_filename', '(', 'ti', ',', 'try_number', ',', 'filename_template', ')', ':', 'filename_template', ',', 'filename_jinja_template', '=', 'parse_template_string', '(', 'filename_template', ')', 'if', 'filename_jinja_template', ':', 'jinja_context', '=', 'ti', '.', 'get_template_context', '(', ')', 'jinja_context', '[', ""'try_number'"", ']', '=', 'try_number', 'return', 'filename_jinja_template', '.', 'render', '(', '*', '*', 'jinja_context', ')', 'return', 'filename_template', '.', 'format', '(', 'dag_id', '=', 'ti', '.', 'dag_id', ',', 'task_id', '=', 'ti', '.', 'task_id', ',', 'execution_date', '=', 'ti', '.', 'execution_date', '.', 'isoformat', '(', ')', ',', 'try_number', '=', 'try_number', ')']","def render_log_filename ( ti , try_number , filename_template ) : filename_template , filename_jinja_template = parse_template_string ( filename_template ) if filename_jinja_template : jinja_context = ti . get_template_context ( ) jinja_context [ 'try_number' ] = try_number return filename_jinja_template . render ( * * jinja_context ) return filename_template . format ( dag_id = ti . dag_id , task_id = ti . task_id , execution_date = ti . execution_date . isoformat ( ) , try_number = try_number )"
257,apache/airflow,airflow/api/common/experimental/get_task_instance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/api/common/experimental/get_task_instance.py#L25-L55,"def get_task_instance(dag_id, task_id, execution_date):
    """"""Return the task object identified by the given dag_id and task_id.""""""

    dagbag = DagBag()

    # Check DAG exists.
    if dag_id not in dagbag.dags:
        error_message = ""Dag id {} not found"".format(dag_id)
        raise DagNotFound(error_message)

    # Get DAG object and check Task Exists
    dag = dagbag.get_dag(dag_id)
    if not dag.has_task(task_id):
        error_message = 'Task {} not found in dag {}'.format(task_id, dag_id)
        raise TaskNotFound(error_message)

    # Get DagRun object and check that it exists
    dagrun = dag.get_dagrun(execution_date=execution_date)
    if not dagrun:
        error_message = ('Dag Run for date {} not found in dag {}'
                         .format(execution_date, dag_id))
        raise DagRunNotFound(error_message)

    # Get task instance object and check that it exists
    task_instance = dagrun.get_task_instance(task_id)
    if not task_instance:
        error_message = ('Task {} instance for date {} not found'
                         .format(task_id, execution_date))
        raise TaskInstanceNotFound(error_message)

    return task_instance","['def', 'get_task_instance', '(', 'dag_id', ',', 'task_id', ',', 'execution_date', ')', ':', 'dagbag', '=', 'DagBag', '(', ')', '# Check DAG exists.', 'if', 'dag_id', 'not', 'in', 'dagbag', '.', 'dags', ':', 'error_message', '=', '""Dag id {} not found""', '.', 'format', '(', 'dag_id', ')', 'raise', 'DagNotFound', '(', 'error_message', ')', '# Get DAG object and check Task Exists', 'dag', '=', 'dagbag', '.', 'get_dag', '(', 'dag_id', ')', 'if', 'not', 'dag', '.', 'has_task', '(', 'task_id', ')', ':', 'error_message', '=', ""'Task {} not found in dag {}'"", '.', 'format', '(', 'task_id', ',', 'dag_id', ')', 'raise', 'TaskNotFound', '(', 'error_message', ')', '# Get DagRun object and check that it exists', 'dagrun', '=', 'dag', '.', 'get_dagrun', '(', 'execution_date', '=', 'execution_date', ')', 'if', 'not', 'dagrun', ':', 'error_message', '=', '(', ""'Dag Run for date {} not found in dag {}'"", '.', 'format', '(', 'execution_date', ',', 'dag_id', ')', ')', 'raise', 'DagRunNotFound', '(', 'error_message', ')', '# Get task instance object and check that it exists', 'task_instance', '=', 'dagrun', '.', 'get_task_instance', '(', 'task_id', ')', 'if', 'not', 'task_instance', ':', 'error_message', '=', '(', ""'Task {} instance for date {} not found'"", '.', 'format', '(', 'task_id', ',', 'execution_date', ')', ')', 'raise', 'TaskInstanceNotFound', '(', 'error_message', ')', 'return', 'task_instance']",Return the task object identified by the given dag_id and task_id.,"['Return', 'the', 'task', 'object', 'identified', 'by', 'the', 'given', 'dag_id', 'and', 'task_id', '.']",python,test,"['return', 'the', 'task', 'object', 'identified', 'by', 'the', 'given', 'dag_id', 'and', 'task_id', '.']",return the task object identified by the given dag_id and task_id .,"['def', 'get_task_instance', '(', 'dag_id', ',', 'task_id', ',', 'execution_date', ')', ':', 'dagbag', '=', 'dagbag', '(', ')', '# check dag exists.', 'if', 'dag_id', 'not', 'in', 'dagbag', '.', 'dags', ':', 'error_message', '=', '""dag id {} not found""', '.', 'format', '(', 'dag_id', ')', 'raise', 'dagnotfound', '(', 'error_message', ')', '# get dag object and check task exists', 'dag', '=', 'dagbag', '.', 'get_dag', '(', 'dag_id', ')', 'if', 'not', 'dag', '.', 'has_task', '(', 'task_id', ')', ':', 'error_message', '=', ""'task {} not found in dag {}'"", '.', 'format', '(', 'task_id', ',', 'dag_id', ')', 'raise', 'tasknotfound', '(', 'error_message', ')', '# get dagrun object and check that it exists', 'dagrun', '=', 'dag', '.', 'get_dagrun', '(', 'execution_date', '=', 'execution_date', ')', 'if', 'not', 'dagrun', ':', 'error_message', '=', '(', ""'dag run for date {} not found in dag {}'"", '.', 'format', '(', 'execution_date', ',', 'dag_id', ')', ')', 'raise', 'dagrunnotfound', '(', 'error_message', ')', '# get task instance object and check that it exists', 'task_instance', '=', 'dagrun', '.', 'get_task_instance', '(', 'task_id', ')', 'if', 'not', 'task_instance', ':', 'error_message', '=', '(', ""'task {} instance for date {} not found'"", '.', 'format', '(', 'task_id', ',', 'execution_date', ')', ')', 'raise', 'taskinstancenotfound', '(', 'error_message', ')', 'return', 'task_instance']","def get_task_instance ( dag_id , task_id , execution_date ) : dagbag = dagbag ( ) # check dag exists. if dag_id not in dagbag . dags : error_message = ""dag id {} not found"" . format ( dag_id ) raise dagnotfound ( error_message ) # get dag object and check task exists dag = dagbag . get_dag ( dag_id ) if not dag . has_task ( task_id ) : error_message = 'task {} not found in dag {}' . format ( task_id , dag_id ) raise tasknotfound ( error_message ) # get dagrun object and check that it exists dagrun = dag . get_dagrun ( execution_date = execution_date ) if not dagrun : error_message = ( 'dag run for date {} not found in dag {}' . format ( execution_date , dag_id ) ) raise dagrunnotfound ( error_message ) # get task instance object and check that it exists task_instance = dagrun . get_task_instance ( task_id ) if not task_instance : error_message = ( 'task {} instance for date {} not found' . format ( task_id , execution_date ) ) raise taskinstancenotfound ( error_message ) return task_instance"
258,apache/airflow,airflow/operators/__init__.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/operators/__init__.py#L21-L27,"def _integrate_plugins():
    """"""Integrate plugins to the context""""""
    import sys
    from airflow.plugins_manager import operators_modules
    for operators_module in operators_modules:
        sys.modules[operators_module.__name__] = operators_module
        globals()[operators_module._name] = operators_module","['def', '_integrate_plugins', '(', ')', ':', 'import', 'sys', 'from', 'airflow', '.', 'plugins_manager', 'import', 'operators_modules', 'for', 'operators_module', 'in', 'operators_modules', ':', 'sys', '.', 'modules', '[', 'operators_module', '.', '__name__', ']', '=', 'operators_module', 'globals', '(', ')', '[', 'operators_module', '.', '_name', ']', '=', 'operators_module']",Integrate plugins to the context,"['Integrate', 'plugins', 'to', 'the', 'context']",python,test,"['integrate', 'plugins', 'to', 'the', 'context']",integrate plugins to the context,"['def', '_integrate_plugins', '(', ')', ':', 'import', 'sys', 'from', 'airflow', '.', 'plugins_manager', 'import', 'operators_modules', 'for', 'operators_module', 'in', 'operators_modules', ':', 'sys', '.', 'modules', '[', 'operators_module', '.', '__name__', ']', '=', 'operators_module', 'globals', '(', ')', '[', 'operators_module', '.', '_name', ']', '=', 'operators_module']",def _integrate_plugins ( ) : import sys from airflow . plugins_manager import operators_modules for operators_module in operators_modules : sys . modules [ operators_module . __name__ ] = operators_module globals ( ) [ operators_module . _name ] = operators_module
259,apache/airflow,airflow/contrib/hooks/gcp_dataproc_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_dataproc_hook.py#L218-L223,"def get_conn(self):
        """"""Returns a Google Cloud Dataproc service object.""""""
        http_authorized = self._authorize()
        return build(
            'dataproc', self.api_version, http=http_authorized,
            cache_discovery=False)","['def', 'get_conn', '(', 'self', ')', ':', 'http_authorized', '=', 'self', '.', '_authorize', '(', ')', 'return', 'build', '(', ""'dataproc'"", ',', 'self', '.', 'api_version', ',', 'http', '=', 'http_authorized', ',', 'cache_discovery', '=', 'False', ')']",Returns a Google Cloud Dataproc service object.,"['Returns', 'a', 'Google', 'Cloud', 'Dataproc', 'service', 'object', '.']",python,test,"['returns', 'a', 'google', 'cloud', 'dataproc', 'service', 'object', '.']",returns a google cloud dataproc service object .,"['def', 'get_conn', '(', 'self', ')', ':', 'http_authorized', '=', 'self', '.', '_authorize', '(', ')', 'return', 'build', '(', ""'dataproc'"", ',', 'self', '.', 'api_version', ',', 'http', '=', 'http_authorized', ',', 'cache_discovery', '=', 'false', ')']","def get_conn ( self ) : http_authorized = self . _authorize ( ) return build ( 'dataproc' , self . api_version , http = http_authorized , cache_discovery = false )"
260,apache/airflow,airflow/contrib/hooks/gcp_dataproc_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_dataproc_hook.py#L243-L247,"def wait(self, operation):
        """"""Awaits for Google Cloud Dataproc Operation to complete.""""""
        submitted = _DataProcOperation(self.get_conn(), operation,
                                       self.num_retries)
        submitted.wait_for_done()","['def', 'wait', '(', 'self', ',', 'operation', ')', ':', 'submitted', '=', '_DataProcOperation', '(', 'self', '.', 'get_conn', '(', ')', ',', 'operation', ',', 'self', '.', 'num_retries', ')', 'submitted', '.', 'wait_for_done', '(', ')']",Awaits for Google Cloud Dataproc Operation to complete.,"['Awaits', 'for', 'Google', 'Cloud', 'Dataproc', 'Operation', 'to', 'complete', '.']",python,test,"['awaits', 'for', 'google', 'cloud', 'dataproc', 'operation', 'to', 'complete', '.']",awaits for google cloud dataproc operation to complete .,"['def', 'wait', '(', 'self', ',', 'operation', ')', ':', 'submitted', '=', '_dataprocoperation', '(', 'self', '.', 'get_conn', '(', ')', ',', 'operation', ',', 'self', '.', 'num_retries', ')', 'submitted', '.', 'wait_for_done', '(', ')']","def wait ( self , operation ) : submitted = _dataprocoperation ( self . get_conn ( ) , operation , self . num_retries ) submitted . wait_for_done ( )"
261,apache/airflow,airflow/contrib/operators/databricks_operator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/databricks_operator.py#L34-L58,"def _deep_string_coerce(content, json_path='json'):
    """"""
    Coerces content or all values of content if it is a dict to a string. The
    function will throw if content contains non-string or non-numeric types.

    The reason why we have this function is because the ``self.json`` field must be a
    dict with only string values. This is because ``render_template`` will fail
    for numerical values.
    """"""
    c = _deep_string_coerce
    if isinstance(content, six.string_types):
        return content
    elif isinstance(content, six.integer_types + (float,)):
        # Databricks can tolerate either numeric or string types in the API backend.
        return str(content)
    elif isinstance(content, (list, tuple)):
        return [c(e, '{0}[{1}]'.format(json_path, i)) for i, e in enumerate(content)]
    elif isinstance(content, dict):
        return {k: c(v, '{0}[{1}]'.format(json_path, k))
                for k, v in list(content.items())}
    else:
        param_type = type(content)
        msg = 'Type {0} used for parameter {1} is not a number or a string' \
            .format(param_type, json_path)
        raise AirflowException(msg)","['def', '_deep_string_coerce', '(', 'content', ',', 'json_path', '=', ""'json'"", ')', ':', 'c', '=', '_deep_string_coerce', 'if', 'isinstance', '(', 'content', ',', 'six', '.', 'string_types', ')', ':', 'return', 'content', 'elif', 'isinstance', '(', 'content', ',', 'six', '.', 'integer_types', '+', '(', 'float', ',', ')', ')', ':', '# Databricks can tolerate either numeric or string types in the API backend.', 'return', 'str', '(', 'content', ')', 'elif', 'isinstance', '(', 'content', ',', '(', 'list', ',', 'tuple', ')', ')', ':', 'return', '[', 'c', '(', 'e', ',', ""'{0}[{1}]'"", '.', 'format', '(', 'json_path', ',', 'i', ')', ')', 'for', 'i', ',', 'e', 'in', 'enumerate', '(', 'content', ')', ']', 'elif', 'isinstance', '(', 'content', ',', 'dict', ')', ':', 'return', '{', 'k', ':', 'c', '(', 'v', ',', ""'{0}[{1}]'"", '.', 'format', '(', 'json_path', ',', 'k', ')', ')', 'for', 'k', ',', 'v', 'in', 'list', '(', 'content', '.', 'items', '(', ')', ')', '}', 'else', ':', 'param_type', '=', 'type', '(', 'content', ')', 'msg', '=', ""'Type {0} used for parameter {1} is not a number or a string'"", '.', 'format', '(', 'param_type', ',', 'json_path', ')', 'raise', 'AirflowException', '(', 'msg', ')']","Coerces content or all values of content if it is a dict to a string. The
    function will throw if content contains non-string or non-numeric types.

    The reason why we have this function is because the ``self.json`` field must be a
    dict with only string values. This is because ``render_template`` will fail
    for numerical values.","['Coerces', 'content', 'or', 'all', 'values', 'of', 'content', 'if', 'it', 'is', 'a', 'dict', 'to', 'a', 'string', '.', 'The', 'function', 'will', 'throw', 'if', 'content', 'contains', 'non', '-', 'string', 'or', 'non', '-', 'numeric', 'types', '.']",python,test,"['coerces', 'content', 'or', 'all', 'values', 'of', 'content', 'if', 'it', 'is', 'a', 'dict', 'to', 'a', 'string', '.', 'the', 'function', 'will', 'throw', 'if', 'content', 'contains', 'non', '-', 'string', 'or', 'non', '-', 'numeric', 'types', '.']",coerces content or all values of content if it is a dict to a string . the function will throw if content contains non - string or non - numeric types .,"['def', '_deep_string_coerce', '(', 'content', ',', 'json_path', '=', ""'json'"", ')', ':', 'c', '=', '_deep_string_coerce', 'if', 'isinstance', '(', 'content', ',', 'six', '.', 'string_types', ')', ':', 'return', 'content', 'elif', 'isinstance', '(', 'content', ',', 'six', '.', 'integer_types', '+', '(', 'float', ',', ')', ')', ':', '# databricks can tolerate either numeric or string types in the api backend.', 'return', 'str', '(', 'content', ')', 'elif', 'isinstance', '(', 'content', ',', '(', 'list', ',', 'tuple', ')', ')', ':', 'return', '[', 'c', '(', 'e', ',', ""'{0}[{1}]'"", '.', 'format', '(', 'json_path', ',', 'i', ')', ')', 'for', 'i', ',', 'e', 'in', 'enumerate', '(', 'content', ')', ']', 'elif', 'isinstance', '(', 'content', ',', 'dict', ')', ':', 'return', '{', 'k', ':', 'c', '(', 'v', ',', ""'{0}[{1}]'"", '.', 'format', '(', 'json_path', ',', 'k', ')', ')', 'for', 'k', ',', 'v', 'in', 'list', '(', 'content', '.', 'items', '(', ')', ')', '}', 'else', ':', 'param_type', '=', 'type', '(', 'content', ')', 'msg', '=', ""'type {0} used for parameter {1} is not a number or a string'"", '.', 'format', '(', 'param_type', ',', 'json_path', ')', 'raise', 'airflowexception', '(', 'msg', ')']","def _deep_string_coerce ( content , json_path = 'json' ) : c = _deep_string_coerce if isinstance ( content , six . string_types ) : return content elif isinstance ( content , six . integer_types + ( float , ) ) : # databricks can tolerate either numeric or string types in the api backend. return str ( content ) elif isinstance ( content , ( list , tuple ) ) : return [ c ( e , '{0}[{1}]' . format ( json_path , i ) ) for i , e in enumerate ( content ) ] elif isinstance ( content , dict ) : return { k : c ( v , '{0}[{1}]' . format ( json_path , k ) ) for k , v in list ( content . items ( ) ) } else : param_type = type ( content ) msg = 'type {0} used for parameter {1} is not a number or a string' . format ( param_type , json_path ) raise airflowexception ( msg )"
262,apache/airflow,airflow/contrib/operators/databricks_operator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/databricks_operator.py#L61-L92,"def _handle_databricks_operator_execution(operator, hook, log, context):
    """"""
    Handles the Airflow + Databricks lifecycle logic for a Databricks operator

    :param operator: Databricks operator being handled
    :param context: Airflow context
    """"""
    if operator.do_xcom_push:
        context['ti'].xcom_push(key=XCOM_RUN_ID_KEY, value=operator.run_id)
    log.info('Run submitted with run_id: %s', operator.run_id)
    run_page_url = hook.get_run_page_url(operator.run_id)
    if operator.do_xcom_push:
        context['ti'].xcom_push(key=XCOM_RUN_PAGE_URL_KEY, value=run_page_url)

    log.info('View run status, Spark UI, and logs at %s', run_page_url)
    while True:
        run_state = hook.get_run_state(operator.run_id)
        if run_state.is_terminal:
            if run_state.is_successful:
                log.info('%s completed successfully.', operator.task_id)
                log.info('View run status, Spark UI, and logs at %s', run_page_url)
                return
            else:
                error_message = '{t} failed with terminal state: {s}'.format(
                    t=operator.task_id,
                    s=run_state)
                raise AirflowException(error_message)
        else:
            log.info('%s in run state: %s', operator.task_id, run_state)
            log.info('View run status, Spark UI, and logs at %s', run_page_url)
            log.info('Sleeping for %s seconds.', operator.polling_period_seconds)
            time.sleep(operator.polling_period_seconds)","['def', '_handle_databricks_operator_execution', '(', 'operator', ',', 'hook', ',', 'log', ',', 'context', ')', ':', 'if', 'operator', '.', 'do_xcom_push', ':', 'context', '[', ""'ti'"", ']', '.', 'xcom_push', '(', 'key', '=', 'XCOM_RUN_ID_KEY', ',', 'value', '=', 'operator', '.', 'run_id', ')', 'log', '.', 'info', '(', ""'Run submitted with run_id: %s'"", ',', 'operator', '.', 'run_id', ')', 'run_page_url', '=', 'hook', '.', 'get_run_page_url', '(', 'operator', '.', 'run_id', ')', 'if', 'operator', '.', 'do_xcom_push', ':', 'context', '[', ""'ti'"", ']', '.', 'xcom_push', '(', 'key', '=', 'XCOM_RUN_PAGE_URL_KEY', ',', 'value', '=', 'run_page_url', ')', 'log', '.', 'info', '(', ""'View run status, Spark UI, and logs at %s'"", ',', 'run_page_url', ')', 'while', 'True', ':', 'run_state', '=', 'hook', '.', 'get_run_state', '(', 'operator', '.', 'run_id', ')', 'if', 'run_state', '.', 'is_terminal', ':', 'if', 'run_state', '.', 'is_successful', ':', 'log', '.', 'info', '(', ""'%s completed successfully.'"", ',', 'operator', '.', 'task_id', ')', 'log', '.', 'info', '(', ""'View run status, Spark UI, and logs at %s'"", ',', 'run_page_url', ')', 'return', 'else', ':', 'error_message', '=', ""'{t} failed with terminal state: {s}'"", '.', 'format', '(', 't', '=', 'operator', '.', 'task_id', ',', 's', '=', 'run_state', ')', 'raise', 'AirflowException', '(', 'error_message', ')', 'else', ':', 'log', '.', 'info', '(', ""'%s in run state: %s'"", ',', 'operator', '.', 'task_id', ',', 'run_state', ')', 'log', '.', 'info', '(', ""'View run status, Spark UI, and logs at %s'"", ',', 'run_page_url', ')', 'log', '.', 'info', '(', ""'Sleeping for %s seconds.'"", ',', 'operator', '.', 'polling_period_seconds', ')', 'time', '.', 'sleep', '(', 'operator', '.', 'polling_period_seconds', ')']","Handles the Airflow + Databricks lifecycle logic for a Databricks operator

    :param operator: Databricks operator being handled
    :param context: Airflow context","['Handles', 'the', 'Airflow', '+', 'Databricks', 'lifecycle', 'logic', 'for', 'a', 'Databricks', 'operator']",python,test,"['handles', 'the', 'airflow', '+', 'databricks', 'lifecycle', 'logic', 'for', 'a', 'databricks', 'operator']",handles the airflow + databricks lifecycle logic for a databricks operator,"['def', '_handle_databricks_operator_execution', '(', 'operator', ',', 'hook', ',', 'log', ',', 'context', ')', ':', 'if', 'operator', '.', 'do_xcom_push', ':', 'context', '[', ""'ti'"", ']', '.', 'xcom_push', '(', 'key', '=', 'xcom_run_id_key', ',', 'value', '=', 'operator', '.', 'run_id', ')', 'log', '.', 'info', '(', ""'run submitted with run_id: %s'"", ',', 'operator', '.', 'run_id', ')', 'run_page_url', '=', 'hook', '.', 'get_run_page_url', '(', 'operator', '.', 'run_id', ')', 'if', 'operator', '.', 'do_xcom_push', ':', 'context', '[', ""'ti'"", ']', '.', 'xcom_push', '(', 'key', '=', 'xcom_run_page_url_key', ',', 'value', '=', 'run_page_url', ')', 'log', '.', 'info', '(', ""'view run status, spark ui, and logs at %s'"", ',', 'run_page_url', ')', 'while', 'true', ':', 'run_state', '=', 'hook', '.', 'get_run_state', '(', 'operator', '.', 'run_id', ')', 'if', 'run_state', '.', 'is_terminal', ':', 'if', 'run_state', '.', 'is_successful', ':', 'log', '.', 'info', '(', ""'%s completed successfully.'"", ',', 'operator', '.', 'task_id', ')', 'log', '.', 'info', '(', ""'view run status, spark ui, and logs at %s'"", ',', 'run_page_url', ')', 'return', 'else', ':', 'error_message', '=', ""'{t} failed with terminal state: {s}'"", '.', 'format', '(', 't', '=', 'operator', '.', 'task_id', ',', 's', '=', 'run_state', ')', 'raise', 'airflowexception', '(', 'error_message', ')', 'else', ':', 'log', '.', 'info', '(', ""'%s in run state: %s'"", ',', 'operator', '.', 'task_id', ',', 'run_state', ')', 'log', '.', 'info', '(', ""'view run status, spark ui, and logs at %s'"", ',', 'run_page_url', ')', 'log', '.', 'info', '(', ""'sleeping for %s seconds.'"", ',', 'operator', '.', 'polling_period_seconds', ')', 'time', '.', 'sleep', '(', 'operator', '.', 'polling_period_seconds', ')']","def _handle_databricks_operator_execution ( operator , hook , log , context ) : if operator . do_xcom_push : context [ 'ti' ] . xcom_push ( key = xcom_run_id_key , value = operator . run_id ) log . info ( 'run submitted with run_id: %s' , operator . run_id ) run_page_url = hook . get_run_page_url ( operator . run_id ) if operator . do_xcom_push : context [ 'ti' ] . xcom_push ( key = xcom_run_page_url_key , value = run_page_url ) log . info ( 'view run status, spark ui, and logs at %s' , run_page_url ) while true : run_state = hook . get_run_state ( operator . run_id ) if run_state . is_terminal : if run_state . is_successful : log . info ( '%s completed successfully.' , operator . task_id ) log . info ( 'view run status, spark ui, and logs at %s' , run_page_url ) return else : error_message = '{t} failed with terminal state: {s}' . format ( t = operator . task_id , s = run_state ) raise airflowexception ( error_message ) else : log . info ( '%s in run state: %s' , operator . task_id , run_state ) log . info ( 'view run status, spark ui, and logs at %s' , run_page_url ) log . info ( 'sleeping for %s seconds.' , operator . polling_period_seconds ) time . sleep ( operator . polling_period_seconds )"
263,apache/airflow,airflow/hooks/pig_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/pig_hook.py#L45-L87,"def run_cli(self, pig, verbose=True):
        """"""
        Run an pig script using the pig cli

        >>> ph = PigCliHook()
        >>> result = ph.run_cli(""ls /;"")
        >>> (""hdfs://"" in result)
        True
        """"""

        with TemporaryDirectory(prefix='airflow_pigop_') as tmp_dir:
            with NamedTemporaryFile(dir=tmp_dir) as f:
                f.write(pig.encode('utf-8'))
                f.flush()
                fname = f.name
                pig_bin = 'pig'
                cmd_extra = []

                pig_cmd = [pig_bin, '-f', fname] + cmd_extra

                if self.pig_properties:
                    pig_properties_list = self.pig_properties.split()
                    pig_cmd.extend(pig_properties_list)
                if verbose:
                    self.log.info(""%s"", "" "".join(pig_cmd))
                sp = subprocess.Popen(
                    pig_cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    cwd=tmp_dir,
                    close_fds=True)
                self.sp = sp
                stdout = ''
                for line in iter(sp.stdout.readline, b''):
                    stdout += line.decode('utf-8')
                    if verbose:
                        self.log.info(line.strip())
                sp.wait()

                if sp.returncode:
                    raise AirflowException(stdout)

                return stdout","['def', 'run_cli', '(', 'self', ',', 'pig', ',', 'verbose', '=', 'True', ')', ':', 'with', 'TemporaryDirectory', '(', 'prefix', '=', ""'airflow_pigop_'"", ')', 'as', 'tmp_dir', ':', 'with', 'NamedTemporaryFile', '(', 'dir', '=', 'tmp_dir', ')', 'as', 'f', ':', 'f', '.', 'write', '(', 'pig', '.', 'encode', '(', ""'utf-8'"", ')', ')', 'f', '.', 'flush', '(', ')', 'fname', '=', 'f', '.', 'name', 'pig_bin', '=', ""'pig'"", 'cmd_extra', '=', '[', ']', 'pig_cmd', '=', '[', 'pig_bin', ',', ""'-f'"", ',', 'fname', ']', '+', 'cmd_extra', 'if', 'self', '.', 'pig_properties', ':', 'pig_properties_list', '=', 'self', '.', 'pig_properties', '.', 'split', '(', ')', 'pig_cmd', '.', 'extend', '(', 'pig_properties_list', ')', 'if', 'verbose', ':', 'self', '.', 'log', '.', 'info', '(', '""%s""', ',', '"" ""', '.', 'join', '(', 'pig_cmd', ')', ')', 'sp', '=', 'subprocess', '.', 'Popen', '(', 'pig_cmd', ',', 'stdout', '=', 'subprocess', '.', 'PIPE', ',', 'stderr', '=', 'subprocess', '.', 'STDOUT', ',', 'cwd', '=', 'tmp_dir', ',', 'close_fds', '=', 'True', ')', 'self', '.', 'sp', '=', 'sp', 'stdout', '=', ""''"", 'for', 'line', 'in', 'iter', '(', 'sp', '.', 'stdout', '.', 'readline', ',', ""b''"", ')', ':', 'stdout', '+=', 'line', '.', 'decode', '(', ""'utf-8'"", ')', 'if', 'verbose', ':', 'self', '.', 'log', '.', 'info', '(', 'line', '.', 'strip', '(', ')', ')', 'sp', '.', 'wait', '(', ')', 'if', 'sp', '.', 'returncode', ':', 'raise', 'AirflowException', '(', 'stdout', ')', 'return', 'stdout']","Run an pig script using the pig cli

        >>> ph = PigCliHook()
        >>> result = ph.run_cli(""ls /;"")
        >>> (""hdfs://"" in result)
        True","['Run', 'an', 'pig', 'script', 'using', 'the', 'pig', 'cli']",python,test,"['run', 'an', 'pig', 'script', 'using', 'the', 'pig', 'cli']",run an pig script using the pig cli,"['def', 'run_cli', '(', 'self', ',', 'pig', ',', 'verbose', '=', 'true', ')', ':', 'with', 'temporarydirectory', '(', 'prefix', '=', ""'airflow_pigop_'"", ')', 'as', 'tmp_dir', ':', 'with', 'namedtemporaryfile', '(', 'dir', '=', 'tmp_dir', ')', 'as', 'f', ':', 'f', '.', 'write', '(', 'pig', '.', 'encode', '(', ""'utf-8'"", ')', ')', 'f', '.', 'flush', '(', ')', 'fname', '=', 'f', '.', 'name', 'pig_bin', '=', ""'pig'"", 'cmd_extra', '=', '[', ']', 'pig_cmd', '=', '[', 'pig_bin', ',', ""'-f'"", ',', 'fname', ']', '+', 'cmd_extra', 'if', 'self', '.', 'pig_properties', ':', 'pig_properties_list', '=', 'self', '.', 'pig_properties', '.', 'split', '(', ')', 'pig_cmd', '.', 'extend', '(', 'pig_properties_list', ')', 'if', 'verbose', ':', 'self', '.', 'log', '.', 'info', '(', '""%s""', ',', '"" ""', '.', 'join', '(', 'pig_cmd', ')', ')', 'sp', '=', 'subprocess', '.', 'popen', '(', 'pig_cmd', ',', 'stdout', '=', 'subprocess', '.', 'pipe', ',', 'stderr', '=', 'subprocess', '.', 'stdout', ',', 'cwd', '=', 'tmp_dir', ',', 'close_fds', '=', 'true', ')', 'self', '.', 'sp', '=', 'sp', 'stdout', '=', ""''"", 'for', 'line', 'in', 'iter', '(', 'sp', '.', 'stdout', '.', 'readline', ',', ""b''"", ')', ':', 'stdout', '+=', 'line', '.', 'decode', '(', ""'utf-8'"", ')', 'if', 'verbose', ':', 'self', '.', 'log', '.', 'info', '(', 'line', '.', 'strip', '(', ')', ')', 'sp', '.', 'wait', '(', ')', 'if', 'sp', '.', 'returncode', ':', 'raise', 'airflowexception', '(', 'stdout', ')', 'return', 'stdout']","def run_cli ( self , pig , verbose = true ) : with temporarydirectory ( prefix = 'airflow_pigop_' ) as tmp_dir : with namedtemporaryfile ( dir = tmp_dir ) as f : f . write ( pig . encode ( 'utf-8' ) ) f . flush ( ) fname = f . name pig_bin = 'pig' cmd_extra = [ ] pig_cmd = [ pig_bin , '-f' , fname ] + cmd_extra if self . pig_properties : pig_properties_list = self . pig_properties . split ( ) pig_cmd . extend ( pig_properties_list ) if verbose : self . log . info ( ""%s"" , "" "" . join ( pig_cmd ) ) sp = subprocess . popen ( pig_cmd , stdout = subprocess . pipe , stderr = subprocess . stdout , cwd = tmp_dir , close_fds = true ) self . sp = sp stdout = '' for line in iter ( sp . stdout . readline , b'' ) : stdout += line . decode ( 'utf-8' ) if verbose : self . log . info ( line . strip ( ) ) sp . wait ( ) if sp . returncode : raise airflowexception ( stdout ) return stdout"
264,apache/airflow,airflow/executors/celery_executor.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/celery_executor.py#L90-L111,"def fetch_celery_task_state(celery_task):
    """"""
    Fetch and return the state of the given celery task. The scope of this function is
    global so that it can be called by subprocesses in the pool.

    :param celery_task: a tuple of the Celery task key and the async Celery object used
        to fetch the task's state
    :type celery_task: tuple(str, celery.result.AsyncResult)
    :return: a tuple of the Celery task key and the Celery state of the task
    :rtype: tuple[str, str]
    """"""

    try:
        with timeout(seconds=2):
            # Accessing state property of celery task will make actual network request
            # to get the current state of the task.
            res = (celery_task[0], celery_task[1].state)
    except Exception as e:
        exception_traceback = ""Celery Task ID: {}\n{}"".format(celery_task[0],
                                                              traceback.format_exc())
        res = ExceptionWithTraceback(e, exception_traceback)
    return res","['def', 'fetch_celery_task_state', '(', 'celery_task', ')', ':', 'try', ':', 'with', 'timeout', '(', 'seconds', '=', '2', ')', ':', '# Accessing state property of celery task will make actual network request', '# to get the current state of the task.', 'res', '=', '(', 'celery_task', '[', '0', ']', ',', 'celery_task', '[', '1', ']', '.', 'state', ')', 'except', 'Exception', 'as', 'e', ':', 'exception_traceback', '=', '""Celery Task ID: {}\\n{}""', '.', 'format', '(', 'celery_task', '[', '0', ']', ',', 'traceback', '.', 'format_exc', '(', ')', ')', 'res', '=', 'ExceptionWithTraceback', '(', 'e', ',', 'exception_traceback', ')', 'return', 'res']","Fetch and return the state of the given celery task. The scope of this function is
    global so that it can be called by subprocesses in the pool.

    :param celery_task: a tuple of the Celery task key and the async Celery object used
        to fetch the task's state
    :type celery_task: tuple(str, celery.result.AsyncResult)
    :return: a tuple of the Celery task key and the Celery state of the task
    :rtype: tuple[str, str]","['Fetch', 'and', 'return', 'the', 'state', 'of', 'the', 'given', 'celery', 'task', '.', 'The', 'scope', 'of', 'this', 'function', 'is', 'global', 'so', 'that', 'it', 'can', 'be', 'called', 'by', 'subprocesses', 'in', 'the', 'pool', '.']",python,test,"['fetch', 'and', 'return', 'the', 'state', 'of', 'the', 'given', 'celery', 'task', '.', 'the', 'scope', 'of', 'this', 'function', 'is', 'global', 'so', 'that', 'it', 'can', 'be', 'called', 'by', 'subprocesses', 'in', 'the', 'pool', '.']",fetch and return the state of the given celery task . the scope of this function is global so that it can be called by subprocesses in the pool .,"['def', 'fetch_celery_task_state', '(', 'celery_task', ')', ':', 'try', ':', 'with', 'timeout', '(', 'seconds', '=', '2', ')', ':', '# accessing state property of celery task will make actual network request', '# to get the current state of the task.', 'res', '=', '(', 'celery_task', '[', '0', ']', ',', 'celery_task', '[', '1', ']', '.', 'state', ')', 'except', 'exception', 'as', 'e', ':', 'exception_traceback', '=', '""celery task id: {}\\n{}""', '.', 'format', '(', 'celery_task', '[', '0', ']', ',', 'traceback', '.', 'format_exc', '(', ')', ')', 'res', '=', 'exceptionwithtraceback', '(', 'e', ',', 'exception_traceback', ')', 'return', 'res']","def fetch_celery_task_state ( celery_task ) : try : with timeout ( seconds = 2 ) : # accessing state property of celery task will make actual network request # to get the current state of the task. res = ( celery_task [ 0 ] , celery_task [ 1 ] . state ) except exception as e : exception_traceback = ""celery task id: {}\n{}"" . format ( celery_task [ 0 ] , traceback . format_exc ( ) ) res = exceptionwithtraceback ( e , exception_traceback ) return res"
265,apache/airflow,airflow/executors/celery_executor.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/celery_executor.py#L158-L166,"def _num_tasks_per_send_process(self, to_send_count):
        """"""
        How many Celery tasks should each worker process send.

        :return: Number of tasks that should be sent per process
        :rtype: int
        """"""
        return max(1,
                   int(math.ceil(1.0 * to_send_count / self._sync_parallelism)))","['def', '_num_tasks_per_send_process', '(', 'self', ',', 'to_send_count', ')', ':', 'return', 'max', '(', '1', ',', 'int', '(', 'math', '.', 'ceil', '(', '1.0', '*', 'to_send_count', '/', 'self', '.', '_sync_parallelism', ')', ')', ')']","How many Celery tasks should each worker process send.

        :return: Number of tasks that should be sent per process
        :rtype: int","['How', 'many', 'Celery', 'tasks', 'should', 'each', 'worker', 'process', 'send', '.']",python,test,"['how', 'many', 'celery', 'tasks', 'should', 'each', 'worker', 'process', 'send', '.']",how many celery tasks should each worker process send .,"['def', '_num_tasks_per_send_process', '(', 'self', ',', 'to_send_count', ')', ':', 'return', 'max', '(', '1', ',', 'int', '(', 'math', '.', 'ceil', '(', '1.0', '*', 'to_send_count', '/', 'self', '.', '_sync_parallelism', ')', ')', ')']","def _num_tasks_per_send_process ( self , to_send_count ) : return max ( 1 , int ( math . ceil ( 1.0 * to_send_count / self . _sync_parallelism ) ) )"
266,apache/airflow,airflow/executors/celery_executor.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/celery_executor.py#L168-L176,"def _num_tasks_per_fetch_process(self):
        """"""
        How many Celery tasks should be sent to each worker process.

        :return: Number of tasks that should be used per process
        :rtype: int
        """"""
        return max(1,
                   int(math.ceil(1.0 * len(self.tasks) / self._sync_parallelism)))","['def', '_num_tasks_per_fetch_process', '(', 'self', ')', ':', 'return', 'max', '(', '1', ',', 'int', '(', 'math', '.', 'ceil', '(', '1.0', '*', 'len', '(', 'self', '.', 'tasks', ')', '/', 'self', '.', '_sync_parallelism', ')', ')', ')']","How many Celery tasks should be sent to each worker process.

        :return: Number of tasks that should be used per process
        :rtype: int","['How', 'many', 'Celery', 'tasks', 'should', 'be', 'sent', 'to', 'each', 'worker', 'process', '.']",python,test,"['how', 'many', 'celery', 'tasks', 'should', 'be', 'sent', 'to', 'each', 'worker', 'process', '.']",how many celery tasks should be sent to each worker process .,"['def', '_num_tasks_per_fetch_process', '(', 'self', ')', ':', 'return', 'max', '(', '1', ',', 'int', '(', 'math', '.', 'ceil', '(', '1.0', '*', 'len', '(', 'self', '.', 'tasks', ')', '/', 'self', '.', '_sync_parallelism', ')', ')', ')']","def _num_tasks_per_fetch_process ( self ) : return max ( 1 , int ( math . ceil ( 1.0 * len ( self . tasks ) / self . _sync_parallelism ) ) )"
267,apache/airflow,airflow/models/variable.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/variable.py#L76-L99,"def setdefault(cls, key, default, deserialize_json=False):
        """"""
        Like a Python builtin dict object, setdefault returns the current value
        for a key, and if it isn't there, stores the default value and returns it.

        :param key: Dict key for this Variable
        :type key: str
        :param default: Default value to set and return if the variable
            isn't already in the DB
        :type default: Mixed
        :param deserialize_json: Store this as a JSON encoded value in the DB
            and un-encode it when retrieving a value
        :return: Mixed
        """"""
        obj = Variable.get(key, default_var=None,
                           deserialize_json=deserialize_json)
        if obj is None:
            if default is not None:
                Variable.set(key, default, serialize_json=deserialize_json)
                return default
            else:
                raise ValueError('Default Value must be set')
        else:
            return obj","['def', 'setdefault', '(', 'cls', ',', 'key', ',', 'default', ',', 'deserialize_json', '=', 'False', ')', ':', 'obj', '=', 'Variable', '.', 'get', '(', 'key', ',', 'default_var', '=', 'None', ',', 'deserialize_json', '=', 'deserialize_json', ')', 'if', 'obj', 'is', 'None', ':', 'if', 'default', 'is', 'not', 'None', ':', 'Variable', '.', 'set', '(', 'key', ',', 'default', ',', 'serialize_json', '=', 'deserialize_json', ')', 'return', 'default', 'else', ':', 'raise', 'ValueError', '(', ""'Default Value must be set'"", ')', 'else', ':', 'return', 'obj']","Like a Python builtin dict object, setdefault returns the current value
        for a key, and if it isn't there, stores the default value and returns it.

        :param key: Dict key for this Variable
        :type key: str
        :param default: Default value to set and return if the variable
            isn't already in the DB
        :type default: Mixed
        :param deserialize_json: Store this as a JSON encoded value in the DB
            and un-encode it when retrieving a value
        :return: Mixed","['Like', 'a', 'Python', 'builtin', 'dict', 'object', 'setdefault', 'returns', 'the', 'current', 'value', 'for', 'a', 'key', 'and', 'if', 'it', 'isn', 't', 'there', 'stores', 'the', 'default', 'value', 'and', 'returns', 'it', '.']",python,test,"['like', 'a', 'python', 'builtin', 'dict', 'object', 'setdefault', 'returns', 'the', 'current', 'value', 'for', 'a', 'key', 'and', 'if', 'it', 'isn', 't', 'there', 'stores', 'the', 'default', 'value', 'and', 'returns', 'it', '.']",like a python builtin dict object setdefault returns the current value for a key and if it isn t there stores the default value and returns it .,"['def', 'setdefault', '(', 'cls', ',', 'key', ',', 'default', ',', 'deserialize_json', '=', 'false', ')', ':', 'obj', '=', 'variable', '.', 'get', '(', 'key', ',', 'default_var', '=', 'none', ',', 'deserialize_json', '=', 'deserialize_json', ')', 'if', 'obj', 'is', 'none', ':', 'if', 'default', 'is', 'not', 'none', ':', 'variable', '.', 'set', '(', 'key', ',', 'default', ',', 'serialize_json', '=', 'deserialize_json', ')', 'return', 'default', 'else', ':', 'raise', 'valueerror', '(', ""'default value must be set'"", ')', 'else', ':', 'return', 'obj']","def setdefault ( cls , key , default , deserialize_json = false ) : obj = variable . get ( key , default_var = none , deserialize_json = deserialize_json ) if obj is none : if default is not none : variable . set ( key , default , serialize_json = deserialize_json ) return default else : raise valueerror ( 'default value must be set' ) else : return obj"
268,apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L53-L58,"def get_conn(self):
        """"""
        Returns a Google MLEngine service object.
        """"""
        authed_http = self._authorize()
        return build('ml', 'v1', http=authed_http, cache_discovery=False)","['def', 'get_conn', '(', 'self', ')', ':', 'authed_http', '=', 'self', '.', '_authorize', '(', ')', 'return', 'build', '(', ""'ml'"", ',', ""'v1'"", ',', 'http', '=', 'authed_http', ',', 'cache_discovery', '=', 'False', ')']",Returns a Google MLEngine service object.,"['Returns', 'a', 'Google', 'MLEngine', 'service', 'object', '.']",python,test,"['returns', 'a', 'google', 'mlengine', 'service', 'object', '.']",returns a google mlengine service object .,"['def', 'get_conn', '(', 'self', ')', ':', 'authed_http', '=', 'self', '.', '_authorize', '(', ')', 'return', 'build', '(', ""'ml'"", ',', ""'v1'"", ',', 'http', '=', 'authed_http', ',', 'cache_discovery', '=', 'false', ')']","def get_conn ( self ) : authed_http = self . _authorize ( ) return build ( 'ml' , 'v1' , http = authed_http , cache_discovery = false )"
269,apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L60-L121,"def create_job(self, project_id, job, use_existing_job_fn=None):
        """"""
        Launches a MLEngine job and wait for it to reach a terminal state.

        :param project_id: The Google Cloud project id within which MLEngine
            job will be launched.
        :type project_id: str

        :param job: MLEngine Job object that should be provided to the MLEngine
            API, such as: ::

                {
                  'jobId': 'my_job_id',
                  'trainingInput': {
                    'scaleTier': 'STANDARD_1',
                    ...
                  }
                }

        :type job: dict

        :param use_existing_job_fn: In case that a MLEngine job with the same
            job_id already exist, this method (if provided) will decide whether
            we should use this existing job, continue waiting for it to finish
            and returning the job object. It should accepts a MLEngine job
            object, and returns a boolean value indicating whether it is OK to
            reuse the existing job. If 'use_existing_job_fn' is not provided,
            we by default reuse the existing MLEngine job.
        :type use_existing_job_fn: function

        :return: The MLEngine job object if the job successfully reach a
            terminal state (which might be FAILED or CANCELLED state).
        :rtype: dict
        """"""
        request = self._mlengine.projects().jobs().create(
            parent='projects/{}'.format(project_id),
            body=job)
        job_id = job['jobId']

        try:
            request.execute()
        except HttpError as e:
            # 409 means there is an existing job with the same job ID.
            if e.resp.status == 409:
                if use_existing_job_fn is not None:
                    existing_job = self._get_job(project_id, job_id)
                    if not use_existing_job_fn(existing_job):
                        self.log.error(
                            'Job with job_id %s already exist, but it does '
                            'not match our expectation: %s',
                            job_id, existing_job
                        )
                        raise
                self.log.info(
                    'Job with job_id %s already exist. Will waiting for it to finish',
                    job_id
                )
            else:
                self.log.error('Failed to create MLEngine job: {}'.format(e))
                raise

        return self._wait_for_job_done(project_id, job_id)","['def', 'create_job', '(', 'self', ',', 'project_id', ',', 'job', ',', 'use_existing_job_fn', '=', 'None', ')', ':', 'request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'jobs', '(', ')', '.', 'create', '(', 'parent', '=', ""'projects/{}'"", '.', 'format', '(', 'project_id', ')', ',', 'body', '=', 'job', ')', 'job_id', '=', 'job', '[', ""'jobId'"", ']', 'try', ':', 'request', '.', 'execute', '(', ')', 'except', 'HttpError', 'as', 'e', ':', '# 409 means there is an existing job with the same job ID.', 'if', 'e', '.', 'resp', '.', 'status', '==', '409', ':', 'if', 'use_existing_job_fn', 'is', 'not', 'None', ':', 'existing_job', '=', 'self', '.', '_get_job', '(', 'project_id', ',', 'job_id', ')', 'if', 'not', 'use_existing_job_fn', '(', 'existing_job', ')', ':', 'self', '.', 'log', '.', 'error', '(', ""'Job with job_id %s already exist, but it does '"", ""'not match our expectation: %s'"", ',', 'job_id', ',', 'existing_job', ')', 'raise', 'self', '.', 'log', '.', 'info', '(', ""'Job with job_id %s already exist. Will waiting for it to finish'"", ',', 'job_id', ')', 'else', ':', 'self', '.', 'log', '.', 'error', '(', ""'Failed to create MLEngine job: {}'"", '.', 'format', '(', 'e', ')', ')', 'raise', 'return', 'self', '.', '_wait_for_job_done', '(', 'project_id', ',', 'job_id', ')']","Launches a MLEngine job and wait for it to reach a terminal state.

        :param project_id: The Google Cloud project id within which MLEngine
            job will be launched.
        :type project_id: str

        :param job: MLEngine Job object that should be provided to the MLEngine
            API, such as: ::

                {
                  'jobId': 'my_job_id',
                  'trainingInput': {
                    'scaleTier': 'STANDARD_1',
                    ...
                  }
                }

        :type job: dict

        :param use_existing_job_fn: In case that a MLEngine job with the same
            job_id already exist, this method (if provided) will decide whether
            we should use this existing job, continue waiting for it to finish
            and returning the job object. It should accepts a MLEngine job
            object, and returns a boolean value indicating whether it is OK to
            reuse the existing job. If 'use_existing_job_fn' is not provided,
            we by default reuse the existing MLEngine job.
        :type use_existing_job_fn: function

        :return: The MLEngine job object if the job successfully reach a
            terminal state (which might be FAILED or CANCELLED state).
        :rtype: dict","['Launches', 'a', 'MLEngine', 'job', 'and', 'wait', 'for', 'it', 'to', 'reach', 'a', 'terminal', 'state', '.']",python,test,"['launches', 'a', 'mlengine', 'job', 'and', 'wait', 'for', 'it', 'to', 'reach', 'a', 'terminal', 'state', '.']",launches a mlengine job and wait for it to reach a terminal state .,"['def', 'create_job', '(', 'self', ',', 'project_id', ',', 'job', ',', 'use_existing_job_fn', '=', 'none', ')', ':', 'request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'jobs', '(', ')', '.', 'create', '(', 'parent', '=', ""'projects/{}'"", '.', 'format', '(', 'project_id', ')', ',', 'body', '=', 'job', ')', 'job_id', '=', 'job', '[', ""'jobid'"", ']', 'try', ':', 'request', '.', 'execute', '(', ')', 'except', 'httperror', 'as', 'e', ':', '# 409 means there is an existing job with the same job id.', 'if', 'e', '.', 'resp', '.', 'status', '==', '409', ':', 'if', 'use_existing_job_fn', 'is', 'not', 'none', ':', 'existing_job', '=', 'self', '.', '_get_job', '(', 'project_id', ',', 'job_id', ')', 'if', 'not', 'use_existing_job_fn', '(', 'existing_job', ')', ':', 'self', '.', 'log', '.', 'error', '(', ""'job with job_id %s already exist, but it does '"", ""'not match our expectation: %s'"", ',', 'job_id', ',', 'existing_job', ')', 'raise', 'self', '.', 'log', '.', 'info', '(', ""'job with job_id %s already exist. will waiting for it to finish'"", ',', 'job_id', ')', 'else', ':', 'self', '.', 'log', '.', 'error', '(', ""'failed to create mlengine job: {}'"", '.', 'format', '(', 'e', ')', ')', 'raise', 'return', 'self', '.', '_wait_for_job_done', '(', 'project_id', ',', 'job_id', ')']","def create_job ( self , project_id , job , use_existing_job_fn = none ) : request = self . _mlengine . projects ( ) . jobs ( ) . create ( parent = 'projects/{}' . format ( project_id ) , body = job ) job_id = job [ 'jobid' ] try : request . execute ( ) except httperror as e : # 409 means there is an existing job with the same job id. if e . resp . status == 409 : if use_existing_job_fn is not none : existing_job = self . _get_job ( project_id , job_id ) if not use_existing_job_fn ( existing_job ) : self . log . error ( 'job with job_id %s already exist, but it does ' 'not match our expectation: %s' , job_id , existing_job ) raise self . log . info ( 'job with job_id %s already exist. will waiting for it to finish' , job_id ) else : self . log . error ( 'failed to create mlengine job: {}' . format ( e ) ) raise return self . _wait_for_job_done ( project_id , job_id )"
270,apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L123-L144,"def _get_job(self, project_id, job_id):
        """"""
        Gets a MLEngine job based on the job name.

        :return: MLEngine job object if succeed.
        :rtype: dict

        Raises:
            googleapiclient.errors.HttpError: if HTTP error is returned from server
        """"""
        job_name = 'projects/{}/jobs/{}'.format(project_id, job_id)
        request = self._mlengine.projects().jobs().get(name=job_name)
        while True:
            try:
                return request.execute()
            except HttpError as e:
                if e.resp.status == 429:
                    # polling after 30 seconds when quota failure occurs
                    time.sleep(30)
                else:
                    self.log.error('Failed to get MLEngine job: {}'.format(e))
                    raise","['def', '_get_job', '(', 'self', ',', 'project_id', ',', 'job_id', ')', ':', 'job_name', '=', ""'projects/{}/jobs/{}'"", '.', 'format', '(', 'project_id', ',', 'job_id', ')', 'request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'jobs', '(', ')', '.', 'get', '(', 'name', '=', 'job_name', ')', 'while', 'True', ':', 'try', ':', 'return', 'request', '.', 'execute', '(', ')', 'except', 'HttpError', 'as', 'e', ':', 'if', 'e', '.', 'resp', '.', 'status', '==', '429', ':', '# polling after 30 seconds when quota failure occurs', 'time', '.', 'sleep', '(', '30', ')', 'else', ':', 'self', '.', 'log', '.', 'error', '(', ""'Failed to get MLEngine job: {}'"", '.', 'format', '(', 'e', ')', ')', 'raise']","Gets a MLEngine job based on the job name.

        :return: MLEngine job object if succeed.
        :rtype: dict

        Raises:
            googleapiclient.errors.HttpError: if HTTP error is returned from server","['Gets', 'a', 'MLEngine', 'job', 'based', 'on', 'the', 'job', 'name', '.']",python,test,"['gets', 'a', 'mlengine', 'job', 'based', 'on', 'the', 'job', 'name', '.']",gets a mlengine job based on the job name .,"['def', '_get_job', '(', 'self', ',', 'project_id', ',', 'job_id', ')', ':', 'job_name', '=', ""'projects/{}/jobs/{}'"", '.', 'format', '(', 'project_id', ',', 'job_id', ')', 'request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'jobs', '(', ')', '.', 'get', '(', 'name', '=', 'job_name', ')', 'while', 'true', ':', 'try', ':', 'return', 'request', '.', 'execute', '(', ')', 'except', 'httperror', 'as', 'e', ':', 'if', 'e', '.', 'resp', '.', 'status', '==', '429', ':', '# polling after 30 seconds when quota failure occurs', 'time', '.', 'sleep', '(', '30', ')', 'else', ':', 'self', '.', 'log', '.', 'error', '(', ""'failed to get mlengine job: {}'"", '.', 'format', '(', 'e', ')', ')', 'raise']","def _get_job ( self , project_id , job_id ) : job_name = 'projects/{}/jobs/{}' . format ( project_id , job_id ) request = self . _mlengine . projects ( ) . jobs ( ) . get ( name = job_name ) while true : try : return request . execute ( ) except httperror as e : if e . resp . status == 429 : # polling after 30 seconds when quota failure occurs time . sleep ( 30 ) else : self . log . error ( 'failed to get mlengine job: {}' . format ( e ) ) raise"
271,apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L146-L163,"def _wait_for_job_done(self, project_id, job_id, interval=30):
        """"""
        Waits for the Job to reach a terminal state.

        This method will periodically check the job state until the job reach
        a terminal state.

        Raises:
            googleapiclient.errors.HttpError: if HTTP error is returned when getting
            the job
        """"""
        if interval <= 0:
            raise ValueError(""Interval must be > 0"")
        while True:
            job = self._get_job(project_id, job_id)
            if job['state'] in ['SUCCEEDED', 'FAILED', 'CANCELLED']:
                return job
            time.sleep(interval)","['def', '_wait_for_job_done', '(', 'self', ',', 'project_id', ',', 'job_id', ',', 'interval', '=', '30', ')', ':', 'if', 'interval', '<=', '0', ':', 'raise', 'ValueError', '(', '""Interval must be > 0""', ')', 'while', 'True', ':', 'job', '=', 'self', '.', '_get_job', '(', 'project_id', ',', 'job_id', ')', 'if', 'job', '[', ""'state'"", ']', 'in', '[', ""'SUCCEEDED'"", ',', ""'FAILED'"", ',', ""'CANCELLED'"", ']', ':', 'return', 'job', 'time', '.', 'sleep', '(', 'interval', ')']","Waits for the Job to reach a terminal state.

        This method will periodically check the job state until the job reach
        a terminal state.

        Raises:
            googleapiclient.errors.HttpError: if HTTP error is returned when getting
            the job","['Waits', 'for', 'the', 'Job', 'to', 'reach', 'a', 'terminal', 'state', '.']",python,test,"['waits', 'for', 'the', 'job', 'to', 'reach', 'a', 'terminal', 'state', '.']",waits for the job to reach a terminal state .,"['def', '_wait_for_job_done', '(', 'self', ',', 'project_id', ',', 'job_id', ',', 'interval', '=', '30', ')', ':', 'if', 'interval', '<=', '0', ':', 'raise', 'valueerror', '(', '""interval must be > 0""', ')', 'while', 'true', ':', 'job', '=', 'self', '.', '_get_job', '(', 'project_id', ',', 'job_id', ')', 'if', 'job', '[', ""'state'"", ']', 'in', '[', ""'succeeded'"", ',', ""'failed'"", ',', ""'cancelled'"", ']', ':', 'return', 'job', 'time', '.', 'sleep', '(', 'interval', ')']","def _wait_for_job_done ( self , project_id , job_id , interval = 30 ) : if interval <= 0 : raise valueerror ( ""interval must be > 0"" ) while true : job = self . _get_job ( project_id , job_id ) if job [ 'state' ] in [ 'succeeded' , 'failed' , 'cancelled' ] : return job time . sleep ( interval )"
272,apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L165-L183,"def create_version(self, project_id, model_name, version_spec):
        """"""
        Creates the Version on Google Cloud ML Engine.

        Returns the operation if the version was created successfully and
        raises an error otherwise.
        """"""
        parent_name = 'projects/{}/models/{}'.format(project_id, model_name)
        create_request = self._mlengine.projects().models().versions().create(
            parent=parent_name, body=version_spec)
        response = create_request.execute()
        get_request = self._mlengine.projects().operations().get(
            name=response['name'])

        return _poll_with_exponential_delay(
            request=get_request,
            max_n=9,
            is_done_func=lambda resp: resp.get('done', False),
            is_error_func=lambda resp: resp.get('error', None) is not None)","['def', 'create_version', '(', 'self', ',', 'project_id', ',', 'model_name', ',', 'version_spec', ')', ':', 'parent_name', '=', ""'projects/{}/models/{}'"", '.', 'format', '(', 'project_id', ',', 'model_name', ')', 'create_request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'models', '(', ')', '.', 'versions', '(', ')', '.', 'create', '(', 'parent', '=', 'parent_name', ',', 'body', '=', 'version_spec', ')', 'response', '=', 'create_request', '.', 'execute', '(', ')', 'get_request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'operations', '(', ')', '.', 'get', '(', 'name', '=', 'response', '[', ""'name'"", ']', ')', 'return', '_poll_with_exponential_delay', '(', 'request', '=', 'get_request', ',', 'max_n', '=', '9', ',', 'is_done_func', '=', 'lambda', 'resp', ':', 'resp', '.', 'get', '(', ""'done'"", ',', 'False', ')', ',', 'is_error_func', '=', 'lambda', 'resp', ':', 'resp', '.', 'get', '(', ""'error'"", ',', 'None', ')', 'is', 'not', 'None', ')']","Creates the Version on Google Cloud ML Engine.

        Returns the operation if the version was created successfully and
        raises an error otherwise.","['Creates', 'the', 'Version', 'on', 'Google', 'Cloud', 'ML', 'Engine', '.']",python,test,"['creates', 'the', 'version', 'on', 'google', 'cloud', 'ml', 'engine', '.']",creates the version on google cloud ml engine .,"['def', 'create_version', '(', 'self', ',', 'project_id', ',', 'model_name', ',', 'version_spec', ')', ':', 'parent_name', '=', ""'projects/{}/models/{}'"", '.', 'format', '(', 'project_id', ',', 'model_name', ')', 'create_request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'models', '(', ')', '.', 'versions', '(', ')', '.', 'create', '(', 'parent', '=', 'parent_name', ',', 'body', '=', 'version_spec', ')', 'response', '=', 'create_request', '.', 'execute', '(', ')', 'get_request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'operations', '(', ')', '.', 'get', '(', 'name', '=', 'response', '[', ""'name'"", ']', ')', 'return', '_poll_with_exponential_delay', '(', 'request', '=', 'get_request', ',', 'max_n', '=', '9', ',', 'is_done_func', '=', 'lambda', 'resp', ':', 'resp', '.', 'get', '(', ""'done'"", ',', 'false', ')', ',', 'is_error_func', '=', 'lambda', 'resp', ':', 'resp', '.', 'get', '(', ""'error'"", ',', 'none', ')', 'is', 'not', 'none', ')']","def create_version ( self , project_id , model_name , version_spec ) : parent_name = 'projects/{}/models/{}' . format ( project_id , model_name ) create_request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . create ( parent = parent_name , body = version_spec ) response = create_request . execute ( ) get_request = self . _mlengine . projects ( ) . operations ( ) . get ( name = response [ 'name' ] ) return _poll_with_exponential_delay ( request = get_request , max_n = 9 , is_done_func = lambda resp : resp . get ( 'done' , false ) , is_error_func = lambda resp : resp . get ( 'error' , none ) is not none )"
273,apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L185-L200,"def set_default_version(self, project_id, model_name, version_name):
        """"""
        Sets a version to be the default. Blocks until finished.
        """"""
        full_version_name = 'projects/{}/models/{}/versions/{}'.format(
            project_id, model_name, version_name)
        request = self._mlengine.projects().models().versions().setDefault(
            name=full_version_name, body={})

        try:
            response = request.execute()
            self.log.info('Successfully set version: %s to default', response)
            return response
        except HttpError as e:
            self.log.error('Something went wrong: %s', e)
            raise","['def', 'set_default_version', '(', 'self', ',', 'project_id', ',', 'model_name', ',', 'version_name', ')', ':', 'full_version_name', '=', ""'projects/{}/models/{}/versions/{}'"", '.', 'format', '(', 'project_id', ',', 'model_name', ',', 'version_name', ')', 'request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'models', '(', ')', '.', 'versions', '(', ')', '.', 'setDefault', '(', 'name', '=', 'full_version_name', ',', 'body', '=', '{', '}', ')', 'try', ':', 'response', '=', 'request', '.', 'execute', '(', ')', 'self', '.', 'log', '.', 'info', '(', ""'Successfully set version: %s to default'"", ',', 'response', ')', 'return', 'response', 'except', 'HttpError', 'as', 'e', ':', 'self', '.', 'log', '.', 'error', '(', ""'Something went wrong: %s'"", ',', 'e', ')', 'raise']",Sets a version to be the default. Blocks until finished.,"['Sets', 'a', 'version', 'to', 'be', 'the', 'default', '.', 'Blocks', 'until', 'finished', '.']",python,test,"['sets', 'a', 'version', 'to', 'be', 'the', 'default', '.', 'blocks', 'until', 'finished', '.']",sets a version to be the default . blocks until finished .,"['def', 'set_default_version', '(', 'self', ',', 'project_id', ',', 'model_name', ',', 'version_name', ')', ':', 'full_version_name', '=', ""'projects/{}/models/{}/versions/{}'"", '.', 'format', '(', 'project_id', ',', 'model_name', ',', 'version_name', ')', 'request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'models', '(', ')', '.', 'versions', '(', ')', '.', 'setdefault', '(', 'name', '=', 'full_version_name', ',', 'body', '=', '{', '}', ')', 'try', ':', 'response', '=', 'request', '.', 'execute', '(', ')', 'self', '.', 'log', '.', 'info', '(', ""'successfully set version: %s to default'"", ',', 'response', ')', 'return', 'response', 'except', 'httperror', 'as', 'e', ':', 'self', '.', 'log', '.', 'error', '(', ""'something went wrong: %s'"", ',', 'e', ')', 'raise']","def set_default_version ( self , project_id , model_name , version_name ) : full_version_name = 'projects/{}/models/{}/versions/{}' . format ( project_id , model_name , version_name ) request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . setdefault ( name = full_version_name , body = { } ) try : response = request . execute ( ) self . log . info ( 'successfully set version: %s to default' , response ) return response except httperror as e : self . log . error ( 'something went wrong: %s' , e ) raise"
274,apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L202-L224,"def list_versions(self, project_id, model_name):
        """"""
        Lists all available versions of a model. Blocks until finished.
        """"""
        result = []
        full_parent_name = 'projects/{}/models/{}'.format(
            project_id, model_name)
        request = self._mlengine.projects().models().versions().list(
            parent=full_parent_name, pageSize=100)

        response = request.execute()
        next_page_token = response.get('nextPageToken', None)
        result.extend(response.get('versions', []))
        while next_page_token is not None:
            next_request = self._mlengine.projects().models().versions().list(
                parent=full_parent_name,
                pageToken=next_page_token,
                pageSize=100)
            response = next_request.execute()
            next_page_token = response.get('nextPageToken', None)
            result.extend(response.get('versions', []))
            time.sleep(5)
        return result","['def', 'list_versions', '(', 'self', ',', 'project_id', ',', 'model_name', ')', ':', 'result', '=', '[', ']', 'full_parent_name', '=', ""'projects/{}/models/{}'"", '.', 'format', '(', 'project_id', ',', 'model_name', ')', 'request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'models', '(', ')', '.', 'versions', '(', ')', '.', 'list', '(', 'parent', '=', 'full_parent_name', ',', 'pageSize', '=', '100', ')', 'response', '=', 'request', '.', 'execute', '(', ')', 'next_page_token', '=', 'response', '.', 'get', '(', ""'nextPageToken'"", ',', 'None', ')', 'result', '.', 'extend', '(', 'response', '.', 'get', '(', ""'versions'"", ',', '[', ']', ')', ')', 'while', 'next_page_token', 'is', 'not', 'None', ':', 'next_request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'models', '(', ')', '.', 'versions', '(', ')', '.', 'list', '(', 'parent', '=', 'full_parent_name', ',', 'pageToken', '=', 'next_page_token', ',', 'pageSize', '=', '100', ')', 'response', '=', 'next_request', '.', 'execute', '(', ')', 'next_page_token', '=', 'response', '.', 'get', '(', ""'nextPageToken'"", ',', 'None', ')', 'result', '.', 'extend', '(', 'response', '.', 'get', '(', ""'versions'"", ',', '[', ']', ')', ')', 'time', '.', 'sleep', '(', '5', ')', 'return', 'result']",Lists all available versions of a model. Blocks until finished.,"['Lists', 'all', 'available', 'versions', 'of', 'a', 'model', '.', 'Blocks', 'until', 'finished', '.']",python,test,"['lists', 'all', 'available', 'versions', 'of', 'a', 'model', '.', 'blocks', 'until', 'finished', '.']",lists all available versions of a model . blocks until finished .,"['def', 'list_versions', '(', 'self', ',', 'project_id', ',', 'model_name', ')', ':', 'result', '=', '[', ']', 'full_parent_name', '=', ""'projects/{}/models/{}'"", '.', 'format', '(', 'project_id', ',', 'model_name', ')', 'request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'models', '(', ')', '.', 'versions', '(', ')', '.', 'list', '(', 'parent', '=', 'full_parent_name', ',', 'pagesize', '=', '100', ')', 'response', '=', 'request', '.', 'execute', '(', ')', 'next_page_token', '=', 'response', '.', 'get', '(', ""'nextpagetoken'"", ',', 'none', ')', 'result', '.', 'extend', '(', 'response', '.', 'get', '(', ""'versions'"", ',', '[', ']', ')', ')', 'while', 'next_page_token', 'is', 'not', 'none', ':', 'next_request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'models', '(', ')', '.', 'versions', '(', ')', '.', 'list', '(', 'parent', '=', 'full_parent_name', ',', 'pagetoken', '=', 'next_page_token', ',', 'pagesize', '=', '100', ')', 'response', '=', 'next_request', '.', 'execute', '(', ')', 'next_page_token', '=', 'response', '.', 'get', '(', ""'nextpagetoken'"", ',', 'none', ')', 'result', '.', 'extend', '(', 'response', '.', 'get', '(', ""'versions'"", ',', '[', ']', ')', ')', 'time', '.', 'sleep', '(', '5', ')', 'return', 'result']","def list_versions ( self , project_id , model_name ) : result = [ ] full_parent_name = 'projects/{}/models/{}' . format ( project_id , model_name ) request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . list ( parent = full_parent_name , pagesize = 100 ) response = request . execute ( ) next_page_token = response . get ( 'nextpagetoken' , none ) result . extend ( response . get ( 'versions' , [ ] ) ) while next_page_token is not none : next_request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . list ( parent = full_parent_name , pagetoken = next_page_token , pagesize = 100 ) response = next_request . execute ( ) next_page_token = response . get ( 'nextpagetoken' , none ) result . extend ( response . get ( 'versions' , [ ] ) ) time . sleep ( 5 ) return result"
275,apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L226-L242,"def delete_version(self, project_id, model_name, version_name):
        """"""
        Deletes the given version of a model. Blocks until finished.
        """"""
        full_name = 'projects/{}/models/{}/versions/{}'.format(
            project_id, model_name, version_name)
        delete_request = self._mlengine.projects().models().versions().delete(
            name=full_name)
        response = delete_request.execute()
        get_request = self._mlengine.projects().operations().get(
            name=response['name'])

        return _poll_with_exponential_delay(
            request=get_request,
            max_n=9,
            is_done_func=lambda resp: resp.get('done', False),
            is_error_func=lambda resp: resp.get('error', None) is not None)","['def', 'delete_version', '(', 'self', ',', 'project_id', ',', 'model_name', ',', 'version_name', ')', ':', 'full_name', '=', ""'projects/{}/models/{}/versions/{}'"", '.', 'format', '(', 'project_id', ',', 'model_name', ',', 'version_name', ')', 'delete_request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'models', '(', ')', '.', 'versions', '(', ')', '.', 'delete', '(', 'name', '=', 'full_name', ')', 'response', '=', 'delete_request', '.', 'execute', '(', ')', 'get_request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'operations', '(', ')', '.', 'get', '(', 'name', '=', 'response', '[', ""'name'"", ']', ')', 'return', '_poll_with_exponential_delay', '(', 'request', '=', 'get_request', ',', 'max_n', '=', '9', ',', 'is_done_func', '=', 'lambda', 'resp', ':', 'resp', '.', 'get', '(', ""'done'"", ',', 'False', ')', ',', 'is_error_func', '=', 'lambda', 'resp', ':', 'resp', '.', 'get', '(', ""'error'"", ',', 'None', ')', 'is', 'not', 'None', ')']",Deletes the given version of a model. Blocks until finished.,"['Deletes', 'the', 'given', 'version', 'of', 'a', 'model', '.', 'Blocks', 'until', 'finished', '.']",python,test,"['deletes', 'the', 'given', 'version', 'of', 'a', 'model', '.', 'blocks', 'until', 'finished', '.']",deletes the given version of a model . blocks until finished .,"['def', 'delete_version', '(', 'self', ',', 'project_id', ',', 'model_name', ',', 'version_name', ')', ':', 'full_name', '=', ""'projects/{}/models/{}/versions/{}'"", '.', 'format', '(', 'project_id', ',', 'model_name', ',', 'version_name', ')', 'delete_request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'models', '(', ')', '.', 'versions', '(', ')', '.', 'delete', '(', 'name', '=', 'full_name', ')', 'response', '=', 'delete_request', '.', 'execute', '(', ')', 'get_request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'operations', '(', ')', '.', 'get', '(', 'name', '=', 'response', '[', ""'name'"", ']', ')', 'return', '_poll_with_exponential_delay', '(', 'request', '=', 'get_request', ',', 'max_n', '=', '9', ',', 'is_done_func', '=', 'lambda', 'resp', ':', 'resp', '.', 'get', '(', ""'done'"", ',', 'false', ')', ',', 'is_error_func', '=', 'lambda', 'resp', ':', 'resp', '.', 'get', '(', ""'error'"", ',', 'none', ')', 'is', 'not', 'none', ')']","def delete_version ( self , project_id , model_name , version_name ) : full_name = 'projects/{}/models/{}/versions/{}' . format ( project_id , model_name , version_name ) delete_request = self . _mlengine . projects ( ) . models ( ) . versions ( ) . delete ( name = full_name ) response = delete_request . execute ( ) get_request = self . _mlengine . projects ( ) . operations ( ) . get ( name = response [ 'name' ] ) return _poll_with_exponential_delay ( request = get_request , max_n = 9 , is_done_func = lambda resp : resp . get ( 'done' , false ) , is_error_func = lambda resp : resp . get ( 'error' , none ) is not none )"
276,apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L244-L255,"def create_model(self, project_id, model):
        """"""
        Create a Model. Blocks until finished.
        """"""
        if not model['name']:
            raise ValueError(""Model name must be provided and ""
                             ""could not be an empty string"")
        project = 'projects/{}'.format(project_id)

        request = self._mlengine.projects().models().create(
            parent=project, body=model)
        return request.execute()","['def', 'create_model', '(', 'self', ',', 'project_id', ',', 'model', ')', ':', 'if', 'not', 'model', '[', ""'name'"", ']', ':', 'raise', 'ValueError', '(', '""Model name must be provided and ""', '""could not be an empty string""', ')', 'project', '=', ""'projects/{}'"", '.', 'format', '(', 'project_id', ')', 'request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'models', '(', ')', '.', 'create', '(', 'parent', '=', 'project', ',', 'body', '=', 'model', ')', 'return', 'request', '.', 'execute', '(', ')']",Create a Model. Blocks until finished.,"['Create', 'a', 'Model', '.', 'Blocks', 'until', 'finished', '.']",python,test,"['create', 'a', 'model', '.', 'blocks', 'until', 'finished', '.']",create a model . blocks until finished .,"['def', 'create_model', '(', 'self', ',', 'project_id', ',', 'model', ')', ':', 'if', 'not', 'model', '[', ""'name'"", ']', ':', 'raise', 'valueerror', '(', '""model name must be provided and ""', '""could not be an empty string""', ')', 'project', '=', ""'projects/{}'"", '.', 'format', '(', 'project_id', ')', 'request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'models', '(', ')', '.', 'create', '(', 'parent', '=', 'project', ',', 'body', '=', 'model', ')', 'return', 'request', '.', 'execute', '(', ')']","def create_model ( self , project_id , model ) : if not model [ 'name' ] : raise valueerror ( ""model name must be provided and "" ""could not be an empty string"" ) project = 'projects/{}' . format ( project_id ) request = self . _mlengine . projects ( ) . models ( ) . create ( parent = project , body = model ) return request . execute ( )"
277,apache/airflow,airflow/contrib/hooks/gcp_mlengine_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_mlengine_hook.py#L257-L273,"def get_model(self, project_id, model_name):
        """"""
        Gets a Model. Blocks until finished.
        """"""
        if not model_name:
            raise ValueError(""Model name must be provided and ""
                             ""it could not be an empty string"")
        full_model_name = 'projects/{}/models/{}'.format(
            project_id, model_name)
        request = self._mlengine.projects().models().get(name=full_model_name)
        try:
            return request.execute()
        except HttpError as e:
            if e.resp.status == 404:
                self.log.error('Model was not found: %s', e)
                return None
            raise","['def', 'get_model', '(', 'self', ',', 'project_id', ',', 'model_name', ')', ':', 'if', 'not', 'model_name', ':', 'raise', 'ValueError', '(', '""Model name must be provided and ""', '""it could not be an empty string""', ')', 'full_model_name', '=', ""'projects/{}/models/{}'"", '.', 'format', '(', 'project_id', ',', 'model_name', ')', 'request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'models', '(', ')', '.', 'get', '(', 'name', '=', 'full_model_name', ')', 'try', ':', 'return', 'request', '.', 'execute', '(', ')', 'except', 'HttpError', 'as', 'e', ':', 'if', 'e', '.', 'resp', '.', 'status', '==', '404', ':', 'self', '.', 'log', '.', 'error', '(', ""'Model was not found: %s'"", ',', 'e', ')', 'return', 'None', 'raise']",Gets a Model. Blocks until finished.,"['Gets', 'a', 'Model', '.', 'Blocks', 'until', 'finished', '.']",python,test,"['gets', 'a', 'model', '.', 'blocks', 'until', 'finished', '.']",gets a model . blocks until finished .,"['def', 'get_model', '(', 'self', ',', 'project_id', ',', 'model_name', ')', ':', 'if', 'not', 'model_name', ':', 'raise', 'valueerror', '(', '""model name must be provided and ""', '""it could not be an empty string""', ')', 'full_model_name', '=', ""'projects/{}/models/{}'"", '.', 'format', '(', 'project_id', ',', 'model_name', ')', 'request', '=', 'self', '.', '_mlengine', '.', 'projects', '(', ')', '.', 'models', '(', ')', '.', 'get', '(', 'name', '=', 'full_model_name', ')', 'try', ':', 'return', 'request', '.', 'execute', '(', ')', 'except', 'httperror', 'as', 'e', ':', 'if', 'e', '.', 'resp', '.', 'status', '==', '404', ':', 'self', '.', 'log', '.', 'error', '(', ""'model was not found: %s'"", ',', 'e', ')', 'return', 'none', 'raise']","def get_model ( self , project_id , model_name ) : if not model_name : raise valueerror ( ""model name must be provided and "" ""it could not be an empty string"" ) full_model_name = 'projects/{}/models/{}' . format ( project_id , model_name ) request = self . _mlengine . projects ( ) . models ( ) . get ( name = full_model_name ) try : return request . execute ( ) except httperror as e : if e . resp . status == 404 : self . log . error ( 'model was not found: %s' , e ) return none raise"
278,apache/airflow,airflow/executors/local_executor.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/local_executor.py#L73-L92,"def execute_work(self, key, command):
        """"""
        Executes command received and stores result state in queue.
        :param key: the key to identify the TI
        :type key: tuple(dag_id, task_id, execution_date)
        :param command: the command to execute
        :type command: str
        """"""
        if key is None:
            return
        self.log.info(""%s running %s"", self.__class__.__name__, command)
        try:
            subprocess.check_call(command, close_fds=True)
            state = State.SUCCESS
        except subprocess.CalledProcessError as e:
            state = State.FAILED
            self.log.error(""Failed to execute task %s."", str(e))
            # TODO: Why is this commented out?
            # raise e
        self.result_queue.put((key, state))","['def', 'execute_work', '(', 'self', ',', 'key', ',', 'command', ')', ':', 'if', 'key', 'is', 'None', ':', 'return', 'self', '.', 'log', '.', 'info', '(', '""%s running %s""', ',', 'self', '.', '__class__', '.', '__name__', ',', 'command', ')', 'try', ':', 'subprocess', '.', 'check_call', '(', 'command', ',', 'close_fds', '=', 'True', ')', 'state', '=', 'State', '.', 'SUCCESS', 'except', 'subprocess', '.', 'CalledProcessError', 'as', 'e', ':', 'state', '=', 'State', '.', 'FAILED', 'self', '.', 'log', '.', 'error', '(', '""Failed to execute task %s.""', ',', 'str', '(', 'e', ')', ')', '# TODO: Why is this commented out?', '# raise e', 'self', '.', 'result_queue', '.', 'put', '(', '(', 'key', ',', 'state', ')', ')']","Executes command received and stores result state in queue.
        :param key: the key to identify the TI
        :type key: tuple(dag_id, task_id, execution_date)
        :param command: the command to execute
        :type command: str","['Executes', 'command', 'received', 'and', 'stores', 'result', 'state', 'in', 'queue', '.', ':', 'param', 'key', ':', 'the', 'key', 'to', 'identify', 'the', 'TI', ':', 'type', 'key', ':', 'tuple', '(', 'dag_id', 'task_id', 'execution_date', ')', ':', 'param', 'command', ':', 'the', 'command', 'to', 'execute', ':', 'type', 'command', ':', 'str']",python,test,"['executes', 'command', 'received', 'and', 'stores', 'result', 'state', 'in', 'queue', '.', ':', 'param', 'key', ':', 'the', 'key', 'to', 'identify', 'the', 'ti', ':', 'type', 'key', ':', 'tuple', '(', 'dag_id', 'task_id', 'execution_date', ')', ':', 'param', 'command', ':', 'the', 'command', 'to', 'execute', ':', 'type', 'command', ':', 'str']",executes command received and stores result state in queue . : param key : the key to identify the ti : type key : tuple ( dag_id task_id execution_date ) : param command : the command to execute : type command : str,"['def', 'execute_work', '(', 'self', ',', 'key', ',', 'command', ')', ':', 'if', 'key', 'is', 'none', ':', 'return', 'self', '.', 'log', '.', 'info', '(', '""%s running %s""', ',', 'self', '.', '__class__', '.', '__name__', ',', 'command', ')', 'try', ':', 'subprocess', '.', 'check_call', '(', 'command', ',', 'close_fds', '=', 'true', ')', 'state', '=', 'state', '.', 'success', 'except', 'subprocess', '.', 'calledprocesserror', 'as', 'e', ':', 'state', '=', 'state', '.', 'failed', 'self', '.', 'log', '.', 'error', '(', '""failed to execute task %s.""', ',', 'str', '(', 'e', ')', ')', '# todo: why is this commented out?', '# raise e', 'self', '.', 'result_queue', '.', 'put', '(', '(', 'key', ',', 'state', ')', ')']","def execute_work ( self , key , command ) : if key is none : return self . log . info ( ""%s running %s"" , self . __class__ . __name__ , command ) try : subprocess . check_call ( command , close_fds = true ) state = state . success except subprocess . calledprocesserror as e : state = state . failed self . log . error ( ""failed to execute task %s."" , str ( e ) ) # todo: why is this commented out? # raise e self . result_queue . put ( ( key , state ) )"
279,apache/airflow,airflow/contrib/hooks/aws_dynamodb_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_dynamodb_hook.py#L50-L69,"def write_batch_data(self, items):
        """"""
        Write batch items to dynamodb table with provisioned throughout capacity.
        """"""

        dynamodb_conn = self.get_conn()

        try:
            table = dynamodb_conn.Table(self.table_name)

            with table.batch_writer(overwrite_by_pkeys=self.table_keys) as batch:
                for item in items:
                    batch.put_item(Item=item)
            return True
        except Exception as general_error:
            raise AirflowException(
                'Failed to insert items in dynamodb, error: {error}'.format(
                    error=str(general_error)
                )
            )","['def', 'write_batch_data', '(', 'self', ',', 'items', ')', ':', 'dynamodb_conn', '=', 'self', '.', 'get_conn', '(', ')', 'try', ':', 'table', '=', 'dynamodb_conn', '.', 'Table', '(', 'self', '.', 'table_name', ')', 'with', 'table', '.', 'batch_writer', '(', 'overwrite_by_pkeys', '=', 'self', '.', 'table_keys', ')', 'as', 'batch', ':', 'for', 'item', 'in', 'items', ':', 'batch', '.', 'put_item', '(', 'Item', '=', 'item', ')', 'return', 'True', 'except', 'Exception', 'as', 'general_error', ':', 'raise', 'AirflowException', '(', ""'Failed to insert items in dynamodb, error: {error}'"", '.', 'format', '(', 'error', '=', 'str', '(', 'general_error', ')', ')', ')']",Write batch items to dynamodb table with provisioned throughout capacity.,"['Write', 'batch', 'items', 'to', 'dynamodb', 'table', 'with', 'provisioned', 'throughout', 'capacity', '.']",python,test,"['write', 'batch', 'items', 'to', 'dynamodb', 'table', 'with', 'provisioned', 'throughout', 'capacity', '.']",write batch items to dynamodb table with provisioned throughout capacity .,"['def', 'write_batch_data', '(', 'self', ',', 'items', ')', ':', 'dynamodb_conn', '=', 'self', '.', 'get_conn', '(', ')', 'try', ':', 'table', '=', 'dynamodb_conn', '.', 'table', '(', 'self', '.', 'table_name', ')', 'with', 'table', '.', 'batch_writer', '(', 'overwrite_by_pkeys', '=', 'self', '.', 'table_keys', ')', 'as', 'batch', ':', 'for', 'item', 'in', 'items', ':', 'batch', '.', 'put_item', '(', 'item', '=', 'item', ')', 'return', 'true', 'except', 'exception', 'as', 'general_error', ':', 'raise', 'airflowexception', '(', ""'failed to insert items in dynamodb, error: {error}'"", '.', 'format', '(', 'error', '=', 'str', '(', 'general_error', ')', ')', ')']","def write_batch_data ( self , items ) : dynamodb_conn = self . get_conn ( ) try : table = dynamodb_conn . table ( self . table_name ) with table . batch_writer ( overwrite_by_pkeys = self . table_keys ) as batch : for item in items : batch . put_item ( item = item ) return true except exception as general_error : raise airflowexception ( 'failed to insert items in dynamodb, error: {error}' . format ( error = str ( general_error ) ) )"
280,apache/airflow,airflow/executors/__init__.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/__init__.py#L31-L36,"def _integrate_plugins():
    """"""Integrate plugins to the context.""""""
    from airflow.plugins_manager import executors_modules
    for executors_module in executors_modules:
        sys.modules[executors_module.__name__] = executors_module
        globals()[executors_module._name] = executors_module","['def', '_integrate_plugins', '(', ')', ':', 'from', 'airflow', '.', 'plugins_manager', 'import', 'executors_modules', 'for', 'executors_module', 'in', 'executors_modules', ':', 'sys', '.', 'modules', '[', 'executors_module', '.', '__name__', ']', '=', 'executors_module', 'globals', '(', ')', '[', 'executors_module', '.', '_name', ']', '=', 'executors_module']",Integrate plugins to the context.,"['Integrate', 'plugins', 'to', 'the', 'context', '.']",python,test,"['integrate', 'plugins', 'to', 'the', 'context', '.']",integrate plugins to the context .,"['def', '_integrate_plugins', '(', ')', ':', 'from', 'airflow', '.', 'plugins_manager', 'import', 'executors_modules', 'for', 'executors_module', 'in', 'executors_modules', ':', 'sys', '.', 'modules', '[', 'executors_module', '.', '__name__', ']', '=', 'executors_module', 'globals', '(', ')', '[', 'executors_module', '.', '_name', ']', '=', 'executors_module']",def _integrate_plugins ( ) : from airflow . plugins_manager import executors_modules for executors_module in executors_modules : sys . modules [ executors_module . __name__ ] = executors_module globals ( ) [ executors_module . _name ] = executors_module
281,apache/airflow,airflow/executors/__init__.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/__init__.py#L39-L53,"def get_default_executor():
    """"""Creates a new instance of the configured executor if none exists and returns it""""""
    global DEFAULT_EXECUTOR

    if DEFAULT_EXECUTOR is not None:
        return DEFAULT_EXECUTOR

    executor_name = configuration.conf.get('core', 'EXECUTOR')

    DEFAULT_EXECUTOR = _get_executor(executor_name)

    log = LoggingMixin().log
    log.info(""Using executor %s"", executor_name)

    return DEFAULT_EXECUTOR","['def', 'get_default_executor', '(', ')', ':', 'global', 'DEFAULT_EXECUTOR', 'if', 'DEFAULT_EXECUTOR', 'is', 'not', 'None', ':', 'return', 'DEFAULT_EXECUTOR', 'executor_name', '=', 'configuration', '.', 'conf', '.', 'get', '(', ""'core'"", ',', ""'EXECUTOR'"", ')', 'DEFAULT_EXECUTOR', '=', '_get_executor', '(', 'executor_name', ')', 'log', '=', 'LoggingMixin', '(', ')', '.', 'log', 'log', '.', 'info', '(', '""Using executor %s""', ',', 'executor_name', ')', 'return', 'DEFAULT_EXECUTOR']",Creates a new instance of the configured executor if none exists and returns it,"['Creates', 'a', 'new', 'instance', 'of', 'the', 'configured', 'executor', 'if', 'none', 'exists', 'and', 'returns', 'it']",python,test,"['creates', 'a', 'new', 'instance', 'of', 'the', 'configured', 'executor', 'if', 'none', 'exists', 'and', 'returns', 'it']",creates a new instance of the configured executor if none exists and returns it,"['def', 'get_default_executor', '(', ')', ':', 'global', 'default_executor', 'if', 'default_executor', 'is', 'not', 'none', ':', 'return', 'default_executor', 'executor_name', '=', 'configuration', '.', 'conf', '.', 'get', '(', ""'core'"", ',', ""'executor'"", ')', 'default_executor', '=', '_get_executor', '(', 'executor_name', ')', 'log', '=', 'loggingmixin', '(', ')', '.', 'log', 'log', '.', 'info', '(', '""using executor %s""', ',', 'executor_name', ')', 'return', 'default_executor']","def get_default_executor ( ) : global default_executor if default_executor is not none : return default_executor executor_name = configuration . conf . get ( 'core' , 'executor' ) default_executor = _get_executor ( executor_name ) log = loggingmixin ( ) . log log . info ( ""using executor %s"" , executor_name ) return default_executor"
282,apache/airflow,airflow/executors/__init__.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/executors/__init__.py#L64-L95,"def _get_executor(executor_name):
    """"""
    Creates a new instance of the named executor.
    In case the executor name is not know in airflow,
    look for it in the plugins
    """"""
    if executor_name == Executors.LocalExecutor:
        return LocalExecutor()
    elif executor_name == Executors.SequentialExecutor:
        return SequentialExecutor()
    elif executor_name == Executors.CeleryExecutor:
        from airflow.executors.celery_executor import CeleryExecutor
        return CeleryExecutor()
    elif executor_name == Executors.DaskExecutor:
        from airflow.executors.dask_executor import DaskExecutor
        return DaskExecutor()
    elif executor_name == Executors.KubernetesExecutor:
        from airflow.contrib.executors.kubernetes_executor import KubernetesExecutor
        return KubernetesExecutor()
    else:
        # Loading plugins
        _integrate_plugins()
        executor_path = executor_name.split('.')
        if len(executor_path) != 2:
            raise AirflowException(
                ""Executor {0} not supported: ""
                ""please specify in format plugin_module.executor"".format(executor_name))

        if executor_path[0] in globals():
            return globals()[executor_path[0]].__dict__[executor_path[1]]()
        else:
            raise AirflowException(""Executor {0} not supported."".format(executor_name))","['def', '_get_executor', '(', 'executor_name', ')', ':', 'if', 'executor_name', '==', 'Executors', '.', 'LocalExecutor', ':', 'return', 'LocalExecutor', '(', ')', 'elif', 'executor_name', '==', 'Executors', '.', 'SequentialExecutor', ':', 'return', 'SequentialExecutor', '(', ')', 'elif', 'executor_name', '==', 'Executors', '.', 'CeleryExecutor', ':', 'from', 'airflow', '.', 'executors', '.', 'celery_executor', 'import', 'CeleryExecutor', 'return', 'CeleryExecutor', '(', ')', 'elif', 'executor_name', '==', 'Executors', '.', 'DaskExecutor', ':', 'from', 'airflow', '.', 'executors', '.', 'dask_executor', 'import', 'DaskExecutor', 'return', 'DaskExecutor', '(', ')', 'elif', 'executor_name', '==', 'Executors', '.', 'KubernetesExecutor', ':', 'from', 'airflow', '.', 'contrib', '.', 'executors', '.', 'kubernetes_executor', 'import', 'KubernetesExecutor', 'return', 'KubernetesExecutor', '(', ')', 'else', ':', '# Loading plugins', '_integrate_plugins', '(', ')', 'executor_path', '=', 'executor_name', '.', 'split', '(', ""'.'"", ')', 'if', 'len', '(', 'executor_path', ')', '!=', '2', ':', 'raise', 'AirflowException', '(', '""Executor {0} not supported: ""', '""please specify in format plugin_module.executor""', '.', 'format', '(', 'executor_name', ')', ')', 'if', 'executor_path', '[', '0', ']', 'in', 'globals', '(', ')', ':', 'return', 'globals', '(', ')', '[', 'executor_path', '[', '0', ']', ']', '.', '__dict__', '[', 'executor_path', '[', '1', ']', ']', '(', ')', 'else', ':', 'raise', 'AirflowException', '(', '""Executor {0} not supported.""', '.', 'format', '(', 'executor_name', ')', ')']","Creates a new instance of the named executor.
    In case the executor name is not know in airflow,
    look for it in the plugins","['Creates', 'a', 'new', 'instance', 'of', 'the', 'named', 'executor', '.', 'In', 'case', 'the', 'executor', 'name', 'is', 'not', 'know', 'in', 'airflow', 'look', 'for', 'it', 'in', 'the', 'plugins']",python,test,"['creates', 'a', 'new', 'instance', 'of', 'the', 'named', 'executor', '.', 'in', 'case', 'the', 'executor', 'name', 'is', 'not', 'know', 'in', 'airflow', 'look', 'for', 'it', 'in', 'the', 'plugins']",creates a new instance of the named executor . in case the executor name is not know in airflow look for it in the plugins,"['def', '_get_executor', '(', 'executor_name', ')', ':', 'if', 'executor_name', '==', 'executors', '.', 'localexecutor', ':', 'return', 'localexecutor', '(', ')', 'elif', 'executor_name', '==', 'executors', '.', 'sequentialexecutor', ':', 'return', 'sequentialexecutor', '(', ')', 'elif', 'executor_name', '==', 'executors', '.', 'celeryexecutor', ':', 'from', 'airflow', '.', 'executors', '.', 'celery_executor', 'import', 'celeryexecutor', 'return', 'celeryexecutor', '(', ')', 'elif', 'executor_name', '==', 'executors', '.', 'daskexecutor', ':', 'from', 'airflow', '.', 'executors', '.', 'dask_executor', 'import', 'daskexecutor', 'return', 'daskexecutor', '(', ')', 'elif', 'executor_name', '==', 'executors', '.', 'kubernetesexecutor', ':', 'from', 'airflow', '.', 'contrib', '.', 'executors', '.', 'kubernetes_executor', 'import', 'kubernetesexecutor', 'return', 'kubernetesexecutor', '(', ')', 'else', ':', '# loading plugins', '_integrate_plugins', '(', ')', 'executor_path', '=', 'executor_name', '.', 'split', '(', ""'.'"", ')', 'if', 'len', '(', 'executor_path', ')', '!=', '2', ':', 'raise', 'airflowexception', '(', '""executor {0} not supported: ""', '""please specify in format plugin_module.executor""', '.', 'format', '(', 'executor_name', ')', ')', 'if', 'executor_path', '[', '0', ']', 'in', 'globals', '(', ')', ':', 'return', 'globals', '(', ')', '[', 'executor_path', '[', '0', ']', ']', '.', '__dict__', '[', 'executor_path', '[', '1', ']', ']', '(', ')', 'else', ':', 'raise', 'airflowexception', '(', '""executor {0} not supported.""', '.', 'format', '(', 'executor_name', ')', ')']","def _get_executor ( executor_name ) : if executor_name == executors . localexecutor : return localexecutor ( ) elif executor_name == executors . sequentialexecutor : return sequentialexecutor ( ) elif executor_name == executors . celeryexecutor : from airflow . executors . celery_executor import celeryexecutor return celeryexecutor ( ) elif executor_name == executors . daskexecutor : from airflow . executors . dask_executor import daskexecutor return daskexecutor ( ) elif executor_name == executors . kubernetesexecutor : from airflow . contrib . executors . kubernetes_executor import kubernetesexecutor return kubernetesexecutor ( ) else : # loading plugins _integrate_plugins ( ) executor_path = executor_name . split ( '.' ) if len ( executor_path ) != 2 : raise airflowexception ( ""executor {0} not supported: "" ""please specify in format plugin_module.executor"" . format ( executor_name ) ) if executor_path [ 0 ] in globals ( ) : return globals ( ) [ executor_path [ 0 ] ] . __dict__ [ executor_path [ 1 ] ] ( ) else : raise airflowexception ( ""executor {0} not supported."" . format ( executor_name ) )"
283,apache/airflow,airflow/contrib/hooks/segment_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/segment_hook.py#L83-L90,"def on_error(self, error, items):
        """"""
        Handles error callbacks when using Segment with segment_debug_mode set to True
        """"""
        self.log.error('Encountered Segment error: {segment_error} with '
                       'items: {with_items}'.format(segment_error=error,
                                                    with_items=items))
        raise AirflowException('Segment error: {}'.format(error))","['def', 'on_error', '(', 'self', ',', 'error', ',', 'items', ')', ':', 'self', '.', 'log', '.', 'error', '(', ""'Encountered Segment error: {segment_error} with '"", ""'items: {with_items}'"", '.', 'format', '(', 'segment_error', '=', 'error', ',', 'with_items', '=', 'items', ')', ')', 'raise', 'AirflowException', '(', ""'Segment error: {}'"", '.', 'format', '(', 'error', ')', ')']",Handles error callbacks when using Segment with segment_debug_mode set to True,"['Handles', 'error', 'callbacks', 'when', 'using', 'Segment', 'with', 'segment_debug_mode', 'set', 'to', 'True']",python,test,"['handles', 'error', 'callbacks', 'when', 'using', 'segment', 'with', 'segment_debug_mode', 'set', 'to', 'true']",handles error callbacks when using segment with segment_debug_mode set to true,"['def', 'on_error', '(', 'self', ',', 'error', ',', 'items', ')', ':', 'self', '.', 'log', '.', 'error', '(', ""'encountered segment error: {segment_error} with '"", ""'items: {with_items}'"", '.', 'format', '(', 'segment_error', '=', 'error', ',', 'with_items', '=', 'items', ')', ')', 'raise', 'airflowexception', '(', ""'segment error: {}'"", '.', 'format', '(', 'error', ')', ')']","def on_error ( self , error , items ) : self . log . error ( 'encountered segment error: {segment_error} with ' 'items: {with_items}' . format ( segment_error = error , with_items = items ) ) raise airflowexception ( 'segment error: {}' . format ( error ) )"
284,apache/airflow,airflow/contrib/kubernetes/pod_launcher.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/kubernetes/pod_launcher.py#L73-L92,"def run_pod(self, pod, startup_timeout=120, get_logs=True):
        # type: (Pod, int, bool) -> Tuple[State, Optional[str]]
        """"""
        Launches the pod synchronously and waits for completion.
        Args:
            pod (Pod):
            startup_timeout (int): Timeout for startup of the pod (if pod is pending for
             too long, considers task a failure
        """"""
        resp = self.run_pod_async(pod)
        curr_time = dt.now()
        if resp.status.start_time is None:
            while self.pod_not_started(pod):
                delta = dt.now() - curr_time
                if delta.seconds >= startup_timeout:
                    raise AirflowException(""Pod took too long to start"")
                time.sleep(1)
            self.log.debug('Pod not yet started')

        return self._monitor_pod(pod, get_logs)","['def', 'run_pod', '(', 'self', ',', 'pod', ',', 'startup_timeout', '=', '120', ',', 'get_logs', '=', 'True', ')', ':', '# type: (Pod, int, bool) -> Tuple[State, Optional[str]]', 'resp', '=', 'self', '.', 'run_pod_async', '(', 'pod', ')', 'curr_time', '=', 'dt', '.', 'now', '(', ')', 'if', 'resp', '.', 'status', '.', 'start_time', 'is', 'None', ':', 'while', 'self', '.', 'pod_not_started', '(', 'pod', ')', ':', 'delta', '=', 'dt', '.', 'now', '(', ')', '-', 'curr_time', 'if', 'delta', '.', 'seconds', '>=', 'startup_timeout', ':', 'raise', 'AirflowException', '(', '""Pod took too long to start""', ')', 'time', '.', 'sleep', '(', '1', ')', 'self', '.', 'log', '.', 'debug', '(', ""'Pod not yet started'"", ')', 'return', 'self', '.', '_monitor_pod', '(', 'pod', ',', 'get_logs', ')']","Launches the pod synchronously and waits for completion.
        Args:
            pod (Pod):
            startup_timeout (int): Timeout for startup of the pod (if pod is pending for
             too long, considers task a failure","['Launches', 'the', 'pod', 'synchronously', 'and', 'waits', 'for', 'completion', '.', 'Args', ':', 'pod', '(', 'Pod', ')', ':', 'startup_timeout', '(', 'int', ')', ':', 'Timeout', 'for', 'startup', 'of', 'the', 'pod', '(', 'if', 'pod', 'is', 'pending', 'for', 'too', 'long', 'considers', 'task', 'a', 'failure']",python,test,"['launches', 'the', 'pod', 'synchronously', 'and', 'waits', 'for', 'completion', '.', 'args', ':', 'pod', '(', 'pod', ')', ':', 'startup_timeout', '(', 'int', ')', ':', 'timeout', 'for', 'startup', 'of', 'the', 'pod', '(', 'if', 'pod', 'is', 'pending', 'for', 'too', 'long', 'considers', 'task', 'a', 'failure']",launches the pod synchronously and waits for completion . args : pod ( pod ) : startup_timeout ( int ) : timeout for startup of the pod ( if pod is pending for too long considers task a failure,"['def', 'run_pod', '(', 'self', ',', 'pod', ',', 'startup_timeout', '=', '120', ',', 'get_logs', '=', 'true', ')', ':', '# type: (pod, int, bool) -> tuple[state, optional[str]]', 'resp', '=', 'self', '.', 'run_pod_async', '(', 'pod', ')', 'curr_time', '=', 'dt', '.', 'now', '(', ')', 'if', 'resp', '.', 'status', '.', 'start_time', 'is', 'none', ':', 'while', 'self', '.', 'pod_not_started', '(', 'pod', ')', ':', 'delta', '=', 'dt', '.', 'now', '(', ')', '-', 'curr_time', 'if', 'delta', '.', 'seconds', '>=', 'startup_timeout', ':', 'raise', 'airflowexception', '(', '""pod took too long to start""', ')', 'time', '.', 'sleep', '(', '1', ')', 'self', '.', 'log', '.', 'debug', '(', ""'pod not yet started'"", ')', 'return', 'self', '.', '_monitor_pod', '(', 'pod', ',', 'get_logs', ')']","def run_pod ( self , pod , startup_timeout = 120 , get_logs = true ) : # type: (pod, int, bool) -> tuple[state, optional[str]] resp = self . run_pod_async ( pod ) curr_time = dt . now ( ) if resp . status . start_time is none : while self . pod_not_started ( pod ) : delta = dt . now ( ) - curr_time if delta . seconds >= startup_timeout : raise airflowexception ( ""pod took too long to start"" ) time . sleep ( 1 ) self . log . debug ( 'pod not yet started' ) return self . _monitor_pod ( pod , get_logs )"
285,apache/airflow,airflow/hooks/mssql_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/mssql_hook.py#L38-L49,"def get_conn(self):
        """"""
        Returns a mssql connection object
        """"""
        conn = self.get_connection(self.mssql_conn_id)
        conn = pymssql.connect(
            server=conn.host,
            user=conn.login,
            password=conn.password,
            database=self.schema or conn.schema,
            port=conn.port)
        return conn","['def', 'get_conn', '(', 'self', ')', ':', 'conn', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'mssql_conn_id', ')', 'conn', '=', 'pymssql', '.', 'connect', '(', 'server', '=', 'conn', '.', 'host', ',', 'user', '=', 'conn', '.', 'login', ',', 'password', '=', 'conn', '.', 'password', ',', 'database', '=', 'self', '.', 'schema', 'or', 'conn', '.', 'schema', ',', 'port', '=', 'conn', '.', 'port', ')', 'return', 'conn']",Returns a mssql connection object,"['Returns', 'a', 'mssql', 'connection', 'object']",python,test,"['returns', 'a', 'mssql', 'connection', 'object']",returns a mssql connection object,"['def', 'get_conn', '(', 'self', ')', ':', 'conn', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'mssql_conn_id', ')', 'conn', '=', 'pymssql', '.', 'connect', '(', 'server', '=', 'conn', '.', 'host', ',', 'user', '=', 'conn', '.', 'login', ',', 'password', '=', 'conn', '.', 'password', ',', 'database', '=', 'self', '.', 'schema', 'or', 'conn', '.', 'schema', ',', 'port', '=', 'conn', '.', 'port', ')', 'return', 'conn']","def get_conn ( self ) : conn = self . get_connection ( self . mssql_conn_id ) conn = pymssql . connect ( server = conn . host , user = conn . login , password = conn . password , database = self . schema or conn . schema , port = conn . port ) return conn"
286,apache/airflow,airflow/contrib/operators/spark_submit_operator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/spark_submit_operator.py#L147-L176,"def execute(self, context):
        """"""
        Call the SparkSubmitHook to run the provided spark job
        """"""
        self._hook = SparkSubmitHook(
            conf=self._conf,
            conn_id=self._conn_id,
            files=self._files,
            py_files=self._py_files,
            archives=self._archives,
            driver_class_path=self._driver_class_path,
            jars=self._jars,
            java_class=self._java_class,
            packages=self._packages,
            exclude_packages=self._exclude_packages,
            repositories=self._repositories,
            total_executor_cores=self._total_executor_cores,
            executor_cores=self._executor_cores,
            executor_memory=self._executor_memory,
            driver_memory=self._driver_memory,
            keytab=self._keytab,
            principal=self._principal,
            name=self._name,
            num_executors=self._num_executors,
            application_args=self._application_args,
            env_vars=self._env_vars,
            verbose=self._verbose,
            spark_binary=self._spark_binary
        )
        self._hook.submit(self._application)","['def', 'execute', '(', 'self', ',', 'context', ')', ':', 'self', '.', '_hook', '=', 'SparkSubmitHook', '(', 'conf', '=', 'self', '.', '_conf', ',', 'conn_id', '=', 'self', '.', '_conn_id', ',', 'files', '=', 'self', '.', '_files', ',', 'py_files', '=', 'self', '.', '_py_files', ',', 'archives', '=', 'self', '.', '_archives', ',', 'driver_class_path', '=', 'self', '.', '_driver_class_path', ',', 'jars', '=', 'self', '.', '_jars', ',', 'java_class', '=', 'self', '.', '_java_class', ',', 'packages', '=', 'self', '.', '_packages', ',', 'exclude_packages', '=', 'self', '.', '_exclude_packages', ',', 'repositories', '=', 'self', '.', '_repositories', ',', 'total_executor_cores', '=', 'self', '.', '_total_executor_cores', ',', 'executor_cores', '=', 'self', '.', '_executor_cores', ',', 'executor_memory', '=', 'self', '.', '_executor_memory', ',', 'driver_memory', '=', 'self', '.', '_driver_memory', ',', 'keytab', '=', 'self', '.', '_keytab', ',', 'principal', '=', 'self', '.', '_principal', ',', 'name', '=', 'self', '.', '_name', ',', 'num_executors', '=', 'self', '.', '_num_executors', ',', 'application_args', '=', 'self', '.', '_application_args', ',', 'env_vars', '=', 'self', '.', '_env_vars', ',', 'verbose', '=', 'self', '.', '_verbose', ',', 'spark_binary', '=', 'self', '.', '_spark_binary', ')', 'self', '.', '_hook', '.', 'submit', '(', 'self', '.', '_application', ')']",Call the SparkSubmitHook to run the provided spark job,"['Call', 'the', 'SparkSubmitHook', 'to', 'run', 'the', 'provided', 'spark', 'job']",python,test,"['call', 'the', 'sparksubmithook', 'to', 'run', 'the', 'provided', 'spark', 'job']",call the sparksubmithook to run the provided spark job,"['def', 'execute', '(', 'self', ',', 'context', ')', ':', 'self', '.', '_hook', '=', 'sparksubmithook', '(', 'conf', '=', 'self', '.', '_conf', ',', 'conn_id', '=', 'self', '.', '_conn_id', ',', 'files', '=', 'self', '.', '_files', ',', 'py_files', '=', 'self', '.', '_py_files', ',', 'archives', '=', 'self', '.', '_archives', ',', 'driver_class_path', '=', 'self', '.', '_driver_class_path', ',', 'jars', '=', 'self', '.', '_jars', ',', 'java_class', '=', 'self', '.', '_java_class', ',', 'packages', '=', 'self', '.', '_packages', ',', 'exclude_packages', '=', 'self', '.', '_exclude_packages', ',', 'repositories', '=', 'self', '.', '_repositories', ',', 'total_executor_cores', '=', 'self', '.', '_total_executor_cores', ',', 'executor_cores', '=', 'self', '.', '_executor_cores', ',', 'executor_memory', '=', 'self', '.', '_executor_memory', ',', 'driver_memory', '=', 'self', '.', '_driver_memory', ',', 'keytab', '=', 'self', '.', '_keytab', ',', 'principal', '=', 'self', '.', '_principal', ',', 'name', '=', 'self', '.', '_name', ',', 'num_executors', '=', 'self', '.', '_num_executors', ',', 'application_args', '=', 'self', '.', '_application_args', ',', 'env_vars', '=', 'self', '.', '_env_vars', ',', 'verbose', '=', 'self', '.', '_verbose', ',', 'spark_binary', '=', 'self', '.', '_spark_binary', ')', 'self', '.', '_hook', '.', 'submit', '(', 'self', '.', '_application', ')']","def execute ( self , context ) : self . _hook = sparksubmithook ( conf = self . _conf , conn_id = self . _conn_id , files = self . _files , py_files = self . _py_files , archives = self . _archives , driver_class_path = self . _driver_class_path , jars = self . _jars , java_class = self . _java_class , packages = self . _packages , exclude_packages = self . _exclude_packages , repositories = self . _repositories , total_executor_cores = self . _total_executor_cores , executor_cores = self . _executor_cores , executor_memory = self . _executor_memory , driver_memory = self . _driver_memory , keytab = self . _keytab , principal = self . _principal , name = self . _name , num_executors = self . _num_executors , application_args = self . _application_args , env_vars = self . _env_vars , verbose = self . _verbose , spark_binary = self . _spark_binary ) self . _hook . submit ( self . _application )"
287,apache/airflow,airflow/www/api/experimental/endpoints.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/api/experimental/endpoints.py#L47-L92,"def trigger_dag(dag_id):
    """"""
    Trigger a new dag run for a Dag with an execution date of now unless
    specified in the data.
    """"""
    data = request.get_json(force=True)

    run_id = None
    if 'run_id' in data:
        run_id = data['run_id']

    conf = None
    if 'conf' in data:
        conf = data['conf']

    execution_date = None
    if 'execution_date' in data and data['execution_date'] is not None:
        execution_date = data['execution_date']

        # Convert string datetime into actual datetime
        try:
            execution_date = timezone.parse(execution_date)
        except ValueError:
            error_message = (
                'Given execution date, {}, could not be identified '
                'as a date. Example date format: 2015-11-16T14:34:15+00:00'
                .format(execution_date))
            _log.info(error_message)
            response = jsonify({'error': error_message})
            response.status_code = 400

            return response

    try:
        dr = trigger.trigger_dag(dag_id, run_id, conf, execution_date)
    except AirflowException as err:
        _log.error(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response

    if getattr(g, 'user', None):
        _log.info(""User %s created %s"", g.user, dr)

    response = jsonify(message=""Created {}"".format(dr))
    return response","['def', 'trigger_dag', '(', 'dag_id', ')', ':', 'data', '=', 'request', '.', 'get_json', '(', 'force', '=', 'True', ')', 'run_id', '=', 'None', 'if', ""'run_id'"", 'in', 'data', ':', 'run_id', '=', 'data', '[', ""'run_id'"", ']', 'conf', '=', 'None', 'if', ""'conf'"", 'in', 'data', ':', 'conf', '=', 'data', '[', ""'conf'"", ']', 'execution_date', '=', 'None', 'if', ""'execution_date'"", 'in', 'data', 'and', 'data', '[', ""'execution_date'"", ']', 'is', 'not', 'None', ':', 'execution_date', '=', 'data', '[', ""'execution_date'"", ']', '# Convert string datetime into actual datetime', 'try', ':', 'execution_date', '=', 'timezone', '.', 'parse', '(', 'execution_date', ')', 'except', 'ValueError', ':', 'error_message', '=', '(', ""'Given execution date, {}, could not be identified '"", ""'as a date. Example date format: 2015-11-16T14:34:15+00:00'"", '.', 'format', '(', 'execution_date', ')', ')', '_log', '.', 'info', '(', 'error_message', ')', 'response', '=', 'jsonify', '(', '{', ""'error'"", ':', 'error_message', '}', ')', 'response', '.', 'status_code', '=', '400', 'return', 'response', 'try', ':', 'dr', '=', 'trigger', '.', 'trigger_dag', '(', 'dag_id', ',', 'run_id', ',', 'conf', ',', 'execution_date', ')', 'except', 'AirflowException', 'as', 'err', ':', '_log', '.', 'error', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', 'err', '.', 'status_code', 'return', 'response', 'if', 'getattr', '(', 'g', ',', ""'user'"", ',', 'None', ')', ':', '_log', '.', 'info', '(', '""User %s created %s""', ',', 'g', '.', 'user', ',', 'dr', ')', 'response', '=', 'jsonify', '(', 'message', '=', '""Created {}""', '.', 'format', '(', 'dr', ')', ')', 'return', 'response']","Trigger a new dag run for a Dag with an execution date of now unless
    specified in the data.","['Trigger', 'a', 'new', 'dag', 'run', 'for', 'a', 'Dag', 'with', 'an', 'execution', 'date', 'of', 'now', 'unless', 'specified', 'in', 'the', 'data', '.']",python,test,"['trigger', 'a', 'new', 'dag', 'run', 'for', 'a', 'dag', 'with', 'an', 'execution', 'date', 'of', 'now', 'unless', 'specified', 'in', 'the', 'data', '.']",trigger a new dag run for a dag with an execution date of now unless specified in the data .,"['def', 'trigger_dag', '(', 'dag_id', ')', ':', 'data', '=', 'request', '.', 'get_json', '(', 'force', '=', 'true', ')', 'run_id', '=', 'none', 'if', ""'run_id'"", 'in', 'data', ':', 'run_id', '=', 'data', '[', ""'run_id'"", ']', 'conf', '=', 'none', 'if', ""'conf'"", 'in', 'data', ':', 'conf', '=', 'data', '[', ""'conf'"", ']', 'execution_date', '=', 'none', 'if', ""'execution_date'"", 'in', 'data', 'and', 'data', '[', ""'execution_date'"", ']', 'is', 'not', 'none', ':', 'execution_date', '=', 'data', '[', ""'execution_date'"", ']', '# convert string datetime into actual datetime', 'try', ':', 'execution_date', '=', 'timezone', '.', 'parse', '(', 'execution_date', ')', 'except', 'valueerror', ':', 'error_message', '=', '(', ""'given execution date, {}, could not be identified '"", ""'as a date. example date format: 2015-11-16t14:34:15+00:00'"", '.', 'format', '(', 'execution_date', ')', ')', '_log', '.', 'info', '(', 'error_message', ')', 'response', '=', 'jsonify', '(', '{', ""'error'"", ':', 'error_message', '}', ')', 'response', '.', 'status_code', '=', '400', 'return', 'response', 'try', ':', 'dr', '=', 'trigger', '.', 'trigger_dag', '(', 'dag_id', ',', 'run_id', ',', 'conf', ',', 'execution_date', ')', 'except', 'airflowexception', 'as', 'err', ':', '_log', '.', 'error', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', 'err', '.', 'status_code', 'return', 'response', 'if', 'getattr', '(', 'g', ',', ""'user'"", ',', 'none', ')', ':', '_log', '.', 'info', '(', '""user %s created %s""', ',', 'g', '.', 'user', ',', 'dr', ')', 'response', '=', 'jsonify', '(', 'message', '=', '""created {}""', '.', 'format', '(', 'dr', ')', ')', 'return', 'response']","def trigger_dag ( dag_id ) : data = request . get_json ( force = true ) run_id = none if 'run_id' in data : run_id = data [ 'run_id' ] conf = none if 'conf' in data : conf = data [ 'conf' ] execution_date = none if 'execution_date' in data and data [ 'execution_date' ] is not none : execution_date = data [ 'execution_date' ] # convert string datetime into actual datetime try : execution_date = timezone . parse ( execution_date ) except valueerror : error_message = ( 'given execution date, {}, could not be identified ' 'as a date. example date format: 2015-11-16t14:34:15+00:00' . format ( execution_date ) ) _log . info ( error_message ) response = jsonify ( { 'error' : error_message } ) response . status_code = 400 return response try : dr = trigger . trigger_dag ( dag_id , run_id , conf , execution_date ) except airflowexception as err : _log . error ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = err . status_code return response if getattr ( g , 'user' , none ) : _log . info ( ""user %s created %s"" , g . user , dr ) response = jsonify ( message = ""created {}"" . format ( dr ) ) return response"
288,apache/airflow,airflow/www/api/experimental/endpoints.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/api/experimental/endpoints.py#L98-L109,"def delete_dag(dag_id):
    """"""
    Delete all DB records related to the specified Dag.
    """"""
    try:
        count = delete.delete_dag(dag_id)
    except AirflowException as err:
        _log.error(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response
    return jsonify(message=""Removed {} record(s)"".format(count), count=count)","['def', 'delete_dag', '(', 'dag_id', ')', ':', 'try', ':', 'count', '=', 'delete', '.', 'delete_dag', '(', 'dag_id', ')', 'except', 'AirflowException', 'as', 'err', ':', '_log', '.', 'error', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', 'err', '.', 'status_code', 'return', 'response', 'return', 'jsonify', '(', 'message', '=', '""Removed {} record(s)""', '.', 'format', '(', 'count', ')', ',', 'count', '=', 'count', ')']",Delete all DB records related to the specified Dag.,"['Delete', 'all', 'DB', 'records', 'related', 'to', 'the', 'specified', 'Dag', '.']",python,test,"['delete', 'all', 'db', 'records', 'related', 'to', 'the', 'specified', 'dag', '.']",delete all db records related to the specified dag .,"['def', 'delete_dag', '(', 'dag_id', ')', ':', 'try', ':', 'count', '=', 'delete', '.', 'delete_dag', '(', 'dag_id', ')', 'except', 'airflowexception', 'as', 'err', ':', '_log', '.', 'error', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', 'err', '.', 'status_code', 'return', 'response', 'return', 'jsonify', '(', 'message', '=', '""removed {} record(s)""', '.', 'format', '(', 'count', ')', ',', 'count', '=', 'count', ')']","def delete_dag ( dag_id ) : try : count = delete . delete_dag ( dag_id ) except airflowexception as err : _log . error ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = err . status_code return response return jsonify ( message = ""removed {} record(s)"" . format ( count ) , count = count )"
289,apache/airflow,airflow/www/api/experimental/endpoints.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/api/experimental/endpoints.py#L114-L131,"def dag_runs(dag_id):
    """"""
    Returns a list of Dag Runs for a specific DAG ID.
    :query param state: a query string parameter '?state=queued|running|success...'
    :param dag_id: String identifier of a DAG
    :return: List of DAG runs of a DAG with requested state,
    or all runs if the state is not specified
    """"""
    try:
        state = request.args.get('state')
        dagruns = get_dag_runs(dag_id, state)
    except AirflowException as err:
        _log.info(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = 400
        return response

    return jsonify(dagruns)","['def', 'dag_runs', '(', 'dag_id', ')', ':', 'try', ':', 'state', '=', 'request', '.', 'args', '.', 'get', '(', ""'state'"", ')', 'dagruns', '=', 'get_dag_runs', '(', 'dag_id', ',', 'state', ')', 'except', 'AirflowException', 'as', 'err', ':', '_log', '.', 'info', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', '400', 'return', 'response', 'return', 'jsonify', '(', 'dagruns', ')']","Returns a list of Dag Runs for a specific DAG ID.
    :query param state: a query string parameter '?state=queued|running|success...'
    :param dag_id: String identifier of a DAG
    :return: List of DAG runs of a DAG with requested state,
    or all runs if the state is not specified","['Returns', 'a', 'list', 'of', 'Dag', 'Runs', 'for', 'a', 'specific', 'DAG', 'ID', '.', ':', 'query', 'param', 'state', ':', 'a', 'query', 'string', 'parameter', '?state', '=', 'queued|running|success', '...', ':', 'param', 'dag_id', ':', 'String', 'identifier', 'of', 'a', 'DAG', ':', 'return', ':', 'List', 'of', 'DAG', 'runs', 'of', 'a', 'DAG', 'with', 'requested', 'state', 'or', 'all', 'runs', 'if', 'the', 'state', 'is', 'not', 'specified']",python,test,"['returns', 'a', 'list', 'of', 'dag', 'runs', 'for', 'a', 'specific', 'dag', 'id', '.', ':', 'query', 'param', 'state', ':', 'a', 'query', 'string', 'parameter', '?state', '=', 'queued|running|success', '...', ':', 'param', 'dag_id', ':', 'string', 'identifier', 'of', 'a', 'dag', ':', 'return', ':', 'list', 'of', 'dag', 'runs', 'of', 'a', 'dag', 'with', 'requested', 'state', 'or', 'all', 'runs', 'if', 'the', 'state', 'is', 'not', 'specified']",returns a list of dag runs for a specific dag id . : query param state : a query string parameter ?state = queued|running|success ... : param dag_id : string identifier of a dag : return : list of dag runs of a dag with requested state or all runs if the state is not specified,"['def', 'dag_runs', '(', 'dag_id', ')', ':', 'try', ':', 'state', '=', 'request', '.', 'args', '.', 'get', '(', ""'state'"", ')', 'dagruns', '=', 'get_dag_runs', '(', 'dag_id', ',', 'state', ')', 'except', 'airflowexception', 'as', 'err', ':', '_log', '.', 'info', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', '400', 'return', 'response', 'return', 'jsonify', '(', 'dagruns', ')']","def dag_runs ( dag_id ) : try : state = request . args . get ( 'state' ) dagruns = get_dag_runs ( dag_id , state ) except airflowexception as err : _log . info ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = 400 return response return jsonify ( dagruns )"
290,apache/airflow,airflow/www/api/experimental/endpoints.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/api/experimental/endpoints.py#L142-L150,"def get_dag_code(dag_id):
    """"""Return python code of a given dag_id.""""""
    try:
        return get_code(dag_id)
    except AirflowException as err:
        _log.info(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response","['def', 'get_dag_code', '(', 'dag_id', ')', ':', 'try', ':', 'return', 'get_code', '(', 'dag_id', ')', 'except', 'AirflowException', 'as', 'err', ':', '_log', '.', 'info', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', 'err', '.', 'status_code', 'return', 'response']",Return python code of a given dag_id.,"['Return', 'python', 'code', 'of', 'a', 'given', 'dag_id', '.']",python,test,"['return', 'python', 'code', 'of', 'a', 'given', 'dag_id', '.']",return python code of a given dag_id .,"['def', 'get_dag_code', '(', 'dag_id', ')', ':', 'try', ':', 'return', 'get_code', '(', 'dag_id', ')', 'except', 'airflowexception', 'as', 'err', ':', '_log', '.', 'info', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', 'err', '.', 'status_code', 'return', 'response']","def get_dag_code ( dag_id ) : try : return get_code ( dag_id ) except airflowexception as err : _log . info ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = err . status_code return response"
291,apache/airflow,airflow/www/api/experimental/endpoints.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/api/experimental/endpoints.py#L155-L169,"def task_info(dag_id, task_id):
    """"""Returns a JSON with a task's public instance variables. """"""
    try:
        info = get_task(dag_id, task_id)
    except AirflowException as err:
        _log.info(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response

    # JSONify and return.
    fields = {k: str(v)
              for k, v in vars(info).items()
              if not k.startswith('_')}
    return jsonify(fields)","['def', 'task_info', '(', 'dag_id', ',', 'task_id', ')', ':', 'try', ':', 'info', '=', 'get_task', '(', 'dag_id', ',', 'task_id', ')', 'except', 'AirflowException', 'as', 'err', ':', '_log', '.', 'info', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', 'err', '.', 'status_code', 'return', 'response', '# JSONify and return.', 'fields', '=', '{', 'k', ':', 'str', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'vars', '(', 'info', ')', '.', 'items', '(', ')', 'if', 'not', 'k', '.', 'startswith', '(', ""'_'"", ')', '}', 'return', 'jsonify', '(', 'fields', ')']",Returns a JSON with a task's public instance variables.,"['Returns', 'a', 'JSON', 'with', 'a', 'task', 's', 'public', 'instance', 'variables', '.']",python,test,"['returns', 'a', 'json', 'with', 'a', 'task', 's', 'public', 'instance', 'variables', '.']",returns a json with a task s public instance variables .,"['def', 'task_info', '(', 'dag_id', ',', 'task_id', ')', ':', 'try', ':', 'info', '=', 'get_task', '(', 'dag_id', ',', 'task_id', ')', 'except', 'airflowexception', 'as', 'err', ':', '_log', '.', 'info', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', 'err', '.', 'status_code', 'return', 'response', '# jsonify and return.', 'fields', '=', '{', 'k', ':', 'str', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'vars', '(', 'info', ')', '.', 'items', '(', ')', 'if', 'not', 'k', '.', 'startswith', '(', ""'_'"", ')', '}', 'return', 'jsonify', '(', 'fields', ')']","def task_info ( dag_id , task_id ) : try : info = get_task ( dag_id , task_id ) except airflowexception as err : _log . info ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = err . status_code return response # jsonify and return. fields = { k : str ( v ) for k , v in vars ( info ) . items ( ) if not k . startswith ( '_' ) } return jsonify ( fields )"
292,apache/airflow,airflow/www/api/experimental/endpoints.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/api/experimental/endpoints.py#L175-L191,"def dag_paused(dag_id, paused):
    """"""(Un)pauses a dag""""""

    DagModel = models.DagModel
    with create_session() as session:
        orm_dag = (
            session.query(DagModel)
                   .filter(DagModel.dag_id == dag_id).first()
        )
        if paused == 'true':
            orm_dag.is_paused = True
        else:
            orm_dag.is_paused = False
        session.merge(orm_dag)
        session.commit()

    return jsonify({'response': 'ok'})","['def', 'dag_paused', '(', 'dag_id', ',', 'paused', ')', ':', 'DagModel', '=', 'models', '.', 'DagModel', 'with', 'create_session', '(', ')', 'as', 'session', ':', 'orm_dag', '=', '(', 'session', '.', 'query', '(', 'DagModel', ')', '.', 'filter', '(', 'DagModel', '.', 'dag_id', '==', 'dag_id', ')', '.', 'first', '(', ')', ')', 'if', 'paused', '==', ""'true'"", ':', 'orm_dag', '.', 'is_paused', '=', 'True', 'else', ':', 'orm_dag', '.', 'is_paused', '=', 'False', 'session', '.', 'merge', '(', 'orm_dag', ')', 'session', '.', 'commit', '(', ')', 'return', 'jsonify', '(', '{', ""'response'"", ':', ""'ok'"", '}', ')']",(Un)pauses a dag,"['(', 'Un', ')', 'pauses', 'a', 'dag']",python,test,"['(', 'un', ')', 'pauses', 'a', 'dag']",( un ) pauses a dag,"['def', 'dag_paused', '(', 'dag_id', ',', 'paused', ')', ':', 'dagmodel', '=', 'models', '.', 'dagmodel', 'with', 'create_session', '(', ')', 'as', 'session', ':', 'orm_dag', '=', '(', 'session', '.', 'query', '(', 'dagmodel', ')', '.', 'filter', '(', 'dagmodel', '.', 'dag_id', '==', 'dag_id', ')', '.', 'first', '(', ')', ')', 'if', 'paused', '==', ""'true'"", ':', 'orm_dag', '.', 'is_paused', '=', 'true', 'else', ':', 'orm_dag', '.', 'is_paused', '=', 'false', 'session', '.', 'merge', '(', 'orm_dag', ')', 'session', '.', 'commit', '(', ')', 'return', 'jsonify', '(', '{', ""'response'"", ':', ""'ok'"", '}', ')']","def dag_paused ( dag_id , paused ) : dagmodel = models . dagmodel with create_session ( ) as session : orm_dag = ( session . query ( dagmodel ) . filter ( dagmodel . dag_id == dag_id ) . first ( ) ) if paused == 'true' : orm_dag . is_paused = true else : orm_dag . is_paused = false session . merge ( orm_dag ) session . commit ( ) return jsonify ( { 'response' : 'ok' } )"
293,apache/airflow,airflow/www/api/experimental/endpoints.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/api/experimental/endpoints.py#L198-L232,"def task_instance_info(dag_id, execution_date, task_id):
    """"""
    Returns a JSON with a task instance's public instance variables.
    The format for the exec_date is expected to be
    ""YYYY-mm-DDTHH:MM:SS"", for example: ""2016-11-16T11:34:15"". This will
    of course need to have been encoded for URL in the request.
    """"""

    # Convert string datetime into actual datetime
    try:
        execution_date = timezone.parse(execution_date)
    except ValueError:
        error_message = (
            'Given execution date, {}, could not be identified '
            'as a date. Example date format: 2015-11-16T14:34:15+00:00'
            .format(execution_date))
        _log.info(error_message)
        response = jsonify({'error': error_message})
        response.status_code = 400

        return response

    try:
        info = get_task_instance(dag_id, task_id, execution_date)
    except AirflowException as err:
        _log.info(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response

    # JSONify and return.
    fields = {k: str(v)
              for k, v in vars(info).items()
              if not k.startswith('_')}
    return jsonify(fields)","['def', 'task_instance_info', '(', 'dag_id', ',', 'execution_date', ',', 'task_id', ')', ':', '# Convert string datetime into actual datetime', 'try', ':', 'execution_date', '=', 'timezone', '.', 'parse', '(', 'execution_date', ')', 'except', 'ValueError', ':', 'error_message', '=', '(', ""'Given execution date, {}, could not be identified '"", ""'as a date. Example date format: 2015-11-16T14:34:15+00:00'"", '.', 'format', '(', 'execution_date', ')', ')', '_log', '.', 'info', '(', 'error_message', ')', 'response', '=', 'jsonify', '(', '{', ""'error'"", ':', 'error_message', '}', ')', 'response', '.', 'status_code', '=', '400', 'return', 'response', 'try', ':', 'info', '=', 'get_task_instance', '(', 'dag_id', ',', 'task_id', ',', 'execution_date', ')', 'except', 'AirflowException', 'as', 'err', ':', '_log', '.', 'info', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', 'err', '.', 'status_code', 'return', 'response', '# JSONify and return.', 'fields', '=', '{', 'k', ':', 'str', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'vars', '(', 'info', ')', '.', 'items', '(', ')', 'if', 'not', 'k', '.', 'startswith', '(', ""'_'"", ')', '}', 'return', 'jsonify', '(', 'fields', ')']","Returns a JSON with a task instance's public instance variables.
    The format for the exec_date is expected to be
    ""YYYY-mm-DDTHH:MM:SS"", for example: ""2016-11-16T11:34:15"". This will
    of course need to have been encoded for URL in the request.","['Returns', 'a', 'JSON', 'with', 'a', 'task', 'instance', 's', 'public', 'instance', 'variables', '.', 'The', 'format', 'for', 'the', 'exec_date', 'is', 'expected', 'to', 'be', 'YYYY', '-', 'mm', '-', 'DDTHH', ':', 'MM', ':', 'SS', 'for', 'example', ':', '2016', '-', '11', '-', '16T11', ':', '34', ':', '15', '.', 'This', 'will', 'of', 'course', 'need', 'to', 'have', 'been', 'encoded', 'for', 'URL', 'in', 'the', 'request', '.']",python,test,"['returns', 'a', 'json', 'with', 'a', 'task', 'instance', 's', 'public', 'instance', 'variables', '.', 'the', 'format', 'for', 'the', 'exec_date', 'is', 'expected', 'to', 'be', 'yyyy', '-', 'mm', '-', 'ddthh', ':', 'mm', ':', 'ss', 'for', 'example', ':', '2016', '-', '11', '-', '16t11', ':', '34', ':', '15', '.', 'this', 'will', 'of', 'course', 'need', 'to', 'have', 'been', 'encoded', 'for', 'url', 'in', 'the', 'request', '.']",returns a json with a task instance s public instance variables . the format for the exec_date is expected to be yyyy - mm - ddthh : mm : ss for example : 2016 - 11 - 16t11 : 34 : 15 . this will of course need to have been encoded for url in the request .,"['def', 'task_instance_info', '(', 'dag_id', ',', 'execution_date', ',', 'task_id', ')', ':', '# convert string datetime into actual datetime', 'try', ':', 'execution_date', '=', 'timezone', '.', 'parse', '(', 'execution_date', ')', 'except', 'valueerror', ':', 'error_message', '=', '(', ""'given execution date, {}, could not be identified '"", ""'as a date. example date format: 2015-11-16t14:34:15+00:00'"", '.', 'format', '(', 'execution_date', ')', ')', '_log', '.', 'info', '(', 'error_message', ')', 'response', '=', 'jsonify', '(', '{', ""'error'"", ':', 'error_message', '}', ')', 'response', '.', 'status_code', '=', '400', 'return', 'response', 'try', ':', 'info', '=', 'get_task_instance', '(', 'dag_id', ',', 'task_id', ',', 'execution_date', ')', 'except', 'airflowexception', 'as', 'err', ':', '_log', '.', 'info', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', 'err', '.', 'status_code', 'return', 'response', '# jsonify and return.', 'fields', '=', '{', 'k', ':', 'str', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'vars', '(', 'info', ')', '.', 'items', '(', ')', 'if', 'not', 'k', '.', 'startswith', '(', ""'_'"", ')', '}', 'return', 'jsonify', '(', 'fields', ')']","def task_instance_info ( dag_id , execution_date , task_id ) : # convert string datetime into actual datetime try : execution_date = timezone . parse ( execution_date ) except valueerror : error_message = ( 'given execution date, {}, could not be identified ' 'as a date. example date format: 2015-11-16t14:34:15+00:00' . format ( execution_date ) ) _log . info ( error_message ) response = jsonify ( { 'error' : error_message } ) response . status_code = 400 return response try : info = get_task_instance ( dag_id , task_id , execution_date ) except airflowexception as err : _log . info ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = err . status_code return response # jsonify and return. fields = { k : str ( v ) for k , v in vars ( info ) . items ( ) if not k . startswith ( '_' ) } return jsonify ( fields )"
294,apache/airflow,airflow/www/api/experimental/endpoints.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/api/experimental/endpoints.py#L239-L269,"def dag_run_status(dag_id, execution_date):
    """"""
    Returns a JSON with a dag_run's public instance variables.
    The format for the exec_date is expected to be
    ""YYYY-mm-DDTHH:MM:SS"", for example: ""2016-11-16T11:34:15"". This will
    of course need to have been encoded for URL in the request.
    """"""

    # Convert string datetime into actual datetime
    try:
        execution_date = timezone.parse(execution_date)
    except ValueError:
        error_message = (
            'Given execution date, {}, could not be identified '
            'as a date. Example date format: 2015-11-16T14:34:15+00:00'.format(
                execution_date))
        _log.info(error_message)
        response = jsonify({'error': error_message})
        response.status_code = 400

        return response

    try:
        info = get_dag_run_state(dag_id, execution_date)
    except AirflowException as err:
        _log.info(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response

    return jsonify(info)","['def', 'dag_run_status', '(', 'dag_id', ',', 'execution_date', ')', ':', '# Convert string datetime into actual datetime', 'try', ':', 'execution_date', '=', 'timezone', '.', 'parse', '(', 'execution_date', ')', 'except', 'ValueError', ':', 'error_message', '=', '(', ""'Given execution date, {}, could not be identified '"", ""'as a date. Example date format: 2015-11-16T14:34:15+00:00'"", '.', 'format', '(', 'execution_date', ')', ')', '_log', '.', 'info', '(', 'error_message', ')', 'response', '=', 'jsonify', '(', '{', ""'error'"", ':', 'error_message', '}', ')', 'response', '.', 'status_code', '=', '400', 'return', 'response', 'try', ':', 'info', '=', 'get_dag_run_state', '(', 'dag_id', ',', 'execution_date', ')', 'except', 'AirflowException', 'as', 'err', ':', '_log', '.', 'info', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', 'err', '.', 'status_code', 'return', 'response', 'return', 'jsonify', '(', 'info', ')']","Returns a JSON with a dag_run's public instance variables.
    The format for the exec_date is expected to be
    ""YYYY-mm-DDTHH:MM:SS"", for example: ""2016-11-16T11:34:15"". This will
    of course need to have been encoded for URL in the request.","['Returns', 'a', 'JSON', 'with', 'a', 'dag_run', 's', 'public', 'instance', 'variables', '.', 'The', 'format', 'for', 'the', 'exec_date', 'is', 'expected', 'to', 'be', 'YYYY', '-', 'mm', '-', 'DDTHH', ':', 'MM', ':', 'SS', 'for', 'example', ':', '2016', '-', '11', '-', '16T11', ':', '34', ':', '15', '.', 'This', 'will', 'of', 'course', 'need', 'to', 'have', 'been', 'encoded', 'for', 'URL', 'in', 'the', 'request', '.']",python,test,"['returns', 'a', 'json', 'with', 'a', 'dag_run', 's', 'public', 'instance', 'variables', '.', 'the', 'format', 'for', 'the', 'exec_date', 'is', 'expected', 'to', 'be', 'yyyy', '-', 'mm', '-', 'ddthh', ':', 'mm', ':', 'ss', 'for', 'example', ':', '2016', '-', '11', '-', '16t11', ':', '34', ':', '15', '.', 'this', 'will', 'of', 'course', 'need', 'to', 'have', 'been', 'encoded', 'for', 'url', 'in', 'the', 'request', '.']",returns a json with a dag_run s public instance variables . the format for the exec_date is expected to be yyyy - mm - ddthh : mm : ss for example : 2016 - 11 - 16t11 : 34 : 15 . this will of course need to have been encoded for url in the request .,"['def', 'dag_run_status', '(', 'dag_id', ',', 'execution_date', ')', ':', '# convert string datetime into actual datetime', 'try', ':', 'execution_date', '=', 'timezone', '.', 'parse', '(', 'execution_date', ')', 'except', 'valueerror', ':', 'error_message', '=', '(', ""'given execution date, {}, could not be identified '"", ""'as a date. example date format: 2015-11-16t14:34:15+00:00'"", '.', 'format', '(', 'execution_date', ')', ')', '_log', '.', 'info', '(', 'error_message', ')', 'response', '=', 'jsonify', '(', '{', ""'error'"", ':', 'error_message', '}', ')', 'response', '.', 'status_code', '=', '400', 'return', 'response', 'try', ':', 'info', '=', 'get_dag_run_state', '(', 'dag_id', ',', 'execution_date', ')', 'except', 'airflowexception', 'as', 'err', ':', '_log', '.', 'info', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', 'err', '.', 'status_code', 'return', 'response', 'return', 'jsonify', '(', 'info', ')']","def dag_run_status ( dag_id , execution_date ) : # convert string datetime into actual datetime try : execution_date = timezone . parse ( execution_date ) except valueerror : error_message = ( 'given execution date, {}, could not be identified ' 'as a date. example date format: 2015-11-16t14:34:15+00:00' . format ( execution_date ) ) _log . info ( error_message ) response = jsonify ( { 'error' : error_message } ) response . status_code = 400 return response try : info = get_dag_run_state ( dag_id , execution_date ) except airflowexception as err : _log . info ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = err . status_code return response return jsonify ( info )"
295,apache/airflow,airflow/www/api/experimental/endpoints.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/api/experimental/endpoints.py#L309-L319,"def get_pools():
    """"""Get all pools.""""""
    try:
        pools = pool_api.get_pools()
    except AirflowException as err:
        _log.error(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response
    else:
        return jsonify([p.to_json() for p in pools])","['def', 'get_pools', '(', ')', ':', 'try', ':', 'pools', '=', 'pool_api', '.', 'get_pools', '(', ')', 'except', 'AirflowException', 'as', 'err', ':', '_log', '.', 'error', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', 'err', '.', 'status_code', 'return', 'response', 'else', ':', 'return', 'jsonify', '(', '[', 'p', '.', 'to_json', '(', ')', 'for', 'p', 'in', 'pools', ']', ')']",Get all pools.,"['Get', 'all', 'pools', '.']",python,test,"['get', 'all', 'pools', '.']",get all pools .,"['def', 'get_pools', '(', ')', ':', 'try', ':', 'pools', '=', 'pool_api', '.', 'get_pools', '(', ')', 'except', 'airflowexception', 'as', 'err', ':', '_log', '.', 'error', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', 'err', '.', 'status_code', 'return', 'response', 'else', ':', 'return', 'jsonify', '(', '[', 'p', '.', 'to_json', '(', ')', 'for', 'p', 'in', 'pools', ']', ')']","def get_pools ( ) : try : pools = pool_api . get_pools ( ) except airflowexception as err : _log . error ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = err . status_code return response else : return jsonify ( [ p . to_json ( ) for p in pools ] )"
296,apache/airflow,airflow/www/api/experimental/endpoints.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/api/experimental/endpoints.py#L325-L336,"def create_pool():
    """"""Create a pool.""""""
    params = request.get_json(force=True)
    try:
        pool = pool_api.create_pool(**params)
    except AirflowException as err:
        _log.error(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response
    else:
        return jsonify(pool.to_json())","['def', 'create_pool', '(', ')', ':', 'params', '=', 'request', '.', 'get_json', '(', 'force', '=', 'True', ')', 'try', ':', 'pool', '=', 'pool_api', '.', 'create_pool', '(', '*', '*', 'params', ')', 'except', 'AirflowException', 'as', 'err', ':', '_log', '.', 'error', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', 'err', '.', 'status_code', 'return', 'response', 'else', ':', 'return', 'jsonify', '(', 'pool', '.', 'to_json', '(', ')', ')']",Create a pool.,"['Create', 'a', 'pool', '.']",python,test,"['create', 'a', 'pool', '.']",create a pool .,"['def', 'create_pool', '(', ')', ':', 'params', '=', 'request', '.', 'get_json', '(', 'force', '=', 'true', ')', 'try', ':', 'pool', '=', 'pool_api', '.', 'create_pool', '(', '*', '*', 'params', ')', 'except', 'airflowexception', 'as', 'err', ':', '_log', '.', 'error', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', 'err', '.', 'status_code', 'return', 'response', 'else', ':', 'return', 'jsonify', '(', 'pool', '.', 'to_json', '(', ')', ')']","def create_pool ( ) : params = request . get_json ( force = true ) try : pool = pool_api . create_pool ( * * params ) except airflowexception as err : _log . error ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = err . status_code return response else : return jsonify ( pool . to_json ( ) )"
297,apache/airflow,airflow/www/api/experimental/endpoints.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/www/api/experimental/endpoints.py#L342-L352,"def delete_pool(name):
    """"""Delete pool.""""""
    try:
        pool = pool_api.delete_pool(name=name)
    except AirflowException as err:
        _log.error(err)
        response = jsonify(error=""{}"".format(err))
        response.status_code = err.status_code
        return response
    else:
        return jsonify(pool.to_json())","['def', 'delete_pool', '(', 'name', ')', ':', 'try', ':', 'pool', '=', 'pool_api', '.', 'delete_pool', '(', 'name', '=', 'name', ')', 'except', 'AirflowException', 'as', 'err', ':', '_log', '.', 'error', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', 'err', '.', 'status_code', 'return', 'response', 'else', ':', 'return', 'jsonify', '(', 'pool', '.', 'to_json', '(', ')', ')']",Delete pool.,"['Delete', 'pool', '.']",python,test,"['delete', 'pool', '.']",delete pool .,"['def', 'delete_pool', '(', 'name', ')', ':', 'try', ':', 'pool', '=', 'pool_api', '.', 'delete_pool', '(', 'name', '=', 'name', ')', 'except', 'airflowexception', 'as', 'err', ':', '_log', '.', 'error', '(', 'err', ')', 'response', '=', 'jsonify', '(', 'error', '=', '""{}""', '.', 'format', '(', 'err', ')', ')', 'response', '.', 'status_code', '=', 'err', '.', 'status_code', 'return', 'response', 'else', ':', 'return', 'jsonify', '(', 'pool', '.', 'to_json', '(', ')', ')']","def delete_pool ( name ) : try : pool = pool_api . delete_pool ( name = name ) except airflowexception as err : _log . error ( err ) response = jsonify ( error = ""{}"" . format ( err ) ) response . status_code = err . status_code return response else : return jsonify ( pool . to_json ( ) )"
298,apache/airflow,airflow/contrib/hooks/azure_container_instance_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_container_instance_hook.py#L80-L93,"def create_or_update(self, resource_group, name, container_group):
        """"""
        Create a new container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :param container_group: the properties of the container group
        :type container_group: azure.mgmt.containerinstance.models.ContainerGroup
        """"""
        self.connection.container_groups.create_or_update(resource_group,
                                                          name,
                                                          container_group)","['def', 'create_or_update', '(', 'self', ',', 'resource_group', ',', 'name', ',', 'container_group', ')', ':', 'self', '.', 'connection', '.', 'container_groups', '.', 'create_or_update', '(', 'resource_group', ',', 'name', ',', 'container_group', ')']","Create a new container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :param container_group: the properties of the container group
        :type container_group: azure.mgmt.containerinstance.models.ContainerGroup","['Create', 'a', 'new', 'container', 'group']",python,test,"['create', 'a', 'new', 'container', 'group']",create a new container group,"['def', 'create_or_update', '(', 'self', ',', 'resource_group', ',', 'name', ',', 'container_group', ')', ':', 'self', '.', 'connection', '.', 'container_groups', '.', 'create_or_update', '(', 'resource_group', ',', 'name', ',', 'container_group', ')']","def create_or_update ( self , resource_group , name , container_group ) : self . connection . container_groups . create_or_update ( resource_group , name , container_group )"
299,apache/airflow,airflow/contrib/hooks/azure_container_instance_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_container_instance_hook.py#L95-L110,"def get_state_exitcode_details(self, resource_group, name):
        """"""
        Get the state and exitcode of a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :return: A tuple with the state, exitcode, and details.
            If the exitcode is unknown 0 is returned.
        :rtype: tuple(state,exitcode,details)
        """"""
        current_state = self._get_instance_view(resource_group, name).current_state
        return (current_state.state,
                current_state.exit_code,
                current_state.detail_status)","['def', 'get_state_exitcode_details', '(', 'self', ',', 'resource_group', ',', 'name', ')', ':', 'current_state', '=', 'self', '.', '_get_instance_view', '(', 'resource_group', ',', 'name', ')', '.', 'current_state', 'return', '(', 'current_state', '.', 'state', ',', 'current_state', '.', 'exit_code', ',', 'current_state', '.', 'detail_status', ')']","Get the state and exitcode of a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :return: A tuple with the state, exitcode, and details.
            If the exitcode is unknown 0 is returned.
        :rtype: tuple(state,exitcode,details)","['Get', 'the', 'state', 'and', 'exitcode', 'of', 'a', 'container', 'group']",python,test,"['get', 'the', 'state', 'and', 'exitcode', 'of', 'a', 'container', 'group']",get the state and exitcode of a container group,"['def', 'get_state_exitcode_details', '(', 'self', ',', 'resource_group', ',', 'name', ')', ':', 'current_state', '=', 'self', '.', '_get_instance_view', '(', 'resource_group', ',', 'name', ')', '.', 'current_state', 'return', '(', 'current_state', '.', 'state', ',', 'current_state', '.', 'exit_code', ',', 'current_state', '.', 'detail_status', ')']","def get_state_exitcode_details ( self , resource_group , name ) : current_state = self . _get_instance_view ( resource_group , name ) . current_state return ( current_state . state , current_state . exit_code , current_state . detail_status )"
300,apache/airflow,airflow/contrib/hooks/azure_container_instance_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_container_instance_hook.py#L118-L131,"def get_messages(self, resource_group, name):
        """"""
        Get the messages of a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :return: A list of the event messages
        :rtype: list[str]
        """"""
        instance_view = self._get_instance_view(resource_group, name)

        return [event.message for event in instance_view.events]","['def', 'get_messages', '(', 'self', ',', 'resource_group', ',', 'name', ')', ':', 'instance_view', '=', 'self', '.', '_get_instance_view', '(', 'resource_group', ',', 'name', ')', 'return', '[', 'event', '.', 'message', 'for', 'event', 'in', 'instance_view', '.', 'events', ']']","Get the messages of a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :return: A list of the event messages
        :rtype: list[str]","['Get', 'the', 'messages', 'of', 'a', 'container', 'group']",python,test,"['get', 'the', 'messages', 'of', 'a', 'container', 'group']",get the messages of a container group,"['def', 'get_messages', '(', 'self', ',', 'resource_group', ',', 'name', ')', ':', 'instance_view', '=', 'self', '.', '_get_instance_view', '(', 'resource_group', ',', 'name', ')', 'return', '[', 'event', '.', 'message', 'for', 'event', 'in', 'instance_view', '.', 'events', ']']","def get_messages ( self , resource_group , name ) : instance_view = self . _get_instance_view ( resource_group , name ) return [ event . message for event in instance_view . events ]"
301,apache/airflow,airflow/contrib/hooks/azure_container_instance_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_container_instance_hook.py#L133-L147,"def get_logs(self, resource_group, name, tail=1000):
        """"""
        Get the tail from logs of a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :param tail: the size of the tail
        :type tail: int
        :return: A list of log messages
        :rtype: list[str]
        """"""
        logs = self.connection.container.list_logs(resource_group, name, name, tail=tail)
        return logs.content.splitlines(True)","['def', 'get_logs', '(', 'self', ',', 'resource_group', ',', 'name', ',', 'tail', '=', '1000', ')', ':', 'logs', '=', 'self', '.', 'connection', '.', 'container', '.', 'list_logs', '(', 'resource_group', ',', 'name', ',', 'name', ',', 'tail', '=', 'tail', ')', 'return', 'logs', '.', 'content', '.', 'splitlines', '(', 'True', ')']","Get the tail from logs of a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        :param tail: the size of the tail
        :type tail: int
        :return: A list of log messages
        :rtype: list[str]","['Get', 'the', 'tail', 'from', 'logs', 'of', 'a', 'container', 'group']",python,test,"['get', 'the', 'tail', 'from', 'logs', 'of', 'a', 'container', 'group']",get the tail from logs of a container group,"['def', 'get_logs', '(', 'self', ',', 'resource_group', ',', 'name', ',', 'tail', '=', '1000', ')', ':', 'logs', '=', 'self', '.', 'connection', '.', 'container', '.', 'list_logs', '(', 'resource_group', ',', 'name', ',', 'name', ',', 'tail', '=', 'tail', ')', 'return', 'logs', '.', 'content', '.', 'splitlines', '(', 'true', ')']","def get_logs ( self , resource_group , name , tail = 1000 ) : logs = self . connection . container . list_logs ( resource_group , name , name , tail = tail ) return logs . content . splitlines ( true )"
302,apache/airflow,airflow/contrib/hooks/azure_container_instance_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_container_instance_hook.py#L149-L158,"def delete(self, resource_group, name):
        """"""
        Delete a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        """"""
        self.connection.container_groups.delete(resource_group, name)","['def', 'delete', '(', 'self', ',', 'resource_group', ',', 'name', ')', ':', 'self', '.', 'connection', '.', 'container_groups', '.', 'delete', '(', 'resource_group', ',', 'name', ')']","Delete a container group

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str","['Delete', 'a', 'container', 'group']",python,test,"['delete', 'a', 'container', 'group']",delete a container group,"['def', 'delete', '(', 'self', ',', 'resource_group', ',', 'name', ')', ':', 'self', '.', 'connection', '.', 'container_groups', '.', 'delete', '(', 'resource_group', ',', 'name', ')']","def delete ( self , resource_group , name ) : self . connection . container_groups . delete ( resource_group , name )"
303,apache/airflow,airflow/contrib/hooks/azure_container_instance_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_container_instance_hook.py#L160-L172,"def exists(self, resource_group, name):
        """"""
        Test if a container group exists

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str
        """"""
        for container in self.connection.container_groups.list_by_resource_group(resource_group):
            if container.name == name:
                return True
        return False","['def', 'exists', '(', 'self', ',', 'resource_group', ',', 'name', ')', ':', 'for', 'container', 'in', 'self', '.', 'connection', '.', 'container_groups', '.', 'list_by_resource_group', '(', 'resource_group', ')', ':', 'if', 'container', '.', 'name', '==', 'name', ':', 'return', 'True', 'return', 'False']","Test if a container group exists

        :param resource_group: the name of the resource group
        :type resource_group: str
        :param name: the name of the container group
        :type name: str","['Test', 'if', 'a', 'container', 'group', 'exists']",python,test,"['test', 'if', 'a', 'container', 'group', 'exists']",test if a container group exists,"['def', 'exists', '(', 'self', ',', 'resource_group', ',', 'name', ')', ':', 'for', 'container', 'in', 'self', '.', 'connection', '.', 'container_groups', '.', 'list_by_resource_group', '(', 'resource_group', ')', ':', 'if', 'container', '.', 'name', '==', 'name', ':', 'return', 'true', 'return', 'false']","def exists ( self , resource_group , name ) : for container in self . connection . container_groups . list_by_resource_group ( resource_group ) : if container . name == name : return true return false"
304,apache/airflow,airflow/utils/decorators.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/decorators.py#L38-L100,"def apply_defaults(func):
    """"""
    Function decorator that Looks for an argument named ""default_args"", and
    fills the unspecified arguments from it.

    Since python2.* isn't clear about which arguments are missing when
    calling a function, and that this can be quite confusing with multi-level
    inheritance and argument defaults, this decorator also alerts with
    specific information about the missing arguments.
    """"""

    # Cache inspect.signature for the wrapper closure to avoid calling it
    # at every decorated invocation. This is separate sig_cache created
    # per decoration, i.e. each function decorated using apply_defaults will
    # have a different sig_cache.
    sig_cache = signature(func)
    non_optional_args = {
        name for (name, param) in sig_cache.parameters.items()
        if param.default == param.empty and
        param.name != 'self' and
        param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD)}

    @wraps(func)
    def wrapper(*args, **kwargs):
        if len(args) > 1:
            raise AirflowException(
                ""Use keyword arguments when initializing operators"")
        dag_args = {}
        dag_params = {}

        dag = kwargs.get('dag', None) or settings.CONTEXT_MANAGER_DAG
        if dag:
            dag_args = copy(dag.default_args) or {}
            dag_params = copy(dag.params) or {}

        params = {}
        if 'params' in kwargs:
            params = kwargs['params']
        dag_params.update(params)

        default_args = {}
        if 'default_args' in kwargs:
            default_args = kwargs['default_args']
            if 'params' in default_args:
                dag_params.update(default_args['params'])
                del default_args['params']

        dag_args.update(default_args)
        default_args = dag_args

        for arg in sig_cache.parameters:
            if arg not in kwargs and arg in default_args:
                kwargs[arg] = default_args[arg]
        missing_args = list(non_optional_args - set(kwargs))
        if missing_args:
            msg = ""Argument {0} is required"".format(missing_args)
            raise AirflowException(msg)

        kwargs['params'] = dag_params

        result = func(*args, **kwargs)
        return result
    return wrapper","['def', 'apply_defaults', '(', 'func', ')', ':', '# Cache inspect.signature for the wrapper closure to avoid calling it', '# at every decorated invocation. This is separate sig_cache created', '# per decoration, i.e. each function decorated using apply_defaults will', '# have a different sig_cache.', 'sig_cache', '=', 'signature', '(', 'func', ')', 'non_optional_args', '=', '{', 'name', 'for', '(', 'name', ',', 'param', ')', 'in', 'sig_cache', '.', 'parameters', '.', 'items', '(', ')', 'if', 'param', '.', 'default', '==', 'param', '.', 'empty', 'and', 'param', '.', 'name', '!=', ""'self'"", 'and', 'param', '.', 'kind', 'not', 'in', '(', 'param', '.', 'VAR_POSITIONAL', ',', 'param', '.', 'VAR_KEYWORD', ')', '}', '@', 'wraps', '(', 'func', ')', 'def', 'wrapper', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'if', 'len', '(', 'args', ')', '>', '1', ':', 'raise', 'AirflowException', '(', '""Use keyword arguments when initializing operators""', ')', 'dag_args', '=', '{', '}', 'dag_params', '=', '{', '}', 'dag', '=', 'kwargs', '.', 'get', '(', ""'dag'"", ',', 'None', ')', 'or', 'settings', '.', 'CONTEXT_MANAGER_DAG', 'if', 'dag', ':', 'dag_args', '=', 'copy', '(', 'dag', '.', 'default_args', ')', 'or', '{', '}', 'dag_params', '=', 'copy', '(', 'dag', '.', 'params', ')', 'or', '{', '}', 'params', '=', '{', '}', 'if', ""'params'"", 'in', 'kwargs', ':', 'params', '=', 'kwargs', '[', ""'params'"", ']', 'dag_params', '.', 'update', '(', 'params', ')', 'default_args', '=', '{', '}', 'if', ""'default_args'"", 'in', 'kwargs', ':', 'default_args', '=', 'kwargs', '[', ""'default_args'"", ']', 'if', ""'params'"", 'in', 'default_args', ':', 'dag_params', '.', 'update', '(', 'default_args', '[', ""'params'"", ']', ')', 'del', 'default_args', '[', ""'params'"", ']', 'dag_args', '.', 'update', '(', 'default_args', ')', 'default_args', '=', 'dag_args', 'for', 'arg', 'in', 'sig_cache', '.', 'parameters', ':', 'if', 'arg', 'not', 'in', 'kwargs', 'and', 'arg', 'in', 'default_args', ':', 'kwargs', '[', 'arg', ']', '=', 'default_args', '[', 'arg', ']', 'missing_args', '=', 'list', '(', 'non_optional_args', '-', 'set', '(', 'kwargs', ')', ')', 'if', 'missing_args', ':', 'msg', '=', '""Argument {0} is required""', '.', 'format', '(', 'missing_args', ')', 'raise', 'AirflowException', '(', 'msg', ')', 'kwargs', '[', ""'params'"", ']', '=', 'dag_params', 'result', '=', 'func', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', 'return', 'result', 'return', 'wrapper']","Function decorator that Looks for an argument named ""default_args"", and
    fills the unspecified arguments from it.

    Since python2.* isn't clear about which arguments are missing when
    calling a function, and that this can be quite confusing with multi-level
    inheritance and argument defaults, this decorator also alerts with
    specific information about the missing arguments.","['Function', 'decorator', 'that', 'Looks', 'for', 'an', 'argument', 'named', 'default_args', 'and', 'fills', 'the', 'unspecified', 'arguments', 'from', 'it', '.']",python,test,"['function', 'decorator', 'that', 'looks', 'for', 'an', 'argument', 'named', 'default_args', 'and', 'fills', 'the', 'unspecified', 'arguments', 'from', 'it', '.']",function decorator that looks for an argument named default_args and fills the unspecified arguments from it .,"['def', 'apply_defaults', '(', 'func', ')', ':', '# cache inspect.signature for the wrapper closure to avoid calling it', '# at every decorated invocation. this is separate sig_cache created', '# per decoration, i.e. each function decorated using apply_defaults will', '# have a different sig_cache.', 'sig_cache', '=', 'signature', '(', 'func', ')', 'non_optional_args', '=', '{', 'name', 'for', '(', 'name', ',', 'param', ')', 'in', 'sig_cache', '.', 'parameters', '.', 'items', '(', ')', 'if', 'param', '.', 'default', '==', 'param', '.', 'empty', 'and', 'param', '.', 'name', '!=', ""'self'"", 'and', 'param', '.', 'kind', 'not', 'in', '(', 'param', '.', 'var_positional', ',', 'param', '.', 'var_keyword', ')', '}', '@', 'wraps', '(', 'func', ')', 'def', 'wrapper', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'if', 'len', '(', 'args', ')', '>', '1', ':', 'raise', 'airflowexception', '(', '""use keyword arguments when initializing operators""', ')', 'dag_args', '=', '{', '}', 'dag_params', '=', '{', '}', 'dag', '=', 'kwargs', '.', 'get', '(', ""'dag'"", ',', 'none', ')', 'or', 'settings', '.', 'context_manager_dag', 'if', 'dag', ':', 'dag_args', '=', 'copy', '(', 'dag', '.', 'default_args', ')', 'or', '{', '}', 'dag_params', '=', 'copy', '(', 'dag', '.', 'params', ')', 'or', '{', '}', 'params', '=', '{', '}', 'if', ""'params'"", 'in', 'kwargs', ':', 'params', '=', 'kwargs', '[', ""'params'"", ']', 'dag_params', '.', 'update', '(', 'params', ')', 'default_args', '=', '{', '}', 'if', ""'default_args'"", 'in', 'kwargs', ':', 'default_args', '=', 'kwargs', '[', ""'default_args'"", ']', 'if', ""'params'"", 'in', 'default_args', ':', 'dag_params', '.', 'update', '(', 'default_args', '[', ""'params'"", ']', ')', 'del', 'default_args', '[', ""'params'"", ']', 'dag_args', '.', 'update', '(', 'default_args', ')', 'default_args', '=', 'dag_args', 'for', 'arg', 'in', 'sig_cache', '.', 'parameters', ':', 'if', 'arg', 'not', 'in', 'kwargs', 'and', 'arg', 'in', 'default_args', ':', 'kwargs', '[', 'arg', ']', '=', 'default_args', '[', 'arg', ']', 'missing_args', '=', 'list', '(', 'non_optional_args', '-', 'set', '(', 'kwargs', ')', ')', 'if', 'missing_args', ':', 'msg', '=', '""argument {0} is required""', '.', 'format', '(', 'missing_args', ')', 'raise', 'airflowexception', '(', 'msg', ')', 'kwargs', '[', ""'params'"", ']', '=', 'dag_params', 'result', '=', 'func', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', 'return', 'result', 'return', 'wrapper']","def apply_defaults ( func ) : # cache inspect.signature for the wrapper closure to avoid calling it # at every decorated invocation. this is separate sig_cache created # per decoration, i.e. each function decorated using apply_defaults will # have a different sig_cache. sig_cache = signature ( func ) non_optional_args = { name for ( name , param ) in sig_cache . parameters . items ( ) if param . default == param . empty and param . name != 'self' and param . kind not in ( param . var_positional , param . var_keyword ) } @ wraps ( func ) def wrapper ( * args , * * kwargs ) : if len ( args ) > 1 : raise airflowexception ( ""use keyword arguments when initializing operators"" ) dag_args = { } dag_params = { } dag = kwargs . get ( 'dag' , none ) or settings . context_manager_dag if dag : dag_args = copy ( dag . default_args ) or { } dag_params = copy ( dag . params ) or { } params = { } if 'params' in kwargs : params = kwargs [ 'params' ] dag_params . update ( params ) default_args = { } if 'default_args' in kwargs : default_args = kwargs [ 'default_args' ] if 'params' in default_args : dag_params . update ( default_args [ 'params' ] ) del default_args [ 'params' ] dag_args . update ( default_args ) default_args = dag_args for arg in sig_cache . parameters : if arg not in kwargs and arg in default_args : kwargs [ arg ] = default_args [ arg ] missing_args = list ( non_optional_args - set ( kwargs ) ) if missing_args : msg = ""argument {0} is required"" . format ( missing_args ) raise airflowexception ( msg ) kwargs [ 'params' ] = dag_params result = func ( * args , * * kwargs ) return result return wrapper"
305,apache/airflow,airflow/operators/hive_to_druid.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/operators/hive_to_druid.py#L157-L244,"def construct_ingest_query(self, static_path, columns):
        """"""
        Builds an ingest query for an HDFS TSV load.

        :param static_path: The path on hdfs where the data is
        :type static_path: str
        :param columns: List of all the columns that are available
        :type columns: list
        """"""

        # backward compatibility for num_shards,
        # but target_partition_size is the default setting
        # and overwrites the num_shards
        num_shards = self.num_shards
        target_partition_size = self.target_partition_size
        if self.target_partition_size == -1:
            if self.num_shards == -1:
                target_partition_size = DEFAULT_TARGET_PARTITION_SIZE
        else:
            num_shards = -1

        metric_names = [m['fieldName'] for m in self.metric_spec if m['type'] != 'count']

        # Take all the columns, which are not the time dimension
        # or a metric, as the dimension columns
        dimensions = [c for c in columns if c not in metric_names and c != self.ts_dim]

        ingest_query_dict = {
            ""type"": ""index_hadoop"",
            ""spec"": {
                ""dataSchema"": {
                    ""metricsSpec"": self.metric_spec,
                    ""granularitySpec"": {
                        ""queryGranularity"": self.query_granularity,
                        ""intervals"": self.intervals,
                        ""type"": ""uniform"",
                        ""segmentGranularity"": self.segment_granularity,
                    },
                    ""parser"": {
                        ""type"": ""string"",
                        ""parseSpec"": {
                            ""columns"": columns,
                            ""dimensionsSpec"": {
                                ""dimensionExclusions"": [],
                                ""dimensions"": dimensions,  # list of names
                                ""spatialDimensions"": []
                            },
                            ""timestampSpec"": {
                                ""column"": self.ts_dim,
                                ""format"": ""auto""
                            },
                            ""format"": ""tsv""
                        }
                    },
                    ""dataSource"": self.druid_datasource
                },
                ""tuningConfig"": {
                    ""type"": ""hadoop"",
                    ""jobProperties"": {
                        ""mapreduce.job.user.classpath.first"": ""false"",
                        ""mapreduce.map.output.compress"": ""false"",
                        ""mapreduce.output.fileoutputformat.compress"": ""false"",
                    },
                    ""partitionsSpec"": {
                        ""type"": ""hashed"",
                        ""targetPartitionSize"": target_partition_size,
                        ""numShards"": num_shards,
                    },
                },
                ""ioConfig"": {
                    ""inputSpec"": {
                        ""paths"": static_path,
                        ""type"": ""static""
                    },
                    ""type"": ""hadoop""
                }
            }
        }

        if self.job_properties:
            ingest_query_dict['spec']['tuningConfig']['jobProperties'] \
                .update(self.job_properties)

        if self.hadoop_dependency_coordinates:
            ingest_query_dict['hadoopDependencyCoordinates'] \
                = self.hadoop_dependency_coordinates

        return ingest_query_dict","['def', 'construct_ingest_query', '(', 'self', ',', 'static_path', ',', 'columns', ')', ':', '# backward compatibility for num_shards,', '# but target_partition_size is the default setting', '# and overwrites the num_shards', 'num_shards', '=', 'self', '.', 'num_shards', 'target_partition_size', '=', 'self', '.', 'target_partition_size', 'if', 'self', '.', 'target_partition_size', '==', '-', '1', ':', 'if', 'self', '.', 'num_shards', '==', '-', '1', ':', 'target_partition_size', '=', 'DEFAULT_TARGET_PARTITION_SIZE', 'else', ':', 'num_shards', '=', '-', '1', 'metric_names', '=', '[', 'm', '[', ""'fieldName'"", ']', 'for', 'm', 'in', 'self', '.', 'metric_spec', 'if', 'm', '[', ""'type'"", ']', '!=', ""'count'"", ']', '# Take all the columns, which are not the time dimension', '# or a metric, as the dimension columns', 'dimensions', '=', '[', 'c', 'for', 'c', 'in', 'columns', 'if', 'c', 'not', 'in', 'metric_names', 'and', 'c', '!=', 'self', '.', 'ts_dim', ']', 'ingest_query_dict', '=', '{', '""type""', ':', '""index_hadoop""', ',', '""spec""', ':', '{', '""dataSchema""', ':', '{', '""metricsSpec""', ':', 'self', '.', 'metric_spec', ',', '""granularitySpec""', ':', '{', '""queryGranularity""', ':', 'self', '.', 'query_granularity', ',', '""intervals""', ':', 'self', '.', 'intervals', ',', '""type""', ':', '""uniform""', ',', '""segmentGranularity""', ':', 'self', '.', 'segment_granularity', ',', '}', ',', '""parser""', ':', '{', '""type""', ':', '""string""', ',', '""parseSpec""', ':', '{', '""columns""', ':', 'columns', ',', '""dimensionsSpec""', ':', '{', '""dimensionExclusions""', ':', '[', ']', ',', '""dimensions""', ':', 'dimensions', ',', '# list of names', '""spatialDimensions""', ':', '[', ']', '}', ',', '""timestampSpec""', ':', '{', '""column""', ':', 'self', '.', 'ts_dim', ',', '""format""', ':', '""auto""', '}', ',', '""format""', ':', '""tsv""', '}', '}', ',', '""dataSource""', ':', 'self', '.', 'druid_datasource', '}', ',', '""tuningConfig""', ':', '{', '""type""', ':', '""hadoop""', ',', '""jobProperties""', ':', '{', '""mapreduce.job.user.classpath.first""', ':', '""false""', ',', '""mapreduce.map.output.compress""', ':', '""false""', ',', '""mapreduce.output.fileoutputformat.compress""', ':', '""false""', ',', '}', ',', '""partitionsSpec""', ':', '{', '""type""', ':', '""hashed""', ',', '""targetPartitionSize""', ':', 'target_partition_size', ',', '""numShards""', ':', 'num_shards', ',', '}', ',', '}', ',', '""ioConfig""', ':', '{', '""inputSpec""', ':', '{', '""paths""', ':', 'static_path', ',', '""type""', ':', '""static""', '}', ',', '""type""', ':', '""hadoop""', '}', '}', '}', 'if', 'self', '.', 'job_properties', ':', 'ingest_query_dict', '[', ""'spec'"", ']', '[', ""'tuningConfig'"", ']', '[', ""'jobProperties'"", ']', '.', 'update', '(', 'self', '.', 'job_properties', ')', 'if', 'self', '.', 'hadoop_dependency_coordinates', ':', 'ingest_query_dict', '[', ""'hadoopDependencyCoordinates'"", ']', '=', 'self', '.', 'hadoop_dependency_coordinates', 'return', 'ingest_query_dict']","Builds an ingest query for an HDFS TSV load.

        :param static_path: The path on hdfs where the data is
        :type static_path: str
        :param columns: List of all the columns that are available
        :type columns: list","['Builds', 'an', 'ingest', 'query', 'for', 'an', 'HDFS', 'TSV', 'load', '.']",python,test,"['builds', 'an', 'ingest', 'query', 'for', 'an', 'hdfs', 'tsv', 'load', '.']",builds an ingest query for an hdfs tsv load .,"['def', 'construct_ingest_query', '(', 'self', ',', 'static_path', ',', 'columns', ')', ':', '# backward compatibility for num_shards,', '# but target_partition_size is the default setting', '# and overwrites the num_shards', 'num_shards', '=', 'self', '.', 'num_shards', 'target_partition_size', '=', 'self', '.', 'target_partition_size', 'if', 'self', '.', 'target_partition_size', '==', '-', '1', ':', 'if', 'self', '.', 'num_shards', '==', '-', '1', ':', 'target_partition_size', '=', 'default_target_partition_size', 'else', ':', 'num_shards', '=', '-', '1', 'metric_names', '=', '[', 'm', '[', ""'fieldname'"", ']', 'for', 'm', 'in', 'self', '.', 'metric_spec', 'if', 'm', '[', ""'type'"", ']', '!=', ""'count'"", ']', '# take all the columns, which are not the time dimension', '# or a metric, as the dimension columns', 'dimensions', '=', '[', 'c', 'for', 'c', 'in', 'columns', 'if', 'c', 'not', 'in', 'metric_names', 'and', 'c', '!=', 'self', '.', 'ts_dim', ']', 'ingest_query_dict', '=', '{', '""type""', ':', '""index_hadoop""', ',', '""spec""', ':', '{', '""dataschema""', ':', '{', '""metricsspec""', ':', 'self', '.', 'metric_spec', ',', '""granularityspec""', ':', '{', '""querygranularity""', ':', 'self', '.', 'query_granularity', ',', '""intervals""', ':', 'self', '.', 'intervals', ',', '""type""', ':', '""uniform""', ',', '""segmentgranularity""', ':', 'self', '.', 'segment_granularity', ',', '}', ',', '""parser""', ':', '{', '""type""', ':', '""string""', ',', '""parsespec""', ':', '{', '""columns""', ':', 'columns', ',', '""dimensionsspec""', ':', '{', '""dimensionexclusions""', ':', '[', ']', ',', '""dimensions""', ':', 'dimensions', ',', '# list of names', '""spatialdimensions""', ':', '[', ']', '}', ',', '""timestampspec""', ':', '{', '""column""', ':', 'self', '.', 'ts_dim', ',', '""format""', ':', '""auto""', '}', ',', '""format""', ':', '""tsv""', '}', '}', ',', '""datasource""', ':', 'self', '.', 'druid_datasource', '}', ',', '""tuningconfig""', ':', '{', '""type""', ':', '""hadoop""', ',', '""jobproperties""', ':', '{', '""mapreduce.job.user.classpath.first""', ':', '""false""', ',', '""mapreduce.map.output.compress""', ':', '""false""', ',', '""mapreduce.output.fileoutputformat.compress""', ':', '""false""', ',', '}', ',', '""partitionsspec""', ':', '{', '""type""', ':', '""hashed""', ',', '""targetpartitionsize""', ':', 'target_partition_size', ',', '""numshards""', ':', 'num_shards', ',', '}', ',', '}', ',', '""ioconfig""', ':', '{', '""inputspec""', ':', '{', '""paths""', ':', 'static_path', ',', '""type""', ':', '""static""', '}', ',', '""type""', ':', '""hadoop""', '}', '}', '}', 'if', 'self', '.', 'job_properties', ':', 'ingest_query_dict', '[', ""'spec'"", ']', '[', ""'tuningconfig'"", ']', '[', ""'jobproperties'"", ']', '.', 'update', '(', 'self', '.', 'job_properties', ')', 'if', 'self', '.', 'hadoop_dependency_coordinates', ':', 'ingest_query_dict', '[', ""'hadoopdependencycoordinates'"", ']', '=', 'self', '.', 'hadoop_dependency_coordinates', 'return', 'ingest_query_dict']","def construct_ingest_query ( self , static_path , columns ) : # backward compatibility for num_shards, # but target_partition_size is the default setting # and overwrites the num_shards num_shards = self . num_shards target_partition_size = self . target_partition_size if self . target_partition_size == - 1 : if self . num_shards == - 1 : target_partition_size = default_target_partition_size else : num_shards = - 1 metric_names = [ m [ 'fieldname' ] for m in self . metric_spec if m [ 'type' ] != 'count' ] # take all the columns, which are not the time dimension # or a metric, as the dimension columns dimensions = [ c for c in columns if c not in metric_names and c != self . ts_dim ] ingest_query_dict = { ""type"" : ""index_hadoop"" , ""spec"" : { ""dataschema"" : { ""metricsspec"" : self . metric_spec , ""granularityspec"" : { ""querygranularity"" : self . query_granularity , ""intervals"" : self . intervals , ""type"" : ""uniform"" , ""segmentgranularity"" : self . segment_granularity , } , ""parser"" : { ""type"" : ""string"" , ""parsespec"" : { ""columns"" : columns , ""dimensionsspec"" : { ""dimensionexclusions"" : [ ] , ""dimensions"" : dimensions , # list of names ""spatialdimensions"" : [ ] } , ""timestampspec"" : { ""column"" : self . ts_dim , ""format"" : ""auto"" } , ""format"" : ""tsv"" } } , ""datasource"" : self . druid_datasource } , ""tuningconfig"" : { ""type"" : ""hadoop"" , ""jobproperties"" : { ""mapreduce.job.user.classpath.first"" : ""false"" , ""mapreduce.map.output.compress"" : ""false"" , ""mapreduce.output.fileoutputformat.compress"" : ""false"" , } , ""partitionsspec"" : { ""type"" : ""hashed"" , ""targetpartitionsize"" : target_partition_size , ""numshards"" : num_shards , } , } , ""ioconfig"" : { ""inputspec"" : { ""paths"" : static_path , ""type"" : ""static"" } , ""type"" : ""hadoop"" } } } if self . job_properties : ingest_query_dict [ 'spec' ] [ 'tuningconfig' ] [ 'jobproperties' ] . update ( self . job_properties ) if self . hadoop_dependency_coordinates : ingest_query_dict [ 'hadoopdependencycoordinates' ] = self . hadoop_dependency_coordinates return ingest_query_dict"
306,apache/airflow,airflow/contrib/operators/imap_attachment_to_s3_operator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/imap_attachment_to_s3_operator.py#L67-L88,"def execute(self, context):
        """"""
        This function executes the transfer from the email server (via imap) into s3.

        :param context: The context while executing.
        :type context: dict
        """"""
        self.log.info(
            'Transferring mail attachment %s from mail server via imap to s3 key %s...',
            self.imap_attachment_name, self.s3_key
        )

        with ImapHook(imap_conn_id=self.imap_conn_id) as imap_hook:
            imap_mail_attachments = imap_hook.retrieve_mail_attachments(
                name=self.imap_attachment_name,
                mail_folder=self.imap_mail_folder,
                check_regex=self.imap_check_regex,
                latest_only=True
            )

        s3_hook = S3Hook(aws_conn_id=self.s3_conn_id)
        s3_hook.load_bytes(bytes_data=imap_mail_attachments[0][1], key=self.s3_key)","['def', 'execute', '(', 'self', ',', 'context', ')', ':', 'self', '.', 'log', '.', 'info', '(', ""'Transferring mail attachment %s from mail server via imap to s3 key %s...'"", ',', 'self', '.', 'imap_attachment_name', ',', 'self', '.', 's3_key', ')', 'with', 'ImapHook', '(', 'imap_conn_id', '=', 'self', '.', 'imap_conn_id', ')', 'as', 'imap_hook', ':', 'imap_mail_attachments', '=', 'imap_hook', '.', 'retrieve_mail_attachments', '(', 'name', '=', 'self', '.', 'imap_attachment_name', ',', 'mail_folder', '=', 'self', '.', 'imap_mail_folder', ',', 'check_regex', '=', 'self', '.', 'imap_check_regex', ',', 'latest_only', '=', 'True', ')', 's3_hook', '=', 'S3Hook', '(', 'aws_conn_id', '=', 'self', '.', 's3_conn_id', ')', 's3_hook', '.', 'load_bytes', '(', 'bytes_data', '=', 'imap_mail_attachments', '[', '0', ']', '[', '1', ']', ',', 'key', '=', 'self', '.', 's3_key', ')']","This function executes the transfer from the email server (via imap) into s3.

        :param context: The context while executing.
        :type context: dict","['This', 'function', 'executes', 'the', 'transfer', 'from', 'the', 'email', 'server', '(', 'via', 'imap', ')', 'into', 's3', '.']",python,test,"['this', 'function', 'executes', 'the', 'transfer', 'from', 'the', 'email', 'server', '(', 'via', 'imap', ')', 'into', 's3', '.']",this function executes the transfer from the email server ( via imap ) into s3 .,"['def', 'execute', '(', 'self', ',', 'context', ')', ':', 'self', '.', 'log', '.', 'info', '(', ""'transferring mail attachment %s from mail server via imap to s3 key %s...'"", ',', 'self', '.', 'imap_attachment_name', ',', 'self', '.', 's3_key', ')', 'with', 'imaphook', '(', 'imap_conn_id', '=', 'self', '.', 'imap_conn_id', ')', 'as', 'imap_hook', ':', 'imap_mail_attachments', '=', 'imap_hook', '.', 'retrieve_mail_attachments', '(', 'name', '=', 'self', '.', 'imap_attachment_name', ',', 'mail_folder', '=', 'self', '.', 'imap_mail_folder', ',', 'check_regex', '=', 'self', '.', 'imap_check_regex', ',', 'latest_only', '=', 'true', ')', 's3_hook', '=', 's3hook', '(', 'aws_conn_id', '=', 'self', '.', 's3_conn_id', ')', 's3_hook', '.', 'load_bytes', '(', 'bytes_data', '=', 'imap_mail_attachments', '[', '0', ']', '[', '1', ']', ',', 'key', '=', 'self', '.', 's3_key', ')']","def execute ( self , context ) : self . log . info ( 'transferring mail attachment %s from mail server via imap to s3 key %s...' , self . imap_attachment_name , self . s3_key ) with imaphook ( imap_conn_id = self . imap_conn_id ) as imap_hook : imap_mail_attachments = imap_hook . retrieve_mail_attachments ( name = self . imap_attachment_name , mail_folder = self . imap_mail_folder , check_regex = self . imap_check_regex , latest_only = true ) s3_hook = s3hook ( aws_conn_id = self . s3_conn_id ) s3_hook . load_bytes ( bytes_data = imap_mail_attachments [ 0 ] [ 1 ] , key = self . s3_key )"
307,apache/airflow,airflow/contrib/sensors/redis_pub_sub_sensor.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/redis_pub_sub_sensor.py#L50-L73,"def poke(self, context):
        """"""
        Check for message on subscribed channels and write to xcom the message with key ``message``

        An example of message ``{'type': 'message', 'pattern': None, 'channel': b'test', 'data': b'hello'}``

        :param context: the context object
        :type context: dict
        :return: ``True`` if message (with type 'message') is available or ``False`` if not
        """"""
        self.log.info('RedisPubSubSensor checking for message on channels: %s', self.channels)

        message = self.pubsub.get_message()
        self.log.info('Message %s from channel %s', message, self.channels)

        # Process only message types
        if message and message['type'] == 'message':

            context['ti'].xcom_push(key='message', value=message)
            self.pubsub.unsubscribe(self.channels)

            return True

        return False","['def', 'poke', '(', 'self', ',', 'context', ')', ':', 'self', '.', 'log', '.', 'info', '(', ""'RedisPubSubSensor checking for message on channels: %s'"", ',', 'self', '.', 'channels', ')', 'message', '=', 'self', '.', 'pubsub', '.', 'get_message', '(', ')', 'self', '.', 'log', '.', 'info', '(', ""'Message %s from channel %s'"", ',', 'message', ',', 'self', '.', 'channels', ')', '# Process only message types', 'if', 'message', 'and', 'message', '[', ""'type'"", ']', '==', ""'message'"", ':', 'context', '[', ""'ti'"", ']', '.', 'xcom_push', '(', 'key', '=', ""'message'"", ',', 'value', '=', 'message', ')', 'self', '.', 'pubsub', '.', 'unsubscribe', '(', 'self', '.', 'channels', ')', 'return', 'True', 'return', 'False']","Check for message on subscribed channels and write to xcom the message with key ``message``

        An example of message ``{'type': 'message', 'pattern': None, 'channel': b'test', 'data': b'hello'}``

        :param context: the context object
        :type context: dict
        :return: ``True`` if message (with type 'message') is available or ``False`` if not","['Check', 'for', 'message', 'on', 'subscribed', 'channels', 'and', 'write', 'to', 'xcom', 'the', 'message', 'with', 'key', 'message']",python,test,"['check', 'for', 'message', 'on', 'subscribed', 'channels', 'and', 'write', 'to', 'xcom', 'the', 'message', 'with', 'key', 'message']",check for message on subscribed channels and write to xcom the message with key message,"['def', 'poke', '(', 'self', ',', 'context', ')', ':', 'self', '.', 'log', '.', 'info', '(', ""'redispubsubsensor checking for message on channels: %s'"", ',', 'self', '.', 'channels', ')', 'message', '=', 'self', '.', 'pubsub', '.', 'get_message', '(', ')', 'self', '.', 'log', '.', 'info', '(', ""'message %s from channel %s'"", ',', 'message', ',', 'self', '.', 'channels', ')', '# process only message types', 'if', 'message', 'and', 'message', '[', ""'type'"", ']', '==', ""'message'"", ':', 'context', '[', ""'ti'"", ']', '.', 'xcom_push', '(', 'key', '=', ""'message'"", ',', 'value', '=', 'message', ')', 'self', '.', 'pubsub', '.', 'unsubscribe', '(', 'self', '.', 'channels', ')', 'return', 'true', 'return', 'false']","def poke ( self , context ) : self . log . info ( 'redispubsubsensor checking for message on channels: %s' , self . channels ) message = self . pubsub . get_message ( ) self . log . info ( 'message %s from channel %s' , message , self . channels ) # process only message types if message and message [ 'type' ] == 'message' : context [ 'ti' ] . xcom_push ( key = 'message' , value = message ) self . pubsub . unsubscribe ( self . channels ) return true return false"
308,apache/airflow,airflow/models/dagrun.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagrun.py#L95-L111,"def refresh_from_db(self, session=None):
        """"""
        Reloads the current dagrun from the database
        :param session: database session
        """"""
        DR = DagRun

        exec_date = func.cast(self.execution_date, DateTime)

        dr = session.query(DR).filter(
            DR.dag_id == self.dag_id,
            func.cast(DR.execution_date, DateTime) == exec_date,
            DR.run_id == self.run_id
        ).one()

        self.id = dr.id
        self.state = dr.state","['def', 'refresh_from_db', '(', 'self', ',', 'session', '=', 'None', ')', ':', 'DR', '=', 'DagRun', 'exec_date', '=', 'func', '.', 'cast', '(', 'self', '.', 'execution_date', ',', 'DateTime', ')', 'dr', '=', 'session', '.', 'query', '(', 'DR', ')', '.', 'filter', '(', 'DR', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'func', '.', 'cast', '(', 'DR', '.', 'execution_date', ',', 'DateTime', ')', '==', 'exec_date', ',', 'DR', '.', 'run_id', '==', 'self', '.', 'run_id', ')', '.', 'one', '(', ')', 'self', '.', 'id', '=', 'dr', '.', 'id', 'self', '.', 'state', '=', 'dr', '.', 'state']","Reloads the current dagrun from the database
        :param session: database session","['Reloads', 'the', 'current', 'dagrun', 'from', 'the', 'database', ':', 'param', 'session', ':', 'database', 'session']",python,test,"['reloads', 'the', 'current', 'dagrun', 'from', 'the', 'database', ':', 'param', 'session', ':', 'database', 'session']",reloads the current dagrun from the database : param session : database session,"['def', 'refresh_from_db', '(', 'self', ',', 'session', '=', 'none', ')', ':', 'dr', '=', 'dagrun', 'exec_date', '=', 'func', '.', 'cast', '(', 'self', '.', 'execution_date', ',', 'datetime', ')', 'dr', '=', 'session', '.', 'query', '(', 'dr', ')', '.', 'filter', '(', 'dr', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'func', '.', 'cast', '(', 'dr', '.', 'execution_date', ',', 'datetime', ')', '==', 'exec_date', ',', 'dr', '.', 'run_id', '==', 'self', '.', 'run_id', ')', '.', 'one', '(', ')', 'self', '.', 'id', '=', 'dr', '.', 'id', 'self', '.', 'state', '=', 'dr', '.', 'state']","def refresh_from_db ( self , session = none ) : dr = dagrun exec_date = func . cast ( self . execution_date , datetime ) dr = session . query ( dr ) . filter ( dr . dag_id == self . dag_id , func . cast ( dr . execution_date , datetime ) == exec_date , dr . run_id == self . run_id ) . one ( ) self . id = dr . id self . state = dr . state"
309,apache/airflow,airflow/models/dagrun.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagrun.py#L115-L160,"def find(dag_id=None, run_id=None, execution_date=None,
             state=None, external_trigger=None, no_backfills=False,
             session=None):
        """"""
        Returns a set of dag runs for the given search criteria.

        :param dag_id: the dag_id to find dag runs for
        :type dag_id: int, list
        :param run_id: defines the the run id for this dag run
        :type run_id: str
        :param execution_date: the execution date
        :type execution_date: datetime.datetime
        :param state: the state of the dag run
        :type state: airflow.utils.state.State
        :param external_trigger: whether this dag run is externally triggered
        :type external_trigger: bool
        :param no_backfills: return no backfills (True), return all (False).
            Defaults to False
        :type no_backfills: bool
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        """"""
        DR = DagRun

        qry = session.query(DR)
        if dag_id:
            qry = qry.filter(DR.dag_id == dag_id)
        if run_id:
            qry = qry.filter(DR.run_id == run_id)
        if execution_date:
            if isinstance(execution_date, list):
                qry = qry.filter(DR.execution_date.in_(execution_date))
            else:
                qry = qry.filter(DR.execution_date == execution_date)
        if state:
            qry = qry.filter(DR.state == state)
        if external_trigger is not None:
            qry = qry.filter(DR.external_trigger == external_trigger)
        if no_backfills:
            # in order to prevent a circular dependency
            from airflow.jobs import BackfillJob
            qry = qry.filter(DR.run_id.notlike(BackfillJob.ID_PREFIX + '%'))

        dr = qry.order_by(DR.execution_date).all()

        return dr","['def', 'find', '(', 'dag_id', '=', 'None', ',', 'run_id', '=', 'None', ',', 'execution_date', '=', 'None', ',', 'state', '=', 'None', ',', 'external_trigger', '=', 'None', ',', 'no_backfills', '=', 'False', ',', 'session', '=', 'None', ')', ':', 'DR', '=', 'DagRun', 'qry', '=', 'session', '.', 'query', '(', 'DR', ')', 'if', 'dag_id', ':', 'qry', '=', 'qry', '.', 'filter', '(', 'DR', '.', 'dag_id', '==', 'dag_id', ')', 'if', 'run_id', ':', 'qry', '=', 'qry', '.', 'filter', '(', 'DR', '.', 'run_id', '==', 'run_id', ')', 'if', 'execution_date', ':', 'if', 'isinstance', '(', 'execution_date', ',', 'list', ')', ':', 'qry', '=', 'qry', '.', 'filter', '(', 'DR', '.', 'execution_date', '.', 'in_', '(', 'execution_date', ')', ')', 'else', ':', 'qry', '=', 'qry', '.', 'filter', '(', 'DR', '.', 'execution_date', '==', 'execution_date', ')', 'if', 'state', ':', 'qry', '=', 'qry', '.', 'filter', '(', 'DR', '.', 'state', '==', 'state', ')', 'if', 'external_trigger', 'is', 'not', 'None', ':', 'qry', '=', 'qry', '.', 'filter', '(', 'DR', '.', 'external_trigger', '==', 'external_trigger', ')', 'if', 'no_backfills', ':', '# in order to prevent a circular dependency', 'from', 'airflow', '.', 'jobs', 'import', 'BackfillJob', 'qry', '=', 'qry', '.', 'filter', '(', 'DR', '.', 'run_id', '.', 'notlike', '(', 'BackfillJob', '.', 'ID_PREFIX', '+', ""'%'"", ')', ')', 'dr', '=', 'qry', '.', 'order_by', '(', 'DR', '.', 'execution_date', ')', '.', 'all', '(', ')', 'return', 'dr']","Returns a set of dag runs for the given search criteria.

        :param dag_id: the dag_id to find dag runs for
        :type dag_id: int, list
        :param run_id: defines the the run id for this dag run
        :type run_id: str
        :param execution_date: the execution date
        :type execution_date: datetime.datetime
        :param state: the state of the dag run
        :type state: airflow.utils.state.State
        :param external_trigger: whether this dag run is externally triggered
        :type external_trigger: bool
        :param no_backfills: return no backfills (True), return all (False).
            Defaults to False
        :type no_backfills: bool
        :param session: database session
        :type session: sqlalchemy.orm.session.Session","['Returns', 'a', 'set', 'of', 'dag', 'runs', 'for', 'the', 'given', 'search', 'criteria', '.']",python,test,"['returns', 'a', 'set', 'of', 'dag', 'runs', 'for', 'the', 'given', 'search', 'criteria', '.']",returns a set of dag runs for the given search criteria .,"['def', 'find', '(', 'dag_id', '=', 'none', ',', 'run_id', '=', 'none', ',', 'execution_date', '=', 'none', ',', 'state', '=', 'none', ',', 'external_trigger', '=', 'none', ',', 'no_backfills', '=', 'false', ',', 'session', '=', 'none', ')', ':', 'dr', '=', 'dagrun', 'qry', '=', 'session', '.', 'query', '(', 'dr', ')', 'if', 'dag_id', ':', 'qry', '=', 'qry', '.', 'filter', '(', 'dr', '.', 'dag_id', '==', 'dag_id', ')', 'if', 'run_id', ':', 'qry', '=', 'qry', '.', 'filter', '(', 'dr', '.', 'run_id', '==', 'run_id', ')', 'if', 'execution_date', ':', 'if', 'isinstance', '(', 'execution_date', ',', 'list', ')', ':', 'qry', '=', 'qry', '.', 'filter', '(', 'dr', '.', 'execution_date', '.', 'in_', '(', 'execution_date', ')', ')', 'else', ':', 'qry', '=', 'qry', '.', 'filter', '(', 'dr', '.', 'execution_date', '==', 'execution_date', ')', 'if', 'state', ':', 'qry', '=', 'qry', '.', 'filter', '(', 'dr', '.', 'state', '==', 'state', ')', 'if', 'external_trigger', 'is', 'not', 'none', ':', 'qry', '=', 'qry', '.', 'filter', '(', 'dr', '.', 'external_trigger', '==', 'external_trigger', ')', 'if', 'no_backfills', ':', '# in order to prevent a circular dependency', 'from', 'airflow', '.', 'jobs', 'import', 'backfilljob', 'qry', '=', 'qry', '.', 'filter', '(', 'dr', '.', 'run_id', '.', 'notlike', '(', 'backfilljob', '.', 'id_prefix', '+', ""'%'"", ')', ')', 'dr', '=', 'qry', '.', 'order_by', '(', 'dr', '.', 'execution_date', ')', '.', 'all', '(', ')', 'return', 'dr']","def find ( dag_id = none , run_id = none , execution_date = none , state = none , external_trigger = none , no_backfills = false , session = none ) : dr = dagrun qry = session . query ( dr ) if dag_id : qry = qry . filter ( dr . dag_id == dag_id ) if run_id : qry = qry . filter ( dr . run_id == run_id ) if execution_date : if isinstance ( execution_date , list ) : qry = qry . filter ( dr . execution_date . in_ ( execution_date ) ) else : qry = qry . filter ( dr . execution_date == execution_date ) if state : qry = qry . filter ( dr . state == state ) if external_trigger is not none : qry = qry . filter ( dr . external_trigger == external_trigger ) if no_backfills : # in order to prevent a circular dependency from airflow . jobs import backfilljob qry = qry . filter ( dr . run_id . notlike ( backfilljob . id_prefix + '%' ) ) dr = qry . order_by ( dr . execution_date ) . all ( ) return dr"
310,apache/airflow,airflow/models/dagrun.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagrun.py#L163-L188,"def get_task_instances(self, state=None, session=None):
        """"""
        Returns the task instances for this dag run
        """"""
        from airflow.models.taskinstance import TaskInstance  # Avoid circular import
        tis = session.query(TaskInstance).filter(
            TaskInstance.dag_id == self.dag_id,
            TaskInstance.execution_date == self.execution_date,
        )
        if state:
            if isinstance(state, six.string_types):
                tis = tis.filter(TaskInstance.state == state)
            else:
                # this is required to deal with NULL values
                if None in state:
                    tis = tis.filter(
                        or_(TaskInstance.state.in_(state),
                            TaskInstance.state.is_(None))
                    )
                else:
                    tis = tis.filter(TaskInstance.state.in_(state))

        if self.dag and self.dag.partial:
            tis = tis.filter(TaskInstance.task_id.in_(self.dag.task_ids))

        return tis.all()","['def', 'get_task_instances', '(', 'self', ',', 'state', '=', 'None', ',', 'session', '=', 'None', ')', ':', 'from', 'airflow', '.', 'models', '.', 'taskinstance', 'import', 'TaskInstance', '# Avoid circular import', 'tis', '=', 'session', '.', 'query', '(', 'TaskInstance', ')', '.', 'filter', '(', 'TaskInstance', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'TaskInstance', '.', 'execution_date', '==', 'self', '.', 'execution_date', ',', ')', 'if', 'state', ':', 'if', 'isinstance', '(', 'state', ',', 'six', '.', 'string_types', ')', ':', 'tis', '=', 'tis', '.', 'filter', '(', 'TaskInstance', '.', 'state', '==', 'state', ')', 'else', ':', '# this is required to deal with NULL values', 'if', 'None', 'in', 'state', ':', 'tis', '=', 'tis', '.', 'filter', '(', 'or_', '(', 'TaskInstance', '.', 'state', '.', 'in_', '(', 'state', ')', ',', 'TaskInstance', '.', 'state', '.', 'is_', '(', 'None', ')', ')', ')', 'else', ':', 'tis', '=', 'tis', '.', 'filter', '(', 'TaskInstance', '.', 'state', '.', 'in_', '(', 'state', ')', ')', 'if', 'self', '.', 'dag', 'and', 'self', '.', 'dag', '.', 'partial', ':', 'tis', '=', 'tis', '.', 'filter', '(', 'TaskInstance', '.', 'task_id', '.', 'in_', '(', 'self', '.', 'dag', '.', 'task_ids', ')', ')', 'return', 'tis', '.', 'all', '(', ')']",Returns the task instances for this dag run,"['Returns', 'the', 'task', 'instances', 'for', 'this', 'dag', 'run']",python,test,"['returns', 'the', 'task', 'instances', 'for', 'this', 'dag', 'run']",returns the task instances for this dag run,"['def', 'get_task_instances', '(', 'self', ',', 'state', '=', 'none', ',', 'session', '=', 'none', ')', ':', 'from', 'airflow', '.', 'models', '.', 'taskinstance', 'import', 'taskinstance', '# avoid circular import', 'tis', '=', 'session', '.', 'query', '(', 'taskinstance', ')', '.', 'filter', '(', 'taskinstance', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'taskinstance', '.', 'execution_date', '==', 'self', '.', 'execution_date', ',', ')', 'if', 'state', ':', 'if', 'isinstance', '(', 'state', ',', 'six', '.', 'string_types', ')', ':', 'tis', '=', 'tis', '.', 'filter', '(', 'taskinstance', '.', 'state', '==', 'state', ')', 'else', ':', '# this is required to deal with null values', 'if', 'none', 'in', 'state', ':', 'tis', '=', 'tis', '.', 'filter', '(', 'or_', '(', 'taskinstance', '.', 'state', '.', 'in_', '(', 'state', ')', ',', 'taskinstance', '.', 'state', '.', 'is_', '(', 'none', ')', ')', ')', 'else', ':', 'tis', '=', 'tis', '.', 'filter', '(', 'taskinstance', '.', 'state', '.', 'in_', '(', 'state', ')', ')', 'if', 'self', '.', 'dag', 'and', 'self', '.', 'dag', '.', 'partial', ':', 'tis', '=', 'tis', '.', 'filter', '(', 'taskinstance', '.', 'task_id', '.', 'in_', '(', 'self', '.', 'dag', '.', 'task_ids', ')', ')', 'return', 'tis', '.', 'all', '(', ')']","def get_task_instances ( self , state = none , session = none ) : from airflow . models . taskinstance import taskinstance # avoid circular import tis = session . query ( taskinstance ) . filter ( taskinstance . dag_id == self . dag_id , taskinstance . execution_date == self . execution_date , ) if state : if isinstance ( state , six . string_types ) : tis = tis . filter ( taskinstance . state == state ) else : # this is required to deal with null values if none in state : tis = tis . filter ( or_ ( taskinstance . state . in_ ( state ) , taskinstance . state . is_ ( none ) ) ) else : tis = tis . filter ( taskinstance . state . in_ ( state ) ) if self . dag and self . dag . partial : tis = tis . filter ( taskinstance . task_id . in_ ( self . dag . task_ids ) ) return tis . all ( )"
311,apache/airflow,airflow/models/dagrun.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagrun.py#L191-L206,"def get_task_instance(self, task_id, session=None):
        """"""
        Returns the task instance specified by task_id for this dag run

        :param task_id: the task id
        """"""

        from airflow.models.taskinstance import TaskInstance  # Avoid circular import
        TI = TaskInstance
        ti = session.query(TI).filter(
            TI.dag_id == self.dag_id,
            TI.execution_date == self.execution_date,
            TI.task_id == task_id
        ).first()

        return ti","['def', 'get_task_instance', '(', 'self', ',', 'task_id', ',', 'session', '=', 'None', ')', ':', 'from', 'airflow', '.', 'models', '.', 'taskinstance', 'import', 'TaskInstance', '# Avoid circular import', 'TI', '=', 'TaskInstance', 'ti', '=', 'session', '.', 'query', '(', 'TI', ')', '.', 'filter', '(', 'TI', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'TI', '.', 'execution_date', '==', 'self', '.', 'execution_date', ',', 'TI', '.', 'task_id', '==', 'task_id', ')', '.', 'first', '(', ')', 'return', 'ti']","Returns the task instance specified by task_id for this dag run

        :param task_id: the task id","['Returns', 'the', 'task', 'instance', 'specified', 'by', 'task_id', 'for', 'this', 'dag', 'run']",python,test,"['returns', 'the', 'task', 'instance', 'specified', 'by', 'task_id', 'for', 'this', 'dag', 'run']",returns the task instance specified by task_id for this dag run,"['def', 'get_task_instance', '(', 'self', ',', 'task_id', ',', 'session', '=', 'none', ')', ':', 'from', 'airflow', '.', 'models', '.', 'taskinstance', 'import', 'taskinstance', '# avoid circular import', 'ti', '=', 'taskinstance', 'ti', '=', 'session', '.', 'query', '(', 'ti', ')', '.', 'filter', '(', 'ti', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'ti', '.', 'execution_date', '==', 'self', '.', 'execution_date', ',', 'ti', '.', 'task_id', '==', 'task_id', ')', '.', 'first', '(', ')', 'return', 'ti']","def get_task_instance ( self , task_id , session = none ) : from airflow . models . taskinstance import taskinstance # avoid circular import ti = taskinstance ti = session . query ( ti ) . filter ( ti . dag_id == self . dag_id , ti . execution_date == self . execution_date , ti . task_id == task_id ) . first ( ) return ti"
312,apache/airflow,airflow/models/dagrun.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagrun.py#L221-L229,"def get_previous_dagrun(self, session=None):
        """"""The previous DagRun, if there is one""""""

        return session.query(DagRun).filter(
            DagRun.dag_id == self.dag_id,
            DagRun.execution_date < self.execution_date
        ).order_by(
            DagRun.execution_date.desc()
        ).first()","['def', 'get_previous_dagrun', '(', 'self', ',', 'session', '=', 'None', ')', ':', 'return', 'session', '.', 'query', '(', 'DagRun', ')', '.', 'filter', '(', 'DagRun', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'DagRun', '.', 'execution_date', '<', 'self', '.', 'execution_date', ')', '.', 'order_by', '(', 'DagRun', '.', 'execution_date', '.', 'desc', '(', ')', ')', '.', 'first', '(', ')']","The previous DagRun, if there is one","['The', 'previous', 'DagRun', 'if', 'there', 'is', 'one']",python,test,"['the', 'previous', 'dagrun', 'if', 'there', 'is', 'one']",the previous dagrun if there is one,"['def', 'get_previous_dagrun', '(', 'self', ',', 'session', '=', 'none', ')', ':', 'return', 'session', '.', 'query', '(', 'dagrun', ')', '.', 'filter', '(', 'dagrun', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'dagrun', '.', 'execution_date', '<', 'self', '.', 'execution_date', ')', '.', 'order_by', '(', 'dagrun', '.', 'execution_date', '.', 'desc', '(', ')', ')', '.', 'first', '(', ')']","def get_previous_dagrun ( self , session = none ) : return session . query ( dagrun ) . filter ( dagrun . dag_id == self . dag_id , dagrun . execution_date < self . execution_date ) . order_by ( dagrun . execution_date . desc ( ) ) . first ( )"
313,apache/airflow,airflow/models/dagrun.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagrun.py#L232-L239,"def get_previous_scheduled_dagrun(self, session=None):
        """"""The previous, SCHEDULED DagRun, if there is one""""""
        dag = self.get_dag()

        return session.query(DagRun).filter(
            DagRun.dag_id == self.dag_id,
            DagRun.execution_date == dag.previous_schedule(self.execution_date)
        ).first()","['def', 'get_previous_scheduled_dagrun', '(', 'self', ',', 'session', '=', 'None', ')', ':', 'dag', '=', 'self', '.', 'get_dag', '(', ')', 'return', 'session', '.', 'query', '(', 'DagRun', ')', '.', 'filter', '(', 'DagRun', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'DagRun', '.', 'execution_date', '==', 'dag', '.', 'previous_schedule', '(', 'self', '.', 'execution_date', ')', ')', '.', 'first', '(', ')']","The previous, SCHEDULED DagRun, if there is one","['The', 'previous', 'SCHEDULED', 'DagRun', 'if', 'there', 'is', 'one']",python,test,"['the', 'previous', 'scheduled', 'dagrun', 'if', 'there', 'is', 'one']",the previous scheduled dagrun if there is one,"['def', 'get_previous_scheduled_dagrun', '(', 'self', ',', 'session', '=', 'none', ')', ':', 'dag', '=', 'self', '.', 'get_dag', '(', ')', 'return', 'session', '.', 'query', '(', 'dagrun', ')', '.', 'filter', '(', 'dagrun', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'dagrun', '.', 'execution_date', '==', 'dag', '.', 'previous_schedule', '(', 'self', '.', 'execution_date', ')', ')', '.', 'first', '(', ')']","def get_previous_scheduled_dagrun ( self , session = none ) : dag = self . get_dag ( ) return session . query ( dagrun ) . filter ( dagrun . dag_id == self . dag_id , dagrun . execution_date == dag . previous_schedule ( self . execution_date ) ) . first ( )"
314,apache/airflow,airflow/models/dagrun.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagrun.py#L242-L329,"def update_state(self, session=None):
        """"""
        Determines the overall state of the DagRun based on the state
        of its TaskInstances.

        :return: State
        """"""

        dag = self.get_dag()

        tis = self.get_task_instances(session=session)
        self.log.debug(""Updating state for %s considering %s task(s)"", self, len(tis))

        for ti in list(tis):
            # skip in db?
            if ti.state == State.REMOVED:
                tis.remove(ti)
            else:
                ti.task = dag.get_task(ti.task_id)

        # pre-calculate
        # db is faster
        start_dttm = timezone.utcnow()
        unfinished_tasks = self.get_task_instances(
            state=State.unfinished(),
            session=session
        )
        none_depends_on_past = all(not t.task.depends_on_past for t in unfinished_tasks)
        none_task_concurrency = all(t.task.task_concurrency is None
                                    for t in unfinished_tasks)
        # small speed up
        if unfinished_tasks and none_depends_on_past and none_task_concurrency:
            # todo: this can actually get pretty slow: one task costs between 0.01-015s
            no_dependencies_met = True
            for ut in unfinished_tasks:
                # We need to flag upstream and check for changes because upstream
                # failures/re-schedules can result in deadlock false positives
                old_state = ut.state
                deps_met = ut.are_dependencies_met(
                    dep_context=DepContext(
                        flag_upstream_failed=True,
                        ignore_in_retry_period=True,
                        ignore_in_reschedule_period=True),
                    session=session)
                if deps_met or old_state != ut.current_state(session=session):
                    no_dependencies_met = False
                    break

        duration = (timezone.utcnow() - start_dttm).total_seconds() * 1000
        Stats.timing(""dagrun.dependency-check.{}"".format(self.dag_id), duration)

        root_ids = [t.task_id for t in dag.roots]
        roots = [t for t in tis if t.task_id in root_ids]

        # if all roots finished and at least one failed, the run failed
        if (not unfinished_tasks and
                any(r.state in (State.FAILED, State.UPSTREAM_FAILED) for r in roots)):
            self.log.info('Marking run %s failed', self)
            self.set_state(State.FAILED)
            dag.handle_callback(self, success=False, reason='task_failure',
                                session=session)

        # if all roots succeeded and no unfinished tasks, the run succeeded
        elif not unfinished_tasks and all(r.state in (State.SUCCESS, State.SKIPPED)
                                          for r in roots):
            self.log.info('Marking run %s successful', self)
            self.set_state(State.SUCCESS)
            dag.handle_callback(self, success=True, reason='success', session=session)

        # if *all tasks* are deadlocked, the run failed
        elif (unfinished_tasks and none_depends_on_past and
              none_task_concurrency and no_dependencies_met):
            self.log.info('Deadlock; marking run %s failed', self)
            self.set_state(State.FAILED)
            dag.handle_callback(self, success=False, reason='all_tasks_deadlocked',
                                session=session)

        # finally, if the roots aren't done, the dag is still running
        else:
            self.set_state(State.RUNNING)

        self._emit_duration_stats_for_finished_state()

        # todo: determine we want to use with_for_update to make sure to lock the run
        session.merge(self)
        session.commit()

        return self.state","['def', 'update_state', '(', 'self', ',', 'session', '=', 'None', ')', ':', 'dag', '=', 'self', '.', 'get_dag', '(', ')', 'tis', '=', 'self', '.', 'get_task_instances', '(', 'session', '=', 'session', ')', 'self', '.', 'log', '.', 'debug', '(', '""Updating state for %s considering %s task(s)""', ',', 'self', ',', 'len', '(', 'tis', ')', ')', 'for', 'ti', 'in', 'list', '(', 'tis', ')', ':', '# skip in db?', 'if', 'ti', '.', 'state', '==', 'State', '.', 'REMOVED', ':', 'tis', '.', 'remove', '(', 'ti', ')', 'else', ':', 'ti', '.', 'task', '=', 'dag', '.', 'get_task', '(', 'ti', '.', 'task_id', ')', '# pre-calculate', '# db is faster', 'start_dttm', '=', 'timezone', '.', 'utcnow', '(', ')', 'unfinished_tasks', '=', 'self', '.', 'get_task_instances', '(', 'state', '=', 'State', '.', 'unfinished', '(', ')', ',', 'session', '=', 'session', ')', 'none_depends_on_past', '=', 'all', '(', 'not', 't', '.', 'task', '.', 'depends_on_past', 'for', 't', 'in', 'unfinished_tasks', ')', 'none_task_concurrency', '=', 'all', '(', 't', '.', 'task', '.', 'task_concurrency', 'is', 'None', 'for', 't', 'in', 'unfinished_tasks', ')', '# small speed up', 'if', 'unfinished_tasks', 'and', 'none_depends_on_past', 'and', 'none_task_concurrency', ':', '# todo: this can actually get pretty slow: one task costs between 0.01-015s', 'no_dependencies_met', '=', 'True', 'for', 'ut', 'in', 'unfinished_tasks', ':', '# We need to flag upstream and check for changes because upstream', '# failures/re-schedules can result in deadlock false positives', 'old_state', '=', 'ut', '.', 'state', 'deps_met', '=', 'ut', '.', 'are_dependencies_met', '(', 'dep_context', '=', 'DepContext', '(', 'flag_upstream_failed', '=', 'True', ',', 'ignore_in_retry_period', '=', 'True', ',', 'ignore_in_reschedule_period', '=', 'True', ')', ',', 'session', '=', 'session', ')', 'if', 'deps_met', 'or', 'old_state', '!=', 'ut', '.', 'current_state', '(', 'session', '=', 'session', ')', ':', 'no_dependencies_met', '=', 'False', 'break', 'duration', '=', '(', 'timezone', '.', 'utcnow', '(', ')', '-', 'start_dttm', ')', '.', 'total_seconds', '(', ')', '*', '1000', 'Stats', '.', 'timing', '(', '""dagrun.dependency-check.{}""', '.', 'format', '(', 'self', '.', 'dag_id', ')', ',', 'duration', ')', 'root_ids', '=', '[', 't', '.', 'task_id', 'for', 't', 'in', 'dag', '.', 'roots', ']', 'roots', '=', '[', 't', 'for', 't', 'in', 'tis', 'if', 't', '.', 'task_id', 'in', 'root_ids', ']', '# if all roots finished and at least one failed, the run failed', 'if', '(', 'not', 'unfinished_tasks', 'and', 'any', '(', 'r', '.', 'state', 'in', '(', 'State', '.', 'FAILED', ',', 'State', '.', 'UPSTREAM_FAILED', ')', 'for', 'r', 'in', 'roots', ')', ')', ':', 'self', '.', 'log', '.', 'info', '(', ""'Marking run %s failed'"", ',', 'self', ')', 'self', '.', 'set_state', '(', 'State', '.', 'FAILED', ')', 'dag', '.', 'handle_callback', '(', 'self', ',', 'success', '=', 'False', ',', 'reason', '=', ""'task_failure'"", ',', 'session', '=', 'session', ')', '# if all roots succeeded and no unfinished tasks, the run succeeded', 'elif', 'not', 'unfinished_tasks', 'and', 'all', '(', 'r', '.', 'state', 'in', '(', 'State', '.', 'SUCCESS', ',', 'State', '.', 'SKIPPED', ')', 'for', 'r', 'in', 'roots', ')', ':', 'self', '.', 'log', '.', 'info', '(', ""'Marking run %s successful'"", ',', 'self', ')', 'self', '.', 'set_state', '(', 'State', '.', 'SUCCESS', ')', 'dag', '.', 'handle_callback', '(', 'self', ',', 'success', '=', 'True', ',', 'reason', '=', ""'success'"", ',', 'session', '=', 'session', ')', '# if *all tasks* are deadlocked, the run failed', 'elif', '(', 'unfinished_tasks', 'and', 'none_depends_on_past', 'and', 'none_task_concurrency', 'and', 'no_dependencies_met', ')', ':', 'self', '.', 'log', '.', 'info', '(', ""'Deadlock; marking run %s failed'"", ',', 'self', ')', 'self', '.', 'set_state', '(', 'State', '.', 'FAILED', ')', 'dag', '.', 'handle_callback', '(', 'self', ',', 'success', '=', 'False', ',', 'reason', '=', ""'all_tasks_deadlocked'"", ',', 'session', '=', 'session', ')', ""# finally, if the roots aren't done, the dag is still running"", 'else', ':', 'self', '.', 'set_state', '(', 'State', '.', 'RUNNING', ')', 'self', '.', '_emit_duration_stats_for_finished_state', '(', ')', '# todo: determine we want to use with_for_update to make sure to lock the run', 'session', '.', 'merge', '(', 'self', ')', 'session', '.', 'commit', '(', ')', 'return', 'self', '.', 'state']","Determines the overall state of the DagRun based on the state
        of its TaskInstances.

        :return: State","['Determines', 'the', 'overall', 'state', 'of', 'the', 'DagRun', 'based', 'on', 'the', 'state', 'of', 'its', 'TaskInstances', '.']",python,test,"['determines', 'the', 'overall', 'state', 'of', 'the', 'dagrun', 'based', 'on', 'the', 'state', 'of', 'its', 'taskinstances', '.']",determines the overall state of the dagrun based on the state of its taskinstances .,"['def', 'update_state', '(', 'self', ',', 'session', '=', 'none', ')', ':', 'dag', '=', 'self', '.', 'get_dag', '(', ')', 'tis', '=', 'self', '.', 'get_task_instances', '(', 'session', '=', 'session', ')', 'self', '.', 'log', '.', 'debug', '(', '""updating state for %s considering %s task(s)""', ',', 'self', ',', 'len', '(', 'tis', ')', ')', 'for', 'ti', 'in', 'list', '(', 'tis', ')', ':', '# skip in db?', 'if', 'ti', '.', 'state', '==', 'state', '.', 'removed', ':', 'tis', '.', 'remove', '(', 'ti', ')', 'else', ':', 'ti', '.', 'task', '=', 'dag', '.', 'get_task', '(', 'ti', '.', 'task_id', ')', '# pre-calculate', '# db is faster', 'start_dttm', '=', 'timezone', '.', 'utcnow', '(', ')', 'unfinished_tasks', '=', 'self', '.', 'get_task_instances', '(', 'state', '=', 'state', '.', 'unfinished', '(', ')', ',', 'session', '=', 'session', ')', 'none_depends_on_past', '=', 'all', '(', 'not', 't', '.', 'task', '.', 'depends_on_past', 'for', 't', 'in', 'unfinished_tasks', ')', 'none_task_concurrency', '=', 'all', '(', 't', '.', 'task', '.', 'task_concurrency', 'is', 'none', 'for', 't', 'in', 'unfinished_tasks', ')', '# small speed up', 'if', 'unfinished_tasks', 'and', 'none_depends_on_past', 'and', 'none_task_concurrency', ':', '# todo: this can actually get pretty slow: one task costs between 0.01-015s', 'no_dependencies_met', '=', 'true', 'for', 'ut', 'in', 'unfinished_tasks', ':', '# we need to flag upstream and check for changes because upstream', '# failures/re-schedules can result in deadlock false positives', 'old_state', '=', 'ut', '.', 'state', 'deps_met', '=', 'ut', '.', 'are_dependencies_met', '(', 'dep_context', '=', 'depcontext', '(', 'flag_upstream_failed', '=', 'true', ',', 'ignore_in_retry_period', '=', 'true', ',', 'ignore_in_reschedule_period', '=', 'true', ')', ',', 'session', '=', 'session', ')', 'if', 'deps_met', 'or', 'old_state', '!=', 'ut', '.', 'current_state', '(', 'session', '=', 'session', ')', ':', 'no_dependencies_met', '=', 'false', 'break', 'duration', '=', '(', 'timezone', '.', 'utcnow', '(', ')', '-', 'start_dttm', ')', '.', 'total_seconds', '(', ')', '*', '1000', 'stats', '.', 'timing', '(', '""dagrun.dependency-check.{}""', '.', 'format', '(', 'self', '.', 'dag_id', ')', ',', 'duration', ')', 'root_ids', '=', '[', 't', '.', 'task_id', 'for', 't', 'in', 'dag', '.', 'roots', ']', 'roots', '=', '[', 't', 'for', 't', 'in', 'tis', 'if', 't', '.', 'task_id', 'in', 'root_ids', ']', '# if all roots finished and at least one failed, the run failed', 'if', '(', 'not', 'unfinished_tasks', 'and', 'any', '(', 'r', '.', 'state', 'in', '(', 'state', '.', 'failed', ',', 'state', '.', 'upstream_failed', ')', 'for', 'r', 'in', 'roots', ')', ')', ':', 'self', '.', 'log', '.', 'info', '(', ""'marking run %s failed'"", ',', 'self', ')', 'self', '.', 'set_state', '(', 'state', '.', 'failed', ')', 'dag', '.', 'handle_callback', '(', 'self', ',', 'success', '=', 'false', ',', 'reason', '=', ""'task_failure'"", ',', 'session', '=', 'session', ')', '# if all roots succeeded and no unfinished tasks, the run succeeded', 'elif', 'not', 'unfinished_tasks', 'and', 'all', '(', 'r', '.', 'state', 'in', '(', 'state', '.', 'success', ',', 'state', '.', 'skipped', ')', 'for', 'r', 'in', 'roots', ')', ':', 'self', '.', 'log', '.', 'info', '(', ""'marking run %s successful'"", ',', 'self', ')', 'self', '.', 'set_state', '(', 'state', '.', 'success', ')', 'dag', '.', 'handle_callback', '(', 'self', ',', 'success', '=', 'true', ',', 'reason', '=', ""'success'"", ',', 'session', '=', 'session', ')', '# if *all tasks* are deadlocked, the run failed', 'elif', '(', 'unfinished_tasks', 'and', 'none_depends_on_past', 'and', 'none_task_concurrency', 'and', 'no_dependencies_met', ')', ':', 'self', '.', 'log', '.', 'info', '(', ""'deadlock; marking run %s failed'"", ',', 'self', ')', 'self', '.', 'set_state', '(', 'state', '.', 'failed', ')', 'dag', '.', 'handle_callback', '(', 'self', ',', 'success', '=', 'false', ',', 'reason', '=', ""'all_tasks_deadlocked'"", ',', 'session', '=', 'session', ')', ""# finally, if the roots aren't done, the dag is still running"", 'else', ':', 'self', '.', 'set_state', '(', 'state', '.', 'running', ')', 'self', '.', '_emit_duration_stats_for_finished_state', '(', ')', '# todo: determine we want to use with_for_update to make sure to lock the run', 'session', '.', 'merge', '(', 'self', ')', 'session', '.', 'commit', '(', ')', 'return', 'self', '.', 'state']","def update_state ( self , session = none ) : dag = self . get_dag ( ) tis = self . get_task_instances ( session = session ) self . log . debug ( ""updating state for %s considering %s task(s)"" , self , len ( tis ) ) for ti in list ( tis ) : # skip in db? if ti . state == state . removed : tis . remove ( ti ) else : ti . task = dag . get_task ( ti . task_id ) # pre-calculate # db is faster start_dttm = timezone . utcnow ( ) unfinished_tasks = self . get_task_instances ( state = state . unfinished ( ) , session = session ) none_depends_on_past = all ( not t . task . depends_on_past for t in unfinished_tasks ) none_task_concurrency = all ( t . task . task_concurrency is none for t in unfinished_tasks ) # small speed up if unfinished_tasks and none_depends_on_past and none_task_concurrency : # todo: this can actually get pretty slow: one task costs between 0.01-015s no_dependencies_met = true for ut in unfinished_tasks : # we need to flag upstream and check for changes because upstream # failures/re-schedules can result in deadlock false positives old_state = ut . state deps_met = ut . are_dependencies_met ( dep_context = depcontext ( flag_upstream_failed = true , ignore_in_retry_period = true , ignore_in_reschedule_period = true ) , session = session ) if deps_met or old_state != ut . current_state ( session = session ) : no_dependencies_met = false break duration = ( timezone . utcnow ( ) - start_dttm ) . total_seconds ( ) * 1000 stats . timing ( ""dagrun.dependency-check.{}"" . format ( self . dag_id ) , duration ) root_ids = [ t . task_id for t in dag . roots ] roots = [ t for t in tis if t . task_id in root_ids ] # if all roots finished and at least one failed, the run failed if ( not unfinished_tasks and any ( r . state in ( state . failed , state . upstream_failed ) for r in roots ) ) : self . log . info ( 'marking run %s failed' , self ) self . set_state ( state . failed ) dag . handle_callback ( self , success = false , reason = 'task_failure' , session = session ) # if all roots succeeded and no unfinished tasks, the run succeeded elif not unfinished_tasks and all ( r . state in ( state . success , state . skipped ) for r in roots ) : self . log . info ( 'marking run %s successful' , self ) self . set_state ( state . success ) dag . handle_callback ( self , success = true , reason = 'success' , session = session ) # if *all tasks* are deadlocked, the run failed elif ( unfinished_tasks and none_depends_on_past and none_task_concurrency and no_dependencies_met ) : self . log . info ( 'deadlock; marking run %s failed' , self ) self . set_state ( state . failed ) dag . handle_callback ( self , success = false , reason = 'all_tasks_deadlocked' , session = session ) # finally, if the roots aren't done, the dag is still running else : self . set_state ( state . running ) self . _emit_duration_stats_for_finished_state ( ) # todo: determine we want to use with_for_update to make sure to lock the run session . merge ( self ) session . commit ( ) return self . state"
315,apache/airflow,airflow/models/dagrun.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagrun.py#L342-L389,"def verify_integrity(self, session=None):
        """"""
        Verifies the DagRun by checking for removed tasks or tasks that are not in the
        database yet. It will set state to removed or add the task if required.
        """"""
        from airflow.models.taskinstance import TaskInstance  # Avoid circular import

        dag = self.get_dag()
        tis = self.get_task_instances(session=session)

        # check for removed or restored tasks
        task_ids = []
        for ti in tis:
            task_ids.append(ti.task_id)
            task = None
            try:
                task = dag.get_task(ti.task_id)
            except AirflowException:
                if ti.state == State.REMOVED:
                    pass  # ti has already been removed, just ignore it
                elif self.state is not State.RUNNING and not dag.partial:
                    self.log.warning(""Failed to get task '{}' for dag '{}'. ""
                                     ""Marking it as removed."".format(ti, dag))
                    Stats.incr(
                        ""task_removed_from_dag.{}"".format(dag.dag_id), 1, 1)
                    ti.state = State.REMOVED

            is_task_in_dag = task is not None
            should_restore_task = is_task_in_dag and ti.state == State.REMOVED
            if should_restore_task:
                self.log.info(""Restoring task '{}' which was previously ""
                              ""removed from DAG '{}'"".format(ti, dag))
                Stats.incr(""task_restored_to_dag.{}"".format(dag.dag_id), 1, 1)
                ti.state = State.NONE

        # check for missing tasks
        for task in six.itervalues(dag.task_dict):
            if task.start_date > self.execution_date and not self.is_backfill:
                continue

            if task.task_id not in task_ids:
                Stats.incr(
                    ""task_instance_created-{}"".format(task.__class__.__name__),
                    1, 1)
                ti = TaskInstance(task, self.execution_date)
                session.add(ti)

        session.commit()","['def', 'verify_integrity', '(', 'self', ',', 'session', '=', 'None', ')', ':', 'from', 'airflow', '.', 'models', '.', 'taskinstance', 'import', 'TaskInstance', '# Avoid circular import', 'dag', '=', 'self', '.', 'get_dag', '(', ')', 'tis', '=', 'self', '.', 'get_task_instances', '(', 'session', '=', 'session', ')', '# check for removed or restored tasks', 'task_ids', '=', '[', ']', 'for', 'ti', 'in', 'tis', ':', 'task_ids', '.', 'append', '(', 'ti', '.', 'task_id', ')', 'task', '=', 'None', 'try', ':', 'task', '=', 'dag', '.', 'get_task', '(', 'ti', '.', 'task_id', ')', 'except', 'AirflowException', ':', 'if', 'ti', '.', 'state', '==', 'State', '.', 'REMOVED', ':', 'pass', '# ti has already been removed, just ignore it', 'elif', 'self', '.', 'state', 'is', 'not', 'State', '.', 'RUNNING', 'and', 'not', 'dag', '.', 'partial', ':', 'self', '.', 'log', '.', 'warning', '(', '""Failed to get task \'{}\' for dag \'{}\'. ""', '""Marking it as removed.""', '.', 'format', '(', 'ti', ',', 'dag', ')', ')', 'Stats', '.', 'incr', '(', '""task_removed_from_dag.{}""', '.', 'format', '(', 'dag', '.', 'dag_id', ')', ',', '1', ',', '1', ')', 'ti', '.', 'state', '=', 'State', '.', 'REMOVED', 'is_task_in_dag', '=', 'task', 'is', 'not', 'None', 'should_restore_task', '=', 'is_task_in_dag', 'and', 'ti', '.', 'state', '==', 'State', '.', 'REMOVED', 'if', 'should_restore_task', ':', 'self', '.', 'log', '.', 'info', '(', '""Restoring task \'{}\' which was previously ""', '""removed from DAG \'{}\'""', '.', 'format', '(', 'ti', ',', 'dag', ')', ')', 'Stats', '.', 'incr', '(', '""task_restored_to_dag.{}""', '.', 'format', '(', 'dag', '.', 'dag_id', ')', ',', '1', ',', '1', ')', 'ti', '.', 'state', '=', 'State', '.', 'NONE', '# check for missing tasks', 'for', 'task', 'in', 'six', '.', 'itervalues', '(', 'dag', '.', 'task_dict', ')', ':', 'if', 'task', '.', 'start_date', '>', 'self', '.', 'execution_date', 'and', 'not', 'self', '.', 'is_backfill', ':', 'continue', 'if', 'task', '.', 'task_id', 'not', 'in', 'task_ids', ':', 'Stats', '.', 'incr', '(', '""task_instance_created-{}""', '.', 'format', '(', 'task', '.', '__class__', '.', '__name__', ')', ',', '1', ',', '1', ')', 'ti', '=', 'TaskInstance', '(', 'task', ',', 'self', '.', 'execution_date', ')', 'session', '.', 'add', '(', 'ti', ')', 'session', '.', 'commit', '(', ')']","Verifies the DagRun by checking for removed tasks or tasks that are not in the
        database yet. It will set state to removed or add the task if required.","['Verifies', 'the', 'DagRun', 'by', 'checking', 'for', 'removed', 'tasks', 'or', 'tasks', 'that', 'are', 'not', 'in', 'the', 'database', 'yet', '.', 'It', 'will', 'set', 'state', 'to', 'removed', 'or', 'add', 'the', 'task', 'if', 'required', '.']",python,test,"['verifies', 'the', 'dagrun', 'by', 'checking', 'for', 'removed', 'tasks', 'or', 'tasks', 'that', 'are', 'not', 'in', 'the', 'database', 'yet', '.', 'it', 'will', 'set', 'state', 'to', 'removed', 'or', 'add', 'the', 'task', 'if', 'required', '.']",verifies the dagrun by checking for removed tasks or tasks that are not in the database yet . it will set state to removed or add the task if required .,"['def', 'verify_integrity', '(', 'self', ',', 'session', '=', 'none', ')', ':', 'from', 'airflow', '.', 'models', '.', 'taskinstance', 'import', 'taskinstance', '# avoid circular import', 'dag', '=', 'self', '.', 'get_dag', '(', ')', 'tis', '=', 'self', '.', 'get_task_instances', '(', 'session', '=', 'session', ')', '# check for removed or restored tasks', 'task_ids', '=', '[', ']', 'for', 'ti', 'in', 'tis', ':', 'task_ids', '.', 'append', '(', 'ti', '.', 'task_id', ')', 'task', '=', 'none', 'try', ':', 'task', '=', 'dag', '.', 'get_task', '(', 'ti', '.', 'task_id', ')', 'except', 'airflowexception', ':', 'if', 'ti', '.', 'state', '==', 'state', '.', 'removed', ':', 'pass', '# ti has already been removed, just ignore it', 'elif', 'self', '.', 'state', 'is', 'not', 'state', '.', 'running', 'and', 'not', 'dag', '.', 'partial', ':', 'self', '.', 'log', '.', 'warning', '(', '""failed to get task \'{}\' for dag \'{}\'. ""', '""marking it as removed.""', '.', 'format', '(', 'ti', ',', 'dag', ')', ')', 'stats', '.', 'incr', '(', '""task_removed_from_dag.{}""', '.', 'format', '(', 'dag', '.', 'dag_id', ')', ',', '1', ',', '1', ')', 'ti', '.', 'state', '=', 'state', '.', 'removed', 'is_task_in_dag', '=', 'task', 'is', 'not', 'none', 'should_restore_task', '=', 'is_task_in_dag', 'and', 'ti', '.', 'state', '==', 'state', '.', 'removed', 'if', 'should_restore_task', ':', 'self', '.', 'log', '.', 'info', '(', '""restoring task \'{}\' which was previously ""', '""removed from dag \'{}\'""', '.', 'format', '(', 'ti', ',', 'dag', ')', ')', 'stats', '.', 'incr', '(', '""task_restored_to_dag.{}""', '.', 'format', '(', 'dag', '.', 'dag_id', ')', ',', '1', ',', '1', ')', 'ti', '.', 'state', '=', 'state', '.', 'none', '# check for missing tasks', 'for', 'task', 'in', 'six', '.', 'itervalues', '(', 'dag', '.', 'task_dict', ')', ':', 'if', 'task', '.', 'start_date', '>', 'self', '.', 'execution_date', 'and', 'not', 'self', '.', 'is_backfill', ':', 'continue', 'if', 'task', '.', 'task_id', 'not', 'in', 'task_ids', ':', 'stats', '.', 'incr', '(', '""task_instance_created-{}""', '.', 'format', '(', 'task', '.', '__class__', '.', '__name__', ')', ',', '1', ',', '1', ')', 'ti', '=', 'taskinstance', '(', 'task', ',', 'self', '.', 'execution_date', ')', 'session', '.', 'add', '(', 'ti', ')', 'session', '.', 'commit', '(', ')']","def verify_integrity ( self , session = none ) : from airflow . models . taskinstance import taskinstance # avoid circular import dag = self . get_dag ( ) tis = self . get_task_instances ( session = session ) # check for removed or restored tasks task_ids = [ ] for ti in tis : task_ids . append ( ti . task_id ) task = none try : task = dag . get_task ( ti . task_id ) except airflowexception : if ti . state == state . removed : pass # ti has already been removed, just ignore it elif self . state is not state . running and not dag . partial : self . log . warning ( ""failed to get task '{}' for dag '{}'. "" ""marking it as removed."" . format ( ti , dag ) ) stats . incr ( ""task_removed_from_dag.{}"" . format ( dag . dag_id ) , 1 , 1 ) ti . state = state . removed is_task_in_dag = task is not none should_restore_task = is_task_in_dag and ti . state == state . removed if should_restore_task : self . log . info ( ""restoring task '{}' which was previously "" ""removed from dag '{}'"" . format ( ti , dag ) ) stats . incr ( ""task_restored_to_dag.{}"" . format ( dag . dag_id ) , 1 , 1 ) ti . state = state . none # check for missing tasks for task in six . itervalues ( dag . task_dict ) : if task . start_date > self . execution_date and not self . is_backfill : continue if task . task_id not in task_ids : stats . incr ( ""task_instance_created-{}"" . format ( task . __class__ . __name__ ) , 1 , 1 ) ti = taskinstance ( task , self . execution_date ) session . add ( ti ) session . commit ( )"
316,apache/airflow,airflow/models/dagrun.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagrun.py#L392-L407,"def get_run(session, dag_id, execution_date):
        """"""
        :param dag_id: DAG ID
        :type dag_id: unicode
        :param execution_date: execution date
        :type execution_date: datetime
        :return: DagRun corresponding to the given dag_id and execution date
            if one exists. None otherwise.
        :rtype: airflow.models.DagRun
        """"""
        qry = session.query(DagRun).filter(
            DagRun.dag_id == dag_id,
            DagRun.external_trigger == False, # noqa
            DagRun.execution_date == execution_date,
        )
        return qry.first()","['def', 'get_run', '(', 'session', ',', 'dag_id', ',', 'execution_date', ')', ':', 'qry', '=', 'session', '.', 'query', '(', 'DagRun', ')', '.', 'filter', '(', 'DagRun', '.', 'dag_id', '==', 'dag_id', ',', 'DagRun', '.', 'external_trigger', '==', 'False', ',', '# noqa', 'DagRun', '.', 'execution_date', '==', 'execution_date', ',', ')', 'return', 'qry', '.', 'first', '(', ')']",":param dag_id: DAG ID
        :type dag_id: unicode
        :param execution_date: execution date
        :type execution_date: datetime
        :return: DagRun corresponding to the given dag_id and execution date
            if one exists. None otherwise.
        :rtype: airflow.models.DagRun","[':', 'param', 'dag_id', ':', 'DAG', 'ID', ':', 'type', 'dag_id', ':', 'unicode', ':', 'param', 'execution_date', ':', 'execution', 'date', ':', 'type', 'execution_date', ':', 'datetime', ':', 'return', ':', 'DagRun', 'corresponding', 'to', 'the', 'given', 'dag_id', 'and', 'execution', 'date', 'if', 'one', 'exists', '.', 'None', 'otherwise', '.', ':', 'rtype', ':', 'airflow', '.', 'models', '.', 'DagRun']",python,test,"[':', 'param', 'dag_id', ':', 'dag', 'id', ':', 'type', 'dag_id', ':', 'unicode', ':', 'param', 'execution_date', ':', 'execution', 'date', ':', 'type', 'execution_date', ':', 'datetime', ':', 'return', ':', 'dagrun', 'corresponding', 'to', 'the', 'given', 'dag_id', 'and', 'execution', 'date', 'if', 'one', 'exists', '.', 'none', 'otherwise', '.', ':', 'rtype', ':', 'airflow', '.', 'models', '.', 'dagrun']",: param dag_id : dag id : type dag_id : unicode : param execution_date : execution date : type execution_date : datetime : return : dagrun corresponding to the given dag_id and execution date if one exists . none otherwise . : rtype : airflow . models . dagrun,"['def', 'get_run', '(', 'session', ',', 'dag_id', ',', 'execution_date', ')', ':', 'qry', '=', 'session', '.', 'query', '(', 'dagrun', ')', '.', 'filter', '(', 'dagrun', '.', 'dag_id', '==', 'dag_id', ',', 'dagrun', '.', 'external_trigger', '==', 'false', ',', '# noqa', 'dagrun', '.', 'execution_date', '==', 'execution_date', ',', ')', 'return', 'qry', '.', 'first', '(', ')']","def get_run ( session , dag_id , execution_date ) : qry = session . query ( dagrun ) . filter ( dagrun . dag_id == dag_id , dagrun . external_trigger == false , # noqa dagrun . execution_date == execution_date , ) return qry . first ( )"
317,apache/airflow,airflow/contrib/operators/jenkins_job_trigger_operator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/jenkins_job_trigger_operator.py#L34-L79,"def jenkins_request_with_headers(jenkins_server, req):
    """"""
    We need to get the headers in addition to the body answer
    to get the location from them
    This function uses jenkins_request method from python-jenkins library
    with just the return call changed

    :param jenkins_server: The server to query
    :param req: The request to execute
    :return: Dict containing the response body (key body)
        and the headers coming along (headers)
    """"""
    try:
        response = jenkins_server.jenkins_request(req)
        response_body = response.content
        response_headers = response.headers
        if response_body is None:
            raise jenkins.EmptyResponseException(
                ""Error communicating with server[%s]: ""
                ""empty response"" % jenkins_server.server)
        return {'body': response_body.decode('utf-8'), 'headers': response_headers}
    except HTTPError as e:
        # Jenkins's funky authentication means its nigh impossible to
        # distinguish errors.
        if e.code in [401, 403, 500]:
            # six.moves.urllib.error.HTTPError provides a 'reason'
            # attribute for all python version except for ver 2.6
            # Falling back to HTTPError.msg since it contains the
            # same info as reason
            raise JenkinsException(
                'Error in request. ' +
                'Possibly authentication failed [%s]: %s' % (
                    e.code, e.msg)
            )
        elif e.code == 404:
            raise jenkins.NotFoundException('Requested item could not be found')
        else:
            raise
    except socket.timeout as e:
        raise jenkins.TimeoutException('Error in request: %s' % e)
    except URLError as e:
        # python 2.6 compatibility to ensure same exception raised
        # since URLError wraps a socket timeout on python 2.6.
        if str(e.reason) == ""timed out"":
            raise jenkins.TimeoutException('Error in request: %s' % e.reason)
        raise JenkinsException('Error in request: %s' % e.reason)","['def', 'jenkins_request_with_headers', '(', 'jenkins_server', ',', 'req', ')', ':', 'try', ':', 'response', '=', 'jenkins_server', '.', 'jenkins_request', '(', 'req', ')', 'response_body', '=', 'response', '.', 'content', 'response_headers', '=', 'response', '.', 'headers', 'if', 'response_body', 'is', 'None', ':', 'raise', 'jenkins', '.', 'EmptyResponseException', '(', '""Error communicating with server[%s]: ""', '""empty response""', '%', 'jenkins_server', '.', 'server', ')', 'return', '{', ""'body'"", ':', 'response_body', '.', 'decode', '(', ""'utf-8'"", ')', ',', ""'headers'"", ':', 'response_headers', '}', 'except', 'HTTPError', 'as', 'e', ':', ""# Jenkins's funky authentication means its nigh impossible to"", '# distinguish errors.', 'if', 'e', '.', 'code', 'in', '[', '401', ',', '403', ',', '500', ']', ':', ""# six.moves.urllib.error.HTTPError provides a 'reason'"", '# attribute for all python version except for ver 2.6', '# Falling back to HTTPError.msg since it contains the', '# same info as reason', 'raise', 'JenkinsException', '(', ""'Error in request. '"", '+', ""'Possibly authentication failed [%s]: %s'"", '%', '(', 'e', '.', 'code', ',', 'e', '.', 'msg', ')', ')', 'elif', 'e', '.', 'code', '==', '404', ':', 'raise', 'jenkins', '.', 'NotFoundException', '(', ""'Requested item could not be found'"", ')', 'else', ':', 'raise', 'except', 'socket', '.', 'timeout', 'as', 'e', ':', 'raise', 'jenkins', '.', 'TimeoutException', '(', ""'Error in request: %s'"", '%', 'e', ')', 'except', 'URLError', 'as', 'e', ':', '# python 2.6 compatibility to ensure same exception raised', '# since URLError wraps a socket timeout on python 2.6.', 'if', 'str', '(', 'e', '.', 'reason', ')', '==', '""timed out""', ':', 'raise', 'jenkins', '.', 'TimeoutException', '(', ""'Error in request: %s'"", '%', 'e', '.', 'reason', ')', 'raise', 'JenkinsException', '(', ""'Error in request: %s'"", '%', 'e', '.', 'reason', ')']","We need to get the headers in addition to the body answer
    to get the location from them
    This function uses jenkins_request method from python-jenkins library
    with just the return call changed

    :param jenkins_server: The server to query
    :param req: The request to execute
    :return: Dict containing the response body (key body)
        and the headers coming along (headers)","['We', 'need', 'to', 'get', 'the', 'headers', 'in', 'addition', 'to', 'the', 'body', 'answer', 'to', 'get', 'the', 'location', 'from', 'them', 'This', 'function', 'uses', 'jenkins_request', 'method', 'from', 'python', '-', 'jenkins', 'library', 'with', 'just', 'the', 'return', 'call', 'changed']",python,test,"['we', 'need', 'to', 'get', 'the', 'headers', 'in', 'addition', 'to', 'the', 'body', 'answer', 'to', 'get', 'the', 'location', 'from', 'them', 'this', 'function', 'uses', 'jenkins_request', 'method', 'from', 'python', '-', 'jenkins', 'library', 'with', 'just', 'the', 'return', 'call', 'changed']",we need to get the headers in addition to the body answer to get the location from them this function uses jenkins_request method from python - jenkins library with just the return call changed,"['def', 'jenkins_request_with_headers', '(', 'jenkins_server', ',', 'req', ')', ':', 'try', ':', 'response', '=', 'jenkins_server', '.', 'jenkins_request', '(', 'req', ')', 'response_body', '=', 'response', '.', 'content', 'response_headers', '=', 'response', '.', 'headers', 'if', 'response_body', 'is', 'none', ':', 'raise', 'jenkins', '.', 'emptyresponseexception', '(', '""error communicating with server[%s]: ""', '""empty response""', '%', 'jenkins_server', '.', 'server', ')', 'return', '{', ""'body'"", ':', 'response_body', '.', 'decode', '(', ""'utf-8'"", ')', ',', ""'headers'"", ':', 'response_headers', '}', 'except', 'httperror', 'as', 'e', ':', ""# jenkins's funky authentication means its nigh impossible to"", '# distinguish errors.', 'if', 'e', '.', 'code', 'in', '[', '401', ',', '403', ',', '500', ']', ':', ""# six.moves.urllib.error.httperror provides a 'reason'"", '# attribute for all python version except for ver 2.6', '# falling back to httperror.msg since it contains the', '# same info as reason', 'raise', 'jenkinsexception', '(', ""'error in request. '"", '+', ""'possibly authentication failed [%s]: %s'"", '%', '(', 'e', '.', 'code', ',', 'e', '.', 'msg', ')', ')', 'elif', 'e', '.', 'code', '==', '404', ':', 'raise', 'jenkins', '.', 'notfoundexception', '(', ""'requested item could not be found'"", ')', 'else', ':', 'raise', 'except', 'socket', '.', 'timeout', 'as', 'e', ':', 'raise', 'jenkins', '.', 'timeoutexception', '(', ""'error in request: %s'"", '%', 'e', ')', 'except', 'urlerror', 'as', 'e', ':', '# python 2.6 compatibility to ensure same exception raised', '# since urlerror wraps a socket timeout on python 2.6.', 'if', 'str', '(', 'e', '.', 'reason', ')', '==', '""timed out""', ':', 'raise', 'jenkins', '.', 'timeoutexception', '(', ""'error in request: %s'"", '%', 'e', '.', 'reason', ')', 'raise', 'jenkinsexception', '(', ""'error in request: %s'"", '%', 'e', '.', 'reason', ')']","def jenkins_request_with_headers ( jenkins_server , req ) : try : response = jenkins_server . jenkins_request ( req ) response_body = response . content response_headers = response . headers if response_body is none : raise jenkins . emptyresponseexception ( ""error communicating with server[%s]: "" ""empty response"" % jenkins_server . server ) return { 'body' : response_body . decode ( 'utf-8' ) , 'headers' : response_headers } except httperror as e : # jenkins's funky authentication means its nigh impossible to # distinguish errors. if e . code in [ 401 , 403 , 500 ] : # six.moves.urllib.error.httperror provides a 'reason' # attribute for all python version except for ver 2.6 # falling back to httperror.msg since it contains the # same info as reason raise jenkinsexception ( 'error in request. ' + 'possibly authentication failed [%s]: %s' % ( e . code , e . msg ) ) elif e . code == 404 : raise jenkins . notfoundexception ( 'requested item could not be found' ) else : raise except socket . timeout as e : raise jenkins . timeoutexception ( 'error in request: %s' % e ) except urlerror as e : # python 2.6 compatibility to ensure same exception raised # since urlerror wraps a socket timeout on python 2.6. if str ( e . reason ) == ""timed out"" : raise jenkins . timeoutexception ( 'error in request: %s' % e . reason ) raise jenkinsexception ( 'error in request: %s' % e . reason )"
318,apache/airflow,airflow/contrib/operators/jenkins_job_trigger_operator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/jenkins_job_trigger_operator.py#L124-L147,"def build_job(self, jenkins_server):
        """"""
        This function makes an API call to Jenkins to trigger a build for 'job_name'
        It returned a dict with 2 keys : body and headers.
        headers contains also a dict-like object which can be queried to get
        the location to poll in the queue.

        :param jenkins_server: The jenkins server where the job should be triggered
        :return: Dict containing the response body (key body)
            and the headers coming along (headers)
        """"""
        # Warning if the parameter is too long, the URL can be longer than
        # the maximum allowed size
        if self.parameters and isinstance(self.parameters, six.string_types):
            import ast
            self.parameters = ast.literal_eval(self.parameters)

        if not self.parameters:
            # We need a None to call the non parametrized jenkins api end point
            self.parameters = None

        request = Request(jenkins_server.build_job_url(self.job_name,
                                                       self.parameters, None))
        return jenkins_request_with_headers(jenkins_server, request)","['def', 'build_job', '(', 'self', ',', 'jenkins_server', ')', ':', '# Warning if the parameter is too long, the URL can be longer than', '# the maximum allowed size', 'if', 'self', '.', 'parameters', 'and', 'isinstance', '(', 'self', '.', 'parameters', ',', 'six', '.', 'string_types', ')', ':', 'import', 'ast', 'self', '.', 'parameters', '=', 'ast', '.', 'literal_eval', '(', 'self', '.', 'parameters', ')', 'if', 'not', 'self', '.', 'parameters', ':', '# We need a None to call the non parametrized jenkins api end point', 'self', '.', 'parameters', '=', 'None', 'request', '=', 'Request', '(', 'jenkins_server', '.', 'build_job_url', '(', 'self', '.', 'job_name', ',', 'self', '.', 'parameters', ',', 'None', ')', ')', 'return', 'jenkins_request_with_headers', '(', 'jenkins_server', ',', 'request', ')']","This function makes an API call to Jenkins to trigger a build for 'job_name'
        It returned a dict with 2 keys : body and headers.
        headers contains also a dict-like object which can be queried to get
        the location to poll in the queue.

        :param jenkins_server: The jenkins server where the job should be triggered
        :return: Dict containing the response body (key body)
            and the headers coming along (headers)","['This', 'function', 'makes', 'an', 'API', 'call', 'to', 'Jenkins', 'to', 'trigger', 'a', 'build', 'for', 'job_name', 'It', 'returned', 'a', 'dict', 'with', '2', 'keys', ':', 'body', 'and', 'headers', '.', 'headers', 'contains', 'also', 'a', 'dict', '-', 'like', 'object', 'which', 'can', 'be', 'queried', 'to', 'get', 'the', 'location', 'to', 'poll', 'in', 'the', 'queue', '.']",python,test,"['this', 'function', 'makes', 'an', 'api', 'call', 'to', 'jenkins', 'to', 'trigger', 'a', 'build', 'for', 'job_name', 'it', 'returned', 'a', 'dict', 'with', '2', 'keys', ':', 'body', 'and', 'headers', '.', 'headers', 'contains', 'also', 'a', 'dict', '-', 'like', 'object', 'which', 'can', 'be', 'queried', 'to', 'get', 'the', 'location', 'to', 'poll', 'in', 'the', 'queue', '.']",this function makes an api call to jenkins to trigger a build for job_name it returned a dict with 2 keys : body and headers . headers contains also a dict - like object which can be queried to get the location to poll in the queue .,"['def', 'build_job', '(', 'self', ',', 'jenkins_server', ')', ':', '# warning if the parameter is too long, the url can be longer than', '# the maximum allowed size', 'if', 'self', '.', 'parameters', 'and', 'isinstance', '(', 'self', '.', 'parameters', ',', 'six', '.', 'string_types', ')', ':', 'import', 'ast', 'self', '.', 'parameters', '=', 'ast', '.', 'literal_eval', '(', 'self', '.', 'parameters', ')', 'if', 'not', 'self', '.', 'parameters', ':', '# we need a none to call the non parametrized jenkins api end point', 'self', '.', 'parameters', '=', 'none', 'request', '=', 'request', '(', 'jenkins_server', '.', 'build_job_url', '(', 'self', '.', 'job_name', ',', 'self', '.', 'parameters', ',', 'none', ')', ')', 'return', 'jenkins_request_with_headers', '(', 'jenkins_server', ',', 'request', ')']","def build_job ( self , jenkins_server ) : # warning if the parameter is too long, the url can be longer than # the maximum allowed size if self . parameters and isinstance ( self . parameters , six . string_types ) : import ast self . parameters = ast . literal_eval ( self . parameters ) if not self . parameters : # we need a none to call the non parametrized jenkins api end point self . parameters = none request = request ( jenkins_server . build_job_url ( self . job_name , self . parameters , none ) ) return jenkins_request_with_headers ( jenkins_server , request )"
319,apache/airflow,airflow/contrib/operators/jenkins_job_trigger_operator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/jenkins_job_trigger_operator.py#L149-L183,"def poll_job_in_queue(self, location, jenkins_server):
        """"""
        This method poll the jenkins queue until the job is executed.
        When we trigger a job through an API call,
        the job is first put in the queue without having a build number assigned.
        Thus we have to wait the job exit the queue to know its build number.
        To do so, we have to add /api/json (or /api/xml) to the location
        returned by the build_job call and poll this file.
        When a 'executable' block appears in the json, it means the job execution started
        and the field 'number' then contains the build number.

        :param location: Location to poll, returned in the header of the build_job call
        :param jenkins_server: The jenkins server to poll
        :return: The build_number corresponding to the triggered job
        """"""
        try_count = 0
        location = location + '/api/json'
        # TODO Use get_queue_info instead
        # once it will be available in python-jenkins (v > 0.4.15)
        self.log.info('Polling jenkins queue at the url %s', location)
        while try_count < self.max_try_before_job_appears:
            location_answer = jenkins_request_with_headers(jenkins_server,
                                                           Request(location))
            if location_answer is not None:
                json_response = json.loads(location_answer['body'])
                if 'executable' in json_response:
                    build_number = json_response['executable']['number']
                    self.log.info('Job executed on Jenkins side with the build number %s',
                                  build_number)
                    return build_number
            try_count += 1
            time.sleep(self.sleep_time)
        raise AirflowException(""The job hasn't been executed""
                               "" after polling the queue %d times"",
                               self.max_try_before_job_appears)","['def', 'poll_job_in_queue', '(', 'self', ',', 'location', ',', 'jenkins_server', ')', ':', 'try_count', '=', '0', 'location', '=', 'location', '+', ""'/api/json'"", '# TODO Use get_queue_info instead', '# once it will be available in python-jenkins (v > 0.4.15)', 'self', '.', 'log', '.', 'info', '(', ""'Polling jenkins queue at the url %s'"", ',', 'location', ')', 'while', 'try_count', '<', 'self', '.', 'max_try_before_job_appears', ':', 'location_answer', '=', 'jenkins_request_with_headers', '(', 'jenkins_server', ',', 'Request', '(', 'location', ')', ')', 'if', 'location_answer', 'is', 'not', 'None', ':', 'json_response', '=', 'json', '.', 'loads', '(', 'location_answer', '[', ""'body'"", ']', ')', 'if', ""'executable'"", 'in', 'json_response', ':', 'build_number', '=', 'json_response', '[', ""'executable'"", ']', '[', ""'number'"", ']', 'self', '.', 'log', '.', 'info', '(', ""'Job executed on Jenkins side with the build number %s'"", ',', 'build_number', ')', 'return', 'build_number', 'try_count', '+=', '1', 'time', '.', 'sleep', '(', 'self', '.', 'sleep_time', ')', 'raise', 'AirflowException', '(', '""The job hasn\'t been executed""', '"" after polling the queue %d times""', ',', 'self', '.', 'max_try_before_job_appears', ')']","This method poll the jenkins queue until the job is executed.
        When we trigger a job through an API call,
        the job is first put in the queue without having a build number assigned.
        Thus we have to wait the job exit the queue to know its build number.
        To do so, we have to add /api/json (or /api/xml) to the location
        returned by the build_job call and poll this file.
        When a 'executable' block appears in the json, it means the job execution started
        and the field 'number' then contains the build number.

        :param location: Location to poll, returned in the header of the build_job call
        :param jenkins_server: The jenkins server to poll
        :return: The build_number corresponding to the triggered job","['This', 'method', 'poll', 'the', 'jenkins', 'queue', 'until', 'the', 'job', 'is', 'executed', '.', 'When', 'we', 'trigger', 'a', 'job', 'through', 'an', 'API', 'call', 'the', 'job', 'is', 'first', 'put', 'in', 'the', 'queue', 'without', 'having', 'a', 'build', 'number', 'assigned', '.', 'Thus', 'we', 'have', 'to', 'wait', 'the', 'job', 'exit', 'the', 'queue', 'to', 'know', 'its', 'build', 'number', '.', 'To', 'do', 'so', 'we', 'have', 'to', 'add', '/', 'api', '/', 'json', '(', 'or', '/', 'api', '/', 'xml', ')', 'to', 'the', 'location', 'returned', 'by', 'the', 'build_job', 'call', 'and', 'poll', 'this', 'file', '.', 'When', 'a', 'executable', 'block', 'appears', 'in', 'the', 'json', 'it', 'means', 'the', 'job', 'execution', 'started', 'and', 'the', 'field', 'number', 'then', 'contains', 'the', 'build', 'number', '.']",python,test,"['this', 'method', 'poll', 'the', 'jenkins', 'queue', 'until', 'the', 'job', 'is', 'executed', '.', 'when', 'we', 'trigger', 'a', 'job', 'through', 'an', 'api', 'call', 'the', 'job', 'is', 'first', 'put', 'in', 'the', 'queue', 'without', 'having', 'a', 'build', 'number', 'assigned', '.', 'thus', 'we', 'have', 'to', 'wait', 'the', 'job', 'exit', 'the', 'queue', 'to', 'know', 'its', 'build', 'number', '.', 'to', 'do', 'so', 'we', 'have', 'to', 'add', '/', 'api', '/', 'json', '(', 'or', '/', 'api', '/', 'xml', ')', 'to', 'the', 'location', 'returned', 'by', 'the', 'build_job', 'call', 'and', 'poll', 'this', 'file', '.', 'when', 'a', 'executable', 'block', 'appears', 'in', 'the', 'json', 'it', 'means', 'the', 'job', 'execution', 'started', 'and', 'the', 'field', 'number', 'then', 'contains', 'the', 'build', 'number', '.']",this method poll the jenkins queue until the job is executed . when we trigger a job through an api call the job is first put in the queue without having a build number assigned . thus we have to wait the job exit the queue to know its build number . to do so we have to add / api / json ( or / api / xml ) to the location returned by the build_job call and poll this file . when a executable block appears in the json it means the job execution started and the field number then contains the build number .,"['def', 'poll_job_in_queue', '(', 'self', ',', 'location', ',', 'jenkins_server', ')', ':', 'try_count', '=', '0', 'location', '=', 'location', '+', ""'/api/json'"", '# todo use get_queue_info instead', '# once it will be available in python-jenkins (v > 0.4.15)', 'self', '.', 'log', '.', 'info', '(', ""'polling jenkins queue at the url %s'"", ',', 'location', ')', 'while', 'try_count', '<', 'self', '.', 'max_try_before_job_appears', ':', 'location_answer', '=', 'jenkins_request_with_headers', '(', 'jenkins_server', ',', 'request', '(', 'location', ')', ')', 'if', 'location_answer', 'is', 'not', 'none', ':', 'json_response', '=', 'json', '.', 'loads', '(', 'location_answer', '[', ""'body'"", ']', ')', 'if', ""'executable'"", 'in', 'json_response', ':', 'build_number', '=', 'json_response', '[', ""'executable'"", ']', '[', ""'number'"", ']', 'self', '.', 'log', '.', 'info', '(', ""'job executed on jenkins side with the build number %s'"", ',', 'build_number', ')', 'return', 'build_number', 'try_count', '+=', '1', 'time', '.', 'sleep', '(', 'self', '.', 'sleep_time', ')', 'raise', 'airflowexception', '(', '""the job hasn\'t been executed""', '"" after polling the queue %d times""', ',', 'self', '.', 'max_try_before_job_appears', ')']","def poll_job_in_queue ( self , location , jenkins_server ) : try_count = 0 location = location + '/api/json' # todo use get_queue_info instead # once it will be available in python-jenkins (v > 0.4.15) self . log . info ( 'polling jenkins queue at the url %s' , location ) while try_count < self . max_try_before_job_appears : location_answer = jenkins_request_with_headers ( jenkins_server , request ( location ) ) if location_answer is not none : json_response = json . loads ( location_answer [ 'body' ] ) if 'executable' in json_response : build_number = json_response [ 'executable' ] [ 'number' ] self . log . info ( 'job executed on jenkins side with the build number %s' , build_number ) return build_number try_count += 1 time . sleep ( self . sleep_time ) raise airflowexception ( ""the job hasn't been executed"" "" after polling the queue %d times"" , self . max_try_before_job_appears )"
320,apache/airflow,airflow/utils/operator_helpers.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/operator_helpers.py#L33-L66,"def context_to_airflow_vars(context, in_env_var_format=False):
    """"""
    Given a context, this function provides a dictionary of values that can be used to
    externally reconstruct relations between dags, dag_runs, tasks and task_instances.
    Default to abc.def.ghi format and can be made to ABC_DEF_GHI format if
    in_env_var_format is set to True.

    :param context: The context for the task_instance of interest.
    :type context: dict
    :param in_env_var_format: If returned vars should be in ABC_DEF_GHI format.
    :type in_env_var_format: bool
    :return: task_instance context as dict.
    """"""
    params = dict()
    if in_env_var_format:
        name_format = 'env_var_format'
    else:
        name_format = 'default'
    task_instance = context.get('task_instance')
    if task_instance and task_instance.dag_id:
        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_DAG_ID'][
            name_format]] = task_instance.dag_id
    if task_instance and task_instance.task_id:
        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_TASK_ID'][
            name_format]] = task_instance.task_id
    if task_instance and task_instance.execution_date:
        params[
            AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_EXECUTION_DATE'][
                name_format]] = task_instance.execution_date.isoformat()
    dag_run = context.get('dag_run')
    if dag_run and dag_run.run_id:
        params[AIRFLOW_VAR_NAME_FORMAT_MAPPING['AIRFLOW_CONTEXT_DAG_RUN_ID'][
            name_format]] = dag_run.run_id
    return params","['def', 'context_to_airflow_vars', '(', 'context', ',', 'in_env_var_format', '=', 'False', ')', ':', 'params', '=', 'dict', '(', ')', 'if', 'in_env_var_format', ':', 'name_format', '=', ""'env_var_format'"", 'else', ':', 'name_format', '=', ""'default'"", 'task_instance', '=', 'context', '.', 'get', '(', ""'task_instance'"", ')', 'if', 'task_instance', 'and', 'task_instance', '.', 'dag_id', ':', 'params', '[', 'AIRFLOW_VAR_NAME_FORMAT_MAPPING', '[', ""'AIRFLOW_CONTEXT_DAG_ID'"", ']', '[', 'name_format', ']', ']', '=', 'task_instance', '.', 'dag_id', 'if', 'task_instance', 'and', 'task_instance', '.', 'task_id', ':', 'params', '[', 'AIRFLOW_VAR_NAME_FORMAT_MAPPING', '[', ""'AIRFLOW_CONTEXT_TASK_ID'"", ']', '[', 'name_format', ']', ']', '=', 'task_instance', '.', 'task_id', 'if', 'task_instance', 'and', 'task_instance', '.', 'execution_date', ':', 'params', '[', 'AIRFLOW_VAR_NAME_FORMAT_MAPPING', '[', ""'AIRFLOW_CONTEXT_EXECUTION_DATE'"", ']', '[', 'name_format', ']', ']', '=', 'task_instance', '.', 'execution_date', '.', 'isoformat', '(', ')', 'dag_run', '=', 'context', '.', 'get', '(', ""'dag_run'"", ')', 'if', 'dag_run', 'and', 'dag_run', '.', 'run_id', ':', 'params', '[', 'AIRFLOW_VAR_NAME_FORMAT_MAPPING', '[', ""'AIRFLOW_CONTEXT_DAG_RUN_ID'"", ']', '[', 'name_format', ']', ']', '=', 'dag_run', '.', 'run_id', 'return', 'params']","Given a context, this function provides a dictionary of values that can be used to
    externally reconstruct relations between dags, dag_runs, tasks and task_instances.
    Default to abc.def.ghi format and can be made to ABC_DEF_GHI format if
    in_env_var_format is set to True.

    :param context: The context for the task_instance of interest.
    :type context: dict
    :param in_env_var_format: If returned vars should be in ABC_DEF_GHI format.
    :type in_env_var_format: bool
    :return: task_instance context as dict.","['Given', 'a', 'context', 'this', 'function', 'provides', 'a', 'dictionary', 'of', 'values', 'that', 'can', 'be', 'used', 'to', 'externally', 'reconstruct', 'relations', 'between', 'dags', 'dag_runs', 'tasks', 'and', 'task_instances', '.', 'Default', 'to', 'abc', '.', 'def', '.', 'ghi', 'format', 'and', 'can', 'be', 'made', 'to', 'ABC_DEF_GHI', 'format', 'if', 'in_env_var_format', 'is', 'set', 'to', 'True', '.']",python,test,"['given', 'a', 'context', 'this', 'function', 'provides', 'a', 'dictionary', 'of', 'values', 'that', 'can', 'be', 'used', 'to', 'externally', 'reconstruct', 'relations', 'between', 'dags', 'dag_runs', 'tasks', 'and', 'task_instances', '.', 'default', 'to', 'abc', '.', 'def', '.', 'ghi', 'format', 'and', 'can', 'be', 'made', 'to', 'abc_def_ghi', 'format', 'if', 'in_env_var_format', 'is', 'set', 'to', 'true', '.']",given a context this function provides a dictionary of values that can be used to externally reconstruct relations between dags dag_runs tasks and task_instances . default to abc . def . ghi format and can be made to abc_def_ghi format if in_env_var_format is set to true .,"['def', 'context_to_airflow_vars', '(', 'context', ',', 'in_env_var_format', '=', 'false', ')', ':', 'params', '=', 'dict', '(', ')', 'if', 'in_env_var_format', ':', 'name_format', '=', ""'env_var_format'"", 'else', ':', 'name_format', '=', ""'default'"", 'task_instance', '=', 'context', '.', 'get', '(', ""'task_instance'"", ')', 'if', 'task_instance', 'and', 'task_instance', '.', 'dag_id', ':', 'params', '[', 'airflow_var_name_format_mapping', '[', ""'airflow_context_dag_id'"", ']', '[', 'name_format', ']', ']', '=', 'task_instance', '.', 'dag_id', 'if', 'task_instance', 'and', 'task_instance', '.', 'task_id', ':', 'params', '[', 'airflow_var_name_format_mapping', '[', ""'airflow_context_task_id'"", ']', '[', 'name_format', ']', ']', '=', 'task_instance', '.', 'task_id', 'if', 'task_instance', 'and', 'task_instance', '.', 'execution_date', ':', 'params', '[', 'airflow_var_name_format_mapping', '[', ""'airflow_context_execution_date'"", ']', '[', 'name_format', ']', ']', '=', 'task_instance', '.', 'execution_date', '.', 'isoformat', '(', ')', 'dag_run', '=', 'context', '.', 'get', '(', ""'dag_run'"", ')', 'if', 'dag_run', 'and', 'dag_run', '.', 'run_id', ':', 'params', '[', 'airflow_var_name_format_mapping', '[', ""'airflow_context_dag_run_id'"", ']', '[', 'name_format', ']', ']', '=', 'dag_run', '.', 'run_id', 'return', 'params']","def context_to_airflow_vars ( context , in_env_var_format = false ) : params = dict ( ) if in_env_var_format : name_format = 'env_var_format' else : name_format = 'default' task_instance = context . get ( 'task_instance' ) if task_instance and task_instance . dag_id : params [ airflow_var_name_format_mapping [ 'airflow_context_dag_id' ] [ name_format ] ] = task_instance . dag_id if task_instance and task_instance . task_id : params [ airflow_var_name_format_mapping [ 'airflow_context_task_id' ] [ name_format ] ] = task_instance . task_id if task_instance and task_instance . execution_date : params [ airflow_var_name_format_mapping [ 'airflow_context_execution_date' ] [ name_format ] ] = task_instance . execution_date . isoformat ( ) dag_run = context . get ( 'dag_run' ) if dag_run and dag_run . run_id : params [ airflow_var_name_format_mapping [ 'airflow_context_dag_run_id' ] [ name_format ] ] = dag_run . run_id return params"
321,apache/airflow,airflow/utils/cli_action_loggers.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/cli_action_loggers.py#L57-L69,"def on_pre_execution(**kwargs):
    """"""
    Calls callbacks before execution.
    Note that any exception from callback will be logged but won't be propagated.
    :param kwargs:
    :return: None
    """"""
    logging.debug(""Calling callbacks: %s"", __pre_exec_callbacks)
    for cb in __pre_exec_callbacks:
        try:
            cb(**kwargs)
        except Exception:
            logging.exception('Failed on pre-execution callback using %s', cb)","['def', 'on_pre_execution', '(', '*', '*', 'kwargs', ')', ':', 'logging', '.', 'debug', '(', '""Calling callbacks: %s""', ',', '__pre_exec_callbacks', ')', 'for', 'cb', 'in', '__pre_exec_callbacks', ':', 'try', ':', 'cb', '(', '*', '*', 'kwargs', ')', 'except', 'Exception', ':', 'logging', '.', 'exception', '(', ""'Failed on pre-execution callback using %s'"", ',', 'cb', ')']","Calls callbacks before execution.
    Note that any exception from callback will be logged but won't be propagated.
    :param kwargs:
    :return: None","['Calls', 'callbacks', 'before', 'execution', '.', 'Note', 'that', 'any', 'exception', 'from', 'callback', 'will', 'be', 'logged', 'but', 'won', 't', 'be', 'propagated', '.', ':', 'param', 'kwargs', ':', ':', 'return', ':', 'None']",python,test,"['calls', 'callbacks', 'before', 'execution', '.', 'note', 'that', 'any', 'exception', 'from', 'callback', 'will', 'be', 'logged', 'but', 'won', 't', 'be', 'propagated', '.', ':', 'param', 'kwargs', ':', ':', 'return', ':', 'none']",calls callbacks before execution . note that any exception from callback will be logged but won t be propagated . : param kwargs : : return : none,"['def', 'on_pre_execution', '(', '*', '*', 'kwargs', ')', ':', 'logging', '.', 'debug', '(', '""calling callbacks: %s""', ',', '__pre_exec_callbacks', ')', 'for', 'cb', 'in', '__pre_exec_callbacks', ':', 'try', ':', 'cb', '(', '*', '*', 'kwargs', ')', 'except', 'exception', ':', 'logging', '.', 'exception', '(', ""'failed on pre-execution callback using %s'"", ',', 'cb', ')']","def on_pre_execution ( * * kwargs ) : logging . debug ( ""calling callbacks: %s"" , __pre_exec_callbacks ) for cb in __pre_exec_callbacks : try : cb ( * * kwargs ) except exception : logging . exception ( 'failed on pre-execution callback using %s' , cb )"
322,apache/airflow,airflow/utils/cli_action_loggers.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/cli_action_loggers.py#L72-L86,"def on_post_execution(**kwargs):
    """"""
    Calls callbacks after execution.
    As it's being called after execution, it can capture status of execution,
    duration, etc. Note that any exception from callback will be logged but
    won't be propagated.
    :param kwargs:
    :return: None
    """"""
    logging.debug(""Calling callbacks: %s"", __post_exec_callbacks)
    for cb in __post_exec_callbacks:
        try:
            cb(**kwargs)
        except Exception:
            logging.exception('Failed on post-execution callback using %s', cb)","['def', 'on_post_execution', '(', '*', '*', 'kwargs', ')', ':', 'logging', '.', 'debug', '(', '""Calling callbacks: %s""', ',', '__post_exec_callbacks', ')', 'for', 'cb', 'in', '__post_exec_callbacks', ':', 'try', ':', 'cb', '(', '*', '*', 'kwargs', ')', 'except', 'Exception', ':', 'logging', '.', 'exception', '(', ""'Failed on post-execution callback using %s'"", ',', 'cb', ')']","Calls callbacks after execution.
    As it's being called after execution, it can capture status of execution,
    duration, etc. Note that any exception from callback will be logged but
    won't be propagated.
    :param kwargs:
    :return: None","['Calls', 'callbacks', 'after', 'execution', '.', 'As', 'it', 's', 'being', 'called', 'after', 'execution', 'it', 'can', 'capture', 'status', 'of', 'execution', 'duration', 'etc', '.', 'Note', 'that', 'any', 'exception', 'from', 'callback', 'will', 'be', 'logged', 'but', 'won', 't', 'be', 'propagated', '.', ':', 'param', 'kwargs', ':', ':', 'return', ':', 'None']",python,test,"['calls', 'callbacks', 'after', 'execution', '.', 'as', 'it', 's', 'being', 'called', 'after', 'execution', 'it', 'can', 'capture', 'status', 'of', 'execution', 'duration', 'etc', '.', 'note', 'that', 'any', 'exception', 'from', 'callback', 'will', 'be', 'logged', 'but', 'won', 't', 'be', 'propagated', '.', ':', 'param', 'kwargs', ':', ':', 'return', ':', 'none']",calls callbacks after execution . as it s being called after execution it can capture status of execution duration etc . note that any exception from callback will be logged but won t be propagated . : param kwargs : : return : none,"['def', 'on_post_execution', '(', '*', '*', 'kwargs', ')', ':', 'logging', '.', 'debug', '(', '""calling callbacks: %s""', ',', '__post_exec_callbacks', ')', 'for', 'cb', 'in', '__post_exec_callbacks', ':', 'try', ':', 'cb', '(', '*', '*', 'kwargs', ')', 'except', 'exception', ':', 'logging', '.', 'exception', '(', ""'failed on post-execution callback using %s'"", ',', 'cb', ')']","def on_post_execution ( * * kwargs ) : logging . debug ( ""calling callbacks: %s"" , __post_exec_callbacks ) for cb in __post_exec_callbacks : try : cb ( * * kwargs ) except exception : logging . exception ( 'failed on post-execution callback using %s' , cb )"
323,apache/airflow,airflow/example_dags/example_trigger_controller_dag.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/example_dags/example_trigger_controller_dag.py#L45-L52,"def conditionally_trigger(context, dag_run_obj):
    """"""This function decides whether or not to Trigger the remote DAG""""""
    c_p = context['params']['condition_param']
    print(""Controller DAG : conditionally_trigger = {}"".format(c_p))
    if context['params']['condition_param']:
        dag_run_obj.payload = {'message': context['params']['message']}
        pp.pprint(dag_run_obj.payload)
        return dag_run_obj","['def', 'conditionally_trigger', '(', 'context', ',', 'dag_run_obj', ')', ':', 'c_p', '=', 'context', '[', ""'params'"", ']', '[', ""'condition_param'"", ']', 'print', '(', '""Controller DAG : conditionally_trigger = {}""', '.', 'format', '(', 'c_p', ')', ')', 'if', 'context', '[', ""'params'"", ']', '[', ""'condition_param'"", ']', ':', 'dag_run_obj', '.', 'payload', '=', '{', ""'message'"", ':', 'context', '[', ""'params'"", ']', '[', ""'message'"", ']', '}', 'pp', '.', 'pprint', '(', 'dag_run_obj', '.', 'payload', ')', 'return', 'dag_run_obj']",This function decides whether or not to Trigger the remote DAG,"['This', 'function', 'decides', 'whether', 'or', 'not', 'to', 'Trigger', 'the', 'remote', 'DAG']",python,test,"['this', 'function', 'decides', 'whether', 'or', 'not', 'to', 'trigger', 'the', 'remote', 'dag']",this function decides whether or not to trigger the remote dag,"['def', 'conditionally_trigger', '(', 'context', ',', 'dag_run_obj', ')', ':', 'c_p', '=', 'context', '[', ""'params'"", ']', '[', ""'condition_param'"", ']', 'print', '(', '""controller dag : conditionally_trigger = {}""', '.', 'format', '(', 'c_p', ')', ')', 'if', 'context', '[', ""'params'"", ']', '[', ""'condition_param'"", ']', ':', 'dag_run_obj', '.', 'payload', '=', '{', ""'message'"", ':', 'context', '[', ""'params'"", ']', '[', ""'message'"", ']', '}', 'pp', '.', 'pprint', '(', 'dag_run_obj', '.', 'payload', ')', 'return', 'dag_run_obj']","def conditionally_trigger ( context , dag_run_obj ) : c_p = context [ 'params' ] [ 'condition_param' ] print ( ""controller dag : conditionally_trigger = {}"" . format ( c_p ) ) if context [ 'params' ] [ 'condition_param' ] : dag_run_obj . payload = { 'message' : context [ 'params' ] [ 'message' ] } pp . pprint ( dag_run_obj . payload ) return dag_run_obj"
324,apache/airflow,airflow/contrib/hooks/datadog_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datadog_hook.py#L62-L86,"def send_metric(self, metric_name, datapoint, tags=None, type_=None, interval=None):
        """"""
        Sends a single datapoint metric to DataDog

        :param metric_name: The name of the metric
        :type metric_name: str
        :param datapoint: A single integer or float related to the metric
        :type datapoint: int or float
        :param tags: A list of tags associated with the metric
        :type tags: list
        :param type_: Type of your metric: gauge, rate, or count
        :type type_: str
        :param interval: If the type of the metric is rate or count, define the corresponding interval
        :type interval: int
        """"""
        response = api.Metric.send(
            metric=metric_name,
            points=datapoint,
            host=self.host,
            tags=tags,
            type=type_,
            interval=interval)

        self.validate_response(response)
        return response","['def', 'send_metric', '(', 'self', ',', 'metric_name', ',', 'datapoint', ',', 'tags', '=', 'None', ',', 'type_', '=', 'None', ',', 'interval', '=', 'None', ')', ':', 'response', '=', 'api', '.', 'Metric', '.', 'send', '(', 'metric', '=', 'metric_name', ',', 'points', '=', 'datapoint', ',', 'host', '=', 'self', '.', 'host', ',', 'tags', '=', 'tags', ',', 'type', '=', 'type_', ',', 'interval', '=', 'interval', ')', 'self', '.', 'validate_response', '(', 'response', ')', 'return', 'response']","Sends a single datapoint metric to DataDog

        :param metric_name: The name of the metric
        :type metric_name: str
        :param datapoint: A single integer or float related to the metric
        :type datapoint: int or float
        :param tags: A list of tags associated with the metric
        :type tags: list
        :param type_: Type of your metric: gauge, rate, or count
        :type type_: str
        :param interval: If the type of the metric is rate or count, define the corresponding interval
        :type interval: int","['Sends', 'a', 'single', 'datapoint', 'metric', 'to', 'DataDog']",python,test,"['sends', 'a', 'single', 'datapoint', 'metric', 'to', 'datadog']",sends a single datapoint metric to datadog,"['def', 'send_metric', '(', 'self', ',', 'metric_name', ',', 'datapoint', ',', 'tags', '=', 'none', ',', 'type_', '=', 'none', ',', 'interval', '=', 'none', ')', ':', 'response', '=', 'api', '.', 'metric', '.', 'send', '(', 'metric', '=', 'metric_name', ',', 'points', '=', 'datapoint', ',', 'host', '=', 'self', '.', 'host', ',', 'tags', '=', 'tags', ',', 'type', '=', 'type_', ',', 'interval', '=', 'interval', ')', 'self', '.', 'validate_response', '(', 'response', ')', 'return', 'response']","def send_metric ( self , metric_name , datapoint , tags = none , type_ = none , interval = none ) : response = api . metric . send ( metric = metric_name , points = datapoint , host = self . host , tags = tags , type = type_ , interval = interval ) self . validate_response ( response ) return response"
325,apache/airflow,airflow/contrib/hooks/datadog_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datadog_hook.py#L88-L111,"def query_metric(self,
                     query,
                     from_seconds_ago,
                     to_seconds_ago):
        """"""
        Queries datadog for a specific metric, potentially with some
        function applied to it and returns the results.

        :param query: The datadog query to execute (see datadog docs)
        :type query: str
        :param from_seconds_ago: How many seconds ago to start querying for.
        :type from_seconds_ago: int
        :param to_seconds_ago: Up to how many seconds ago to query for.
        :type to_seconds_ago: int
        """"""
        now = int(time.time())

        response = api.Metric.query(
            start=now - from_seconds_ago,
            end=now - to_seconds_ago,
            query=query)

        self.validate_response(response)
        return response","['def', 'query_metric', '(', 'self', ',', 'query', ',', 'from_seconds_ago', ',', 'to_seconds_ago', ')', ':', 'now', '=', 'int', '(', 'time', '.', 'time', '(', ')', ')', 'response', '=', 'api', '.', 'Metric', '.', 'query', '(', 'start', '=', 'now', '-', 'from_seconds_ago', ',', 'end', '=', 'now', '-', 'to_seconds_ago', ',', 'query', '=', 'query', ')', 'self', '.', 'validate_response', '(', 'response', ')', 'return', 'response']","Queries datadog for a specific metric, potentially with some
        function applied to it and returns the results.

        :param query: The datadog query to execute (see datadog docs)
        :type query: str
        :param from_seconds_ago: How many seconds ago to start querying for.
        :type from_seconds_ago: int
        :param to_seconds_ago: Up to how many seconds ago to query for.
        :type to_seconds_ago: int","['Queries', 'datadog', 'for', 'a', 'specific', 'metric', 'potentially', 'with', 'some', 'function', 'applied', 'to', 'it', 'and', 'returns', 'the', 'results', '.']",python,test,"['queries', 'datadog', 'for', 'a', 'specific', 'metric', 'potentially', 'with', 'some', 'function', 'applied', 'to', 'it', 'and', 'returns', 'the', 'results', '.']",queries datadog for a specific metric potentially with some function applied to it and returns the results .,"['def', 'query_metric', '(', 'self', ',', 'query', ',', 'from_seconds_ago', ',', 'to_seconds_ago', ')', ':', 'now', '=', 'int', '(', 'time', '.', 'time', '(', ')', ')', 'response', '=', 'api', '.', 'metric', '.', 'query', '(', 'start', '=', 'now', '-', 'from_seconds_ago', ',', 'end', '=', 'now', '-', 'to_seconds_ago', ',', 'query', '=', 'query', ')', 'self', '.', 'validate_response', '(', 'response', ')', 'return', 'response']","def query_metric ( self , query , from_seconds_ago , to_seconds_ago ) : now = int ( time . time ( ) ) response = api . metric . query ( start = now - from_seconds_ago , end = now - to_seconds_ago , query = query ) self . validate_response ( response ) return response"
326,apache/airflow,airflow/contrib/hooks/datadog_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/datadog_hook.py#L113-L158,"def post_event(self, title, text, aggregation_key=None, alert_type=None, date_happened=None,
                   handle=None, priority=None, related_event_id=None, tags=None, device_name=None):
        """"""
        Posts an event to datadog (processing finished, potentially alerts, other issues)
        Think about this as a means to maintain persistence of alerts, rather than
        alerting itself.

        :param title: The title of the event
        :type title: str
        :param text: The body of the event (more information)
        :type text: str
        :param aggregation_key: Key that can be used to aggregate this event in a stream
        :type aggregation_key: str
        :param alert_type: The alert type for the event, one of
            [""error"", ""warning"", ""info"", ""success""]
        :type alert_type: str
        :param date_happened: POSIX timestamp of the event; defaults to now
        :type date_happened: int
        :handle: User to post the event as; defaults to owner of the application key used
            to submit.
        :param handle: str
        :param priority: Priority to post the event as. (""normal"" or ""low"", defaults to ""normal"")
        :type priority: str
        :param related_event_id: Post event as a child of the given event
        :type related_event_id: id
        :param tags: List of tags to apply to the event
        :type tags: list[str]
        :param device_name: device_name to post the event with
        :type device_name: list
        """"""
        response = api.Event.create(
            title=title,
            text=text,
            aggregation_key=aggregation_key,
            alert_type=alert_type,
            date_happened=date_happened,
            handle=handle,
            priority=priority,
            related_event_id=related_event_id,
            tags=tags,
            host=self.host,
            device_name=device_name,
            source_type_name=self.source_type_name)

        self.validate_response(response)
        return response","['def', 'post_event', '(', 'self', ',', 'title', ',', 'text', ',', 'aggregation_key', '=', 'None', ',', 'alert_type', '=', 'None', ',', 'date_happened', '=', 'None', ',', 'handle', '=', 'None', ',', 'priority', '=', 'None', ',', 'related_event_id', '=', 'None', ',', 'tags', '=', 'None', ',', 'device_name', '=', 'None', ')', ':', 'response', '=', 'api', '.', 'Event', '.', 'create', '(', 'title', '=', 'title', ',', 'text', '=', 'text', ',', 'aggregation_key', '=', 'aggregation_key', ',', 'alert_type', '=', 'alert_type', ',', 'date_happened', '=', 'date_happened', ',', 'handle', '=', 'handle', ',', 'priority', '=', 'priority', ',', 'related_event_id', '=', 'related_event_id', ',', 'tags', '=', 'tags', ',', 'host', '=', 'self', '.', 'host', ',', 'device_name', '=', 'device_name', ',', 'source_type_name', '=', 'self', '.', 'source_type_name', ')', 'self', '.', 'validate_response', '(', 'response', ')', 'return', 'response']","Posts an event to datadog (processing finished, potentially alerts, other issues)
        Think about this as a means to maintain persistence of alerts, rather than
        alerting itself.

        :param title: The title of the event
        :type title: str
        :param text: The body of the event (more information)
        :type text: str
        :param aggregation_key: Key that can be used to aggregate this event in a stream
        :type aggregation_key: str
        :param alert_type: The alert type for the event, one of
            [""error"", ""warning"", ""info"", ""success""]
        :type alert_type: str
        :param date_happened: POSIX timestamp of the event; defaults to now
        :type date_happened: int
        :handle: User to post the event as; defaults to owner of the application key used
            to submit.
        :param handle: str
        :param priority: Priority to post the event as. (""normal"" or ""low"", defaults to ""normal"")
        :type priority: str
        :param related_event_id: Post event as a child of the given event
        :type related_event_id: id
        :param tags: List of tags to apply to the event
        :type tags: list[str]
        :param device_name: device_name to post the event with
        :type device_name: list","['Posts', 'an', 'event', 'to', 'datadog', '(', 'processing', 'finished', 'potentially', 'alerts', 'other', 'issues', ')', 'Think', 'about', 'this', 'as', 'a', 'means', 'to', 'maintain', 'persistence', 'of', 'alerts', 'rather', 'than', 'alerting', 'itself', '.']",python,test,"['posts', 'an', 'event', 'to', 'datadog', '(', 'processing', 'finished', 'potentially', 'alerts', 'other', 'issues', ')', 'think', 'about', 'this', 'as', 'a', 'means', 'to', 'maintain', 'persistence', 'of', 'alerts', 'rather', 'than', 'alerting', 'itself', '.']",posts an event to datadog ( processing finished potentially alerts other issues ) think about this as a means to maintain persistence of alerts rather than alerting itself .,"['def', 'post_event', '(', 'self', ',', 'title', ',', 'text', ',', 'aggregation_key', '=', 'none', ',', 'alert_type', '=', 'none', ',', 'date_happened', '=', 'none', ',', 'handle', '=', 'none', ',', 'priority', '=', 'none', ',', 'related_event_id', '=', 'none', ',', 'tags', '=', 'none', ',', 'device_name', '=', 'none', ')', ':', 'response', '=', 'api', '.', 'event', '.', 'create', '(', 'title', '=', 'title', ',', 'text', '=', 'text', ',', 'aggregation_key', '=', 'aggregation_key', ',', 'alert_type', '=', 'alert_type', ',', 'date_happened', '=', 'date_happened', ',', 'handle', '=', 'handle', ',', 'priority', '=', 'priority', ',', 'related_event_id', '=', 'related_event_id', ',', 'tags', '=', 'tags', ',', 'host', '=', 'self', '.', 'host', ',', 'device_name', '=', 'device_name', ',', 'source_type_name', '=', 'self', '.', 'source_type_name', ')', 'self', '.', 'validate_response', '(', 'response', ')', 'return', 'response']","def post_event ( self , title , text , aggregation_key = none , alert_type = none , date_happened = none , handle = none , priority = none , related_event_id = none , tags = none , device_name = none ) : response = api . event . create ( title = title , text = text , aggregation_key = aggregation_key , alert_type = alert_type , date_happened = date_happened , handle = handle , priority = priority , related_event_id = related_event_id , tags = tags , host = self . host , device_name = device_name , source_type_name = self . source_type_name ) self . validate_response ( response ) return response"
327,apache/airflow,airflow/contrib/hooks/slack_webhook_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/slack_webhook_hook.py#L80-L97,"def _get_token(self, token, http_conn_id):
        """"""
        Given either a manually set token or a conn_id, return the webhook_token to use
        :param token: The manually provided token
        :type token: str
        :param http_conn_id: The conn_id provided
        :type http_conn_id: str
        :return: webhook_token (str) to use
        """"""
        if token:
            return token
        elif http_conn_id:
            conn = self.get_connection(http_conn_id)
            extra = conn.extra_dejson
            return extra.get('webhook_token', '')
        else:
            raise AirflowException('Cannot get token: No valid Slack '
                                   'webhook token nor conn_id supplied')","['def', '_get_token', '(', 'self', ',', 'token', ',', 'http_conn_id', ')', ':', 'if', 'token', ':', 'return', 'token', 'elif', 'http_conn_id', ':', 'conn', '=', 'self', '.', 'get_connection', '(', 'http_conn_id', ')', 'extra', '=', 'conn', '.', 'extra_dejson', 'return', 'extra', '.', 'get', '(', ""'webhook_token'"", ',', ""''"", ')', 'else', ':', 'raise', 'AirflowException', '(', ""'Cannot get token: No valid Slack '"", ""'webhook token nor conn_id supplied'"", ')']","Given either a manually set token or a conn_id, return the webhook_token to use
        :param token: The manually provided token
        :type token: str
        :param http_conn_id: The conn_id provided
        :type http_conn_id: str
        :return: webhook_token (str) to use","['Given', 'either', 'a', 'manually', 'set', 'token', 'or', 'a', 'conn_id', 'return', 'the', 'webhook_token', 'to', 'use', ':', 'param', 'token', ':', 'The', 'manually', 'provided', 'token', ':', 'type', 'token', ':', 'str', ':', 'param', 'http_conn_id', ':', 'The', 'conn_id', 'provided', ':', 'type', 'http_conn_id', ':', 'str', ':', 'return', ':', 'webhook_token', '(', 'str', ')', 'to', 'use']",python,test,"['given', 'either', 'a', 'manually', 'set', 'token', 'or', 'a', 'conn_id', 'return', 'the', 'webhook_token', 'to', 'use', ':', 'param', 'token', ':', 'the', 'manually', 'provided', 'token', ':', 'type', 'token', ':', 'str', ':', 'param', 'http_conn_id', ':', 'the', 'conn_id', 'provided', ':', 'type', 'http_conn_id', ':', 'str', ':', 'return', ':', 'webhook_token', '(', 'str', ')', 'to', 'use']",given either a manually set token or a conn_id return the webhook_token to use : param token : the manually provided token : type token : str : param http_conn_id : the conn_id provided : type http_conn_id : str : return : webhook_token ( str ) to use,"['def', '_get_token', '(', 'self', ',', 'token', ',', 'http_conn_id', ')', ':', 'if', 'token', ':', 'return', 'token', 'elif', 'http_conn_id', ':', 'conn', '=', 'self', '.', 'get_connection', '(', 'http_conn_id', ')', 'extra', '=', 'conn', '.', 'extra_dejson', 'return', 'extra', '.', 'get', '(', ""'webhook_token'"", ',', ""''"", ')', 'else', ':', 'raise', 'airflowexception', '(', ""'cannot get token: no valid slack '"", ""'webhook token nor conn_id supplied'"", ')']","def _get_token ( self , token , http_conn_id ) : if token : return token elif http_conn_id : conn = self . get_connection ( http_conn_id ) extra = conn . extra_dejson return extra . get ( 'webhook_token' , '' ) else : raise airflowexception ( 'cannot get token: no valid slack ' 'webhook token nor conn_id supplied' )"
328,apache/airflow,airflow/contrib/hooks/slack_webhook_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/slack_webhook_hook.py#L99-L119,"def _build_slack_message(self):
        """"""
        Construct the Slack message. All relevant parameters are combined here to a valid
        Slack json message
        :return: Slack message (str) to send
        """"""
        cmd = {}

        if self.channel:
            cmd['channel'] = self.channel
        if self.username:
            cmd['username'] = self.username
        if self.icon_emoji:
            cmd['icon_emoji'] = self.icon_emoji
        if self.link_names:
            cmd['link_names'] = 1
        if self.attachments:
            cmd['attachments'] = self.attachments

        cmd['text'] = self.message
        return json.dumps(cmd)","['def', '_build_slack_message', '(', 'self', ')', ':', 'cmd', '=', '{', '}', 'if', 'self', '.', 'channel', ':', 'cmd', '[', ""'channel'"", ']', '=', 'self', '.', 'channel', 'if', 'self', '.', 'username', ':', 'cmd', '[', ""'username'"", ']', '=', 'self', '.', 'username', 'if', 'self', '.', 'icon_emoji', ':', 'cmd', '[', ""'icon_emoji'"", ']', '=', 'self', '.', 'icon_emoji', 'if', 'self', '.', 'link_names', ':', 'cmd', '[', ""'link_names'"", ']', '=', '1', 'if', 'self', '.', 'attachments', ':', 'cmd', '[', ""'attachments'"", ']', '=', 'self', '.', 'attachments', 'cmd', '[', ""'text'"", ']', '=', 'self', '.', 'message', 'return', 'json', '.', 'dumps', '(', 'cmd', ')']","Construct the Slack message. All relevant parameters are combined here to a valid
        Slack json message
        :return: Slack message (str) to send","['Construct', 'the', 'Slack', 'message', '.', 'All', 'relevant', 'parameters', 'are', 'combined', 'here', 'to', 'a', 'valid', 'Slack', 'json', 'message', ':', 'return', ':', 'Slack', 'message', '(', 'str', ')', 'to', 'send']",python,test,"['construct', 'the', 'slack', 'message', '.', 'all', 'relevant', 'parameters', 'are', 'combined', 'here', 'to', 'a', 'valid', 'slack', 'json', 'message', ':', 'return', ':', 'slack', 'message', '(', 'str', ')', 'to', 'send']",construct the slack message . all relevant parameters are combined here to a valid slack json message : return : slack message ( str ) to send,"['def', '_build_slack_message', '(', 'self', ')', ':', 'cmd', '=', '{', '}', 'if', 'self', '.', 'channel', ':', 'cmd', '[', ""'channel'"", ']', '=', 'self', '.', 'channel', 'if', 'self', '.', 'username', ':', 'cmd', '[', ""'username'"", ']', '=', 'self', '.', 'username', 'if', 'self', '.', 'icon_emoji', ':', 'cmd', '[', ""'icon_emoji'"", ']', '=', 'self', '.', 'icon_emoji', 'if', 'self', '.', 'link_names', ':', 'cmd', '[', ""'link_names'"", ']', '=', '1', 'if', 'self', '.', 'attachments', ':', 'cmd', '[', ""'attachments'"", ']', '=', 'self', '.', 'attachments', 'cmd', '[', ""'text'"", ']', '=', 'self', '.', 'message', 'return', 'json', '.', 'dumps', '(', 'cmd', ')']",def _build_slack_message ( self ) : cmd = { } if self . channel : cmd [ 'channel' ] = self . channel if self . username : cmd [ 'username' ] = self . username if self . icon_emoji : cmd [ 'icon_emoji' ] = self . icon_emoji if self . link_names : cmd [ 'link_names' ] = 1 if self . attachments : cmd [ 'attachments' ] = self . attachments cmd [ 'text' ] = self . message return json . dumps ( cmd )
329,apache/airflow,airflow/contrib/hooks/slack_webhook_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/slack_webhook_hook.py#L121-L134,"def execute(self):
        """"""
        Remote Popen (actually execute the slack webhook call)
        """"""
        proxies = {}
        if self.proxy:
            # we only need https proxy for Slack, as the endpoint is https
            proxies = {'https': self.proxy}

        slack_message = self._build_slack_message()
        self.run(endpoint=self.webhook_token,
                 data=slack_message,
                 headers={'Content-type': 'application/json'},
                 extra_options={'proxies': proxies})","['def', 'execute', '(', 'self', ')', ':', 'proxies', '=', '{', '}', 'if', 'self', '.', 'proxy', ':', '# we only need https proxy for Slack, as the endpoint is https', 'proxies', '=', '{', ""'https'"", ':', 'self', '.', 'proxy', '}', 'slack_message', '=', 'self', '.', '_build_slack_message', '(', ')', 'self', '.', 'run', '(', 'endpoint', '=', 'self', '.', 'webhook_token', ',', 'data', '=', 'slack_message', ',', 'headers', '=', '{', ""'Content-type'"", ':', ""'application/json'"", '}', ',', 'extra_options', '=', '{', ""'proxies'"", ':', 'proxies', '}', ')']",Remote Popen (actually execute the slack webhook call),"['Remote', 'Popen', '(', 'actually', 'execute', 'the', 'slack', 'webhook', 'call', ')']",python,test,"['remote', 'popen', '(', 'actually', 'execute', 'the', 'slack', 'webhook', 'call', ')']",remote popen ( actually execute the slack webhook call ),"['def', 'execute', '(', 'self', ')', ':', 'proxies', '=', '{', '}', 'if', 'self', '.', 'proxy', ':', '# we only need https proxy for slack, as the endpoint is https', 'proxies', '=', '{', ""'https'"", ':', 'self', '.', 'proxy', '}', 'slack_message', '=', 'self', '.', '_build_slack_message', '(', ')', 'self', '.', 'run', '(', 'endpoint', '=', 'self', '.', 'webhook_token', ',', 'data', '=', 'slack_message', ',', 'headers', '=', '{', ""'content-type'"", ':', ""'application/json'"", '}', ',', 'extra_options', '=', '{', ""'proxies'"", ':', 'proxies', '}', ')']","def execute ( self ) : proxies = { } if self . proxy : # we only need https proxy for slack, as the endpoint is https proxies = { 'https' : self . proxy } slack_message = self . _build_slack_message ( ) self . run ( endpoint = self . webhook_token , data = slack_message , headers = { 'content-type' : 'application/json' } , extra_options = { 'proxies' : proxies } )"
330,apache/airflow,airflow/models/dagbag.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagbag.py#L112-L143,"def get_dag(self, dag_id):
        """"""
        Gets the DAG out of the dictionary, and refreshes it if expired
        """"""
        from airflow.models.dag import DagModel  # Avoid circular import

        # If asking for a known subdag, we want to refresh the parent
        root_dag_id = dag_id
        if dag_id in self.dags:
            dag = self.dags[dag_id]
            if dag.is_subdag:
                root_dag_id = dag.parent_dag.dag_id

        # If the dag corresponding to root_dag_id is absent or expired
        orm_dag = DagModel.get_current(root_dag_id)
        if orm_dag and (
                root_dag_id not in self.dags or
                (
                    orm_dag.last_expired and
                    dag.last_loaded < orm_dag.last_expired
                )
        ):
            # Reprocess source file
            found_dags = self.process_file(
                filepath=orm_dag.fileloc, only_if_updated=False)

            # If the source file no longer exports `dag_id`, delete it from self.dags
            if found_dags and dag_id in [found_dag.dag_id for found_dag in found_dags]:
                return self.dags[dag_id]
            elif dag_id in self.dags:
                del self.dags[dag_id]
        return self.dags.get(dag_id)","['def', 'get_dag', '(', 'self', ',', 'dag_id', ')', ':', 'from', 'airflow', '.', 'models', '.', 'dag', 'import', 'DagModel', '# Avoid circular import', '# If asking for a known subdag, we want to refresh the parent', 'root_dag_id', '=', 'dag_id', 'if', 'dag_id', 'in', 'self', '.', 'dags', ':', 'dag', '=', 'self', '.', 'dags', '[', 'dag_id', ']', 'if', 'dag', '.', 'is_subdag', ':', 'root_dag_id', '=', 'dag', '.', 'parent_dag', '.', 'dag_id', '# If the dag corresponding to root_dag_id is absent or expired', 'orm_dag', '=', 'DagModel', '.', 'get_current', '(', 'root_dag_id', ')', 'if', 'orm_dag', 'and', '(', 'root_dag_id', 'not', 'in', 'self', '.', 'dags', 'or', '(', 'orm_dag', '.', 'last_expired', 'and', 'dag', '.', 'last_loaded', '<', 'orm_dag', '.', 'last_expired', ')', ')', ':', '# Reprocess source file', 'found_dags', '=', 'self', '.', 'process_file', '(', 'filepath', '=', 'orm_dag', '.', 'fileloc', ',', 'only_if_updated', '=', 'False', ')', '# If the source file no longer exports `dag_id`, delete it from self.dags', 'if', 'found_dags', 'and', 'dag_id', 'in', '[', 'found_dag', '.', 'dag_id', 'for', 'found_dag', 'in', 'found_dags', ']', ':', 'return', 'self', '.', 'dags', '[', 'dag_id', ']', 'elif', 'dag_id', 'in', 'self', '.', 'dags', ':', 'del', 'self', '.', 'dags', '[', 'dag_id', ']', 'return', 'self', '.', 'dags', '.', 'get', '(', 'dag_id', ')']","Gets the DAG out of the dictionary, and refreshes it if expired","['Gets', 'the', 'DAG', 'out', 'of', 'the', 'dictionary', 'and', 'refreshes', 'it', 'if', 'expired']",python,test,"['gets', 'the', 'dag', 'out', 'of', 'the', 'dictionary', 'and', 'refreshes', 'it', 'if', 'expired']",gets the dag out of the dictionary and refreshes it if expired,"['def', 'get_dag', '(', 'self', ',', 'dag_id', ')', ':', 'from', 'airflow', '.', 'models', '.', 'dag', 'import', 'dagmodel', '# avoid circular import', '# if asking for a known subdag, we want to refresh the parent', 'root_dag_id', '=', 'dag_id', 'if', 'dag_id', 'in', 'self', '.', 'dags', ':', 'dag', '=', 'self', '.', 'dags', '[', 'dag_id', ']', 'if', 'dag', '.', 'is_subdag', ':', 'root_dag_id', '=', 'dag', '.', 'parent_dag', '.', 'dag_id', '# if the dag corresponding to root_dag_id is absent or expired', 'orm_dag', '=', 'dagmodel', '.', 'get_current', '(', 'root_dag_id', ')', 'if', 'orm_dag', 'and', '(', 'root_dag_id', 'not', 'in', 'self', '.', 'dags', 'or', '(', 'orm_dag', '.', 'last_expired', 'and', 'dag', '.', 'last_loaded', '<', 'orm_dag', '.', 'last_expired', ')', ')', ':', '# reprocess source file', 'found_dags', '=', 'self', '.', 'process_file', '(', 'filepath', '=', 'orm_dag', '.', 'fileloc', ',', 'only_if_updated', '=', 'false', ')', '# if the source file no longer exports `dag_id`, delete it from self.dags', 'if', 'found_dags', 'and', 'dag_id', 'in', '[', 'found_dag', '.', 'dag_id', 'for', 'found_dag', 'in', 'found_dags', ']', ':', 'return', 'self', '.', 'dags', '[', 'dag_id', ']', 'elif', 'dag_id', 'in', 'self', '.', 'dags', ':', 'del', 'self', '.', 'dags', '[', 'dag_id', ']', 'return', 'self', '.', 'dags', '.', 'get', '(', 'dag_id', ')']","def get_dag ( self , dag_id ) : from airflow . models . dag import dagmodel # avoid circular import # if asking for a known subdag, we want to refresh the parent root_dag_id = dag_id if dag_id in self . dags : dag = self . dags [ dag_id ] if dag . is_subdag : root_dag_id = dag . parent_dag . dag_id # if the dag corresponding to root_dag_id is absent or expired orm_dag = dagmodel . get_current ( root_dag_id ) if orm_dag and ( root_dag_id not in self . dags or ( orm_dag . last_expired and dag . last_loaded < orm_dag . last_expired ) ) : # reprocess source file found_dags = self . process_file ( filepath = orm_dag . fileloc , only_if_updated = false ) # if the source file no longer exports `dag_id`, delete it from self.dags if found_dags and dag_id in [ found_dag . dag_id for found_dag in found_dags ] : return self . dags [ dag_id ] elif dag_id in self . dags : del self . dags [ dag_id ] return self . dags . get ( dag_id )"
331,apache/airflow,airflow/models/dagbag.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagbag.py#L145-L271,"def process_file(self, filepath, only_if_updated=True, safe_mode=True):
        """"""
        Given a path to a python module or zip file, this method imports
        the module and look for dag objects within it.
        """"""
        from airflow.models.dag import DAG  # Avoid circular import

        found_dags = []

        # if the source file no longer exists in the DB or in the filesystem,
        # return an empty list
        # todo: raise exception?
        if filepath is None or not os.path.isfile(filepath):
            return found_dags

        try:
            # This failed before in what may have been a git sync
            # race condition
            file_last_changed_on_disk = datetime.fromtimestamp(os.path.getmtime(filepath))
            if only_if_updated \
                    and filepath in self.file_last_changed \
                    and file_last_changed_on_disk == self.file_last_changed[filepath]:
                return found_dags

        except Exception as e:
            self.log.exception(e)
            return found_dags

        mods = []
        is_zipfile = zipfile.is_zipfile(filepath)
        if not is_zipfile:
            if safe_mode:
                with open(filepath, 'rb') as f:
                    content = f.read()
                    if not all([s in content for s in (b'DAG', b'airflow')]):
                        self.file_last_changed[filepath] = file_last_changed_on_disk
                        # Don't want to spam user with skip messages
                        if not self.has_logged:
                            self.has_logged = True
                            self.log.info(
                                ""File %s assumed to contain no DAGs. Skipping."",
                                filepath)
                        return found_dags

            self.log.debug(""Importing %s"", filepath)
            org_mod_name, _ = os.path.splitext(os.path.split(filepath)[-1])
            mod_name = ('unusual_prefix_' +
                        hashlib.sha1(filepath.encode('utf-8')).hexdigest() +
                        '_' + org_mod_name)

            if mod_name in sys.modules:
                del sys.modules[mod_name]

            with timeout(configuration.conf.getint('core', ""DAGBAG_IMPORT_TIMEOUT"")):
                try:
                    m = imp.load_source(mod_name, filepath)
                    mods.append(m)
                except Exception as e:
                    self.log.exception(""Failed to import: %s"", filepath)
                    self.import_errors[filepath] = str(e)
                    self.file_last_changed[filepath] = file_last_changed_on_disk

        else:
            zip_file = zipfile.ZipFile(filepath)
            for mod in zip_file.infolist():
                head, _ = os.path.split(mod.filename)
                mod_name, ext = os.path.splitext(mod.filename)
                if not head and (ext == '.py' or ext == '.pyc'):
                    if mod_name == '__init__':
                        self.log.warning(""Found __init__.%s at root of %s"", ext, filepath)
                    if safe_mode:
                        with zip_file.open(mod.filename) as zf:
                            self.log.debug(""Reading %s from %s"", mod.filename, filepath)
                            content = zf.read()
                            if not all([s in content for s in (b'DAG', b'airflow')]):
                                self.file_last_changed[filepath] = (
                                    file_last_changed_on_disk)
                                # todo: create ignore list
                                # Don't want to spam user with skip messages
                                if not self.has_logged:
                                    self.has_logged = True
                                    self.log.info(
                                        ""File %s assumed to contain no DAGs. Skipping."",
                                        filepath)

                    if mod_name in sys.modules:
                        del sys.modules[mod_name]

                    try:
                        sys.path.insert(0, filepath)
                        m = importlib.import_module(mod_name)
                        mods.append(m)
                    except Exception as e:
                        self.log.exception(""Failed to import: %s"", filepath)
                        self.import_errors[filepath] = str(e)
                        self.file_last_changed[filepath] = file_last_changed_on_disk

        for m in mods:
            for dag in list(m.__dict__.values()):
                if isinstance(dag, DAG):
                    if not dag.full_filepath:
                        dag.full_filepath = filepath
                        if dag.fileloc != filepath and not is_zipfile:
                            dag.fileloc = filepath
                    try:
                        dag.is_subdag = False
                        self.bag_dag(dag, parent_dag=dag, root_dag=dag)
                        if isinstance(dag._schedule_interval, six.string_types):
                            croniter(dag._schedule_interval)
                        found_dags.append(dag)
                        found_dags += dag.subdags
                    except (CroniterBadCronError,
                            CroniterBadDateError,
                            CroniterNotAlphaError) as cron_e:
                        self.log.exception(""Failed to bag_dag: %s"", dag.full_filepath)
                        self.import_errors[dag.full_filepath] = \
                            ""Invalid Cron expression: "" + str(cron_e)
                        self.file_last_changed[dag.full_filepath] = \
                            file_last_changed_on_disk
                    except AirflowDagCycleException as cycle_exception:
                        self.log.exception(""Failed to bag_dag: %s"", dag.full_filepath)
                        self.import_errors[dag.full_filepath] = str(cycle_exception)
                        self.file_last_changed[dag.full_filepath] = \
                            file_last_changed_on_disk

        self.file_last_changed[filepath] = file_last_changed_on_disk
        return found_dags","['def', 'process_file', '(', 'self', ',', 'filepath', ',', 'only_if_updated', '=', 'True', ',', 'safe_mode', '=', 'True', ')', ':', 'from', 'airflow', '.', 'models', '.', 'dag', 'import', 'DAG', '# Avoid circular import', 'found_dags', '=', '[', ']', '# if the source file no longer exists in the DB or in the filesystem,', '# return an empty list', '# todo: raise exception?', 'if', 'filepath', 'is', 'None', 'or', 'not', 'os', '.', 'path', '.', 'isfile', '(', 'filepath', ')', ':', 'return', 'found_dags', 'try', ':', '# This failed before in what may have been a git sync', '# race condition', 'file_last_changed_on_disk', '=', 'datetime', '.', 'fromtimestamp', '(', 'os', '.', 'path', '.', 'getmtime', '(', 'filepath', ')', ')', 'if', 'only_if_updated', 'and', 'filepath', 'in', 'self', '.', 'file_last_changed', 'and', 'file_last_changed_on_disk', '==', 'self', '.', 'file_last_changed', '[', 'filepath', ']', ':', 'return', 'found_dags', 'except', 'Exception', 'as', 'e', ':', 'self', '.', 'log', '.', 'exception', '(', 'e', ')', 'return', 'found_dags', 'mods', '=', '[', ']', 'is_zipfile', '=', 'zipfile', '.', 'is_zipfile', '(', 'filepath', ')', 'if', 'not', 'is_zipfile', ':', 'if', 'safe_mode', ':', 'with', 'open', '(', 'filepath', ',', ""'rb'"", ')', 'as', 'f', ':', 'content', '=', 'f', '.', 'read', '(', ')', 'if', 'not', 'all', '(', '[', 's', 'in', 'content', 'for', 's', 'in', '(', ""b'DAG'"", ',', ""b'airflow'"", ')', ']', ')', ':', 'self', '.', 'file_last_changed', '[', 'filepath', ']', '=', 'file_last_changed_on_disk', ""# Don't want to spam user with skip messages"", 'if', 'not', 'self', '.', 'has_logged', ':', 'self', '.', 'has_logged', '=', 'True', 'self', '.', 'log', '.', 'info', '(', '""File %s assumed to contain no DAGs. Skipping.""', ',', 'filepath', ')', 'return', 'found_dags', 'self', '.', 'log', '.', 'debug', '(', '""Importing %s""', ',', 'filepath', ')', 'org_mod_name', ',', '_', '=', 'os', '.', 'path', '.', 'splitext', '(', 'os', '.', 'path', '.', 'split', '(', 'filepath', ')', '[', '-', '1', ']', ')', 'mod_name', '=', '(', ""'unusual_prefix_'"", '+', 'hashlib', '.', 'sha1', '(', 'filepath', '.', 'encode', '(', ""'utf-8'"", ')', ')', '.', 'hexdigest', '(', ')', '+', ""'_'"", '+', 'org_mod_name', ')', 'if', 'mod_name', 'in', 'sys', '.', 'modules', ':', 'del', 'sys', '.', 'modules', '[', 'mod_name', ']', 'with', 'timeout', '(', 'configuration', '.', 'conf', '.', 'getint', '(', ""'core'"", ',', '""DAGBAG_IMPORT_TIMEOUT""', ')', ')', ':', 'try', ':', 'm', '=', 'imp', '.', 'load_source', '(', 'mod_name', ',', 'filepath', ')', 'mods', '.', 'append', '(', 'm', ')', 'except', 'Exception', 'as', 'e', ':', 'self', '.', 'log', '.', 'exception', '(', '""Failed to import: %s""', ',', 'filepath', ')', 'self', '.', 'import_errors', '[', 'filepath', ']', '=', 'str', '(', 'e', ')', 'self', '.', 'file_last_changed', '[', 'filepath', ']', '=', 'file_last_changed_on_disk', 'else', ':', 'zip_file', '=', 'zipfile', '.', 'ZipFile', '(', 'filepath', ')', 'for', 'mod', 'in', 'zip_file', '.', 'infolist', '(', ')', ':', 'head', ',', '_', '=', 'os', '.', 'path', '.', 'split', '(', 'mod', '.', 'filename', ')', 'mod_name', ',', 'ext', '=', 'os', '.', 'path', '.', 'splitext', '(', 'mod', '.', 'filename', ')', 'if', 'not', 'head', 'and', '(', 'ext', '==', ""'.py'"", 'or', 'ext', '==', ""'.pyc'"", ')', ':', 'if', 'mod_name', '==', ""'__init__'"", ':', 'self', '.', 'log', '.', 'warning', '(', '""Found __init__.%s at root of %s""', ',', 'ext', ',', 'filepath', ')', 'if', 'safe_mode', ':', 'with', 'zip_file', '.', 'open', '(', 'mod', '.', 'filename', ')', 'as', 'zf', ':', 'self', '.', 'log', '.', 'debug', '(', '""Reading %s from %s""', ',', 'mod', '.', 'filename', ',', 'filepath', ')', 'content', '=', 'zf', '.', 'read', '(', ')', 'if', 'not', 'all', '(', '[', 's', 'in', 'content', 'for', 's', 'in', '(', ""b'DAG'"", ',', ""b'airflow'"", ')', ']', ')', ':', 'self', '.', 'file_last_changed', '[', 'filepath', ']', '=', '(', 'file_last_changed_on_disk', ')', '# todo: create ignore list', ""# Don't want to spam user with skip messages"", 'if', 'not', 'self', '.', 'has_logged', ':', 'self', '.', 'has_logged', '=', 'True', 'self', '.', 'log', '.', 'info', '(', '""File %s assumed to contain no DAGs. Skipping.""', ',', 'filepath', ')', 'if', 'mod_name', 'in', 'sys', '.', 'modules', ':', 'del', 'sys', '.', 'modules', '[', 'mod_name', ']', 'try', ':', 'sys', '.', 'path', '.', 'insert', '(', '0', ',', 'filepath', ')', 'm', '=', 'importlib', '.', 'import_module', '(', 'mod_name', ')', 'mods', '.', 'append', '(', 'm', ')', 'except', 'Exception', 'as', 'e', ':', 'self', '.', 'log', '.', 'exception', '(', '""Failed to import: %s""', ',', 'filepath', ')', 'self', '.', 'import_errors', '[', 'filepath', ']', '=', 'str', '(', 'e', ')', 'self', '.', 'file_last_changed', '[', 'filepath', ']', '=', 'file_last_changed_on_disk', 'for', 'm', 'in', 'mods', ':', 'for', 'dag', 'in', 'list', '(', 'm', '.', '__dict__', '.', 'values', '(', ')', ')', ':', 'if', 'isinstance', '(', 'dag', ',', 'DAG', ')', ':', 'if', 'not', 'dag', '.', 'full_filepath', ':', 'dag', '.', 'full_filepath', '=', 'filepath', 'if', 'dag', '.', 'fileloc', '!=', 'filepath', 'and', 'not', 'is_zipfile', ':', 'dag', '.', 'fileloc', '=', 'filepath', 'try', ':', 'dag', '.', 'is_subdag', '=', 'False', 'self', '.', 'bag_dag', '(', 'dag', ',', 'parent_dag', '=', 'dag', ',', 'root_dag', '=', 'dag', ')', 'if', 'isinstance', '(', 'dag', '.', '_schedule_interval', ',', 'six', '.', 'string_types', ')', ':', 'croniter', '(', 'dag', '.', '_schedule_interval', ')', 'found_dags', '.', 'append', '(', 'dag', ')', 'found_dags', '+=', 'dag', '.', 'subdags', 'except', '(', 'CroniterBadCronError', ',', 'CroniterBadDateError', ',', 'CroniterNotAlphaError', ')', 'as', 'cron_e', ':', 'self', '.', 'log', '.', 'exception', '(', '""Failed to bag_dag: %s""', ',', 'dag', '.', 'full_filepath', ')', 'self', '.', 'import_errors', '[', 'dag', '.', 'full_filepath', ']', '=', '""Invalid Cron expression: ""', '+', 'str', '(', 'cron_e', ')', 'self', '.', 'file_last_changed', '[', 'dag', '.', 'full_filepath', ']', '=', 'file_last_changed_on_disk', 'except', 'AirflowDagCycleException', 'as', 'cycle_exception', ':', 'self', '.', 'log', '.', 'exception', '(', '""Failed to bag_dag: %s""', ',', 'dag', '.', 'full_filepath', ')', 'self', '.', 'import_errors', '[', 'dag', '.', 'full_filepath', ']', '=', 'str', '(', 'cycle_exception', ')', 'self', '.', 'file_last_changed', '[', 'dag', '.', 'full_filepath', ']', '=', 'file_last_changed_on_disk', 'self', '.', 'file_last_changed', '[', 'filepath', ']', '=', 'file_last_changed_on_disk', 'return', 'found_dags']","Given a path to a python module or zip file, this method imports
        the module and look for dag objects within it.","['Given', 'a', 'path', 'to', 'a', 'python', 'module', 'or', 'zip', 'file', 'this', 'method', 'imports', 'the', 'module', 'and', 'look', 'for', 'dag', 'objects', 'within', 'it', '.']",python,test,"['given', 'a', 'path', 'to', 'a', 'python', 'module', 'or', 'zip', 'file', 'this', 'method', 'imports', 'the', 'module', 'and', 'look', 'for', 'dag', 'objects', 'within', 'it', '.']",given a path to a python module or zip file this method imports the module and look for dag objects within it .,"['def', 'process_file', '(', 'self', ',', 'filepath', ',', 'only_if_updated', '=', 'true', ',', 'safe_mode', '=', 'true', ')', ':', 'from', 'airflow', '.', 'models', '.', 'dag', 'import', 'dag', '# avoid circular import', 'found_dags', '=', '[', ']', '# if the source file no longer exists in the db or in the filesystem,', '# return an empty list', '# todo: raise exception?', 'if', 'filepath', 'is', 'none', 'or', 'not', 'os', '.', 'path', '.', 'isfile', '(', 'filepath', ')', ':', 'return', 'found_dags', 'try', ':', '# this failed before in what may have been a git sync', '# race condition', 'file_last_changed_on_disk', '=', 'datetime', '.', 'fromtimestamp', '(', 'os', '.', 'path', '.', 'getmtime', '(', 'filepath', ')', ')', 'if', 'only_if_updated', 'and', 'filepath', 'in', 'self', '.', 'file_last_changed', 'and', 'file_last_changed_on_disk', '==', 'self', '.', 'file_last_changed', '[', 'filepath', ']', ':', 'return', 'found_dags', 'except', 'exception', 'as', 'e', ':', 'self', '.', 'log', '.', 'exception', '(', 'e', ')', 'return', 'found_dags', 'mods', '=', '[', ']', 'is_zipfile', '=', 'zipfile', '.', 'is_zipfile', '(', 'filepath', ')', 'if', 'not', 'is_zipfile', ':', 'if', 'safe_mode', ':', 'with', 'open', '(', 'filepath', ',', ""'rb'"", ')', 'as', 'f', ':', 'content', '=', 'f', '.', 'read', '(', ')', 'if', 'not', 'all', '(', '[', 's', 'in', 'content', 'for', 's', 'in', '(', ""b'dag'"", ',', ""b'airflow'"", ')', ']', ')', ':', 'self', '.', 'file_last_changed', '[', 'filepath', ']', '=', 'file_last_changed_on_disk', ""# don't want to spam user with skip messages"", 'if', 'not', 'self', '.', 'has_logged', ':', 'self', '.', 'has_logged', '=', 'true', 'self', '.', 'log', '.', 'info', '(', '""file %s assumed to contain no dags. skipping.""', ',', 'filepath', ')', 'return', 'found_dags', 'self', '.', 'log', '.', 'debug', '(', '""importing %s""', ',', 'filepath', ')', 'org_mod_name', ',', '_', '=', 'os', '.', 'path', '.', 'splitext', '(', 'os', '.', 'path', '.', 'split', '(', 'filepath', ')', '[', '-', '1', ']', ')', 'mod_name', '=', '(', ""'unusual_prefix_'"", '+', 'hashlib', '.', 'sha1', '(', 'filepath', '.', 'encode', '(', ""'utf-8'"", ')', ')', '.', 'hexdigest', '(', ')', '+', ""'_'"", '+', 'org_mod_name', ')', 'if', 'mod_name', 'in', 'sys', '.', 'modules', ':', 'del', 'sys', '.', 'modules', '[', 'mod_name', ']', 'with', 'timeout', '(', 'configuration', '.', 'conf', '.', 'getint', '(', ""'core'"", ',', '""dagbag_import_timeout""', ')', ')', ':', 'try', ':', 'm', '=', 'imp', '.', 'load_source', '(', 'mod_name', ',', 'filepath', ')', 'mods', '.', 'append', '(', 'm', ')', 'except', 'exception', 'as', 'e', ':', 'self', '.', 'log', '.', 'exception', '(', '""failed to import: %s""', ',', 'filepath', ')', 'self', '.', 'import_errors', '[', 'filepath', ']', '=', 'str', '(', 'e', ')', 'self', '.', 'file_last_changed', '[', 'filepath', ']', '=', 'file_last_changed_on_disk', 'else', ':', 'zip_file', '=', 'zipfile', '.', 'zipfile', '(', 'filepath', ')', 'for', 'mod', 'in', 'zip_file', '.', 'infolist', '(', ')', ':', 'head', ',', '_', '=', 'os', '.', 'path', '.', 'split', '(', 'mod', '.', 'filename', ')', 'mod_name', ',', 'ext', '=', 'os', '.', 'path', '.', 'splitext', '(', 'mod', '.', 'filename', ')', 'if', 'not', 'head', 'and', '(', 'ext', '==', ""'.py'"", 'or', 'ext', '==', ""'.pyc'"", ')', ':', 'if', 'mod_name', '==', ""'__init__'"", ':', 'self', '.', 'log', '.', 'warning', '(', '""found __init__.%s at root of %s""', ',', 'ext', ',', 'filepath', ')', 'if', 'safe_mode', ':', 'with', 'zip_file', '.', 'open', '(', 'mod', '.', 'filename', ')', 'as', 'zf', ':', 'self', '.', 'log', '.', 'debug', '(', '""reading %s from %s""', ',', 'mod', '.', 'filename', ',', 'filepath', ')', 'content', '=', 'zf', '.', 'read', '(', ')', 'if', 'not', 'all', '(', '[', 's', 'in', 'content', 'for', 's', 'in', '(', ""b'dag'"", ',', ""b'airflow'"", ')', ']', ')', ':', 'self', '.', 'file_last_changed', '[', 'filepath', ']', '=', '(', 'file_last_changed_on_disk', ')', '# todo: create ignore list', ""# don't want to spam user with skip messages"", 'if', 'not', 'self', '.', 'has_logged', ':', 'self', '.', 'has_logged', '=', 'true', 'self', '.', 'log', '.', 'info', '(', '""file %s assumed to contain no dags. skipping.""', ',', 'filepath', ')', 'if', 'mod_name', 'in', 'sys', '.', 'modules', ':', 'del', 'sys', '.', 'modules', '[', 'mod_name', ']', 'try', ':', 'sys', '.', 'path', '.', 'insert', '(', '0', ',', 'filepath', ')', 'm', '=', 'importlib', '.', 'import_module', '(', 'mod_name', ')', 'mods', '.', 'append', '(', 'm', ')', 'except', 'exception', 'as', 'e', ':', 'self', '.', 'log', '.', 'exception', '(', '""failed to import: %s""', ',', 'filepath', ')', 'self', '.', 'import_errors', '[', 'filepath', ']', '=', 'str', '(', 'e', ')', 'self', '.', 'file_last_changed', '[', 'filepath', ']', '=', 'file_last_changed_on_disk', 'for', 'm', 'in', 'mods', ':', 'for', 'dag', 'in', 'list', '(', 'm', '.', '__dict__', '.', 'values', '(', ')', ')', ':', 'if', 'isinstance', '(', 'dag', ',', 'dag', ')', ':', 'if', 'not', 'dag', '.', 'full_filepath', ':', 'dag', '.', 'full_filepath', '=', 'filepath', 'if', 'dag', '.', 'fileloc', '!=', 'filepath', 'and', 'not', 'is_zipfile', ':', 'dag', '.', 'fileloc', '=', 'filepath', 'try', ':', 'dag', '.', 'is_subdag', '=', 'false', 'self', '.', 'bag_dag', '(', 'dag', ',', 'parent_dag', '=', 'dag', ',', 'root_dag', '=', 'dag', ')', 'if', 'isinstance', '(', 'dag', '.', '_schedule_interval', ',', 'six', '.', 'string_types', ')', ':', 'croniter', '(', 'dag', '.', '_schedule_interval', ')', 'found_dags', '.', 'append', '(', 'dag', ')', 'found_dags', '+=', 'dag', '.', 'subdags', 'except', '(', 'croniterbadcronerror', ',', 'croniterbaddateerror', ',', 'croniternotalphaerror', ')', 'as', 'cron_e', ':', 'self', '.', 'log', '.', 'exception', '(', '""failed to bag_dag: %s""', ',', 'dag', '.', 'full_filepath', ')', 'self', '.', 'import_errors', '[', 'dag', '.', 'full_filepath', ']', '=', '""invalid cron expression: ""', '+', 'str', '(', 'cron_e', ')', 'self', '.', 'file_last_changed', '[', 'dag', '.', 'full_filepath', ']', '=', 'file_last_changed_on_disk', 'except', 'airflowdagcycleexception', 'as', 'cycle_exception', ':', 'self', '.', 'log', '.', 'exception', '(', '""failed to bag_dag: %s""', ',', 'dag', '.', 'full_filepath', ')', 'self', '.', 'import_errors', '[', 'dag', '.', 'full_filepath', ']', '=', 'str', '(', 'cycle_exception', ')', 'self', '.', 'file_last_changed', '[', 'dag', '.', 'full_filepath', ']', '=', 'file_last_changed_on_disk', 'self', '.', 'file_last_changed', '[', 'filepath', ']', '=', 'file_last_changed_on_disk', 'return', 'found_dags']","def process_file ( self , filepath , only_if_updated = true , safe_mode = true ) : from airflow . models . dag import dag # avoid circular import found_dags = [ ] # if the source file no longer exists in the db or in the filesystem, # return an empty list # todo: raise exception? if filepath is none or not os . path . isfile ( filepath ) : return found_dags try : # this failed before in what may have been a git sync # race condition file_last_changed_on_disk = datetime . fromtimestamp ( os . path . getmtime ( filepath ) ) if only_if_updated and filepath in self . file_last_changed and file_last_changed_on_disk == self . file_last_changed [ filepath ] : return found_dags except exception as e : self . log . exception ( e ) return found_dags mods = [ ] is_zipfile = zipfile . is_zipfile ( filepath ) if not is_zipfile : if safe_mode : with open ( filepath , 'rb' ) as f : content = f . read ( ) if not all ( [ s in content for s in ( b'dag' , b'airflow' ) ] ) : self . file_last_changed [ filepath ] = file_last_changed_on_disk # don't want to spam user with skip messages if not self . has_logged : self . has_logged = true self . log . info ( ""file %s assumed to contain no dags. skipping."" , filepath ) return found_dags self . log . debug ( ""importing %s"" , filepath ) org_mod_name , _ = os . path . splitext ( os . path . split ( filepath ) [ - 1 ] ) mod_name = ( 'unusual_prefix_' + hashlib . sha1 ( filepath . encode ( 'utf-8' ) ) . hexdigest ( ) + '_' + org_mod_name ) if mod_name in sys . modules : del sys . modules [ mod_name ] with timeout ( configuration . conf . getint ( 'core' , ""dagbag_import_timeout"" ) ) : try : m = imp . load_source ( mod_name , filepath ) mods . append ( m ) except exception as e : self . log . exception ( ""failed to import: %s"" , filepath ) self . import_errors [ filepath ] = str ( e ) self . file_last_changed [ filepath ] = file_last_changed_on_disk else : zip_file = zipfile . zipfile ( filepath ) for mod in zip_file . infolist ( ) : head , _ = os . path . split ( mod . filename ) mod_name , ext = os . path . splitext ( mod . filename ) if not head and ( ext == '.py' or ext == '.pyc' ) : if mod_name == '__init__' : self . log . warning ( ""found __init__.%s at root of %s"" , ext , filepath ) if safe_mode : with zip_file . open ( mod . filename ) as zf : self . log . debug ( ""reading %s from %s"" , mod . filename , filepath ) content = zf . read ( ) if not all ( [ s in content for s in ( b'dag' , b'airflow' ) ] ) : self . file_last_changed [ filepath ] = ( file_last_changed_on_disk ) # todo: create ignore list # don't want to spam user with skip messages if not self . has_logged : self . has_logged = true self . log . info ( ""file %s assumed to contain no dags. skipping."" , filepath ) if mod_name in sys . modules : del sys . modules [ mod_name ] try : sys . path . insert ( 0 , filepath ) m = importlib . import_module ( mod_name ) mods . append ( m ) except exception as e : self . log . exception ( ""failed to import: %s"" , filepath ) self . import_errors [ filepath ] = str ( e ) self . file_last_changed [ filepath ] = file_last_changed_on_disk for m in mods : for dag in list ( m . __dict__ . values ( ) ) : if isinstance ( dag , dag ) : if not dag . full_filepath : dag . full_filepath = filepath if dag . fileloc != filepath and not is_zipfile : dag . fileloc = filepath try : dag . is_subdag = false self . bag_dag ( dag , parent_dag = dag , root_dag = dag ) if isinstance ( dag . _schedule_interval , six . string_types ) : croniter ( dag . _schedule_interval ) found_dags . append ( dag ) found_dags += dag . subdags except ( croniterbadcronerror , croniterbaddateerror , croniternotalphaerror ) as cron_e : self . log . exception ( ""failed to bag_dag: %s"" , dag . full_filepath ) self . import_errors [ dag . full_filepath ] = ""invalid cron expression: "" + str ( cron_e ) self . file_last_changed [ dag . full_filepath ] = file_last_changed_on_disk except airflowdagcycleexception as cycle_exception : self . log . exception ( ""failed to bag_dag: %s"" , dag . full_filepath ) self . import_errors [ dag . full_filepath ] = str ( cycle_exception ) self . file_last_changed [ dag . full_filepath ] = file_last_changed_on_disk self . file_last_changed [ filepath ] = file_last_changed_on_disk return found_dags"
332,apache/airflow,airflow/models/dagbag.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagbag.py#L274-L303,"def kill_zombies(self, zombies, session=None):
        """"""
        Fail given zombie tasks, which are tasks that haven't
        had a heartbeat for too long, in the current DagBag.

        :param zombies: zombie task instances to kill.
        :type zombies: airflow.utils.dag_processing.SimpleTaskInstance
        :param session: DB session.
        :type session: sqlalchemy.orm.session.Session
        """"""
        from airflow.models.taskinstance import TaskInstance  # Avoid circular import

        for zombie in zombies:
            if zombie.dag_id in self.dags:
                dag = self.dags[zombie.dag_id]
                if zombie.task_id in dag.task_ids:
                    task = dag.get_task(zombie.task_id)
                    ti = TaskInstance(task, zombie.execution_date)
                    # Get properties needed for failure handling from SimpleTaskInstance.
                    ti.start_date = zombie.start_date
                    ti.end_date = zombie.end_date
                    ti.try_number = zombie.try_number
                    ti.state = zombie.state
                    ti.test_mode = configuration.getboolean('core', 'unit_test_mode')
                    ti.handle_failure(""{} detected as zombie"".format(ti),
                                      ti.test_mode, ti.get_template_context())
                    self.log.info(
                        'Marked zombie job %s as %s', ti, ti.state)
                    Stats.incr('zombies_killed')
        session.commit()","['def', 'kill_zombies', '(', 'self', ',', 'zombies', ',', 'session', '=', 'None', ')', ':', 'from', 'airflow', '.', 'models', '.', 'taskinstance', 'import', 'TaskInstance', '# Avoid circular import', 'for', 'zombie', 'in', 'zombies', ':', 'if', 'zombie', '.', 'dag_id', 'in', 'self', '.', 'dags', ':', 'dag', '=', 'self', '.', 'dags', '[', 'zombie', '.', 'dag_id', ']', 'if', 'zombie', '.', 'task_id', 'in', 'dag', '.', 'task_ids', ':', 'task', '=', 'dag', '.', 'get_task', '(', 'zombie', '.', 'task_id', ')', 'ti', '=', 'TaskInstance', '(', 'task', ',', 'zombie', '.', 'execution_date', ')', '# Get properties needed for failure handling from SimpleTaskInstance.', 'ti', '.', 'start_date', '=', 'zombie', '.', 'start_date', 'ti', '.', 'end_date', '=', 'zombie', '.', 'end_date', 'ti', '.', 'try_number', '=', 'zombie', '.', 'try_number', 'ti', '.', 'state', '=', 'zombie', '.', 'state', 'ti', '.', 'test_mode', '=', 'configuration', '.', 'getboolean', '(', ""'core'"", ',', ""'unit_test_mode'"", ')', 'ti', '.', 'handle_failure', '(', '""{} detected as zombie""', '.', 'format', '(', 'ti', ')', ',', 'ti', '.', 'test_mode', ',', 'ti', '.', 'get_template_context', '(', ')', ')', 'self', '.', 'log', '.', 'info', '(', ""'Marked zombie job %s as %s'"", ',', 'ti', ',', 'ti', '.', 'state', ')', 'Stats', '.', 'incr', '(', ""'zombies_killed'"", ')', 'session', '.', 'commit', '(', ')']","Fail given zombie tasks, which are tasks that haven't
        had a heartbeat for too long, in the current DagBag.

        :param zombies: zombie task instances to kill.
        :type zombies: airflow.utils.dag_processing.SimpleTaskInstance
        :param session: DB session.
        :type session: sqlalchemy.orm.session.Session","['Fail', 'given', 'zombie', 'tasks', 'which', 'are', 'tasks', 'that', 'haven', 't', 'had', 'a', 'heartbeat', 'for', 'too', 'long', 'in', 'the', 'current', 'DagBag', '.']",python,test,"['fail', 'given', 'zombie', 'tasks', 'which', 'are', 'tasks', 'that', 'haven', 't', 'had', 'a', 'heartbeat', 'for', 'too', 'long', 'in', 'the', 'current', 'dagbag', '.']",fail given zombie tasks which are tasks that haven t had a heartbeat for too long in the current dagbag .,"['def', 'kill_zombies', '(', 'self', ',', 'zombies', ',', 'session', '=', 'none', ')', ':', 'from', 'airflow', '.', 'models', '.', 'taskinstance', 'import', 'taskinstance', '# avoid circular import', 'for', 'zombie', 'in', 'zombies', ':', 'if', 'zombie', '.', 'dag_id', 'in', 'self', '.', 'dags', ':', 'dag', '=', 'self', '.', 'dags', '[', 'zombie', '.', 'dag_id', ']', 'if', 'zombie', '.', 'task_id', 'in', 'dag', '.', 'task_ids', ':', 'task', '=', 'dag', '.', 'get_task', '(', 'zombie', '.', 'task_id', ')', 'ti', '=', 'taskinstance', '(', 'task', ',', 'zombie', '.', 'execution_date', ')', '# get properties needed for failure handling from simpletaskinstance.', 'ti', '.', 'start_date', '=', 'zombie', '.', 'start_date', 'ti', '.', 'end_date', '=', 'zombie', '.', 'end_date', 'ti', '.', 'try_number', '=', 'zombie', '.', 'try_number', 'ti', '.', 'state', '=', 'zombie', '.', 'state', 'ti', '.', 'test_mode', '=', 'configuration', '.', 'getboolean', '(', ""'core'"", ',', ""'unit_test_mode'"", ')', 'ti', '.', 'handle_failure', '(', '""{} detected as zombie""', '.', 'format', '(', 'ti', ')', ',', 'ti', '.', 'test_mode', ',', 'ti', '.', 'get_template_context', '(', ')', ')', 'self', '.', 'log', '.', 'info', '(', ""'marked zombie job %s as %s'"", ',', 'ti', ',', 'ti', '.', 'state', ')', 'stats', '.', 'incr', '(', ""'zombies_killed'"", ')', 'session', '.', 'commit', '(', ')']","def kill_zombies ( self , zombies , session = none ) : from airflow . models . taskinstance import taskinstance # avoid circular import for zombie in zombies : if zombie . dag_id in self . dags : dag = self . dags [ zombie . dag_id ] if zombie . task_id in dag . task_ids : task = dag . get_task ( zombie . task_id ) ti = taskinstance ( task , zombie . execution_date ) # get properties needed for failure handling from simpletaskinstance. ti . start_date = zombie . start_date ti . end_date = zombie . end_date ti . try_number = zombie . try_number ti . state = zombie . state ti . test_mode = configuration . getboolean ( 'core' , 'unit_test_mode' ) ti . handle_failure ( ""{} detected as zombie"" . format ( ti ) , ti . test_mode , ti . get_template_context ( ) ) self . log . info ( 'marked zombie job %s as %s' , ti , ti . state ) stats . incr ( 'zombies_killed' ) session . commit ( )"
333,apache/airflow,airflow/models/dagbag.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagbag.py#L305-L339,"def bag_dag(self, dag, parent_dag, root_dag):
        """"""
        Adds the DAG into the bag, recurses into sub dags.
        Throws AirflowDagCycleException if a cycle is detected in this dag or its subdags
        """"""

        dag.test_cycle()  # throws if a task cycle is found

        dag.resolve_template_files()
        dag.last_loaded = timezone.utcnow()

        for task in dag.tasks:
            settings.policy(task)

        subdags = dag.subdags

        try:
            for subdag in subdags:
                subdag.full_filepath = dag.full_filepath
                subdag.parent_dag = dag
                subdag.is_subdag = True
                self.bag_dag(subdag, parent_dag=dag, root_dag=root_dag)

            self.dags[dag.dag_id] = dag
            self.log.debug('Loaded DAG %s', dag)
        except AirflowDagCycleException as cycle_exception:
            # There was an error in bagging the dag. Remove it from the list of dags
            self.log.exception('Exception bagging dag: %s', dag.dag_id)
            # Only necessary at the root level since DAG.subdags automatically
            # performs DFS to search through all subdags
            if dag == root_dag:
                for subdag in subdags:
                    if subdag.dag_id in self.dags:
                        del self.dags[subdag.dag_id]
            raise cycle_exception","['def', 'bag_dag', '(', 'self', ',', 'dag', ',', 'parent_dag', ',', 'root_dag', ')', ':', 'dag', '.', 'test_cycle', '(', ')', '# throws if a task cycle is found', 'dag', '.', 'resolve_template_files', '(', ')', 'dag', '.', 'last_loaded', '=', 'timezone', '.', 'utcnow', '(', ')', 'for', 'task', 'in', 'dag', '.', 'tasks', ':', 'settings', '.', 'policy', '(', 'task', ')', 'subdags', '=', 'dag', '.', 'subdags', 'try', ':', 'for', 'subdag', 'in', 'subdags', ':', 'subdag', '.', 'full_filepath', '=', 'dag', '.', 'full_filepath', 'subdag', '.', 'parent_dag', '=', 'dag', 'subdag', '.', 'is_subdag', '=', 'True', 'self', '.', 'bag_dag', '(', 'subdag', ',', 'parent_dag', '=', 'dag', ',', 'root_dag', '=', 'root_dag', ')', 'self', '.', 'dags', '[', 'dag', '.', 'dag_id', ']', '=', 'dag', 'self', '.', 'log', '.', 'debug', '(', ""'Loaded DAG %s'"", ',', 'dag', ')', 'except', 'AirflowDagCycleException', 'as', 'cycle_exception', ':', '# There was an error in bagging the dag. Remove it from the list of dags', 'self', '.', 'log', '.', 'exception', '(', ""'Exception bagging dag: %s'"", ',', 'dag', '.', 'dag_id', ')', '# Only necessary at the root level since DAG.subdags automatically', '# performs DFS to search through all subdags', 'if', 'dag', '==', 'root_dag', ':', 'for', 'subdag', 'in', 'subdags', ':', 'if', 'subdag', '.', 'dag_id', 'in', 'self', '.', 'dags', ':', 'del', 'self', '.', 'dags', '[', 'subdag', '.', 'dag_id', ']', 'raise', 'cycle_exception']","Adds the DAG into the bag, recurses into sub dags.
        Throws AirflowDagCycleException if a cycle is detected in this dag or its subdags","['Adds', 'the', 'DAG', 'into', 'the', 'bag', 'recurses', 'into', 'sub', 'dags', '.', 'Throws', 'AirflowDagCycleException', 'if', 'a', 'cycle', 'is', 'detected', 'in', 'this', 'dag', 'or', 'its', 'subdags']",python,test,"['adds', 'the', 'dag', 'into', 'the', 'bag', 'recurses', 'into', 'sub', 'dags', '.', 'throws', 'airflowdagcycleexception', 'if', 'a', 'cycle', 'is', 'detected', 'in', 'this', 'dag', 'or', 'its', 'subdags']",adds the dag into the bag recurses into sub dags . throws airflowdagcycleexception if a cycle is detected in this dag or its subdags,"['def', 'bag_dag', '(', 'self', ',', 'dag', ',', 'parent_dag', ',', 'root_dag', ')', ':', 'dag', '.', 'test_cycle', '(', ')', '# throws if a task cycle is found', 'dag', '.', 'resolve_template_files', '(', ')', 'dag', '.', 'last_loaded', '=', 'timezone', '.', 'utcnow', '(', ')', 'for', 'task', 'in', 'dag', '.', 'tasks', ':', 'settings', '.', 'policy', '(', 'task', ')', 'subdags', '=', 'dag', '.', 'subdags', 'try', ':', 'for', 'subdag', 'in', 'subdags', ':', 'subdag', '.', 'full_filepath', '=', 'dag', '.', 'full_filepath', 'subdag', '.', 'parent_dag', '=', 'dag', 'subdag', '.', 'is_subdag', '=', 'true', 'self', '.', 'bag_dag', '(', 'subdag', ',', 'parent_dag', '=', 'dag', ',', 'root_dag', '=', 'root_dag', ')', 'self', '.', 'dags', '[', 'dag', '.', 'dag_id', ']', '=', 'dag', 'self', '.', 'log', '.', 'debug', '(', ""'loaded dag %s'"", ',', 'dag', ')', 'except', 'airflowdagcycleexception', 'as', 'cycle_exception', ':', '# there was an error in bagging the dag. remove it from the list of dags', 'self', '.', 'log', '.', 'exception', '(', ""'exception bagging dag: %s'"", ',', 'dag', '.', 'dag_id', ')', '# only necessary at the root level since dag.subdags automatically', '# performs dfs to search through all subdags', 'if', 'dag', '==', 'root_dag', ':', 'for', 'subdag', 'in', 'subdags', ':', 'if', 'subdag', '.', 'dag_id', 'in', 'self', '.', 'dags', ':', 'del', 'self', '.', 'dags', '[', 'subdag', '.', 'dag_id', ']', 'raise', 'cycle_exception']","def bag_dag ( self , dag , parent_dag , root_dag ) : dag . test_cycle ( ) # throws if a task cycle is found dag . resolve_template_files ( ) dag . last_loaded = timezone . utcnow ( ) for task in dag . tasks : settings . policy ( task ) subdags = dag . subdags try : for subdag in subdags : subdag . full_filepath = dag . full_filepath subdag . parent_dag = dag subdag . is_subdag = true self . bag_dag ( subdag , parent_dag = dag , root_dag = root_dag ) self . dags [ dag . dag_id ] = dag self . log . debug ( 'loaded dag %s' , dag ) except airflowdagcycleexception as cycle_exception : # there was an error in bagging the dag. remove it from the list of dags self . log . exception ( 'exception bagging dag: %s' , dag . dag_id ) # only necessary at the root level since dag.subdags automatically # performs dfs to search through all subdags if dag == root_dag : for subdag in subdags : if subdag . dag_id in self . dags : del self . dags [ subdag . dag_id ] raise cycle_exception"
334,apache/airflow,airflow/models/dagbag.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagbag.py#L341-L396,"def collect_dags(
            self,
            dag_folder=None,
            only_if_updated=True,
            include_examples=configuration.conf.getboolean('core', 'LOAD_EXAMPLES'),
            safe_mode=configuration.conf.getboolean('core', 'DAG_DISCOVERY_SAFE_MODE')):
        """"""
        Given a file path or a folder, this method looks for python modules,
        imports them and adds them to the dagbag collection.

        Note that if a ``.airflowignore`` file is found while processing
        the directory, it will behave much like a ``.gitignore``,
        ignoring files that match any of the regex patterns specified
        in the file.

        **Note**: The patterns in .airflowignore are treated as
        un-anchored regexes, not shell-like glob patterns.
        """"""
        start_dttm = timezone.utcnow()
        dag_folder = dag_folder or self.dag_folder

        # Used to store stats around DagBag processing
        stats = []
        FileLoadStat = namedtuple(
            'FileLoadStat', ""file duration dag_num task_num dags"")

        dag_folder = correct_maybe_zipped(dag_folder)

        for filepath in list_py_file_paths(dag_folder, safe_mode=safe_mode,
                                           include_examples=include_examples):
            try:
                ts = timezone.utcnow()
                found_dags = self.process_file(
                    filepath, only_if_updated=only_if_updated,
                    safe_mode=safe_mode)

                td = timezone.utcnow() - ts
                td = td.total_seconds() + (
                    float(td.microseconds) / 1000000)
                stats.append(FileLoadStat(
                    filepath.replace(dag_folder, ''),
                    td,
                    len(found_dags),
                    sum([len(dag.tasks) for dag in found_dags]),
                    str([dag.dag_id for dag in found_dags]),
                ))
            except Exception as e:
                self.log.exception(e)
        Stats.gauge(
            'collect_dags', (timezone.utcnow() - start_dttm).total_seconds(), 1)
        Stats.gauge(
            'dagbag_size', len(self.dags), 1)
        Stats.gauge(
            'dagbag_import_errors', len(self.import_errors), 1)
        self.dagbag_stats = sorted(
            stats, key=lambda x: x.duration, reverse=True)","['def', 'collect_dags', '(', 'self', ',', 'dag_folder', '=', 'None', ',', 'only_if_updated', '=', 'True', ',', 'include_examples', '=', 'configuration', '.', 'conf', '.', 'getboolean', '(', ""'core'"", ',', ""'LOAD_EXAMPLES'"", ')', ',', 'safe_mode', '=', 'configuration', '.', 'conf', '.', 'getboolean', '(', ""'core'"", ',', ""'DAG_DISCOVERY_SAFE_MODE'"", ')', ')', ':', 'start_dttm', '=', 'timezone', '.', 'utcnow', '(', ')', 'dag_folder', '=', 'dag_folder', 'or', 'self', '.', 'dag_folder', '# Used to store stats around DagBag processing', 'stats', '=', '[', ']', 'FileLoadStat', '=', 'namedtuple', '(', ""'FileLoadStat'"", ',', '""file duration dag_num task_num dags""', ')', 'dag_folder', '=', 'correct_maybe_zipped', '(', 'dag_folder', ')', 'for', 'filepath', 'in', 'list_py_file_paths', '(', 'dag_folder', ',', 'safe_mode', '=', 'safe_mode', ',', 'include_examples', '=', 'include_examples', ')', ':', 'try', ':', 'ts', '=', 'timezone', '.', 'utcnow', '(', ')', 'found_dags', '=', 'self', '.', 'process_file', '(', 'filepath', ',', 'only_if_updated', '=', 'only_if_updated', ',', 'safe_mode', '=', 'safe_mode', ')', 'td', '=', 'timezone', '.', 'utcnow', '(', ')', '-', 'ts', 'td', '=', 'td', '.', 'total_seconds', '(', ')', '+', '(', 'float', '(', 'td', '.', 'microseconds', ')', '/', '1000000', ')', 'stats', '.', 'append', '(', 'FileLoadStat', '(', 'filepath', '.', 'replace', '(', 'dag_folder', ',', ""''"", ')', ',', 'td', ',', 'len', '(', 'found_dags', ')', ',', 'sum', '(', '[', 'len', '(', 'dag', '.', 'tasks', ')', 'for', 'dag', 'in', 'found_dags', ']', ')', ',', 'str', '(', '[', 'dag', '.', 'dag_id', 'for', 'dag', 'in', 'found_dags', ']', ')', ',', ')', ')', 'except', 'Exception', 'as', 'e', ':', 'self', '.', 'log', '.', 'exception', '(', 'e', ')', 'Stats', '.', 'gauge', '(', ""'collect_dags'"", ',', '(', 'timezone', '.', 'utcnow', '(', ')', '-', 'start_dttm', ')', '.', 'total_seconds', '(', ')', ',', '1', ')', 'Stats', '.', 'gauge', '(', ""'dagbag_size'"", ',', 'len', '(', 'self', '.', 'dags', ')', ',', '1', ')', 'Stats', '.', 'gauge', '(', ""'dagbag_import_errors'"", ',', 'len', '(', 'self', '.', 'import_errors', ')', ',', '1', ')', 'self', '.', 'dagbag_stats', '=', 'sorted', '(', 'stats', ',', 'key', '=', 'lambda', 'x', ':', 'x', '.', 'duration', ',', 'reverse', '=', 'True', ')']","Given a file path or a folder, this method looks for python modules,
        imports them and adds them to the dagbag collection.

        Note that if a ``.airflowignore`` file is found while processing
        the directory, it will behave much like a ``.gitignore``,
        ignoring files that match any of the regex patterns specified
        in the file.

        **Note**: The patterns in .airflowignore are treated as
        un-anchored regexes, not shell-like glob patterns.","['Given', 'a', 'file', 'path', 'or', 'a', 'folder', 'this', 'method', 'looks', 'for', 'python', 'modules', 'imports', 'them', 'and', 'adds', 'them', 'to', 'the', 'dagbag', 'collection', '.']",python,test,"['given', 'a', 'file', 'path', 'or', 'a', 'folder', 'this', 'method', 'looks', 'for', 'python', 'modules', 'imports', 'them', 'and', 'adds', 'them', 'to', 'the', 'dagbag', 'collection', '.']",given a file path or a folder this method looks for python modules imports them and adds them to the dagbag collection .,"['def', 'collect_dags', '(', 'self', ',', 'dag_folder', '=', 'none', ',', 'only_if_updated', '=', 'true', ',', 'include_examples', '=', 'configuration', '.', 'conf', '.', 'getboolean', '(', ""'core'"", ',', ""'load_examples'"", ')', ',', 'safe_mode', '=', 'configuration', '.', 'conf', '.', 'getboolean', '(', ""'core'"", ',', ""'dag_discovery_safe_mode'"", ')', ')', ':', 'start_dttm', '=', 'timezone', '.', 'utcnow', '(', ')', 'dag_folder', '=', 'dag_folder', 'or', 'self', '.', 'dag_folder', '# used to store stats around dagbag processing', 'stats', '=', '[', ']', 'fileloadstat', '=', 'namedtuple', '(', ""'fileloadstat'"", ',', '""file duration dag_num task_num dags""', ')', 'dag_folder', '=', 'correct_maybe_zipped', '(', 'dag_folder', ')', 'for', 'filepath', 'in', 'list_py_file_paths', '(', 'dag_folder', ',', 'safe_mode', '=', 'safe_mode', ',', 'include_examples', '=', 'include_examples', ')', ':', 'try', ':', 'ts', '=', 'timezone', '.', 'utcnow', '(', ')', 'found_dags', '=', 'self', '.', 'process_file', '(', 'filepath', ',', 'only_if_updated', '=', 'only_if_updated', ',', 'safe_mode', '=', 'safe_mode', ')', 'td', '=', 'timezone', '.', 'utcnow', '(', ')', '-', 'ts', 'td', '=', 'td', '.', 'total_seconds', '(', ')', '+', '(', 'float', '(', 'td', '.', 'microseconds', ')', '/', '1000000', ')', 'stats', '.', 'append', '(', 'fileloadstat', '(', 'filepath', '.', 'replace', '(', 'dag_folder', ',', ""''"", ')', ',', 'td', ',', 'len', '(', 'found_dags', ')', ',', 'sum', '(', '[', 'len', '(', 'dag', '.', 'tasks', ')', 'for', 'dag', 'in', 'found_dags', ']', ')', ',', 'str', '(', '[', 'dag', '.', 'dag_id', 'for', 'dag', 'in', 'found_dags', ']', ')', ',', ')', ')', 'except', 'exception', 'as', 'e', ':', 'self', '.', 'log', '.', 'exception', '(', 'e', ')', 'stats', '.', 'gauge', '(', ""'collect_dags'"", ',', '(', 'timezone', '.', 'utcnow', '(', ')', '-', 'start_dttm', ')', '.', 'total_seconds', '(', ')', ',', '1', ')', 'stats', '.', 'gauge', '(', ""'dagbag_size'"", ',', 'len', '(', 'self', '.', 'dags', ')', ',', '1', ')', 'stats', '.', 'gauge', '(', ""'dagbag_import_errors'"", ',', 'len', '(', 'self', '.', 'import_errors', ')', ',', '1', ')', 'self', '.', 'dagbag_stats', '=', 'sorted', '(', 'stats', ',', 'key', '=', 'lambda', 'x', ':', 'x', '.', 'duration', ',', 'reverse', '=', 'true', ')']","def collect_dags ( self , dag_folder = none , only_if_updated = true , include_examples = configuration . conf . getboolean ( 'core' , 'load_examples' ) , safe_mode = configuration . conf . getboolean ( 'core' , 'dag_discovery_safe_mode' ) ) : start_dttm = timezone . utcnow ( ) dag_folder = dag_folder or self . dag_folder # used to store stats around dagbag processing stats = [ ] fileloadstat = namedtuple ( 'fileloadstat' , ""file duration dag_num task_num dags"" ) dag_folder = correct_maybe_zipped ( dag_folder ) for filepath in list_py_file_paths ( dag_folder , safe_mode = safe_mode , include_examples = include_examples ) : try : ts = timezone . utcnow ( ) found_dags = self . process_file ( filepath , only_if_updated = only_if_updated , safe_mode = safe_mode ) td = timezone . utcnow ( ) - ts td = td . total_seconds ( ) + ( float ( td . microseconds ) / 1000000 ) stats . append ( fileloadstat ( filepath . replace ( dag_folder , '' ) , td , len ( found_dags ) , sum ( [ len ( dag . tasks ) for dag in found_dags ] ) , str ( [ dag . dag_id for dag in found_dags ] ) , ) ) except exception as e : self . log . exception ( e ) stats . gauge ( 'collect_dags' , ( timezone . utcnow ( ) - start_dttm ) . total_seconds ( ) , 1 ) stats . gauge ( 'dagbag_size' , len ( self . dags ) , 1 ) stats . gauge ( 'dagbag_import_errors' , len ( self . import_errors ) , 1 ) self . dagbag_stats = sorted ( stats , key = lambda x : x . duration , reverse = true )"
335,apache/airflow,airflow/models/dagbag.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/dagbag.py#L398-L416,"def dagbag_report(self):
        """"""Prints a report around DagBag loading stats""""""
        report = textwrap.dedent(""""""\n
        -------------------------------------------------------------------
        DagBag loading stats for {dag_folder}
        -------------------------------------------------------------------
        Number of DAGs: {dag_num}
        Total task number: {task_num}
        DagBag parsing time: {duration}
        {table}
        """""")
        stats = self.dagbag_stats
        return report.format(
            dag_folder=self.dag_folder,
            duration=sum([o.duration for o in stats]),
            dag_num=sum([o.dag_num for o in stats]),
            task_num=sum([o.task_num for o in stats]),
            table=pprinttable(stats),
        )","['def', 'dagbag_report', '(', 'self', ')', ':', 'report', '=', 'textwrap', '.', 'dedent', '(', '""""""\\n\n        -------------------------------------------------------------------\n        DagBag loading stats for {dag_folder}\n        -------------------------------------------------------------------\n        Number of DAGs: {dag_num}\n        Total task number: {task_num}\n        DagBag parsing time: {duration}\n        {table}\n        """"""', ')', 'stats', '=', 'self', '.', 'dagbag_stats', 'return', 'report', '.', 'format', '(', 'dag_folder', '=', 'self', '.', 'dag_folder', ',', 'duration', '=', 'sum', '(', '[', 'o', '.', 'duration', 'for', 'o', 'in', 'stats', ']', ')', ',', 'dag_num', '=', 'sum', '(', '[', 'o', '.', 'dag_num', 'for', 'o', 'in', 'stats', ']', ')', ',', 'task_num', '=', 'sum', '(', '[', 'o', '.', 'task_num', 'for', 'o', 'in', 'stats', ']', ')', ',', 'table', '=', 'pprinttable', '(', 'stats', ')', ',', ')']",Prints a report around DagBag loading stats,"['Prints', 'a', 'report', 'around', 'DagBag', 'loading', 'stats']",python,test,"['prints', 'a', 'report', 'around', 'dagbag', 'loading', 'stats']",prints a report around dagbag loading stats,"['def', 'dagbag_report', '(', 'self', ')', ':', 'report', '=', 'textwrap', '.', 'dedent', '(', '""""""\\n\n        -------------------------------------------------------------------\n        dagbag loading stats for {dag_folder}\n        -------------------------------------------------------------------\n        number of dags: {dag_num}\n        total task number: {task_num}\n        dagbag parsing time: {duration}\n        {table}\n        """"""', ')', 'stats', '=', 'self', '.', 'dagbag_stats', 'return', 'report', '.', 'format', '(', 'dag_folder', '=', 'self', '.', 'dag_folder', ',', 'duration', '=', 'sum', '(', '[', 'o', '.', 'duration', 'for', 'o', 'in', 'stats', ']', ')', ',', 'dag_num', '=', 'sum', '(', '[', 'o', '.', 'dag_num', 'for', 'o', 'in', 'stats', ']', ')', ',', 'task_num', '=', 'sum', '(', '[', 'o', '.', 'task_num', 'for', 'o', 'in', 'stats', ']', ')', ',', 'table', '=', 'pprinttable', '(', 'stats', ')', ',', ')']","def dagbag_report ( self ) : report = textwrap . dedent ( """"""\n
        -------------------------------------------------------------------
        dagbag loading stats for {dag_folder}
        -------------------------------------------------------------------
        number of dags: {dag_num}
        total task number: {task_num}
        dagbag parsing time: {duration}
        {table}
        """""" ) stats = self . dagbag_stats return report . format ( dag_folder = self . dag_folder , duration = sum ( [ o . duration for o in stats ] ) , dag_num = sum ( [ o . dag_num for o in stats ] ) , task_num = sum ( [ o . task_num for o in stats ] ) , table = pprinttable ( stats ) , )"
336,apache/airflow,airflow/contrib/operators/spark_jdbc_operator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/spark_jdbc_operator.py#L177-L211,"def execute(self, context):
        """"""
        Call the SparkSubmitHook to run the provided spark job
        """"""
        self._hook = SparkJDBCHook(
            spark_app_name=self._spark_app_name,
            spark_conn_id=self._spark_conn_id,
            spark_conf=self._spark_conf,
            spark_py_files=self._spark_py_files,
            spark_files=self._spark_files,
            spark_jars=self._spark_jars,
            num_executors=self._num_executors,
            executor_cores=self._executor_cores,
            executor_memory=self._executor_memory,
            driver_memory=self._driver_memory,
            verbose=self._verbose,
            keytab=self._keytab,
            principal=self._principal,
            cmd_type=self._cmd_type,
            jdbc_table=self._jdbc_table,
            jdbc_conn_id=self._jdbc_conn_id,
            jdbc_driver=self._jdbc_driver,
            metastore_table=self._metastore_table,
            jdbc_truncate=self._jdbc_truncate,
            save_mode=self._save_mode,
            save_format=self._save_format,
            batch_size=self._batch_size,
            fetch_size=self._fetch_size,
            num_partitions=self._num_partitions,
            partition_column=self._partition_column,
            lower_bound=self._lower_bound,
            upper_bound=self._upper_bound,
            create_table_column_types=self._create_table_column_types
        )
        self._hook.submit_jdbc_job()","['def', 'execute', '(', 'self', ',', 'context', ')', ':', 'self', '.', '_hook', '=', 'SparkJDBCHook', '(', 'spark_app_name', '=', 'self', '.', '_spark_app_name', ',', 'spark_conn_id', '=', 'self', '.', '_spark_conn_id', ',', 'spark_conf', '=', 'self', '.', '_spark_conf', ',', 'spark_py_files', '=', 'self', '.', '_spark_py_files', ',', 'spark_files', '=', 'self', '.', '_spark_files', ',', 'spark_jars', '=', 'self', '.', '_spark_jars', ',', 'num_executors', '=', 'self', '.', '_num_executors', ',', 'executor_cores', '=', 'self', '.', '_executor_cores', ',', 'executor_memory', '=', 'self', '.', '_executor_memory', ',', 'driver_memory', '=', 'self', '.', '_driver_memory', ',', 'verbose', '=', 'self', '.', '_verbose', ',', 'keytab', '=', 'self', '.', '_keytab', ',', 'principal', '=', 'self', '.', '_principal', ',', 'cmd_type', '=', 'self', '.', '_cmd_type', ',', 'jdbc_table', '=', 'self', '.', '_jdbc_table', ',', 'jdbc_conn_id', '=', 'self', '.', '_jdbc_conn_id', ',', 'jdbc_driver', '=', 'self', '.', '_jdbc_driver', ',', 'metastore_table', '=', 'self', '.', '_metastore_table', ',', 'jdbc_truncate', '=', 'self', '.', '_jdbc_truncate', ',', 'save_mode', '=', 'self', '.', '_save_mode', ',', 'save_format', '=', 'self', '.', '_save_format', ',', 'batch_size', '=', 'self', '.', '_batch_size', ',', 'fetch_size', '=', 'self', '.', '_fetch_size', ',', 'num_partitions', '=', 'self', '.', '_num_partitions', ',', 'partition_column', '=', 'self', '.', '_partition_column', ',', 'lower_bound', '=', 'self', '.', '_lower_bound', ',', 'upper_bound', '=', 'self', '.', '_upper_bound', ',', 'create_table_column_types', '=', 'self', '.', '_create_table_column_types', ')', 'self', '.', '_hook', '.', 'submit_jdbc_job', '(', ')']",Call the SparkSubmitHook to run the provided spark job,"['Call', 'the', 'SparkSubmitHook', 'to', 'run', 'the', 'provided', 'spark', 'job']",python,test,"['call', 'the', 'sparksubmithook', 'to', 'run', 'the', 'provided', 'spark', 'job']",call the sparksubmithook to run the provided spark job,"['def', 'execute', '(', 'self', ',', 'context', ')', ':', 'self', '.', '_hook', '=', 'sparkjdbchook', '(', 'spark_app_name', '=', 'self', '.', '_spark_app_name', ',', 'spark_conn_id', '=', 'self', '.', '_spark_conn_id', ',', 'spark_conf', '=', 'self', '.', '_spark_conf', ',', 'spark_py_files', '=', 'self', '.', '_spark_py_files', ',', 'spark_files', '=', 'self', '.', '_spark_files', ',', 'spark_jars', '=', 'self', '.', '_spark_jars', ',', 'num_executors', '=', 'self', '.', '_num_executors', ',', 'executor_cores', '=', 'self', '.', '_executor_cores', ',', 'executor_memory', '=', 'self', '.', '_executor_memory', ',', 'driver_memory', '=', 'self', '.', '_driver_memory', ',', 'verbose', '=', 'self', '.', '_verbose', ',', 'keytab', '=', 'self', '.', '_keytab', ',', 'principal', '=', 'self', '.', '_principal', ',', 'cmd_type', '=', 'self', '.', '_cmd_type', ',', 'jdbc_table', '=', 'self', '.', '_jdbc_table', ',', 'jdbc_conn_id', '=', 'self', '.', '_jdbc_conn_id', ',', 'jdbc_driver', '=', 'self', '.', '_jdbc_driver', ',', 'metastore_table', '=', 'self', '.', '_metastore_table', ',', 'jdbc_truncate', '=', 'self', '.', '_jdbc_truncate', ',', 'save_mode', '=', 'self', '.', '_save_mode', ',', 'save_format', '=', 'self', '.', '_save_format', ',', 'batch_size', '=', 'self', '.', '_batch_size', ',', 'fetch_size', '=', 'self', '.', '_fetch_size', ',', 'num_partitions', '=', 'self', '.', '_num_partitions', ',', 'partition_column', '=', 'self', '.', '_partition_column', ',', 'lower_bound', '=', 'self', '.', '_lower_bound', ',', 'upper_bound', '=', 'self', '.', '_upper_bound', ',', 'create_table_column_types', '=', 'self', '.', '_create_table_column_types', ')', 'self', '.', '_hook', '.', 'submit_jdbc_job', '(', ')']","def execute ( self , context ) : self . _hook = sparkjdbchook ( spark_app_name = self . _spark_app_name , spark_conn_id = self . _spark_conn_id , spark_conf = self . _spark_conf , spark_py_files = self . _spark_py_files , spark_files = self . _spark_files , spark_jars = self . _spark_jars , num_executors = self . _num_executors , executor_cores = self . _executor_cores , executor_memory = self . _executor_memory , driver_memory = self . _driver_memory , verbose = self . _verbose , keytab = self . _keytab , principal = self . _principal , cmd_type = self . _cmd_type , jdbc_table = self . _jdbc_table , jdbc_conn_id = self . _jdbc_conn_id , jdbc_driver = self . _jdbc_driver , metastore_table = self . _metastore_table , jdbc_truncate = self . _jdbc_truncate , save_mode = self . _save_mode , save_format = self . _save_format , batch_size = self . _batch_size , fetch_size = self . _fetch_size , num_partitions = self . _num_partitions , partition_column = self . _partition_column , lower_bound = self . _lower_bound , upper_bound = self . _upper_bound , create_table_column_types = self . _create_table_column_types ) self . _hook . submit_jdbc_job ( )"
337,apache/airflow,airflow/macros/__init__.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/macros/__init__.py#L28-L46,"def ds_add(ds, days):
    """"""
    Add or subtract days from a YYYY-MM-DD

    :param ds: anchor date in ``YYYY-MM-DD`` format to add to
    :type ds: str
    :param days: number of days to add to the ds, you can use negative values
    :type days: int

    >>> ds_add('2015-01-01', 5)
    '2015-01-06'
    >>> ds_add('2015-01-06', -5)
    '2015-01-01'
    """"""

    ds = datetime.strptime(ds, '%Y-%m-%d')
    if days:
        ds = ds + timedelta(days)
    return ds.isoformat()[:10]","['def', 'ds_add', '(', 'ds', ',', 'days', ')', ':', 'ds', '=', 'datetime', '.', 'strptime', '(', 'ds', ',', ""'%Y-%m-%d'"", ')', 'if', 'days', ':', 'ds', '=', 'ds', '+', 'timedelta', '(', 'days', ')', 'return', 'ds', '.', 'isoformat', '(', ')', '[', ':', '10', ']']","Add or subtract days from a YYYY-MM-DD

    :param ds: anchor date in ``YYYY-MM-DD`` format to add to
    :type ds: str
    :param days: number of days to add to the ds, you can use negative values
    :type days: int

    >>> ds_add('2015-01-01', 5)
    '2015-01-06'
    >>> ds_add('2015-01-06', -5)
    '2015-01-01'","['Add', 'or', 'subtract', 'days', 'from', 'a', 'YYYY', '-', 'MM', '-', 'DD']",python,test,"['add', 'or', 'subtract', 'days', 'from', 'a', 'yyyy', '-', 'mm', '-', 'dd']",add or subtract days from a yyyy - mm - dd,"['def', 'ds_add', '(', 'ds', ',', 'days', ')', ':', 'ds', '=', 'datetime', '.', 'strptime', '(', 'ds', ',', ""'%y-%m-%d'"", ')', 'if', 'days', ':', 'ds', '=', 'ds', '+', 'timedelta', '(', 'days', ')', 'return', 'ds', '.', 'isoformat', '(', ')', '[', ':', '10', ']']","def ds_add ( ds , days ) : ds = datetime . strptime ( ds , '%y-%m-%d' ) if days : ds = ds + timedelta ( days ) return ds . isoformat ( ) [ : 10 ]"
338,apache/airflow,airflow/macros/__init__.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/macros/__init__.py#L49-L66,"def ds_format(ds, input_format, output_format):
    """"""
    Takes an input string and outputs another string
    as specified in the output format

    :param ds: input string which contains a date
    :type ds: str
    :param input_format: input string format. E.g. %Y-%m-%d
    :type input_format: str
    :param output_format: output string format  E.g. %Y-%m-%d
    :type output_format: str

    >>> ds_format('2015-01-01', ""%Y-%m-%d"", ""%m-%d-%y"")
    '01-01-15'
    >>> ds_format('1/5/2015', ""%m/%d/%Y"",  ""%Y-%m-%d"")
    '2015-01-05'
    """"""
    return datetime.strptime(ds, input_format).strftime(output_format)","['def', 'ds_format', '(', 'ds', ',', 'input_format', ',', 'output_format', ')', ':', 'return', 'datetime', '.', 'strptime', '(', 'ds', ',', 'input_format', ')', '.', 'strftime', '(', 'output_format', ')']","Takes an input string and outputs another string
    as specified in the output format

    :param ds: input string which contains a date
    :type ds: str
    :param input_format: input string format. E.g. %Y-%m-%d
    :type input_format: str
    :param output_format: output string format  E.g. %Y-%m-%d
    :type output_format: str

    >>> ds_format('2015-01-01', ""%Y-%m-%d"", ""%m-%d-%y"")
    '01-01-15'
    >>> ds_format('1/5/2015', ""%m/%d/%Y"",  ""%Y-%m-%d"")
    '2015-01-05'","['Takes', 'an', 'input', 'string', 'and', 'outputs', 'another', 'string', 'as', 'specified', 'in', 'the', 'output', 'format']",python,test,"['takes', 'an', 'input', 'string', 'and', 'outputs', 'another', 'string', 'as', 'specified', 'in', 'the', 'output', 'format']",takes an input string and outputs another string as specified in the output format,"['def', 'ds_format', '(', 'ds', ',', 'input_format', ',', 'output_format', ')', ':', 'return', 'datetime', '.', 'strptime', '(', 'ds', ',', 'input_format', ')', '.', 'strftime', '(', 'output_format', ')']","def ds_format ( ds , input_format , output_format ) : return datetime . strptime ( ds , input_format ) . strftime ( output_format )"
339,apache/airflow,airflow/macros/__init__.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/macros/__init__.py#L69-L75,"def _integrate_plugins():
    """"""Integrate plugins to the context""""""
    import sys
    from airflow.plugins_manager import macros_modules
    for macros_module in macros_modules:
        sys.modules[macros_module.__name__] = macros_module
        globals()[macros_module._name] = macros_module","['def', '_integrate_plugins', '(', ')', ':', 'import', 'sys', 'from', 'airflow', '.', 'plugins_manager', 'import', 'macros_modules', 'for', 'macros_module', 'in', 'macros_modules', ':', 'sys', '.', 'modules', '[', 'macros_module', '.', '__name__', ']', '=', 'macros_module', 'globals', '(', ')', '[', 'macros_module', '.', '_name', ']', '=', 'macros_module']",Integrate plugins to the context,"['Integrate', 'plugins', 'to', 'the', 'context']",python,test,"['integrate', 'plugins', 'to', 'the', 'context']",integrate plugins to the context,"['def', '_integrate_plugins', '(', ')', ':', 'import', 'sys', 'from', 'airflow', '.', 'plugins_manager', 'import', 'macros_modules', 'for', 'macros_module', 'in', 'macros_modules', ':', 'sys', '.', 'modules', '[', 'macros_module', '.', '__name__', ']', '=', 'macros_module', 'globals', '(', ')', '[', 'macros_module', '.', '_name', ']', '=', 'macros_module']",def _integrate_plugins ( ) : import sys from airflow . plugins_manager import macros_modules for macros_module in macros_modules : sys . modules [ macros_module . __name__ ] = macros_module globals ( ) [ macros_module . _name ] = macros_module
340,apache/airflow,airflow/contrib/sensors/hdfs_sensor.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/hdfs_sensor.py#L30-L46,"def poke(self, context):
        """"""
        poke matching files in a directory with self.regex

        :return: Bool depending on the search criteria
        """"""
        sb = self.hook(self.hdfs_conn_id).get_conn()
        self.log.info(
            'Poking for %s to be a directory with files matching %s', self.filepath, self.regex.pattern
        )
        result = [f for f in sb.ls([self.filepath], include_toplevel=False) if
                  f['file_type'] == 'f' and
                  self.regex.match(f['path'].replace('%s/' % self.filepath, ''))]
        result = self.filter_for_ignored_ext(result, self.ignored_ext,
                                             self.ignore_copying)
        result = self.filter_for_filesize(result, self.file_size)
        return bool(result)","['def', 'poke', '(', 'self', ',', 'context', ')', ':', 'sb', '=', 'self', '.', 'hook', '(', 'self', '.', 'hdfs_conn_id', ')', '.', 'get_conn', '(', ')', 'self', '.', 'log', '.', 'info', '(', ""'Poking for %s to be a directory with files matching %s'"", ',', 'self', '.', 'filepath', ',', 'self', '.', 'regex', '.', 'pattern', ')', 'result', '=', '[', 'f', 'for', 'f', 'in', 'sb', '.', 'ls', '(', '[', 'self', '.', 'filepath', ']', ',', 'include_toplevel', '=', 'False', ')', 'if', 'f', '[', ""'file_type'"", ']', '==', ""'f'"", 'and', 'self', '.', 'regex', '.', 'match', '(', 'f', '[', ""'path'"", ']', '.', 'replace', '(', ""'%s/'"", '%', 'self', '.', 'filepath', ',', ""''"", ')', ')', ']', 'result', '=', 'self', '.', 'filter_for_ignored_ext', '(', 'result', ',', 'self', '.', 'ignored_ext', ',', 'self', '.', 'ignore_copying', ')', 'result', '=', 'self', '.', 'filter_for_filesize', '(', 'result', ',', 'self', '.', 'file_size', ')', 'return', 'bool', '(', 'result', ')']","poke matching files in a directory with self.regex

        :return: Bool depending on the search criteria","['poke', 'matching', 'files', 'in', 'a', 'directory', 'with', 'self', '.', 'regex']",python,test,"['poke', 'matching', 'files', 'in', 'a', 'directory', 'with', 'self', '.', 'regex']",poke matching files in a directory with self . regex,"['def', 'poke', '(', 'self', ',', 'context', ')', ':', 'sb', '=', 'self', '.', 'hook', '(', 'self', '.', 'hdfs_conn_id', ')', '.', 'get_conn', '(', ')', 'self', '.', 'log', '.', 'info', '(', ""'poking for %s to be a directory with files matching %s'"", ',', 'self', '.', 'filepath', ',', 'self', '.', 'regex', '.', 'pattern', ')', 'result', '=', '[', 'f', 'for', 'f', 'in', 'sb', '.', 'ls', '(', '[', 'self', '.', 'filepath', ']', ',', 'include_toplevel', '=', 'false', ')', 'if', 'f', '[', ""'file_type'"", ']', '==', ""'f'"", 'and', 'self', '.', 'regex', '.', 'match', '(', 'f', '[', ""'path'"", ']', '.', 'replace', '(', ""'%s/'"", '%', 'self', '.', 'filepath', ',', ""''"", ')', ')', ']', 'result', '=', 'self', '.', 'filter_for_ignored_ext', '(', 'result', ',', 'self', '.', 'ignored_ext', ',', 'self', '.', 'ignore_copying', ')', 'result', '=', 'self', '.', 'filter_for_filesize', '(', 'result', ',', 'self', '.', 'file_size', ')', 'return', 'bool', '(', 'result', ')']","def poke ( self , context ) : sb = self . hook ( self . hdfs_conn_id ) . get_conn ( ) self . log . info ( 'poking for %s to be a directory with files matching %s' , self . filepath , self . regex . pattern ) result = [ f for f in sb . ls ( [ self . filepath ] , include_toplevel = false ) if f [ 'file_type' ] == 'f' and self . regex . match ( f [ 'path' ] . replace ( '%s/' % self . filepath , '' ) ) ] result = self . filter_for_ignored_ext ( result , self . ignored_ext , self . ignore_copying ) result = self . filter_for_filesize ( result , self . file_size ) return bool ( result )"
341,apache/airflow,airflow/contrib/sensors/hdfs_sensor.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/sensors/hdfs_sensor.py#L57-L74,"def poke(self, context):
        """"""
        poke for a non empty directory

        :return: Bool depending on the search criteria
        """"""
        sb = self.hook(self.hdfs_conn_id).get_conn()
        result = [f for f in sb.ls([self.filepath], include_toplevel=True)]
        result = self.filter_for_ignored_ext(result, self.ignored_ext,
                                             self.ignore_copying)
        result = self.filter_for_filesize(result, self.file_size)
        if self.be_empty:
            self.log.info('Poking for filepath %s to a empty directory', self.filepath)
            return len(result) == 1 and result[0]['path'] == self.filepath
        else:
            self.log.info('Poking for filepath %s to a non empty directory', self.filepath)
            result.pop(0)
            return bool(result) and result[0]['file_type'] == 'f'","['def', 'poke', '(', 'self', ',', 'context', ')', ':', 'sb', '=', 'self', '.', 'hook', '(', 'self', '.', 'hdfs_conn_id', ')', '.', 'get_conn', '(', ')', 'result', '=', '[', 'f', 'for', 'f', 'in', 'sb', '.', 'ls', '(', '[', 'self', '.', 'filepath', ']', ',', 'include_toplevel', '=', 'True', ')', ']', 'result', '=', 'self', '.', 'filter_for_ignored_ext', '(', 'result', ',', 'self', '.', 'ignored_ext', ',', 'self', '.', 'ignore_copying', ')', 'result', '=', 'self', '.', 'filter_for_filesize', '(', 'result', ',', 'self', '.', 'file_size', ')', 'if', 'self', '.', 'be_empty', ':', 'self', '.', 'log', '.', 'info', '(', ""'Poking for filepath %s to a empty directory'"", ',', 'self', '.', 'filepath', ')', 'return', 'len', '(', 'result', ')', '==', '1', 'and', 'result', '[', '0', ']', '[', ""'path'"", ']', '==', 'self', '.', 'filepath', 'else', ':', 'self', '.', 'log', '.', 'info', '(', ""'Poking for filepath %s to a non empty directory'"", ',', 'self', '.', 'filepath', ')', 'result', '.', 'pop', '(', '0', ')', 'return', 'bool', '(', 'result', ')', 'and', 'result', '[', '0', ']', '[', ""'file_type'"", ']', '==', ""'f'""]","poke for a non empty directory

        :return: Bool depending on the search criteria","['poke', 'for', 'a', 'non', 'empty', 'directory']",python,test,"['poke', 'for', 'a', 'non', 'empty', 'directory']",poke for a non empty directory,"['def', 'poke', '(', 'self', ',', 'context', ')', ':', 'sb', '=', 'self', '.', 'hook', '(', 'self', '.', 'hdfs_conn_id', ')', '.', 'get_conn', '(', ')', 'result', '=', '[', 'f', 'for', 'f', 'in', 'sb', '.', 'ls', '(', '[', 'self', '.', 'filepath', ']', ',', 'include_toplevel', '=', 'true', ')', ']', 'result', '=', 'self', '.', 'filter_for_ignored_ext', '(', 'result', ',', 'self', '.', 'ignored_ext', ',', 'self', '.', 'ignore_copying', ')', 'result', '=', 'self', '.', 'filter_for_filesize', '(', 'result', ',', 'self', '.', 'file_size', ')', 'if', 'self', '.', 'be_empty', ':', 'self', '.', 'log', '.', 'info', '(', ""'poking for filepath %s to a empty directory'"", ',', 'self', '.', 'filepath', ')', 'return', 'len', '(', 'result', ')', '==', '1', 'and', 'result', '[', '0', ']', '[', ""'path'"", ']', '==', 'self', '.', 'filepath', 'else', ':', 'self', '.', 'log', '.', 'info', '(', ""'poking for filepath %s to a non empty directory'"", ',', 'self', '.', 'filepath', ')', 'result', '.', 'pop', '(', '0', ')', 'return', 'bool', '(', 'result', ')', 'and', 'result', '[', '0', ']', '[', ""'file_type'"", ']', '==', ""'f'""]","def poke ( self , context ) : sb = self . hook ( self . hdfs_conn_id ) . get_conn ( ) result = [ f for f in sb . ls ( [ self . filepath ] , include_toplevel = true ) ] result = self . filter_for_ignored_ext ( result , self . ignored_ext , self . ignore_copying ) result = self . filter_for_filesize ( result , self . file_size ) if self . be_empty : self . log . info ( 'poking for filepath %s to a empty directory' , self . filepath ) return len ( result ) == 1 and result [ 0 ] [ 'path' ] == self . filepath else : self . log . info ( 'poking for filepath %s to a non empty directory' , self . filepath ) result . pop ( 0 ) return bool ( result ) and result [ 0 ] [ 'file_type' ] == 'f'"
342,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L59-L107,"def clear_task_instances(tis,
                         session,
                         activate_dag_runs=True,
                         dag=None,
                         ):
    """"""
    Clears a set of task instances, but makes sure the running ones
    get killed.

    :param tis: a list of task instances
    :param session: current session
    :param activate_dag_runs: flag to check for active dag run
    :param dag: DAG object
    """"""
    job_ids = []
    for ti in tis:
        if ti.state == State.RUNNING:
            if ti.job_id:
                ti.state = State.SHUTDOWN
                job_ids.append(ti.job_id)
        else:
            task_id = ti.task_id
            if dag and dag.has_task(task_id):
                task = dag.get_task(task_id)
                task_retries = task.retries
                ti.max_tries = ti.try_number + task_retries - 1
            else:
                # Ignore errors when updating max_tries if dag is None or
                # task not found in dag since database records could be
                # outdated. We make max_tries the maximum value of its
                # original max_tries or the current task try number.
                ti.max_tries = max(ti.max_tries, ti.try_number - 1)
            ti.state = State.NONE
            session.merge(ti)

    if job_ids:
        from airflow.jobs import BaseJob as BJ
        for job in session.query(BJ).filter(BJ.id.in_(job_ids)).all():
            job.state = State.SHUTDOWN

    if activate_dag_runs and tis:
        from airflow.models.dagrun import DagRun  # Avoid circular import
        drs = session.query(DagRun).filter(
            DagRun.dag_id.in_({ti.dag_id for ti in tis}),
            DagRun.execution_date.in_({ti.execution_date for ti in tis}),
        ).all()
        for dr in drs:
            dr.state = State.RUNNING
            dr.start_date = timezone.utcnow()","['def', 'clear_task_instances', '(', 'tis', ',', 'session', ',', 'activate_dag_runs', '=', 'True', ',', 'dag', '=', 'None', ',', ')', ':', 'job_ids', '=', '[', ']', 'for', 'ti', 'in', 'tis', ':', 'if', 'ti', '.', 'state', '==', 'State', '.', 'RUNNING', ':', 'if', 'ti', '.', 'job_id', ':', 'ti', '.', 'state', '=', 'State', '.', 'SHUTDOWN', 'job_ids', '.', 'append', '(', 'ti', '.', 'job_id', ')', 'else', ':', 'task_id', '=', 'ti', '.', 'task_id', 'if', 'dag', 'and', 'dag', '.', 'has_task', '(', 'task_id', ')', ':', 'task', '=', 'dag', '.', 'get_task', '(', 'task_id', ')', 'task_retries', '=', 'task', '.', 'retries', 'ti', '.', 'max_tries', '=', 'ti', '.', 'try_number', '+', 'task_retries', '-', '1', 'else', ':', '# Ignore errors when updating max_tries if dag is None or', '# task not found in dag since database records could be', '# outdated. We make max_tries the maximum value of its', '# original max_tries or the current task try number.', 'ti', '.', 'max_tries', '=', 'max', '(', 'ti', '.', 'max_tries', ',', 'ti', '.', 'try_number', '-', '1', ')', 'ti', '.', 'state', '=', 'State', '.', 'NONE', 'session', '.', 'merge', '(', 'ti', ')', 'if', 'job_ids', ':', 'from', 'airflow', '.', 'jobs', 'import', 'BaseJob', 'as', 'BJ', 'for', 'job', 'in', 'session', '.', 'query', '(', 'BJ', ')', '.', 'filter', '(', 'BJ', '.', 'id', '.', 'in_', '(', 'job_ids', ')', ')', '.', 'all', '(', ')', ':', 'job', '.', 'state', '=', 'State', '.', 'SHUTDOWN', 'if', 'activate_dag_runs', 'and', 'tis', ':', 'from', 'airflow', '.', 'models', '.', 'dagrun', 'import', 'DagRun', '# Avoid circular import', 'drs', '=', 'session', '.', 'query', '(', 'DagRun', ')', '.', 'filter', '(', 'DagRun', '.', 'dag_id', '.', 'in_', '(', '{', 'ti', '.', 'dag_id', 'for', 'ti', 'in', 'tis', '}', ')', ',', 'DagRun', '.', 'execution_date', '.', 'in_', '(', '{', 'ti', '.', 'execution_date', 'for', 'ti', 'in', 'tis', '}', ')', ',', ')', '.', 'all', '(', ')', 'for', 'dr', 'in', 'drs', ':', 'dr', '.', 'state', '=', 'State', '.', 'RUNNING', 'dr', '.', 'start_date', '=', 'timezone', '.', 'utcnow', '(', ')']","Clears a set of task instances, but makes sure the running ones
    get killed.

    :param tis: a list of task instances
    :param session: current session
    :param activate_dag_runs: flag to check for active dag run
    :param dag: DAG object","['Clears', 'a', 'set', 'of', 'task', 'instances', 'but', 'makes', 'sure', 'the', 'running', 'ones', 'get', 'killed', '.']",python,test,"['clears', 'a', 'set', 'of', 'task', 'instances', 'but', 'makes', 'sure', 'the', 'running', 'ones', 'get', 'killed', '.']",clears a set of task instances but makes sure the running ones get killed .,"['def', 'clear_task_instances', '(', 'tis', ',', 'session', ',', 'activate_dag_runs', '=', 'true', ',', 'dag', '=', 'none', ',', ')', ':', 'job_ids', '=', '[', ']', 'for', 'ti', 'in', 'tis', ':', 'if', 'ti', '.', 'state', '==', 'state', '.', 'running', ':', 'if', 'ti', '.', 'job_id', ':', 'ti', '.', 'state', '=', 'state', '.', 'shutdown', 'job_ids', '.', 'append', '(', 'ti', '.', 'job_id', ')', 'else', ':', 'task_id', '=', 'ti', '.', 'task_id', 'if', 'dag', 'and', 'dag', '.', 'has_task', '(', 'task_id', ')', ':', 'task', '=', 'dag', '.', 'get_task', '(', 'task_id', ')', 'task_retries', '=', 'task', '.', 'retries', 'ti', '.', 'max_tries', '=', 'ti', '.', 'try_number', '+', 'task_retries', '-', '1', 'else', ':', '# ignore errors when updating max_tries if dag is none or', '# task not found in dag since database records could be', '# outdated. we make max_tries the maximum value of its', '# original max_tries or the current task try number.', 'ti', '.', 'max_tries', '=', 'max', '(', 'ti', '.', 'max_tries', ',', 'ti', '.', 'try_number', '-', '1', ')', 'ti', '.', 'state', '=', 'state', '.', 'none', 'session', '.', 'merge', '(', 'ti', ')', 'if', 'job_ids', ':', 'from', 'airflow', '.', 'jobs', 'import', 'basejob', 'as', 'bj', 'for', 'job', 'in', 'session', '.', 'query', '(', 'bj', ')', '.', 'filter', '(', 'bj', '.', 'id', '.', 'in_', '(', 'job_ids', ')', ')', '.', 'all', '(', ')', ':', 'job', '.', 'state', '=', 'state', '.', 'shutdown', 'if', 'activate_dag_runs', 'and', 'tis', ':', 'from', 'airflow', '.', 'models', '.', 'dagrun', 'import', 'dagrun', '# avoid circular import', 'drs', '=', 'session', '.', 'query', '(', 'dagrun', ')', '.', 'filter', '(', 'dagrun', '.', 'dag_id', '.', 'in_', '(', '{', 'ti', '.', 'dag_id', 'for', 'ti', 'in', 'tis', '}', ')', ',', 'dagrun', '.', 'execution_date', '.', 'in_', '(', '{', 'ti', '.', 'execution_date', 'for', 'ti', 'in', 'tis', '}', ')', ',', ')', '.', 'all', '(', ')', 'for', 'dr', 'in', 'drs', ':', 'dr', '.', 'state', '=', 'state', '.', 'running', 'dr', '.', 'start_date', '=', 'timezone', '.', 'utcnow', '(', ')']","def clear_task_instances ( tis , session , activate_dag_runs = true , dag = none , ) : job_ids = [ ] for ti in tis : if ti . state == state . running : if ti . job_id : ti . state = state . shutdown job_ids . append ( ti . job_id ) else : task_id = ti . task_id if dag and dag . has_task ( task_id ) : task = dag . get_task ( task_id ) task_retries = task . retries ti . max_tries = ti . try_number + task_retries - 1 else : # ignore errors when updating max_tries if dag is none or # task not found in dag since database records could be # outdated. we make max_tries the maximum value of its # original max_tries or the current task try number. ti . max_tries = max ( ti . max_tries , ti . try_number - 1 ) ti . state = state . none session . merge ( ti ) if job_ids : from airflow . jobs import basejob as bj for job in session . query ( bj ) . filter ( bj . id . in_ ( job_ids ) ) . all ( ) : job . state = state . shutdown if activate_dag_runs and tis : from airflow . models . dagrun import dagrun # avoid circular import drs = session . query ( dagrun ) . filter ( dagrun . dag_id . in_ ( { ti . dag_id for ti in tis } ) , dagrun . execution_date . in_ ( { ti . execution_date for ti in tis } ) , ) . all ( ) for dr in drs : dr . state = state . running dr . start_date = timezone . utcnow ( )"
343,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L197-L208,"def try_number(self):
        """"""
        Return the try number that this task number will be when it is actually
        run.

        If the TI is currently running, this will match the column in the
        databse, in all othercases this will be incremenetd
        """"""
        # This is designed so that task logs end up in the right file.
        if self.state == State.RUNNING:
            return self._try_number
        return self._try_number + 1","['def', 'try_number', '(', 'self', ')', ':', '# This is designed so that task logs end up in the right file.', 'if', 'self', '.', 'state', '==', 'State', '.', 'RUNNING', ':', 'return', 'self', '.', '_try_number', 'return', 'self', '.', '_try_number', '+', '1']","Return the try number that this task number will be when it is actually
        run.

        If the TI is currently running, this will match the column in the
        databse, in all othercases this will be incremenetd","['Return', 'the', 'try', 'number', 'that', 'this', 'task', 'number', 'will', 'be', 'when', 'it', 'is', 'actually', 'run', '.']",python,test,"['return', 'the', 'try', 'number', 'that', 'this', 'task', 'number', 'will', 'be', 'when', 'it', 'is', 'actually', 'run', '.']",return the try number that this task number will be when it is actually run .,"['def', 'try_number', '(', 'self', ')', ':', '# this is designed so that task logs end up in the right file.', 'if', 'self', '.', 'state', '==', 'state', '.', 'running', ':', 'return', 'self', '.', '_try_number', 'return', 'self', '.', '_try_number', '+', '1']",def try_number ( self ) : # this is designed so that task logs end up in the right file. if self . state == state . running : return self . _try_number return self . _try_number + 1
344,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L218-L247,"def command(
            self,
            mark_success=False,
            ignore_all_deps=False,
            ignore_depends_on_past=False,
            ignore_task_deps=False,
            ignore_ti_state=False,
            local=False,
            pickle_id=None,
            raw=False,
            job_id=None,
            pool=None,
            cfg_path=None):
        """"""
        Returns a command that can be executed anywhere where airflow is
        installed. This command is part of the message sent to executors by
        the orchestrator.
        """"""
        return "" "".join(self.command_as_list(
            mark_success=mark_success,
            ignore_all_deps=ignore_all_deps,
            ignore_depends_on_past=ignore_depends_on_past,
            ignore_task_deps=ignore_task_deps,
            ignore_ti_state=ignore_ti_state,
            local=local,
            pickle_id=pickle_id,
            raw=raw,
            job_id=job_id,
            pool=pool,
            cfg_path=cfg_path))","['def', 'command', '(', 'self', ',', 'mark_success', '=', 'False', ',', 'ignore_all_deps', '=', 'False', ',', 'ignore_depends_on_past', '=', 'False', ',', 'ignore_task_deps', '=', 'False', ',', 'ignore_ti_state', '=', 'False', ',', 'local', '=', 'False', ',', 'pickle_id', '=', 'None', ',', 'raw', '=', 'False', ',', 'job_id', '=', 'None', ',', 'pool', '=', 'None', ',', 'cfg_path', '=', 'None', ')', ':', 'return', '"" ""', '.', 'join', '(', 'self', '.', 'command_as_list', '(', 'mark_success', '=', 'mark_success', ',', 'ignore_all_deps', '=', 'ignore_all_deps', ',', 'ignore_depends_on_past', '=', 'ignore_depends_on_past', ',', 'ignore_task_deps', '=', 'ignore_task_deps', ',', 'ignore_ti_state', '=', 'ignore_ti_state', ',', 'local', '=', 'local', ',', 'pickle_id', '=', 'pickle_id', ',', 'raw', '=', 'raw', ',', 'job_id', '=', 'job_id', ',', 'pool', '=', 'pool', ',', 'cfg_path', '=', 'cfg_path', ')', ')']","Returns a command that can be executed anywhere where airflow is
        installed. This command is part of the message sent to executors by
        the orchestrator.","['Returns', 'a', 'command', 'that', 'can', 'be', 'executed', 'anywhere', 'where', 'airflow', 'is', 'installed', '.', 'This', 'command', 'is', 'part', 'of', 'the', 'message', 'sent', 'to', 'executors', 'by', 'the', 'orchestrator', '.']",python,test,"['returns', 'a', 'command', 'that', 'can', 'be', 'executed', 'anywhere', 'where', 'airflow', 'is', 'installed', '.', 'this', 'command', 'is', 'part', 'of', 'the', 'message', 'sent', 'to', 'executors', 'by', 'the', 'orchestrator', '.']",returns a command that can be executed anywhere where airflow is installed . this command is part of the message sent to executors by the orchestrator .,"['def', 'command', '(', 'self', ',', 'mark_success', '=', 'false', ',', 'ignore_all_deps', '=', 'false', ',', 'ignore_depends_on_past', '=', 'false', ',', 'ignore_task_deps', '=', 'false', ',', 'ignore_ti_state', '=', 'false', ',', 'local', '=', 'false', ',', 'pickle_id', '=', 'none', ',', 'raw', '=', 'false', ',', 'job_id', '=', 'none', ',', 'pool', '=', 'none', ',', 'cfg_path', '=', 'none', ')', ':', 'return', '"" ""', '.', 'join', '(', 'self', '.', 'command_as_list', '(', 'mark_success', '=', 'mark_success', ',', 'ignore_all_deps', '=', 'ignore_all_deps', ',', 'ignore_depends_on_past', '=', 'ignore_depends_on_past', ',', 'ignore_task_deps', '=', 'ignore_task_deps', ',', 'ignore_ti_state', '=', 'ignore_ti_state', ',', 'local', '=', 'local', ',', 'pickle_id', '=', 'pickle_id', ',', 'raw', '=', 'raw', ',', 'job_id', '=', 'job_id', ',', 'pool', '=', 'pool', ',', 'cfg_path', '=', 'cfg_path', ')', ')']","def command ( self , mark_success = false , ignore_all_deps = false , ignore_depends_on_past = false , ignore_task_deps = false , ignore_ti_state = false , local = false , pickle_id = none , raw = false , job_id = none , pool = none , cfg_path = none ) : return "" "" . join ( self . command_as_list ( mark_success = mark_success , ignore_all_deps = ignore_all_deps , ignore_depends_on_past = ignore_depends_on_past , ignore_task_deps = ignore_task_deps , ignore_ti_state = ignore_ti_state , local = local , pickle_id = pickle_id , raw = raw , job_id = job_id , pool = pool , cfg_path = cfg_path ) )"
345,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L249-L292,"def command_as_list(
            self,
            mark_success=False,
            ignore_all_deps=False,
            ignore_task_deps=False,
            ignore_depends_on_past=False,
            ignore_ti_state=False,
            local=False,
            pickle_id=None,
            raw=False,
            job_id=None,
            pool=None,
            cfg_path=None):
        """"""
        Returns a command that can be executed anywhere where airflow is
        installed. This command is part of the message sent to executors by
        the orchestrator.
        """"""
        dag = self.task.dag

        should_pass_filepath = not pickle_id and dag
        if should_pass_filepath and dag.full_filepath != dag.filepath:
            path = ""DAGS_FOLDER/{}"".format(dag.filepath)
        elif should_pass_filepath and dag.full_filepath:
            path = dag.full_filepath
        else:
            path = None

        return TaskInstance.generate_command(
            self.dag_id,
            self.task_id,
            self.execution_date,
            mark_success=mark_success,
            ignore_all_deps=ignore_all_deps,
            ignore_task_deps=ignore_task_deps,
            ignore_depends_on_past=ignore_depends_on_past,
            ignore_ti_state=ignore_ti_state,
            local=local,
            pickle_id=pickle_id,
            file_path=path,
            raw=raw,
            job_id=job_id,
            pool=pool,
            cfg_path=cfg_path)","['def', 'command_as_list', '(', 'self', ',', 'mark_success', '=', 'False', ',', 'ignore_all_deps', '=', 'False', ',', 'ignore_task_deps', '=', 'False', ',', 'ignore_depends_on_past', '=', 'False', ',', 'ignore_ti_state', '=', 'False', ',', 'local', '=', 'False', ',', 'pickle_id', '=', 'None', ',', 'raw', '=', 'False', ',', 'job_id', '=', 'None', ',', 'pool', '=', 'None', ',', 'cfg_path', '=', 'None', ')', ':', 'dag', '=', 'self', '.', 'task', '.', 'dag', 'should_pass_filepath', '=', 'not', 'pickle_id', 'and', 'dag', 'if', 'should_pass_filepath', 'and', 'dag', '.', 'full_filepath', '!=', 'dag', '.', 'filepath', ':', 'path', '=', '""DAGS_FOLDER/{}""', '.', 'format', '(', 'dag', '.', 'filepath', ')', 'elif', 'should_pass_filepath', 'and', 'dag', '.', 'full_filepath', ':', 'path', '=', 'dag', '.', 'full_filepath', 'else', ':', 'path', '=', 'None', 'return', 'TaskInstance', '.', 'generate_command', '(', 'self', '.', 'dag_id', ',', 'self', '.', 'task_id', ',', 'self', '.', 'execution_date', ',', 'mark_success', '=', 'mark_success', ',', 'ignore_all_deps', '=', 'ignore_all_deps', ',', 'ignore_task_deps', '=', 'ignore_task_deps', ',', 'ignore_depends_on_past', '=', 'ignore_depends_on_past', ',', 'ignore_ti_state', '=', 'ignore_ti_state', ',', 'local', '=', 'local', ',', 'pickle_id', '=', 'pickle_id', ',', 'file_path', '=', 'path', ',', 'raw', '=', 'raw', ',', 'job_id', '=', 'job_id', ',', 'pool', '=', 'pool', ',', 'cfg_path', '=', 'cfg_path', ')']","Returns a command that can be executed anywhere where airflow is
        installed. This command is part of the message sent to executors by
        the orchestrator.","['Returns', 'a', 'command', 'that', 'can', 'be', 'executed', 'anywhere', 'where', 'airflow', 'is', 'installed', '.', 'This', 'command', 'is', 'part', 'of', 'the', 'message', 'sent', 'to', 'executors', 'by', 'the', 'orchestrator', '.']",python,test,"['returns', 'a', 'command', 'that', 'can', 'be', 'executed', 'anywhere', 'where', 'airflow', 'is', 'installed', '.', 'this', 'command', 'is', 'part', 'of', 'the', 'message', 'sent', 'to', 'executors', 'by', 'the', 'orchestrator', '.']",returns a command that can be executed anywhere where airflow is installed . this command is part of the message sent to executors by the orchestrator .,"['def', 'command_as_list', '(', 'self', ',', 'mark_success', '=', 'false', ',', 'ignore_all_deps', '=', 'false', ',', 'ignore_task_deps', '=', 'false', ',', 'ignore_depends_on_past', '=', 'false', ',', 'ignore_ti_state', '=', 'false', ',', 'local', '=', 'false', ',', 'pickle_id', '=', 'none', ',', 'raw', '=', 'false', ',', 'job_id', '=', 'none', ',', 'pool', '=', 'none', ',', 'cfg_path', '=', 'none', ')', ':', 'dag', '=', 'self', '.', 'task', '.', 'dag', 'should_pass_filepath', '=', 'not', 'pickle_id', 'and', 'dag', 'if', 'should_pass_filepath', 'and', 'dag', '.', 'full_filepath', '!=', 'dag', '.', 'filepath', ':', 'path', '=', '""dags_folder/{}""', '.', 'format', '(', 'dag', '.', 'filepath', ')', 'elif', 'should_pass_filepath', 'and', 'dag', '.', 'full_filepath', ':', 'path', '=', 'dag', '.', 'full_filepath', 'else', ':', 'path', '=', 'none', 'return', 'taskinstance', '.', 'generate_command', '(', 'self', '.', 'dag_id', ',', 'self', '.', 'task_id', ',', 'self', '.', 'execution_date', ',', 'mark_success', '=', 'mark_success', ',', 'ignore_all_deps', '=', 'ignore_all_deps', ',', 'ignore_task_deps', '=', 'ignore_task_deps', ',', 'ignore_depends_on_past', '=', 'ignore_depends_on_past', ',', 'ignore_ti_state', '=', 'ignore_ti_state', ',', 'local', '=', 'local', ',', 'pickle_id', '=', 'pickle_id', ',', 'file_path', '=', 'path', ',', 'raw', '=', 'raw', ',', 'job_id', '=', 'job_id', ',', 'pool', '=', 'pool', ',', 'cfg_path', '=', 'cfg_path', ')']","def command_as_list ( self , mark_success = false , ignore_all_deps = false , ignore_task_deps = false , ignore_depends_on_past = false , ignore_ti_state = false , local = false , pickle_id = none , raw = false , job_id = none , pool = none , cfg_path = none ) : dag = self . task . dag should_pass_filepath = not pickle_id and dag if should_pass_filepath and dag . full_filepath != dag . filepath : path = ""dags_folder/{}"" . format ( dag . filepath ) elif should_pass_filepath and dag . full_filepath : path = dag . full_filepath else : path = none return taskinstance . generate_command ( self . dag_id , self . task_id , self . execution_date , mark_success = mark_success , ignore_all_deps = ignore_all_deps , ignore_task_deps = ignore_task_deps , ignore_depends_on_past = ignore_depends_on_past , ignore_ti_state = ignore_ti_state , local = local , pickle_id = pickle_id , file_path = path , raw = raw , job_id = job_id , pool = pool , cfg_path = cfg_path )"
346,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L295-L361,"def generate_command(dag_id,
                         task_id,
                         execution_date,
                         mark_success=False,
                         ignore_all_deps=False,
                         ignore_depends_on_past=False,
                         ignore_task_deps=False,
                         ignore_ti_state=False,
                         local=False,
                         pickle_id=None,
                         file_path=None,
                         raw=False,
                         job_id=None,
                         pool=None,
                         cfg_path=None
                         ):
        """"""
        Generates the shell command required to execute this task instance.

        :param dag_id: DAG ID
        :type dag_id: unicode
        :param task_id: Task ID
        :type task_id: unicode
        :param execution_date: Execution date for the task
        :type execution_date: datetime
        :param mark_success: Whether to mark the task as successful
        :type mark_success: bool
        :param ignore_all_deps: Ignore all ignorable dependencies.
            Overrides the other ignore_* parameters.
        :type ignore_all_deps: bool
        :param ignore_depends_on_past: Ignore depends_on_past parameter of DAGs
            (e.g. for Backfills)
        :type ignore_depends_on_past: bool
        :param ignore_task_deps: Ignore task-specific dependencies such as depends_on_past
            and trigger rule
        :type ignore_task_deps: bool
        :param ignore_ti_state: Ignore the task instance's previous failure/success
        :type ignore_ti_state: bool
        :param local: Whether to run the task locally
        :type local: bool
        :param pickle_id: If the DAG was serialized to the DB, the ID
            associated with the pickled DAG
        :type pickle_id: unicode
        :param file_path: path to the file containing the DAG definition
        :param raw: raw mode (needs more details)
        :param job_id: job ID (needs more details)
        :param pool: the Airflow pool that the task should run in
        :type pool: unicode
        :param cfg_path: the Path to the configuration file
        :type cfg_path: basestring
        :return: shell command that can be used to run the task instance
        """"""
        iso = execution_date.isoformat()
        cmd = [""airflow"", ""run"", str(dag_id), str(task_id), str(iso)]
        cmd.extend([""--mark_success""]) if mark_success else None
        cmd.extend([""--pickle"", str(pickle_id)]) if pickle_id else None
        cmd.extend([""--job_id"", str(job_id)]) if job_id else None
        cmd.extend([""-A""]) if ignore_all_deps else None
        cmd.extend([""-i""]) if ignore_task_deps else None
        cmd.extend([""-I""]) if ignore_depends_on_past else None
        cmd.extend([""--force""]) if ignore_ti_state else None
        cmd.extend([""--local""]) if local else None
        cmd.extend([""--pool"", pool]) if pool else None
        cmd.extend([""--raw""]) if raw else None
        cmd.extend([""-sd"", file_path]) if file_path else None
        cmd.extend([""--cfg_path"", cfg_path]) if cfg_path else None
        return cmd","['def', 'generate_command', '(', 'dag_id', ',', 'task_id', ',', 'execution_date', ',', 'mark_success', '=', 'False', ',', 'ignore_all_deps', '=', 'False', ',', 'ignore_depends_on_past', '=', 'False', ',', 'ignore_task_deps', '=', 'False', ',', 'ignore_ti_state', '=', 'False', ',', 'local', '=', 'False', ',', 'pickle_id', '=', 'None', ',', 'file_path', '=', 'None', ',', 'raw', '=', 'False', ',', 'job_id', '=', 'None', ',', 'pool', '=', 'None', ',', 'cfg_path', '=', 'None', ')', ':', 'iso', '=', 'execution_date', '.', 'isoformat', '(', ')', 'cmd', '=', '[', '""airflow""', ',', '""run""', ',', 'str', '(', 'dag_id', ')', ',', 'str', '(', 'task_id', ')', ',', 'str', '(', 'iso', ')', ']', 'cmd', '.', 'extend', '(', '[', '""--mark_success""', ']', ')', 'if', 'mark_success', 'else', 'None', 'cmd', '.', 'extend', '(', '[', '""--pickle""', ',', 'str', '(', 'pickle_id', ')', ']', ')', 'if', 'pickle_id', 'else', 'None', 'cmd', '.', 'extend', '(', '[', '""--job_id""', ',', 'str', '(', 'job_id', ')', ']', ')', 'if', 'job_id', 'else', 'None', 'cmd', '.', 'extend', '(', '[', '""-A""', ']', ')', 'if', 'ignore_all_deps', 'else', 'None', 'cmd', '.', 'extend', '(', '[', '""-i""', ']', ')', 'if', 'ignore_task_deps', 'else', 'None', 'cmd', '.', 'extend', '(', '[', '""-I""', ']', ')', 'if', 'ignore_depends_on_past', 'else', 'None', 'cmd', '.', 'extend', '(', '[', '""--force""', ']', ')', 'if', 'ignore_ti_state', 'else', 'None', 'cmd', '.', 'extend', '(', '[', '""--local""', ']', ')', 'if', 'local', 'else', 'None', 'cmd', '.', 'extend', '(', '[', '""--pool""', ',', 'pool', ']', ')', 'if', 'pool', 'else', 'None', 'cmd', '.', 'extend', '(', '[', '""--raw""', ']', ')', 'if', 'raw', 'else', 'None', 'cmd', '.', 'extend', '(', '[', '""-sd""', ',', 'file_path', ']', ')', 'if', 'file_path', 'else', 'None', 'cmd', '.', 'extend', '(', '[', '""--cfg_path""', ',', 'cfg_path', ']', ')', 'if', 'cfg_path', 'else', 'None', 'return', 'cmd']","Generates the shell command required to execute this task instance.

        :param dag_id: DAG ID
        :type dag_id: unicode
        :param task_id: Task ID
        :type task_id: unicode
        :param execution_date: Execution date for the task
        :type execution_date: datetime
        :param mark_success: Whether to mark the task as successful
        :type mark_success: bool
        :param ignore_all_deps: Ignore all ignorable dependencies.
            Overrides the other ignore_* parameters.
        :type ignore_all_deps: bool
        :param ignore_depends_on_past: Ignore depends_on_past parameter of DAGs
            (e.g. for Backfills)
        :type ignore_depends_on_past: bool
        :param ignore_task_deps: Ignore task-specific dependencies such as depends_on_past
            and trigger rule
        :type ignore_task_deps: bool
        :param ignore_ti_state: Ignore the task instance's previous failure/success
        :type ignore_ti_state: bool
        :param local: Whether to run the task locally
        :type local: bool
        :param pickle_id: If the DAG was serialized to the DB, the ID
            associated with the pickled DAG
        :type pickle_id: unicode
        :param file_path: path to the file containing the DAG definition
        :param raw: raw mode (needs more details)
        :param job_id: job ID (needs more details)
        :param pool: the Airflow pool that the task should run in
        :type pool: unicode
        :param cfg_path: the Path to the configuration file
        :type cfg_path: basestring
        :return: shell command that can be used to run the task instance","['Generates', 'the', 'shell', 'command', 'required', 'to', 'execute', 'this', 'task', 'instance', '.']",python,test,"['generates', 'the', 'shell', 'command', 'required', 'to', 'execute', 'this', 'task', 'instance', '.']",generates the shell command required to execute this task instance .,"['def', 'generate_command', '(', 'dag_id', ',', 'task_id', ',', 'execution_date', ',', 'mark_success', '=', 'false', ',', 'ignore_all_deps', '=', 'false', ',', 'ignore_depends_on_past', '=', 'false', ',', 'ignore_task_deps', '=', 'false', ',', 'ignore_ti_state', '=', 'false', ',', 'local', '=', 'false', ',', 'pickle_id', '=', 'none', ',', 'file_path', '=', 'none', ',', 'raw', '=', 'false', ',', 'job_id', '=', 'none', ',', 'pool', '=', 'none', ',', 'cfg_path', '=', 'none', ')', ':', 'iso', '=', 'execution_date', '.', 'isoformat', '(', ')', 'cmd', '=', '[', '""airflow""', ',', '""run""', ',', 'str', '(', 'dag_id', ')', ',', 'str', '(', 'task_id', ')', ',', 'str', '(', 'iso', ')', ']', 'cmd', '.', 'extend', '(', '[', '""--mark_success""', ']', ')', 'if', 'mark_success', 'else', 'none', 'cmd', '.', 'extend', '(', '[', '""--pickle""', ',', 'str', '(', 'pickle_id', ')', ']', ')', 'if', 'pickle_id', 'else', 'none', 'cmd', '.', 'extend', '(', '[', '""--job_id""', ',', 'str', '(', 'job_id', ')', ']', ')', 'if', 'job_id', 'else', 'none', 'cmd', '.', 'extend', '(', '[', '""-a""', ']', ')', 'if', 'ignore_all_deps', 'else', 'none', 'cmd', '.', 'extend', '(', '[', '""-i""', ']', ')', 'if', 'ignore_task_deps', 'else', 'none', 'cmd', '.', 'extend', '(', '[', '""-i""', ']', ')', 'if', 'ignore_depends_on_past', 'else', 'none', 'cmd', '.', 'extend', '(', '[', '""--force""', ']', ')', 'if', 'ignore_ti_state', 'else', 'none', 'cmd', '.', 'extend', '(', '[', '""--local""', ']', ')', 'if', 'local', 'else', 'none', 'cmd', '.', 'extend', '(', '[', '""--pool""', ',', 'pool', ']', ')', 'if', 'pool', 'else', 'none', 'cmd', '.', 'extend', '(', '[', '""--raw""', ']', ')', 'if', 'raw', 'else', 'none', 'cmd', '.', 'extend', '(', '[', '""-sd""', ',', 'file_path', ']', ')', 'if', 'file_path', 'else', 'none', 'cmd', '.', 'extend', '(', '[', '""--cfg_path""', ',', 'cfg_path', ']', ')', 'if', 'cfg_path', 'else', 'none', 'return', 'cmd']","def generate_command ( dag_id , task_id , execution_date , mark_success = false , ignore_all_deps = false , ignore_depends_on_past = false , ignore_task_deps = false , ignore_ti_state = false , local = false , pickle_id = none , file_path = none , raw = false , job_id = none , pool = none , cfg_path = none ) : iso = execution_date . isoformat ( ) cmd = [ ""airflow"" , ""run"" , str ( dag_id ) , str ( task_id ) , str ( iso ) ] cmd . extend ( [ ""--mark_success"" ] ) if mark_success else none cmd . extend ( [ ""--pickle"" , str ( pickle_id ) ] ) if pickle_id else none cmd . extend ( [ ""--job_id"" , str ( job_id ) ] ) if job_id else none cmd . extend ( [ ""-a"" ] ) if ignore_all_deps else none cmd . extend ( [ ""-i"" ] ) if ignore_task_deps else none cmd . extend ( [ ""-i"" ] ) if ignore_depends_on_past else none cmd . extend ( [ ""--force"" ] ) if ignore_ti_state else none cmd . extend ( [ ""--local"" ] ) if local else none cmd . extend ( [ ""--pool"" , pool ] ) if pool else none cmd . extend ( [ ""--raw"" ] ) if raw else none cmd . extend ( [ ""-sd"" , file_path ] ) if file_path else none cmd . extend ( [ ""--cfg_path"" , cfg_path ] ) if cfg_path else none return cmd"
347,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L395-L411,"def current_state(self, session=None):
        """"""
        Get the very latest state from the database, if a session is passed,
        we use and looking up the state becomes part of the session, otherwise
        a new session is used.
        """"""
        TI = TaskInstance
        ti = session.query(TI).filter(
            TI.dag_id == self.dag_id,
            TI.task_id == self.task_id,
            TI.execution_date == self.execution_date,
        ).all()
        if ti:
            state = ti[0].state
        else:
            state = None
        return state","['def', 'current_state', '(', 'self', ',', 'session', '=', 'None', ')', ':', 'TI', '=', 'TaskInstance', 'ti', '=', 'session', '.', 'query', '(', 'TI', ')', '.', 'filter', '(', 'TI', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'TI', '.', 'task_id', '==', 'self', '.', 'task_id', ',', 'TI', '.', 'execution_date', '==', 'self', '.', 'execution_date', ',', ')', '.', 'all', '(', ')', 'if', 'ti', ':', 'state', '=', 'ti', '[', '0', ']', '.', 'state', 'else', ':', 'state', '=', 'None', 'return', 'state']","Get the very latest state from the database, if a session is passed,
        we use and looking up the state becomes part of the session, otherwise
        a new session is used.","['Get', 'the', 'very', 'latest', 'state', 'from', 'the', 'database', 'if', 'a', 'session', 'is', 'passed', 'we', 'use', 'and', 'looking', 'up', 'the', 'state', 'becomes', 'part', 'of', 'the', 'session', 'otherwise', 'a', 'new', 'session', 'is', 'used', '.']",python,test,"['get', 'the', 'very', 'latest', 'state', 'from', 'the', 'database', 'if', 'a', 'session', 'is', 'passed', 'we', 'use', 'and', 'looking', 'up', 'the', 'state', 'becomes', 'part', 'of', 'the', 'session', 'otherwise', 'a', 'new', 'session', 'is', 'used', '.']",get the very latest state from the database if a session is passed we use and looking up the state becomes part of the session otherwise a new session is used .,"['def', 'current_state', '(', 'self', ',', 'session', '=', 'none', ')', ':', 'ti', '=', 'taskinstance', 'ti', '=', 'session', '.', 'query', '(', 'ti', ')', '.', 'filter', '(', 'ti', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'ti', '.', 'task_id', '==', 'self', '.', 'task_id', ',', 'ti', '.', 'execution_date', '==', 'self', '.', 'execution_date', ',', ')', '.', 'all', '(', ')', 'if', 'ti', ':', 'state', '=', 'ti', '[', '0', ']', '.', 'state', 'else', ':', 'state', '=', 'none', 'return', 'state']","def current_state ( self , session = none ) : ti = taskinstance ti = session . query ( ti ) . filter ( ti . dag_id == self . dag_id , ti . task_id == self . task_id , ti . execution_date == self . execution_date , ) . all ( ) if ti : state = ti [ 0 ] . state else : state = none return state"
348,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L414-L421,"def error(self, session=None):
        """"""
        Forces the task instance's state to FAILED in the database.
        """"""
        self.log.error(""Recording the task instance as FAILED"")
        self.state = State.FAILED
        session.merge(self)
        session.commit()","['def', 'error', '(', 'self', ',', 'session', '=', 'None', ')', ':', 'self', '.', 'log', '.', 'error', '(', '""Recording the task instance as FAILED""', ')', 'self', '.', 'state', '=', 'State', '.', 'FAILED', 'session', '.', 'merge', '(', 'self', ')', 'session', '.', 'commit', '(', ')']",Forces the task instance's state to FAILED in the database.,"['Forces', 'the', 'task', 'instance', 's', 'state', 'to', 'FAILED', 'in', 'the', 'database', '.']",python,test,"['forces', 'the', 'task', 'instance', 's', 'state', 'to', 'failed', 'in', 'the', 'database', '.']",forces the task instance s state to failed in the database .,"['def', 'error', '(', 'self', ',', 'session', '=', 'none', ')', ':', 'self', '.', 'log', '.', 'error', '(', '""recording the task instance as failed""', ')', 'self', '.', 'state', '=', 'state', '.', 'failed', 'session', '.', 'merge', '(', 'self', ')', 'session', '.', 'commit', '(', ')']","def error ( self , session = none ) : self . log . error ( ""recording the task instance as failed"" ) self . state = state . failed session . merge ( self ) session . commit ( )"
349,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L424-L455,"def refresh_from_db(self, session=None, lock_for_update=False):
        """"""
        Refreshes the task instance from the database based on the primary key

        :param lock_for_update: if True, indicates that the database should
            lock the TaskInstance (issuing a FOR UPDATE clause) until the
            session is committed.
        """"""
        TI = TaskInstance

        qry = session.query(TI).filter(
            TI.dag_id == self.dag_id,
            TI.task_id == self.task_id,
            TI.execution_date == self.execution_date)

        if lock_for_update:
            ti = qry.with_for_update().first()
        else:
            ti = qry.first()
        if ti:
            self.state = ti.state
            self.start_date = ti.start_date
            self.end_date = ti.end_date
            # Get the raw value of try_number column, don't read through the
            # accessor here otherwise it will be incremeneted by one already.
            self.try_number = ti._try_number
            self.max_tries = ti.max_tries
            self.hostname = ti.hostname
            self.pid = ti.pid
            self.executor_config = ti.executor_config
        else:
            self.state = None","['def', 'refresh_from_db', '(', 'self', ',', 'session', '=', 'None', ',', 'lock_for_update', '=', 'False', ')', ':', 'TI', '=', 'TaskInstance', 'qry', '=', 'session', '.', 'query', '(', 'TI', ')', '.', 'filter', '(', 'TI', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'TI', '.', 'task_id', '==', 'self', '.', 'task_id', ',', 'TI', '.', 'execution_date', '==', 'self', '.', 'execution_date', ')', 'if', 'lock_for_update', ':', 'ti', '=', 'qry', '.', 'with_for_update', '(', ')', '.', 'first', '(', ')', 'else', ':', 'ti', '=', 'qry', '.', 'first', '(', ')', 'if', 'ti', ':', 'self', '.', 'state', '=', 'ti', '.', 'state', 'self', '.', 'start_date', '=', 'ti', '.', 'start_date', 'self', '.', 'end_date', '=', 'ti', '.', 'end_date', ""# Get the raw value of try_number column, don't read through the"", '# accessor here otherwise it will be incremeneted by one already.', 'self', '.', 'try_number', '=', 'ti', '.', '_try_number', 'self', '.', 'max_tries', '=', 'ti', '.', 'max_tries', 'self', '.', 'hostname', '=', 'ti', '.', 'hostname', 'self', '.', 'pid', '=', 'ti', '.', 'pid', 'self', '.', 'executor_config', '=', 'ti', '.', 'executor_config', 'else', ':', 'self', '.', 'state', '=', 'None']","Refreshes the task instance from the database based on the primary key

        :param lock_for_update: if True, indicates that the database should
            lock the TaskInstance (issuing a FOR UPDATE clause) until the
            session is committed.","['Refreshes', 'the', 'task', 'instance', 'from', 'the', 'database', 'based', 'on', 'the', 'primary', 'key']",python,test,"['refreshes', 'the', 'task', 'instance', 'from', 'the', 'database', 'based', 'on', 'the', 'primary', 'key']",refreshes the task instance from the database based on the primary key,"['def', 'refresh_from_db', '(', 'self', ',', 'session', '=', 'none', ',', 'lock_for_update', '=', 'false', ')', ':', 'ti', '=', 'taskinstance', 'qry', '=', 'session', '.', 'query', '(', 'ti', ')', '.', 'filter', '(', 'ti', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'ti', '.', 'task_id', '==', 'self', '.', 'task_id', ',', 'ti', '.', 'execution_date', '==', 'self', '.', 'execution_date', ')', 'if', 'lock_for_update', ':', 'ti', '=', 'qry', '.', 'with_for_update', '(', ')', '.', 'first', '(', ')', 'else', ':', 'ti', '=', 'qry', '.', 'first', '(', ')', 'if', 'ti', ':', 'self', '.', 'state', '=', 'ti', '.', 'state', 'self', '.', 'start_date', '=', 'ti', '.', 'start_date', 'self', '.', 'end_date', '=', 'ti', '.', 'end_date', ""# get the raw value of try_number column, don't read through the"", '# accessor here otherwise it will be incremeneted by one already.', 'self', '.', 'try_number', '=', 'ti', '.', '_try_number', 'self', '.', 'max_tries', '=', 'ti', '.', 'max_tries', 'self', '.', 'hostname', '=', 'ti', '.', 'hostname', 'self', '.', 'pid', '=', 'ti', '.', 'pid', 'self', '.', 'executor_config', '=', 'ti', '.', 'executor_config', 'else', ':', 'self', '.', 'state', '=', 'none']","def refresh_from_db ( self , session = none , lock_for_update = false ) : ti = taskinstance qry = session . query ( ti ) . filter ( ti . dag_id == self . dag_id , ti . task_id == self . task_id , ti . execution_date == self . execution_date ) if lock_for_update : ti = qry . with_for_update ( ) . first ( ) else : ti = qry . first ( ) if ti : self . state = ti . state self . start_date = ti . start_date self . end_date = ti . end_date # get the raw value of try_number column, don't read through the # accessor here otherwise it will be incremeneted by one already. self . try_number = ti . _try_number self . max_tries = ti . max_tries self . hostname = ti . hostname self . pid = ti . pid self . executor_config = ti . executor_config else : self . state = none"
350,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L458-L467,"def clear_xcom_data(self, session=None):
        """"""
        Clears all XCom data from the database for the task instance
        """"""
        session.query(XCom).filter(
            XCom.dag_id == self.dag_id,
            XCom.task_id == self.task_id,
            XCom.execution_date == self.execution_date
        ).delete()
        session.commit()","['def', 'clear_xcom_data', '(', 'self', ',', 'session', '=', 'None', ')', ':', 'session', '.', 'query', '(', 'XCom', ')', '.', 'filter', '(', 'XCom', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'XCom', '.', 'task_id', '==', 'self', '.', 'task_id', ',', 'XCom', '.', 'execution_date', '==', 'self', '.', 'execution_date', ')', '.', 'delete', '(', ')', 'session', '.', 'commit', '(', ')']",Clears all XCom data from the database for the task instance,"['Clears', 'all', 'XCom', 'data', 'from', 'the', 'database', 'for', 'the', 'task', 'instance']",python,test,"['clears', 'all', 'xcom', 'data', 'from', 'the', 'database', 'for', 'the', 'task', 'instance']",clears all xcom data from the database for the task instance,"['def', 'clear_xcom_data', '(', 'self', ',', 'session', '=', 'none', ')', ':', 'session', '.', 'query', '(', 'xcom', ')', '.', 'filter', '(', 'xcom', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'xcom', '.', 'task_id', '==', 'self', '.', 'task_id', ',', 'xcom', '.', 'execution_date', '==', 'self', '.', 'execution_date', ')', '.', 'delete', '(', ')', 'session', '.', 'commit', '(', ')']","def clear_xcom_data ( self , session = none ) : session . query ( xcom ) . filter ( xcom . dag_id == self . dag_id , xcom . task_id == self . task_id , xcom . execution_date == self . execution_date ) . delete ( ) session . commit ( )"
351,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L470-L474,"def key(self):
        """"""
        Returns a tuple that identifies the task instance uniquely
        """"""
        return self.dag_id, self.task_id, self.execution_date, self.try_number","['def', 'key', '(', 'self', ')', ':', 'return', 'self', '.', 'dag_id', ',', 'self', '.', 'task_id', ',', 'self', '.', 'execution_date', ',', 'self', '.', 'try_number']",Returns a tuple that identifies the task instance uniquely,"['Returns', 'a', 'tuple', 'that', 'identifies', 'the', 'task', 'instance', 'uniquely']",python,test,"['returns', 'a', 'tuple', 'that', 'identifies', 'the', 'task', 'instance', 'uniquely']",returns a tuple that identifies the task instance uniquely,"['def', 'key', '(', 'self', ')', ':', 'return', 'self', '.', 'dag_id', ',', 'self', '.', 'task_id', ',', 'self', '.', 'execution_date', ',', 'self', '.', 'try_number']","def key ( self ) : return self . dag_id , self . task_id , self . execution_date , self . try_number"
352,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L494-L515,"def are_dependents_done(self, session=None):
        """"""
        Checks whether the dependents of this task instance have all succeeded.
        This is meant to be used by wait_for_downstream.

        This is useful when you do not want to start processing the next
        schedule of a task until the dependents are done. For instance,
        if the task DROPs and recreates a table.
        """"""
        task = self.task

        if not task.downstream_task_ids:
            return True

        ti = session.query(func.count(TaskInstance.task_id)).filter(
            TaskInstance.dag_id == self.dag_id,
            TaskInstance.task_id.in_(task.downstream_task_ids),
            TaskInstance.execution_date == self.execution_date,
            TaskInstance.state == State.SUCCESS,
        )
        count = ti[0][0]
        return count == len(task.downstream_task_ids)","['def', 'are_dependents_done', '(', 'self', ',', 'session', '=', 'None', ')', ':', 'task', '=', 'self', '.', 'task', 'if', 'not', 'task', '.', 'downstream_task_ids', ':', 'return', 'True', 'ti', '=', 'session', '.', 'query', '(', 'func', '.', 'count', '(', 'TaskInstance', '.', 'task_id', ')', ')', '.', 'filter', '(', 'TaskInstance', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'TaskInstance', '.', 'task_id', '.', 'in_', '(', 'task', '.', 'downstream_task_ids', ')', ',', 'TaskInstance', '.', 'execution_date', '==', 'self', '.', 'execution_date', ',', 'TaskInstance', '.', 'state', '==', 'State', '.', 'SUCCESS', ',', ')', 'count', '=', 'ti', '[', '0', ']', '[', '0', ']', 'return', 'count', '==', 'len', '(', 'task', '.', 'downstream_task_ids', ')']","Checks whether the dependents of this task instance have all succeeded.
        This is meant to be used by wait_for_downstream.

        This is useful when you do not want to start processing the next
        schedule of a task until the dependents are done. For instance,
        if the task DROPs and recreates a table.","['Checks', 'whether', 'the', 'dependents', 'of', 'this', 'task', 'instance', 'have', 'all', 'succeeded', '.', 'This', 'is', 'meant', 'to', 'be', 'used', 'by', 'wait_for_downstream', '.']",python,test,"['checks', 'whether', 'the', 'dependents', 'of', 'this', 'task', 'instance', 'have', 'all', 'succeeded', '.', 'this', 'is', 'meant', 'to', 'be', 'used', 'by', 'wait_for_downstream', '.']",checks whether the dependents of this task instance have all succeeded . this is meant to be used by wait_for_downstream .,"['def', 'are_dependents_done', '(', 'self', ',', 'session', '=', 'none', ')', ':', 'task', '=', 'self', '.', 'task', 'if', 'not', 'task', '.', 'downstream_task_ids', ':', 'return', 'true', 'ti', '=', 'session', '.', 'query', '(', 'func', '.', 'count', '(', 'taskinstance', '.', 'task_id', ')', ')', '.', 'filter', '(', 'taskinstance', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'taskinstance', '.', 'task_id', '.', 'in_', '(', 'task', '.', 'downstream_task_ids', ')', ',', 'taskinstance', '.', 'execution_date', '==', 'self', '.', 'execution_date', ',', 'taskinstance', '.', 'state', '==', 'state', '.', 'success', ',', ')', 'count', '=', 'ti', '[', '0', ']', '[', '0', ']', 'return', 'count', '==', 'len', '(', 'task', '.', 'downstream_task_ids', ')']","def are_dependents_done ( self , session = none ) : task = self . task if not task . downstream_task_ids : return true ti = session . query ( func . count ( taskinstance . task_id ) ) . filter ( taskinstance . dag_id == self . dag_id , taskinstance . task_id . in_ ( task . downstream_task_ids ) , taskinstance . execution_date == self . execution_date , taskinstance . state == state . success , ) count = ti [ 0 ] [ 0 ] return count == len ( task . downstream_task_ids )"
353,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L550-L586,"def are_dependencies_met(
            self,
            dep_context=None,
            session=None,
            verbose=False):
        """"""
        Returns whether or not all the conditions are met for this task instance to be run
        given the context for the dependencies (e.g. a task instance being force run from
        the UI will ignore some dependencies).

        :param dep_context: The execution context that determines the dependencies that
            should be evaluated.
        :type dep_context: DepContext
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        :param verbose: whether log details on failed dependencies on
            info or debug log level
        :type verbose: bool
        """"""
        dep_context = dep_context or DepContext()
        failed = False
        verbose_aware_logger = self.log.info if verbose else self.log.debug
        for dep_status in self.get_failed_dep_statuses(
                dep_context=dep_context,
                session=session):
            failed = True

            verbose_aware_logger(
                ""Dependencies not met for %s, dependency '%s' FAILED: %s"",
                self, dep_status.dep_name, dep_status.reason
            )

        if failed:
            return False

        verbose_aware_logger(""Dependencies all met for %s"", self)
        return True","['def', 'are_dependencies_met', '(', 'self', ',', 'dep_context', '=', 'None', ',', 'session', '=', 'None', ',', 'verbose', '=', 'False', ')', ':', 'dep_context', '=', 'dep_context', 'or', 'DepContext', '(', ')', 'failed', '=', 'False', 'verbose_aware_logger', '=', 'self', '.', 'log', '.', 'info', 'if', 'verbose', 'else', 'self', '.', 'log', '.', 'debug', 'for', 'dep_status', 'in', 'self', '.', 'get_failed_dep_statuses', '(', 'dep_context', '=', 'dep_context', ',', 'session', '=', 'session', ')', ':', 'failed', '=', 'True', 'verbose_aware_logger', '(', '""Dependencies not met for %s, dependency \'%s\' FAILED: %s""', ',', 'self', ',', 'dep_status', '.', 'dep_name', ',', 'dep_status', '.', 'reason', ')', 'if', 'failed', ':', 'return', 'False', 'verbose_aware_logger', '(', '""Dependencies all met for %s""', ',', 'self', ')', 'return', 'True']","Returns whether or not all the conditions are met for this task instance to be run
        given the context for the dependencies (e.g. a task instance being force run from
        the UI will ignore some dependencies).

        :param dep_context: The execution context that determines the dependencies that
            should be evaluated.
        :type dep_context: DepContext
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        :param verbose: whether log details on failed dependencies on
            info or debug log level
        :type verbose: bool","['Returns', 'whether', 'or', 'not', 'all', 'the', 'conditions', 'are', 'met', 'for', 'this', 'task', 'instance', 'to', 'be', 'run', 'given', 'the', 'context', 'for', 'the', 'dependencies', '(', 'e', '.', 'g', '.', 'a', 'task', 'instance', 'being', 'force', 'run', 'from', 'the', 'UI', 'will', 'ignore', 'some', 'dependencies', ')', '.']",python,test,"['returns', 'whether', 'or', 'not', 'all', 'the', 'conditions', 'are', 'met', 'for', 'this', 'task', 'instance', 'to', 'be', 'run', 'given', 'the', 'context', 'for', 'the', 'dependencies', '(', 'e', '.', 'g', '.', 'a', 'task', 'instance', 'being', 'force', 'run', 'from', 'the', 'ui', 'will', 'ignore', 'some', 'dependencies', ')', '.']",returns whether or not all the conditions are met for this task instance to be run given the context for the dependencies ( e . g . a task instance being force run from the ui will ignore some dependencies ) .,"['def', 'are_dependencies_met', '(', 'self', ',', 'dep_context', '=', 'none', ',', 'session', '=', 'none', ',', 'verbose', '=', 'false', ')', ':', 'dep_context', '=', 'dep_context', 'or', 'depcontext', '(', ')', 'failed', '=', 'false', 'verbose_aware_logger', '=', 'self', '.', 'log', '.', 'info', 'if', 'verbose', 'else', 'self', '.', 'log', '.', 'debug', 'for', 'dep_status', 'in', 'self', '.', 'get_failed_dep_statuses', '(', 'dep_context', '=', 'dep_context', ',', 'session', '=', 'session', ')', ':', 'failed', '=', 'true', 'verbose_aware_logger', '(', '""dependencies not met for %s, dependency \'%s\' failed: %s""', ',', 'self', ',', 'dep_status', '.', 'dep_name', ',', 'dep_status', '.', 'reason', ')', 'if', 'failed', ':', 'return', 'false', 'verbose_aware_logger', '(', '""dependencies all met for %s""', ',', 'self', ')', 'return', 'true']","def are_dependencies_met ( self , dep_context = none , session = none , verbose = false ) : dep_context = dep_context or depcontext ( ) failed = false verbose_aware_logger = self . log . info if verbose else self . log . debug for dep_status in self . get_failed_dep_statuses ( dep_context = dep_context , session = session ) : failed = true verbose_aware_logger ( ""dependencies not met for %s, dependency '%s' failed: %s"" , self , dep_status . dep_name , dep_status . reason ) if failed : return false verbose_aware_logger ( ""dependencies all met for %s"" , self ) return true"
354,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L614-L642,"def next_retry_datetime(self):
        """"""
        Get datetime of the next retry if the task instance fails. For exponential
        backoff, retry_delay is used as base and will be converted to seconds.
        """"""
        delay = self.task.retry_delay
        if self.task.retry_exponential_backoff:
            min_backoff = int(delay.total_seconds() * (2 ** (self.try_number - 2)))
            # deterministic per task instance
            hash = int(hashlib.sha1(""{}#{}#{}#{}"".format(self.dag_id,
                                                         self.task_id,
                                                         self.execution_date,
                                                         self.try_number)
                                    .encode('utf-8')).hexdigest(), 16)
            # between 0.5 * delay * (2^retry_number) and 1.0 * delay * (2^retry_number)
            modded_hash = min_backoff + hash % min_backoff
            # timedelta has a maximum representable value. The exponentiation
            # here means this value can be exceeded after a certain number
            # of tries (around 50 if the initial delay is 1s, even fewer if
            # the delay is larger). Cap the value here before creating a
            # timedelta object so the operation doesn't fail.
            delay_backoff_in_seconds = min(
                modded_hash,
                timedelta.max.total_seconds() - 1
            )
            delay = timedelta(seconds=delay_backoff_in_seconds)
            if self.task.max_retry_delay:
                delay = min(self.task.max_retry_delay, delay)
        return self.end_date + delay","['def', 'next_retry_datetime', '(', 'self', ')', ':', 'delay', '=', 'self', '.', 'task', '.', 'retry_delay', 'if', 'self', '.', 'task', '.', 'retry_exponential_backoff', ':', 'min_backoff', '=', 'int', '(', 'delay', '.', 'total_seconds', '(', ')', '*', '(', '2', '**', '(', 'self', '.', 'try_number', '-', '2', ')', ')', ')', '# deterministic per task instance', 'hash', '=', 'int', '(', 'hashlib', '.', 'sha1', '(', '""{}#{}#{}#{}""', '.', 'format', '(', 'self', '.', 'dag_id', ',', 'self', '.', 'task_id', ',', 'self', '.', 'execution_date', ',', 'self', '.', 'try_number', ')', '.', 'encode', '(', ""'utf-8'"", ')', ')', '.', 'hexdigest', '(', ')', ',', '16', ')', '# between 0.5 * delay * (2^retry_number) and 1.0 * delay * (2^retry_number)', 'modded_hash', '=', 'min_backoff', '+', 'hash', '%', 'min_backoff', '# timedelta has a maximum representable value. The exponentiation', '# here means this value can be exceeded after a certain number', '# of tries (around 50 if the initial delay is 1s, even fewer if', '# the delay is larger). Cap the value here before creating a', ""# timedelta object so the operation doesn't fail."", 'delay_backoff_in_seconds', '=', 'min', '(', 'modded_hash', ',', 'timedelta', '.', 'max', '.', 'total_seconds', '(', ')', '-', '1', ')', 'delay', '=', 'timedelta', '(', 'seconds', '=', 'delay_backoff_in_seconds', ')', 'if', 'self', '.', 'task', '.', 'max_retry_delay', ':', 'delay', '=', 'min', '(', 'self', '.', 'task', '.', 'max_retry_delay', ',', 'delay', ')', 'return', 'self', '.', 'end_date', '+', 'delay']","Get datetime of the next retry if the task instance fails. For exponential
        backoff, retry_delay is used as base and will be converted to seconds.","['Get', 'datetime', 'of', 'the', 'next', 'retry', 'if', 'the', 'task', 'instance', 'fails', '.', 'For', 'exponential', 'backoff', 'retry_delay', 'is', 'used', 'as', 'base', 'and', 'will', 'be', 'converted', 'to', 'seconds', '.']",python,test,"['get', 'datetime', 'of', 'the', 'next', 'retry', 'if', 'the', 'task', 'instance', 'fails', '.', 'for', 'exponential', 'backoff', 'retry_delay', 'is', 'used', 'as', 'base', 'and', 'will', 'be', 'converted', 'to', 'seconds', '.']",get datetime of the next retry if the task instance fails . for exponential backoff retry_delay is used as base and will be converted to seconds .,"['def', 'next_retry_datetime', '(', 'self', ')', ':', 'delay', '=', 'self', '.', 'task', '.', 'retry_delay', 'if', 'self', '.', 'task', '.', 'retry_exponential_backoff', ':', 'min_backoff', '=', 'int', '(', 'delay', '.', 'total_seconds', '(', ')', '*', '(', '2', '**', '(', 'self', '.', 'try_number', '-', '2', ')', ')', ')', '# deterministic per task instance', 'hash', '=', 'int', '(', 'hashlib', '.', 'sha1', '(', '""{}#{}#{}#{}""', '.', 'format', '(', 'self', '.', 'dag_id', ',', 'self', '.', 'task_id', ',', 'self', '.', 'execution_date', ',', 'self', '.', 'try_number', ')', '.', 'encode', '(', ""'utf-8'"", ')', ')', '.', 'hexdigest', '(', ')', ',', '16', ')', '# between 0.5 * delay * (2^retry_number) and 1.0 * delay * (2^retry_number)', 'modded_hash', '=', 'min_backoff', '+', 'hash', '%', 'min_backoff', '# timedelta has a maximum representable value. the exponentiation', '# here means this value can be exceeded after a certain number', '# of tries (around 50 if the initial delay is 1s, even fewer if', '# the delay is larger). cap the value here before creating a', ""# timedelta object so the operation doesn't fail."", 'delay_backoff_in_seconds', '=', 'min', '(', 'modded_hash', ',', 'timedelta', '.', 'max', '.', 'total_seconds', '(', ')', '-', '1', ')', 'delay', '=', 'timedelta', '(', 'seconds', '=', 'delay_backoff_in_seconds', ')', 'if', 'self', '.', 'task', '.', 'max_retry_delay', ':', 'delay', '=', 'min', '(', 'self', '.', 'task', '.', 'max_retry_delay', ',', 'delay', ')', 'return', 'self', '.', 'end_date', '+', 'delay']","def next_retry_datetime ( self ) : delay = self . task . retry_delay if self . task . retry_exponential_backoff : min_backoff = int ( delay . total_seconds ( ) * ( 2 ** ( self . try_number - 2 ) ) ) # deterministic per task instance hash = int ( hashlib . sha1 ( ""{}#{}#{}#{}"" . format ( self . dag_id , self . task_id , self . execution_date , self . try_number ) . encode ( 'utf-8' ) ) . hexdigest ( ) , 16 ) # between 0.5 * delay * (2^retry_number) and 1.0 * delay * (2^retry_number) modded_hash = min_backoff + hash % min_backoff # timedelta has a maximum representable value. the exponentiation # here means this value can be exceeded after a certain number # of tries (around 50 if the initial delay is 1s, even fewer if # the delay is larger). cap the value here before creating a # timedelta object so the operation doesn't fail. delay_backoff_in_seconds = min ( modded_hash , timedelta . max . total_seconds ( ) - 1 ) delay = timedelta ( seconds = delay_backoff_in_seconds ) if self . task . max_retry_delay : delay = min ( self . task . max_retry_delay , delay ) return self . end_date + delay"
355,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L644-L650,"def ready_for_retry(self):
        """"""
        Checks on whether the task instance is in the right state and timeframe
        to be retried.
        """"""
        return (self.state == State.UP_FOR_RETRY and
                self.next_retry_datetime() < timezone.utcnow())","['def', 'ready_for_retry', '(', 'self', ')', ':', 'return', '(', 'self', '.', 'state', '==', 'State', '.', 'UP_FOR_RETRY', 'and', 'self', '.', 'next_retry_datetime', '(', ')', '<', 'timezone', '.', 'utcnow', '(', ')', ')']","Checks on whether the task instance is in the right state and timeframe
        to be retried.","['Checks', 'on', 'whether', 'the', 'task', 'instance', 'is', 'in', 'the', 'right', 'state', 'and', 'timeframe', 'to', 'be', 'retried', '.']",python,test,"['checks', 'on', 'whether', 'the', 'task', 'instance', 'is', 'in', 'the', 'right', 'state', 'and', 'timeframe', 'to', 'be', 'retried', '.']",checks on whether the task instance is in the right state and timeframe to be retried .,"['def', 'ready_for_retry', '(', 'self', ')', ':', 'return', '(', 'self', '.', 'state', '==', 'state', '.', 'up_for_retry', 'and', 'self', '.', 'next_retry_datetime', '(', ')', '<', 'timezone', '.', 'utcnow', '(', ')', ')']",def ready_for_retry ( self ) : return ( self . state == state . up_for_retry and self . next_retry_datetime ( ) < timezone . utcnow ( ) )
356,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L653-L671,"def pool_full(self, session):
        """"""
        Returns a boolean as to whether the slot pool has room for this
        task to run
        """"""
        if not self.task.pool:
            return False

        pool = (
            session
            .query(Pool)
            .filter(Pool.pool == self.task.pool)
            .first()
        )
        if not pool:
            return False
        open_slots = pool.open_slots(session=session)

        return open_slots <= 0","['def', 'pool_full', '(', 'self', ',', 'session', ')', ':', 'if', 'not', 'self', '.', 'task', '.', 'pool', ':', 'return', 'False', 'pool', '=', '(', 'session', '.', 'query', '(', 'Pool', ')', '.', 'filter', '(', 'Pool', '.', 'pool', '==', 'self', '.', 'task', '.', 'pool', ')', '.', 'first', '(', ')', ')', 'if', 'not', 'pool', ':', 'return', 'False', 'open_slots', '=', 'pool', '.', 'open_slots', '(', 'session', '=', 'session', ')', 'return', 'open_slots', '<=', '0']","Returns a boolean as to whether the slot pool has room for this
        task to run","['Returns', 'a', 'boolean', 'as', 'to', 'whether', 'the', 'slot', 'pool', 'has', 'room', 'for', 'this', 'task', 'to', 'run']",python,test,"['returns', 'a', 'boolean', 'as', 'to', 'whether', 'the', 'slot', 'pool', 'has', 'room', 'for', 'this', 'task', 'to', 'run']",returns a boolean as to whether the slot pool has room for this task to run,"['def', 'pool_full', '(', 'self', ',', 'session', ')', ':', 'if', 'not', 'self', '.', 'task', '.', 'pool', ':', 'return', 'false', 'pool', '=', '(', 'session', '.', 'query', '(', 'pool', ')', '.', 'filter', '(', 'pool', '.', 'pool', '==', 'self', '.', 'task', '.', 'pool', ')', '.', 'first', '(', ')', ')', 'if', 'not', 'pool', ':', 'return', 'false', 'open_slots', '=', 'pool', '.', 'open_slots', '(', 'session', '=', 'session', ')', 'return', 'open_slots', '<=', '0']","def pool_full ( self , session ) : if not self . task . pool : return false pool = ( session . query ( pool ) . filter ( pool . pool == self . task . pool ) . first ( ) ) if not pool : return false open_slots = pool . open_slots ( session = session ) return open_slots <= 0"
357,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L674-L687,"def get_dagrun(self, session):
        """"""
        Returns the DagRun for this TaskInstance

        :param session:
        :return: DagRun
        """"""
        from airflow.models.dagrun import DagRun  # Avoid circular import
        dr = session.query(DagRun).filter(
            DagRun.dag_id == self.dag_id,
            DagRun.execution_date == self.execution_date
        ).first()

        return dr","['def', 'get_dagrun', '(', 'self', ',', 'session', ')', ':', 'from', 'airflow', '.', 'models', '.', 'dagrun', 'import', 'DagRun', '# Avoid circular import', 'dr', '=', 'session', '.', 'query', '(', 'DagRun', ')', '.', 'filter', '(', 'DagRun', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'DagRun', '.', 'execution_date', '==', 'self', '.', 'execution_date', ')', '.', 'first', '(', ')', 'return', 'dr']","Returns the DagRun for this TaskInstance

        :param session:
        :return: DagRun","['Returns', 'the', 'DagRun', 'for', 'this', 'TaskInstance']",python,test,"['returns', 'the', 'dagrun', 'for', 'this', 'taskinstance']",returns the dagrun for this taskinstance,"['def', 'get_dagrun', '(', 'self', ',', 'session', ')', ':', 'from', 'airflow', '.', 'models', '.', 'dagrun', 'import', 'dagrun', '# avoid circular import', 'dr', '=', 'session', '.', 'query', '(', 'dagrun', ')', '.', 'filter', '(', 'dagrun', '.', 'dag_id', '==', 'self', '.', 'dag_id', ',', 'dagrun', '.', 'execution_date', '==', 'self', '.', 'execution_date', ')', '.', 'first', '(', ')', 'return', 'dr']","def get_dagrun ( self , session ) : from airflow . models . dagrun import dagrun # avoid circular import dr = session . query ( dagrun ) . filter ( dagrun . dag_id == self . dag_id , dagrun . execution_date == self . execution_date ) . first ( ) return dr"
358,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L690-L822,"def _check_and_change_state_before_execution(
            self,
            verbose=True,
            ignore_all_deps=False,
            ignore_depends_on_past=False,
            ignore_task_deps=False,
            ignore_ti_state=False,
            mark_success=False,
            test_mode=False,
            job_id=None,
            pool=None,
            session=None):
        """"""
        Checks dependencies and then sets state to RUNNING if they are met. Returns
        True if and only if state is set to RUNNING, which implies that task should be
        executed, in preparation for _run_raw_task

        :param verbose: whether to turn on more verbose logging
        :type verbose: bool
        :param ignore_all_deps: Ignore all of the non-critical dependencies, just runs
        :type ignore_all_deps: bool
        :param ignore_depends_on_past: Ignore depends_on_past DAG attribute
        :type ignore_depends_on_past: bool
        :param ignore_task_deps: Don't check the dependencies of this TI's task
        :type ignore_task_deps: bool
        :param ignore_ti_state: Disregards previous task instance state
        :type ignore_ti_state: bool
        :param mark_success: Don't run the task, mark its state as success
        :type mark_success: bool
        :param test_mode: Doesn't record success or failure in the DB
        :type test_mode: bool
        :param pool: specifies the pool to use to run the task instance
        :type pool: str
        :return: whether the state was changed to running or not
        :rtype: bool
        """"""
        task = self.task
        self.pool = pool or task.pool
        self.test_mode = test_mode
        self.refresh_from_db(session=session, lock_for_update=True)
        self.job_id = job_id
        self.hostname = get_hostname()
        self.operator = task.__class__.__name__

        if not ignore_all_deps and not ignore_ti_state and self.state == State.SUCCESS:
            Stats.incr('previously_succeeded', 1, 1)

        queue_dep_context = DepContext(
            deps=QUEUE_DEPS,
            ignore_all_deps=ignore_all_deps,
            ignore_ti_state=ignore_ti_state,
            ignore_depends_on_past=ignore_depends_on_past,
            ignore_task_deps=ignore_task_deps)
        if not self.are_dependencies_met(
                dep_context=queue_dep_context,
                session=session,
                verbose=True):
            session.commit()
            return False

        # TODO: Logging needs cleanup, not clear what is being printed
        hr = ""\n"" + (""-"" * 80)  # Line break

        # For reporting purposes, we report based on 1-indexed,
        # not 0-indexed lists (i.e. Attempt 1 instead of
        # Attempt 0 for the first attempt).
        # Set the task start date. In case it was re-scheduled use the initial
        # start date that is recorded in task_reschedule table
        self.start_date = timezone.utcnow()
        task_reschedules = TaskReschedule.find_for_task_instance(self, session)
        if task_reschedules:
            self.start_date = task_reschedules[0].start_date

        dep_context = DepContext(
            deps=RUN_DEPS - QUEUE_DEPS,
            ignore_all_deps=ignore_all_deps,
            ignore_depends_on_past=ignore_depends_on_past,
            ignore_task_deps=ignore_task_deps,
            ignore_ti_state=ignore_ti_state)
        runnable = self.are_dependencies_met(
            dep_context=dep_context,
            session=session,
            verbose=True)

        if not runnable and not mark_success:
            # FIXME: we might have hit concurrency limits, which means we probably
            # have been running prematurely. This should be handled in the
            # scheduling mechanism.
            self.state = State.NONE
            self.log.warning(hr)
            self.log.warning(
                ""FIXME: Rescheduling due to concurrency limits reached at task runtime. Attempt %s of ""
                ""%s. State set to NONE."", self.try_number, self.max_tries + 1
            )
            self.log.warning(hr)

            self.queued_dttm = timezone.utcnow()
            self.log.info(""Queuing into pool %s"", self.pool)
            session.merge(self)
            session.commit()
            return False

        # Another worker might have started running this task instance while
        # the current worker process was blocked on refresh_from_db
        if self.state == State.RUNNING:
            self.log.warning(""Task Instance already running %s"", self)
            session.commit()
            return False

        # print status message
        self.log.info(hr)
        self.log.info(""Starting attempt %s of %s"", self.try_number, self.max_tries + 1)
        self.log.info(hr)
        self._try_number += 1

        if not test_mode:
            session.add(Log(State.RUNNING, self))
        self.state = State.RUNNING
        self.pid = os.getpid()
        self.end_date = None
        if not test_mode:
            session.merge(self)
        session.commit()

        # Closing all pooled connections to prevent
        # ""max number of connections reached""
        settings.engine.dispose()
        if verbose:
            if mark_success:
                self.log.info(""Marking success for %s on %s"", self.task, self.execution_date)
            else:
                self.log.info(""Executing %s on %s"", self.task, self.execution_date)
        return True","['def', '_check_and_change_state_before_execution', '(', 'self', ',', 'verbose', '=', 'True', ',', 'ignore_all_deps', '=', 'False', ',', 'ignore_depends_on_past', '=', 'False', ',', 'ignore_task_deps', '=', 'False', ',', 'ignore_ti_state', '=', 'False', ',', 'mark_success', '=', 'False', ',', 'test_mode', '=', 'False', ',', 'job_id', '=', 'None', ',', 'pool', '=', 'None', ',', 'session', '=', 'None', ')', ':', 'task', '=', 'self', '.', 'task', 'self', '.', 'pool', '=', 'pool', 'or', 'task', '.', 'pool', 'self', '.', 'test_mode', '=', 'test_mode', 'self', '.', 'refresh_from_db', '(', 'session', '=', 'session', ',', 'lock_for_update', '=', 'True', ')', 'self', '.', 'job_id', '=', 'job_id', 'self', '.', 'hostname', '=', 'get_hostname', '(', ')', 'self', '.', 'operator', '=', 'task', '.', '__class__', '.', '__name__', 'if', 'not', 'ignore_all_deps', 'and', 'not', 'ignore_ti_state', 'and', 'self', '.', 'state', '==', 'State', '.', 'SUCCESS', ':', 'Stats', '.', 'incr', '(', ""'previously_succeeded'"", ',', '1', ',', '1', ')', 'queue_dep_context', '=', 'DepContext', '(', 'deps', '=', 'QUEUE_DEPS', ',', 'ignore_all_deps', '=', 'ignore_all_deps', ',', 'ignore_ti_state', '=', 'ignore_ti_state', ',', 'ignore_depends_on_past', '=', 'ignore_depends_on_past', ',', 'ignore_task_deps', '=', 'ignore_task_deps', ')', 'if', 'not', 'self', '.', 'are_dependencies_met', '(', 'dep_context', '=', 'queue_dep_context', ',', 'session', '=', 'session', ',', 'verbose', '=', 'True', ')', ':', 'session', '.', 'commit', '(', ')', 'return', 'False', '# TODO: Logging needs cleanup, not clear what is being printed', 'hr', '=', '""\\n""', '+', '(', '""-""', '*', '80', ')', '# Line break', '# For reporting purposes, we report based on 1-indexed,', '# not 0-indexed lists (i.e. Attempt 1 instead of', '# Attempt 0 for the first attempt).', '# Set the task start date. In case it was re-scheduled use the initial', '# start date that is recorded in task_reschedule table', 'self', '.', 'start_date', '=', 'timezone', '.', 'utcnow', '(', ')', 'task_reschedules', '=', 'TaskReschedule', '.', 'find_for_task_instance', '(', 'self', ',', 'session', ')', 'if', 'task_reschedules', ':', 'self', '.', 'start_date', '=', 'task_reschedules', '[', '0', ']', '.', 'start_date', 'dep_context', '=', 'DepContext', '(', 'deps', '=', 'RUN_DEPS', '-', 'QUEUE_DEPS', ',', 'ignore_all_deps', '=', 'ignore_all_deps', ',', 'ignore_depends_on_past', '=', 'ignore_depends_on_past', ',', 'ignore_task_deps', '=', 'ignore_task_deps', ',', 'ignore_ti_state', '=', 'ignore_ti_state', ')', 'runnable', '=', 'self', '.', 'are_dependencies_met', '(', 'dep_context', '=', 'dep_context', ',', 'session', '=', 'session', ',', 'verbose', '=', 'True', ')', 'if', 'not', 'runnable', 'and', 'not', 'mark_success', ':', '# FIXME: we might have hit concurrency limits, which means we probably', '# have been running prematurely. This should be handled in the', '# scheduling mechanism.', 'self', '.', 'state', '=', 'State', '.', 'NONE', 'self', '.', 'log', '.', 'warning', '(', 'hr', ')', 'self', '.', 'log', '.', 'warning', '(', '""FIXME: Rescheduling due to concurrency limits reached at task runtime. Attempt %s of ""', '""%s. State set to NONE.""', ',', 'self', '.', 'try_number', ',', 'self', '.', 'max_tries', '+', '1', ')', 'self', '.', 'log', '.', 'warning', '(', 'hr', ')', 'self', '.', 'queued_dttm', '=', 'timezone', '.', 'utcnow', '(', ')', 'self', '.', 'log', '.', 'info', '(', '""Queuing into pool %s""', ',', 'self', '.', 'pool', ')', 'session', '.', 'merge', '(', 'self', ')', 'session', '.', 'commit', '(', ')', 'return', 'False', '# Another worker might have started running this task instance while', '# the current worker process was blocked on refresh_from_db', 'if', 'self', '.', 'state', '==', 'State', '.', 'RUNNING', ':', 'self', '.', 'log', '.', 'warning', '(', '""Task Instance already running %s""', ',', 'self', ')', 'session', '.', 'commit', '(', ')', 'return', 'False', '# print status message', 'self', '.', 'log', '.', 'info', '(', 'hr', ')', 'self', '.', 'log', '.', 'info', '(', '""Starting attempt %s of %s""', ',', 'self', '.', 'try_number', ',', 'self', '.', 'max_tries', '+', '1', ')', 'self', '.', 'log', '.', 'info', '(', 'hr', ')', 'self', '.', '_try_number', '+=', '1', 'if', 'not', 'test_mode', ':', 'session', '.', 'add', '(', 'Log', '(', 'State', '.', 'RUNNING', ',', 'self', ')', ')', 'self', '.', 'state', '=', 'State', '.', 'RUNNING', 'self', '.', 'pid', '=', 'os', '.', 'getpid', '(', ')', 'self', '.', 'end_date', '=', 'None', 'if', 'not', 'test_mode', ':', 'session', '.', 'merge', '(', 'self', ')', 'session', '.', 'commit', '(', ')', '# Closing all pooled connections to prevent', '# ""max number of connections reached""', 'settings', '.', 'engine', '.', 'dispose', '(', ')', 'if', 'verbose', ':', 'if', 'mark_success', ':', 'self', '.', 'log', '.', 'info', '(', '""Marking success for %s on %s""', ',', 'self', '.', 'task', ',', 'self', '.', 'execution_date', ')', 'else', ':', 'self', '.', 'log', '.', 'info', '(', '""Executing %s on %s""', ',', 'self', '.', 'task', ',', 'self', '.', 'execution_date', ')', 'return', 'True']","Checks dependencies and then sets state to RUNNING if they are met. Returns
        True if and only if state is set to RUNNING, which implies that task should be
        executed, in preparation for _run_raw_task

        :param verbose: whether to turn on more verbose logging
        :type verbose: bool
        :param ignore_all_deps: Ignore all of the non-critical dependencies, just runs
        :type ignore_all_deps: bool
        :param ignore_depends_on_past: Ignore depends_on_past DAG attribute
        :type ignore_depends_on_past: bool
        :param ignore_task_deps: Don't check the dependencies of this TI's task
        :type ignore_task_deps: bool
        :param ignore_ti_state: Disregards previous task instance state
        :type ignore_ti_state: bool
        :param mark_success: Don't run the task, mark its state as success
        :type mark_success: bool
        :param test_mode: Doesn't record success or failure in the DB
        :type test_mode: bool
        :param pool: specifies the pool to use to run the task instance
        :type pool: str
        :return: whether the state was changed to running or not
        :rtype: bool","['Checks', 'dependencies', 'and', 'then', 'sets', 'state', 'to', 'RUNNING', 'if', 'they', 'are', 'met', '.', 'Returns', 'True', 'if', 'and', 'only', 'if', 'state', 'is', 'set', 'to', 'RUNNING', 'which', 'implies', 'that', 'task', 'should', 'be', 'executed', 'in', 'preparation', 'for', '_run_raw_task']",python,test,"['checks', 'dependencies', 'and', 'then', 'sets', 'state', 'to', 'running', 'if', 'they', 'are', 'met', '.', 'returns', 'true', 'if', 'and', 'only', 'if', 'state', 'is', 'set', 'to', 'running', 'which', 'implies', 'that', 'task', 'should', 'be', 'executed', 'in', 'preparation', 'for', '_run_raw_task']",checks dependencies and then sets state to running if they are met . returns true if and only if state is set to running which implies that task should be executed in preparation for _run_raw_task,"['def', '_check_and_change_state_before_execution', '(', 'self', ',', 'verbose', '=', 'true', ',', 'ignore_all_deps', '=', 'false', ',', 'ignore_depends_on_past', '=', 'false', ',', 'ignore_task_deps', '=', 'false', ',', 'ignore_ti_state', '=', 'false', ',', 'mark_success', '=', 'false', ',', 'test_mode', '=', 'false', ',', 'job_id', '=', 'none', ',', 'pool', '=', 'none', ',', 'session', '=', 'none', ')', ':', 'task', '=', 'self', '.', 'task', 'self', '.', 'pool', '=', 'pool', 'or', 'task', '.', 'pool', 'self', '.', 'test_mode', '=', 'test_mode', 'self', '.', 'refresh_from_db', '(', 'session', '=', 'session', ',', 'lock_for_update', '=', 'true', ')', 'self', '.', 'job_id', '=', 'job_id', 'self', '.', 'hostname', '=', 'get_hostname', '(', ')', 'self', '.', 'operator', '=', 'task', '.', '__class__', '.', '__name__', 'if', 'not', 'ignore_all_deps', 'and', 'not', 'ignore_ti_state', 'and', 'self', '.', 'state', '==', 'state', '.', 'success', ':', 'stats', '.', 'incr', '(', ""'previously_succeeded'"", ',', '1', ',', '1', ')', 'queue_dep_context', '=', 'depcontext', '(', 'deps', '=', 'queue_deps', ',', 'ignore_all_deps', '=', 'ignore_all_deps', ',', 'ignore_ti_state', '=', 'ignore_ti_state', ',', 'ignore_depends_on_past', '=', 'ignore_depends_on_past', ',', 'ignore_task_deps', '=', 'ignore_task_deps', ')', 'if', 'not', 'self', '.', 'are_dependencies_met', '(', 'dep_context', '=', 'queue_dep_context', ',', 'session', '=', 'session', ',', 'verbose', '=', 'true', ')', ':', 'session', '.', 'commit', '(', ')', 'return', 'false', '# todo: logging needs cleanup, not clear what is being printed', 'hr', '=', '""\\n""', '+', '(', '""-""', '*', '80', ')', '# line break', '# for reporting purposes, we report based on 1-indexed,', '# not 0-indexed lists (i.e. attempt 1 instead of', '# attempt 0 for the first attempt).', '# set the task start date. in case it was re-scheduled use the initial', '# start date that is recorded in task_reschedule table', 'self', '.', 'start_date', '=', 'timezone', '.', 'utcnow', '(', ')', 'task_reschedules', '=', 'taskreschedule', '.', 'find_for_task_instance', '(', 'self', ',', 'session', ')', 'if', 'task_reschedules', ':', 'self', '.', 'start_date', '=', 'task_reschedules', '[', '0', ']', '.', 'start_date', 'dep_context', '=', 'depcontext', '(', 'deps', '=', 'run_deps', '-', 'queue_deps', ',', 'ignore_all_deps', '=', 'ignore_all_deps', ',', 'ignore_depends_on_past', '=', 'ignore_depends_on_past', ',', 'ignore_task_deps', '=', 'ignore_task_deps', ',', 'ignore_ti_state', '=', 'ignore_ti_state', ')', 'runnable', '=', 'self', '.', 'are_dependencies_met', '(', 'dep_context', '=', 'dep_context', ',', 'session', '=', 'session', ',', 'verbose', '=', 'true', ')', 'if', 'not', 'runnable', 'and', 'not', 'mark_success', ':', '# fixme: we might have hit concurrency limits, which means we probably', '# have been running prematurely. this should be handled in the', '# scheduling mechanism.', 'self', '.', 'state', '=', 'state', '.', 'none', 'self', '.', 'log', '.', 'warning', '(', 'hr', ')', 'self', '.', 'log', '.', 'warning', '(', '""fixme: rescheduling due to concurrency limits reached at task runtime. attempt %s of ""', '""%s. state set to none.""', ',', 'self', '.', 'try_number', ',', 'self', '.', 'max_tries', '+', '1', ')', 'self', '.', 'log', '.', 'warning', '(', 'hr', ')', 'self', '.', 'queued_dttm', '=', 'timezone', '.', 'utcnow', '(', ')', 'self', '.', 'log', '.', 'info', '(', '""queuing into pool %s""', ',', 'self', '.', 'pool', ')', 'session', '.', 'merge', '(', 'self', ')', 'session', '.', 'commit', '(', ')', 'return', 'false', '# another worker might have started running this task instance while', '# the current worker process was blocked on refresh_from_db', 'if', 'self', '.', 'state', '==', 'state', '.', 'running', ':', 'self', '.', 'log', '.', 'warning', '(', '""task instance already running %s""', ',', 'self', ')', 'session', '.', 'commit', '(', ')', 'return', 'false', '# print status message', 'self', '.', 'log', '.', 'info', '(', 'hr', ')', 'self', '.', 'log', '.', 'info', '(', '""starting attempt %s of %s""', ',', 'self', '.', 'try_number', ',', 'self', '.', 'max_tries', '+', '1', ')', 'self', '.', 'log', '.', 'info', '(', 'hr', ')', 'self', '.', '_try_number', '+=', '1', 'if', 'not', 'test_mode', ':', 'session', '.', 'add', '(', 'log', '(', 'state', '.', 'running', ',', 'self', ')', ')', 'self', '.', 'state', '=', 'state', '.', 'running', 'self', '.', 'pid', '=', 'os', '.', 'getpid', '(', ')', 'self', '.', 'end_date', '=', 'none', 'if', 'not', 'test_mode', ':', 'session', '.', 'merge', '(', 'self', ')', 'session', '.', 'commit', '(', ')', '# closing all pooled connections to prevent', '# ""max number of connections reached""', 'settings', '.', 'engine', '.', 'dispose', '(', ')', 'if', 'verbose', ':', 'if', 'mark_success', ':', 'self', '.', 'log', '.', 'info', '(', '""marking success for %s on %s""', ',', 'self', '.', 'task', ',', 'self', '.', 'execution_date', ')', 'else', ':', 'self', '.', 'log', '.', 'info', '(', '""executing %s on %s""', ',', 'self', '.', 'task', ',', 'self', '.', 'execution_date', ')', 'return', 'true']","def _check_and_change_state_before_execution ( self , verbose = true , ignore_all_deps = false , ignore_depends_on_past = false , ignore_task_deps = false , ignore_ti_state = false , mark_success = false , test_mode = false , job_id = none , pool = none , session = none ) : task = self . task self . pool = pool or task . pool self . test_mode = test_mode self . refresh_from_db ( session = session , lock_for_update = true ) self . job_id = job_id self . hostname = get_hostname ( ) self . operator = task . __class__ . __name__ if not ignore_all_deps and not ignore_ti_state and self . state == state . success : stats . incr ( 'previously_succeeded' , 1 , 1 ) queue_dep_context = depcontext ( deps = queue_deps , ignore_all_deps = ignore_all_deps , ignore_ti_state = ignore_ti_state , ignore_depends_on_past = ignore_depends_on_past , ignore_task_deps = ignore_task_deps ) if not self . are_dependencies_met ( dep_context = queue_dep_context , session = session , verbose = true ) : session . commit ( ) return false # todo: logging needs cleanup, not clear what is being printed hr = ""\n"" + ( ""-"" * 80 ) # line break # for reporting purposes, we report based on 1-indexed, # not 0-indexed lists (i.e. attempt 1 instead of # attempt 0 for the first attempt). # set the task start date. in case it was re-scheduled use the initial # start date that is recorded in task_reschedule table self . start_date = timezone . utcnow ( ) task_reschedules = taskreschedule . find_for_task_instance ( self , session ) if task_reschedules : self . start_date = task_reschedules [ 0 ] . start_date dep_context = depcontext ( deps = run_deps - queue_deps , ignore_all_deps = ignore_all_deps , ignore_depends_on_past = ignore_depends_on_past , ignore_task_deps = ignore_task_deps , ignore_ti_state = ignore_ti_state ) runnable = self . are_dependencies_met ( dep_context = dep_context , session = session , verbose = true ) if not runnable and not mark_success : # fixme: we might have hit concurrency limits, which means we probably # have been running prematurely. this should be handled in the # scheduling mechanism. self . state = state . none self . log . warning ( hr ) self . log . warning ( ""fixme: rescheduling due to concurrency limits reached at task runtime. attempt %s of "" ""%s. state set to none."" , self . try_number , self . max_tries + 1 ) self . log . warning ( hr ) self . queued_dttm = timezone . utcnow ( ) self . log . info ( ""queuing into pool %s"" , self . pool ) session . merge ( self ) session . commit ( ) return false # another worker might have started running this task instance while # the current worker process was blocked on refresh_from_db if self . state == state . running : self . log . warning ( ""task instance already running %s"" , self ) session . commit ( ) return false # print status message self . log . info ( hr ) self . log . info ( ""starting attempt %s of %s"" , self . try_number , self . max_tries + 1 ) self . log . info ( hr ) self . _try_number += 1 if not test_mode : session . add ( log ( state . running , self ) ) self . state = state . running self . pid = os . getpid ( ) self . end_date = none if not test_mode : session . merge ( self ) session . commit ( ) # closing all pooled connections to prevent # ""max number of connections reached"" settings . engine . dispose ( ) if verbose : if mark_success : self . log . info ( ""marking success for %s on %s"" , self . task , self . execution_date ) else : self . log . info ( ""executing %s on %s"" , self . task , self . execution_date ) return true"
359,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L825-L943,"def _run_raw_task(
            self,
            mark_success=False,
            test_mode=False,
            job_id=None,
            pool=None,
            session=None):
        """"""
        Immediately runs the task (without checking or changing db state
        before execution) and then sets the appropriate final state after
        completion and runs any post-execute callbacks. Meant to be called
        only after another function changes the state to running.

        :param mark_success: Don't run the task, mark its state as success
        :type mark_success: bool
        :param test_mode: Doesn't record success or failure in the DB
        :type test_mode: bool
        :param pool: specifies the pool to use to run the task instance
        :type pool: str
        """"""
        task = self.task
        self.pool = pool or task.pool
        self.test_mode = test_mode
        self.refresh_from_db(session=session)
        self.job_id = job_id
        self.hostname = get_hostname()
        self.operator = task.__class__.__name__

        context = {}
        actual_start_date = timezone.utcnow()
        try:
            if not mark_success:
                context = self.get_template_context()

                task_copy = copy.copy(task)
                self.task = task_copy

                def signal_handler(signum, frame):
                    self.log.error(""Received SIGTERM. Terminating subprocesses."")
                    task_copy.on_kill()
                    raise AirflowException(""Task received SIGTERM signal"")
                signal.signal(signal.SIGTERM, signal_handler)

                # Don't clear Xcom until the task is certain to execute
                self.clear_xcom_data()

                start_time = time.time()

                self.render_templates()
                task_copy.pre_execute(context=context)

                # If a timeout is specified for the task, make it fail
                # if it goes beyond
                result = None
                if task_copy.execution_timeout:
                    try:
                        with timeout(int(
                                task_copy.execution_timeout.total_seconds())):
                            result = task_copy.execute(context=context)
                    except AirflowTaskTimeout:
                        task_copy.on_kill()
                        raise
                else:
                    result = task_copy.execute(context=context)

                # If the task returns a result, push an XCom containing it
                if task_copy.do_xcom_push and result is not None:
                    self.xcom_push(key=XCOM_RETURN_KEY, value=result)

                task_copy.post_execute(context=context, result=result)

                end_time = time.time()
                duration = end_time - start_time
                Stats.timing(
                    'dag.{dag_id}.{task_id}.duration'.format(
                        dag_id=task_copy.dag_id,
                        task_id=task_copy.task_id),
                    duration)

                Stats.incr('operator_successes_{}'.format(
                    self.task.__class__.__name__), 1, 1)
                Stats.incr('ti_successes')
            self.refresh_from_db(lock_for_update=True)
            self.state = State.SUCCESS
        except AirflowSkipException:
            self.refresh_from_db(lock_for_update=True)
            self.state = State.SKIPPED
        except AirflowRescheduleException as reschedule_exception:
            self.refresh_from_db()
            self._handle_reschedule(actual_start_date, reschedule_exception, test_mode, context)
            return
        except AirflowException as e:
            self.refresh_from_db()
            # for case when task is marked as success/failed externally
            # current behavior doesn't hit the success callback
            if self.state in {State.SUCCESS, State.FAILED}:
                return
            else:
                self.handle_failure(e, test_mode, context)
                raise
        except (Exception, KeyboardInterrupt) as e:
            self.handle_failure(e, test_mode, context)
            raise

        # Success callback
        try:
            if task.on_success_callback:
                task.on_success_callback(context)
        except Exception as e3:
            self.log.error(""Failed when executing success callback"")
            self.log.exception(e3)

        # Recording SUCCESS
        self.end_date = timezone.utcnow()
        self.set_duration()
        if not test_mode:
            session.add(Log(self.state, self))
            session.merge(self)
        session.commit()","['def', '_run_raw_task', '(', 'self', ',', 'mark_success', '=', 'False', ',', 'test_mode', '=', 'False', ',', 'job_id', '=', 'None', ',', 'pool', '=', 'None', ',', 'session', '=', 'None', ')', ':', 'task', '=', 'self', '.', 'task', 'self', '.', 'pool', '=', 'pool', 'or', 'task', '.', 'pool', 'self', '.', 'test_mode', '=', 'test_mode', 'self', '.', 'refresh_from_db', '(', 'session', '=', 'session', ')', 'self', '.', 'job_id', '=', 'job_id', 'self', '.', 'hostname', '=', 'get_hostname', '(', ')', 'self', '.', 'operator', '=', 'task', '.', '__class__', '.', '__name__', 'context', '=', '{', '}', 'actual_start_date', '=', 'timezone', '.', 'utcnow', '(', ')', 'try', ':', 'if', 'not', 'mark_success', ':', 'context', '=', 'self', '.', 'get_template_context', '(', ')', 'task_copy', '=', 'copy', '.', 'copy', '(', 'task', ')', 'self', '.', 'task', '=', 'task_copy', 'def', 'signal_handler', '(', 'signum', ',', 'frame', ')', ':', 'self', '.', 'log', '.', 'error', '(', '""Received SIGTERM. Terminating subprocesses.""', ')', 'task_copy', '.', 'on_kill', '(', ')', 'raise', 'AirflowException', '(', '""Task received SIGTERM signal""', ')', 'signal', '.', 'signal', '(', 'signal', '.', 'SIGTERM', ',', 'signal_handler', ')', ""# Don't clear Xcom until the task is certain to execute"", 'self', '.', 'clear_xcom_data', '(', ')', 'start_time', '=', 'time', '.', 'time', '(', ')', 'self', '.', 'render_templates', '(', ')', 'task_copy', '.', 'pre_execute', '(', 'context', '=', 'context', ')', '# If a timeout is specified for the task, make it fail', '# if it goes beyond', 'result', '=', 'None', 'if', 'task_copy', '.', 'execution_timeout', ':', 'try', ':', 'with', 'timeout', '(', 'int', '(', 'task_copy', '.', 'execution_timeout', '.', 'total_seconds', '(', ')', ')', ')', ':', 'result', '=', 'task_copy', '.', 'execute', '(', 'context', '=', 'context', ')', 'except', 'AirflowTaskTimeout', ':', 'task_copy', '.', 'on_kill', '(', ')', 'raise', 'else', ':', 'result', '=', 'task_copy', '.', 'execute', '(', 'context', '=', 'context', ')', '# If the task returns a result, push an XCom containing it', 'if', 'task_copy', '.', 'do_xcom_push', 'and', 'result', 'is', 'not', 'None', ':', 'self', '.', 'xcom_push', '(', 'key', '=', 'XCOM_RETURN_KEY', ',', 'value', '=', 'result', ')', 'task_copy', '.', 'post_execute', '(', 'context', '=', 'context', ',', 'result', '=', 'result', ')', 'end_time', '=', 'time', '.', 'time', '(', ')', 'duration', '=', 'end_time', '-', 'start_time', 'Stats', '.', 'timing', '(', ""'dag.{dag_id}.{task_id}.duration'"", '.', 'format', '(', 'dag_id', '=', 'task_copy', '.', 'dag_id', ',', 'task_id', '=', 'task_copy', '.', 'task_id', ')', ',', 'duration', ')', 'Stats', '.', 'incr', '(', ""'operator_successes_{}'"", '.', 'format', '(', 'self', '.', 'task', '.', '__class__', '.', '__name__', ')', ',', '1', ',', '1', ')', 'Stats', '.', 'incr', '(', ""'ti_successes'"", ')', 'self', '.', 'refresh_from_db', '(', 'lock_for_update', '=', 'True', ')', 'self', '.', 'state', '=', 'State', '.', 'SUCCESS', 'except', 'AirflowSkipException', ':', 'self', '.', 'refresh_from_db', '(', 'lock_for_update', '=', 'True', ')', 'self', '.', 'state', '=', 'State', '.', 'SKIPPED', 'except', 'AirflowRescheduleException', 'as', 'reschedule_exception', ':', 'self', '.', 'refresh_from_db', '(', ')', 'self', '.', '_handle_reschedule', '(', 'actual_start_date', ',', 'reschedule_exception', ',', 'test_mode', ',', 'context', ')', 'return', 'except', 'AirflowException', 'as', 'e', ':', 'self', '.', 'refresh_from_db', '(', ')', '# for case when task is marked as success/failed externally', ""# current behavior doesn't hit the success callback"", 'if', 'self', '.', 'state', 'in', '{', 'State', '.', 'SUCCESS', ',', 'State', '.', 'FAILED', '}', ':', 'return', 'else', ':', 'self', '.', 'handle_failure', '(', 'e', ',', 'test_mode', ',', 'context', ')', 'raise', 'except', '(', 'Exception', ',', 'KeyboardInterrupt', ')', 'as', 'e', ':', 'self', '.', 'handle_failure', '(', 'e', ',', 'test_mode', ',', 'context', ')', 'raise', '# Success callback', 'try', ':', 'if', 'task', '.', 'on_success_callback', ':', 'task', '.', 'on_success_callback', '(', 'context', ')', 'except', 'Exception', 'as', 'e3', ':', 'self', '.', 'log', '.', 'error', '(', '""Failed when executing success callback""', ')', 'self', '.', 'log', '.', 'exception', '(', 'e3', ')', '# Recording SUCCESS', 'self', '.', 'end_date', '=', 'timezone', '.', 'utcnow', '(', ')', 'self', '.', 'set_duration', '(', ')', 'if', 'not', 'test_mode', ':', 'session', '.', 'add', '(', 'Log', '(', 'self', '.', 'state', ',', 'self', ')', ')', 'session', '.', 'merge', '(', 'self', ')', 'session', '.', 'commit', '(', ')']","Immediately runs the task (without checking or changing db state
        before execution) and then sets the appropriate final state after
        completion and runs any post-execute callbacks. Meant to be called
        only after another function changes the state to running.

        :param mark_success: Don't run the task, mark its state as success
        :type mark_success: bool
        :param test_mode: Doesn't record success or failure in the DB
        :type test_mode: bool
        :param pool: specifies the pool to use to run the task instance
        :type pool: str","['Immediately', 'runs', 'the', 'task', '(', 'without', 'checking', 'or', 'changing', 'db', 'state', 'before', 'execution', ')', 'and', 'then', 'sets', 'the', 'appropriate', 'final', 'state', 'after', 'completion', 'and', 'runs', 'any', 'post', '-', 'execute', 'callbacks', '.', 'Meant', 'to', 'be', 'called', 'only', 'after', 'another', 'function', 'changes', 'the', 'state', 'to', 'running', '.']",python,test,"['immediately', 'runs', 'the', 'task', '(', 'without', 'checking', 'or', 'changing', 'db', 'state', 'before', 'execution', ')', 'and', 'then', 'sets', 'the', 'appropriate', 'final', 'state', 'after', 'completion', 'and', 'runs', 'any', 'post', '-', 'execute', 'callbacks', '.', 'meant', 'to', 'be', 'called', 'only', 'after', 'another', 'function', 'changes', 'the', 'state', 'to', 'running', '.']",immediately runs the task ( without checking or changing db state before execution ) and then sets the appropriate final state after completion and runs any post - execute callbacks . meant to be called only after another function changes the state to running .,"['def', '_run_raw_task', '(', 'self', ',', 'mark_success', '=', 'false', ',', 'test_mode', '=', 'false', ',', 'job_id', '=', 'none', ',', 'pool', '=', 'none', ',', 'session', '=', 'none', ')', ':', 'task', '=', 'self', '.', 'task', 'self', '.', 'pool', '=', 'pool', 'or', 'task', '.', 'pool', 'self', '.', 'test_mode', '=', 'test_mode', 'self', '.', 'refresh_from_db', '(', 'session', '=', 'session', ')', 'self', '.', 'job_id', '=', 'job_id', 'self', '.', 'hostname', '=', 'get_hostname', '(', ')', 'self', '.', 'operator', '=', 'task', '.', '__class__', '.', '__name__', 'context', '=', '{', '}', 'actual_start_date', '=', 'timezone', '.', 'utcnow', '(', ')', 'try', ':', 'if', 'not', 'mark_success', ':', 'context', '=', 'self', '.', 'get_template_context', '(', ')', 'task_copy', '=', 'copy', '.', 'copy', '(', 'task', ')', 'self', '.', 'task', '=', 'task_copy', 'def', 'signal_handler', '(', 'signum', ',', 'frame', ')', ':', 'self', '.', 'log', '.', 'error', '(', '""received sigterm. terminating subprocesses.""', ')', 'task_copy', '.', 'on_kill', '(', ')', 'raise', 'airflowexception', '(', '""task received sigterm signal""', ')', 'signal', '.', 'signal', '(', 'signal', '.', 'sigterm', ',', 'signal_handler', ')', ""# don't clear xcom until the task is certain to execute"", 'self', '.', 'clear_xcom_data', '(', ')', 'start_time', '=', 'time', '.', 'time', '(', ')', 'self', '.', 'render_templates', '(', ')', 'task_copy', '.', 'pre_execute', '(', 'context', '=', 'context', ')', '# if a timeout is specified for the task, make it fail', '# if it goes beyond', 'result', '=', 'none', 'if', 'task_copy', '.', 'execution_timeout', ':', 'try', ':', 'with', 'timeout', '(', 'int', '(', 'task_copy', '.', 'execution_timeout', '.', 'total_seconds', '(', ')', ')', ')', ':', 'result', '=', 'task_copy', '.', 'execute', '(', 'context', '=', 'context', ')', 'except', 'airflowtasktimeout', ':', 'task_copy', '.', 'on_kill', '(', ')', 'raise', 'else', ':', 'result', '=', 'task_copy', '.', 'execute', '(', 'context', '=', 'context', ')', '# if the task returns a result, push an xcom containing it', 'if', 'task_copy', '.', 'do_xcom_push', 'and', 'result', 'is', 'not', 'none', ':', 'self', '.', 'xcom_push', '(', 'key', '=', 'xcom_return_key', ',', 'value', '=', 'result', ')', 'task_copy', '.', 'post_execute', '(', 'context', '=', 'context', ',', 'result', '=', 'result', ')', 'end_time', '=', 'time', '.', 'time', '(', ')', 'duration', '=', 'end_time', '-', 'start_time', 'stats', '.', 'timing', '(', ""'dag.{dag_id}.{task_id}.duration'"", '.', 'format', '(', 'dag_id', '=', 'task_copy', '.', 'dag_id', ',', 'task_id', '=', 'task_copy', '.', 'task_id', ')', ',', 'duration', ')', 'stats', '.', 'incr', '(', ""'operator_successes_{}'"", '.', 'format', '(', 'self', '.', 'task', '.', '__class__', '.', '__name__', ')', ',', '1', ',', '1', ')', 'stats', '.', 'incr', '(', ""'ti_successes'"", ')', 'self', '.', 'refresh_from_db', '(', 'lock_for_update', '=', 'true', ')', 'self', '.', 'state', '=', 'state', '.', 'success', 'except', 'airflowskipexception', ':', 'self', '.', 'refresh_from_db', '(', 'lock_for_update', '=', 'true', ')', 'self', '.', 'state', '=', 'state', '.', 'skipped', 'except', 'airflowrescheduleexception', 'as', 'reschedule_exception', ':', 'self', '.', 'refresh_from_db', '(', ')', 'self', '.', '_handle_reschedule', '(', 'actual_start_date', ',', 'reschedule_exception', ',', 'test_mode', ',', 'context', ')', 'return', 'except', 'airflowexception', 'as', 'e', ':', 'self', '.', 'refresh_from_db', '(', ')', '# for case when task is marked as success/failed externally', ""# current behavior doesn't hit the success callback"", 'if', 'self', '.', 'state', 'in', '{', 'state', '.', 'success', ',', 'state', '.', 'failed', '}', ':', 'return', 'else', ':', 'self', '.', 'handle_failure', '(', 'e', ',', 'test_mode', ',', 'context', ')', 'raise', 'except', '(', 'exception', ',', 'keyboardinterrupt', ')', 'as', 'e', ':', 'self', '.', 'handle_failure', '(', 'e', ',', 'test_mode', ',', 'context', ')', 'raise', '# success callback', 'try', ':', 'if', 'task', '.', 'on_success_callback', ':', 'task', '.', 'on_success_callback', '(', 'context', ')', 'except', 'exception', 'as', 'e3', ':', 'self', '.', 'log', '.', 'error', '(', '""failed when executing success callback""', ')', 'self', '.', 'log', '.', 'exception', '(', 'e3', ')', '# recording success', 'self', '.', 'end_date', '=', 'timezone', '.', 'utcnow', '(', ')', 'self', '.', 'set_duration', '(', ')', 'if', 'not', 'test_mode', ':', 'session', '.', 'add', '(', 'log', '(', 'self', '.', 'state', ',', 'self', ')', ')', 'session', '.', 'merge', '(', 'self', ')', 'session', '.', 'commit', '(', ')']","def _run_raw_task ( self , mark_success = false , test_mode = false , job_id = none , pool = none , session = none ) : task = self . task self . pool = pool or task . pool self . test_mode = test_mode self . refresh_from_db ( session = session ) self . job_id = job_id self . hostname = get_hostname ( ) self . operator = task . __class__ . __name__ context = { } actual_start_date = timezone . utcnow ( ) try : if not mark_success : context = self . get_template_context ( ) task_copy = copy . copy ( task ) self . task = task_copy def signal_handler ( signum , frame ) : self . log . error ( ""received sigterm. terminating subprocesses."" ) task_copy . on_kill ( ) raise airflowexception ( ""task received sigterm signal"" ) signal . signal ( signal . sigterm , signal_handler ) # don't clear xcom until the task is certain to execute self . clear_xcom_data ( ) start_time = time . time ( ) self . render_templates ( ) task_copy . pre_execute ( context = context ) # if a timeout is specified for the task, make it fail # if it goes beyond result = none if task_copy . execution_timeout : try : with timeout ( int ( task_copy . execution_timeout . total_seconds ( ) ) ) : result = task_copy . execute ( context = context ) except airflowtasktimeout : task_copy . on_kill ( ) raise else : result = task_copy . execute ( context = context ) # if the task returns a result, push an xcom containing it if task_copy . do_xcom_push and result is not none : self . xcom_push ( key = xcom_return_key , value = result ) task_copy . post_execute ( context = context , result = result ) end_time = time . time ( ) duration = end_time - start_time stats . timing ( 'dag.{dag_id}.{task_id}.duration' . format ( dag_id = task_copy . dag_id , task_id = task_copy . task_id ) , duration ) stats . incr ( 'operator_successes_{}' . format ( self . task . __class__ . __name__ ) , 1 , 1 ) stats . incr ( 'ti_successes' ) self . refresh_from_db ( lock_for_update = true ) self . state = state . success except airflowskipexception : self . refresh_from_db ( lock_for_update = true ) self . state = state . skipped except airflowrescheduleexception as reschedule_exception : self . refresh_from_db ( ) self . _handle_reschedule ( actual_start_date , reschedule_exception , test_mode , context ) return except airflowexception as e : self . refresh_from_db ( ) # for case when task is marked as success/failed externally # current behavior doesn't hit the success callback if self . state in { state . success , state . failed } : return else : self . handle_failure ( e , test_mode , context ) raise except ( exception , keyboardinterrupt ) as e : self . handle_failure ( e , test_mode , context ) raise # success callback try : if task . on_success_callback : task . on_success_callback ( context ) except exception as e3 : self . log . error ( ""failed when executing success callback"" ) self . log . exception ( e3 ) # recording success self . end_date = timezone . utcnow ( ) self . set_duration ( ) if not test_mode : session . add ( log ( self . state , self ) ) session . merge ( self ) session . commit ( )"
360,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L1271-L1301,"def xcom_push(
            self,
            key,
            value,
            execution_date=None):
        """"""
        Make an XCom available for tasks to pull.

        :param key: A key for the XCom
        :type key: str
        :param value: A value for the XCom. The value is pickled and stored
            in the database.
        :type value: any pickleable object
        :param execution_date: if provided, the XCom will not be visible until
            this date. This can be used, for example, to send a message to a
            task on a future date without it being immediately visible.
        :type execution_date: datetime
        """"""

        if execution_date and execution_date < self.execution_date:
            raise ValueError(
                'execution_date can not be in the past (current '
                'execution_date is {}; received {})'.format(
                    self.execution_date, execution_date))

        XCom.set(
            key=key,
            value=value,
            task_id=self.task_id,
            dag_id=self.dag_id,
            execution_date=execution_date or self.execution_date)","['def', 'xcom_push', '(', 'self', ',', 'key', ',', 'value', ',', 'execution_date', '=', 'None', ')', ':', 'if', 'execution_date', 'and', 'execution_date', '<', 'self', '.', 'execution_date', ':', 'raise', 'ValueError', '(', ""'execution_date can not be in the past (current '"", ""'execution_date is {}; received {})'"", '.', 'format', '(', 'self', '.', 'execution_date', ',', 'execution_date', ')', ')', 'XCom', '.', 'set', '(', 'key', '=', 'key', ',', 'value', '=', 'value', ',', 'task_id', '=', 'self', '.', 'task_id', ',', 'dag_id', '=', 'self', '.', 'dag_id', ',', 'execution_date', '=', 'execution_date', 'or', 'self', '.', 'execution_date', ')']","Make an XCom available for tasks to pull.

        :param key: A key for the XCom
        :type key: str
        :param value: A value for the XCom. The value is pickled and stored
            in the database.
        :type value: any pickleable object
        :param execution_date: if provided, the XCom will not be visible until
            this date. This can be used, for example, to send a message to a
            task on a future date without it being immediately visible.
        :type execution_date: datetime","['Make', 'an', 'XCom', 'available', 'for', 'tasks', 'to', 'pull', '.']",python,test,"['make', 'an', 'xcom', 'available', 'for', 'tasks', 'to', 'pull', '.']",make an xcom available for tasks to pull .,"['def', 'xcom_push', '(', 'self', ',', 'key', ',', 'value', ',', 'execution_date', '=', 'none', ')', ':', 'if', 'execution_date', 'and', 'execution_date', '<', 'self', '.', 'execution_date', ':', 'raise', 'valueerror', '(', ""'execution_date can not be in the past (current '"", ""'execution_date is {}; received {})'"", '.', 'format', '(', 'self', '.', 'execution_date', ',', 'execution_date', ')', ')', 'xcom', '.', 'set', '(', 'key', '=', 'key', ',', 'value', '=', 'value', ',', 'task_id', '=', 'self', '.', 'task_id', ',', 'dag_id', '=', 'self', '.', 'dag_id', ',', 'execution_date', '=', 'execution_date', 'or', 'self', '.', 'execution_date', ')']","def xcom_push ( self , key , value , execution_date = none ) : if execution_date and execution_date < self . execution_date : raise valueerror ( 'execution_date can not be in the past (current ' 'execution_date is {}; received {})' . format ( self . execution_date , execution_date ) ) xcom . set ( key = key , value = value , task_id = self . task_id , dag_id = self . dag_id , execution_date = execution_date or self . execution_date )"
361,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L1303-L1352,"def xcom_pull(
            self,
            task_ids=None,
            dag_id=None,
            key=XCOM_RETURN_KEY,
            include_prior_dates=False):
        """"""
        Pull XComs that optionally meet certain criteria.

        The default value for `key` limits the search to XComs
        that were returned by other tasks (as opposed to those that were pushed
        manually). To remove this filter, pass key=None (or any desired value).

        If a single task_id string is provided, the result is the value of the
        most recent matching XCom from that task_id. If multiple task_ids are
        provided, a tuple of matching values is returned. None is returned
        whenever no matches are found.

        :param key: A key for the XCom. If provided, only XComs with matching
            keys will be returned. The default key is 'return_value', also
            available as a constant XCOM_RETURN_KEY. This key is automatically
            given to XComs returned by tasks (as opposed to being pushed
            manually). To remove the filter, pass key=None.
        :type key: str
        :param task_ids: Only XComs from tasks with matching ids will be
            pulled. Can pass None to remove the filter.
        :type task_ids: str or iterable of strings (representing task_ids)
        :param dag_id: If provided, only pulls XComs from this DAG.
            If None (default), the DAG of the calling task is used.
        :type dag_id: str
        :param include_prior_dates: If False, only XComs from the current
            execution_date are returned. If True, XComs from previous dates
            are returned as well.
        :type include_prior_dates: bool
        """"""

        if dag_id is None:
            dag_id = self.dag_id

        pull_fn = functools.partial(
            XCom.get_one,
            execution_date=self.execution_date,
            key=key,
            dag_id=dag_id,
            include_prior_dates=include_prior_dates)

        if is_container(task_ids):
            return tuple(pull_fn(task_id=t) for t in task_ids)
        else:
            return pull_fn(task_id=task_ids)","['def', 'xcom_pull', '(', 'self', ',', 'task_ids', '=', 'None', ',', 'dag_id', '=', 'None', ',', 'key', '=', 'XCOM_RETURN_KEY', ',', 'include_prior_dates', '=', 'False', ')', ':', 'if', 'dag_id', 'is', 'None', ':', 'dag_id', '=', 'self', '.', 'dag_id', 'pull_fn', '=', 'functools', '.', 'partial', '(', 'XCom', '.', 'get_one', ',', 'execution_date', '=', 'self', '.', 'execution_date', ',', 'key', '=', 'key', ',', 'dag_id', '=', 'dag_id', ',', 'include_prior_dates', '=', 'include_prior_dates', ')', 'if', 'is_container', '(', 'task_ids', ')', ':', 'return', 'tuple', '(', 'pull_fn', '(', 'task_id', '=', 't', ')', 'for', 't', 'in', 'task_ids', ')', 'else', ':', 'return', 'pull_fn', '(', 'task_id', '=', 'task_ids', ')']","Pull XComs that optionally meet certain criteria.

        The default value for `key` limits the search to XComs
        that were returned by other tasks (as opposed to those that were pushed
        manually). To remove this filter, pass key=None (or any desired value).

        If a single task_id string is provided, the result is the value of the
        most recent matching XCom from that task_id. If multiple task_ids are
        provided, a tuple of matching values is returned. None is returned
        whenever no matches are found.

        :param key: A key for the XCom. If provided, only XComs with matching
            keys will be returned. The default key is 'return_value', also
            available as a constant XCOM_RETURN_KEY. This key is automatically
            given to XComs returned by tasks (as opposed to being pushed
            manually). To remove the filter, pass key=None.
        :type key: str
        :param task_ids: Only XComs from tasks with matching ids will be
            pulled. Can pass None to remove the filter.
        :type task_ids: str or iterable of strings (representing task_ids)
        :param dag_id: If provided, only pulls XComs from this DAG.
            If None (default), the DAG of the calling task is used.
        :type dag_id: str
        :param include_prior_dates: If False, only XComs from the current
            execution_date are returned. If True, XComs from previous dates
            are returned as well.
        :type include_prior_dates: bool","['Pull', 'XComs', 'that', 'optionally', 'meet', 'certain', 'criteria', '.']",python,test,"['pull', 'xcoms', 'that', 'optionally', 'meet', 'certain', 'criteria', '.']",pull xcoms that optionally meet certain criteria .,"['def', 'xcom_pull', '(', 'self', ',', 'task_ids', '=', 'none', ',', 'dag_id', '=', 'none', ',', 'key', '=', 'xcom_return_key', ',', 'include_prior_dates', '=', 'false', ')', ':', 'if', 'dag_id', 'is', 'none', ':', 'dag_id', '=', 'self', '.', 'dag_id', 'pull_fn', '=', 'functools', '.', 'partial', '(', 'xcom', '.', 'get_one', ',', 'execution_date', '=', 'self', '.', 'execution_date', ',', 'key', '=', 'key', ',', 'dag_id', '=', 'dag_id', ',', 'include_prior_dates', '=', 'include_prior_dates', ')', 'if', 'is_container', '(', 'task_ids', ')', ':', 'return', 'tuple', '(', 'pull_fn', '(', 'task_id', '=', 't', ')', 'for', 't', 'in', 'task_ids', ')', 'else', ':', 'return', 'pull_fn', '(', 'task_id', '=', 'task_ids', ')']","def xcom_pull ( self , task_ids = none , dag_id = none , key = xcom_return_key , include_prior_dates = false ) : if dag_id is none : dag_id = self . dag_id pull_fn = functools . partial ( xcom . get_one , execution_date = self . execution_date , key = key , dag_id = dag_id , include_prior_dates = include_prior_dates ) if is_container ( task_ids ) : return tuple ( pull_fn ( task_id = t ) for t in task_ids ) else : return pull_fn ( task_id = task_ids )"
362,apache/airflow,airflow/models/taskinstance.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/taskinstance.py#L1363-L1368,"def init_run_context(self, raw=False):
        """"""
        Sets the log context.
        """"""
        self.raw = raw
        self._set_context(self)","['def', 'init_run_context', '(', 'self', ',', 'raw', '=', 'False', ')', ':', 'self', '.', 'raw', '=', 'raw', 'self', '.', '_set_context', '(', 'self', ')']",Sets the log context.,"['Sets', 'the', 'log', 'context', '.']",python,test,"['sets', 'the', 'log', 'context', '.']",sets the log context .,"['def', 'init_run_context', '(', 'self', ',', 'raw', '=', 'false', ')', ':', 'self', '.', 'raw', '=', 'raw', 'self', '.', '_set_context', '(', 'self', ')']","def init_run_context ( self , raw = false ) : self . raw = raw self . _set_context ( self )"
363,apache/airflow,airflow/utils/log/wasb_task_handler.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/wasb_task_handler.py#L68-L95,"def close(self):
        """"""
        Close and upload local log file to remote storage Wasb.
        """"""
        # When application exit, system shuts down all handlers by
        # calling close method. Here we check if logger is already
        # closed to prevent uploading the log to remote storage multiple
        # times when `logging.shutdown` is called.
        if self.closed:
            return

        super().close()

        if not self.upload_on_close:
            return

        local_loc = os.path.join(self.local_base, self.log_relative_path)
        remote_loc = os.path.join(self.remote_base, self.log_relative_path)
        if os.path.exists(local_loc):
            # read log and remove old logs to get just the latest additions
            with open(local_loc, 'r') as logfile:
                log = logfile.read()
            self.wasb_write(log, remote_loc, append=True)

            if self.delete_local_copy:
                shutil.rmtree(os.path.dirname(local_loc))
        # Mark closed so we don't double write if close is called twice
        self.closed = True","['def', 'close', '(', 'self', ')', ':', '# When application exit, system shuts down all handlers by', '# calling close method. Here we check if logger is already', '# closed to prevent uploading the log to remote storage multiple', '# times when `logging.shutdown` is called.', 'if', 'self', '.', 'closed', ':', 'return', 'super', '(', ')', '.', 'close', '(', ')', 'if', 'not', 'self', '.', 'upload_on_close', ':', 'return', 'local_loc', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'local_base', ',', 'self', '.', 'log_relative_path', ')', 'remote_loc', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'remote_base', ',', 'self', '.', 'log_relative_path', ')', 'if', 'os', '.', 'path', '.', 'exists', '(', 'local_loc', ')', ':', '# read log and remove old logs to get just the latest additions', 'with', 'open', '(', 'local_loc', ',', ""'r'"", ')', 'as', 'logfile', ':', 'log', '=', 'logfile', '.', 'read', '(', ')', 'self', '.', 'wasb_write', '(', 'log', ',', 'remote_loc', ',', 'append', '=', 'True', ')', 'if', 'self', '.', 'delete_local_copy', ':', 'shutil', '.', 'rmtree', '(', 'os', '.', 'path', '.', 'dirname', '(', 'local_loc', ')', ')', ""# Mark closed so we don't double write if close is called twice"", 'self', '.', 'closed', '=', 'True']",Close and upload local log file to remote storage Wasb.,"['Close', 'and', 'upload', 'local', 'log', 'file', 'to', 'remote', 'storage', 'Wasb', '.']",python,test,"['close', 'and', 'upload', 'local', 'log', 'file', 'to', 'remote', 'storage', 'wasb', '.']",close and upload local log file to remote storage wasb .,"['def', 'close', '(', 'self', ')', ':', '# when application exit, system shuts down all handlers by', '# calling close method. here we check if logger is already', '# closed to prevent uploading the log to remote storage multiple', '# times when `logging.shutdown` is called.', 'if', 'self', '.', 'closed', ':', 'return', 'super', '(', ')', '.', 'close', '(', ')', 'if', 'not', 'self', '.', 'upload_on_close', ':', 'return', 'local_loc', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'local_base', ',', 'self', '.', 'log_relative_path', ')', 'remote_loc', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'remote_base', ',', 'self', '.', 'log_relative_path', ')', 'if', 'os', '.', 'path', '.', 'exists', '(', 'local_loc', ')', ':', '# read log and remove old logs to get just the latest additions', 'with', 'open', '(', 'local_loc', ',', ""'r'"", ')', 'as', 'logfile', ':', 'log', '=', 'logfile', '.', 'read', '(', ')', 'self', '.', 'wasb_write', '(', 'log', ',', 'remote_loc', ',', 'append', '=', 'true', ')', 'if', 'self', '.', 'delete_local_copy', ':', 'shutil', '.', 'rmtree', '(', 'os', '.', 'path', '.', 'dirname', '(', 'local_loc', ')', ')', ""# mark closed so we don't double write if close is called twice"", 'self', '.', 'closed', '=', 'true']","def close ( self ) : # when application exit, system shuts down all handlers by # calling close method. here we check if logger is already # closed to prevent uploading the log to remote storage multiple # times when `logging.shutdown` is called. if self . closed : return super ( ) . close ( ) if not self . upload_on_close : return local_loc = os . path . join ( self . local_base , self . log_relative_path ) remote_loc = os . path . join ( self . remote_base , self . log_relative_path ) if os . path . exists ( local_loc ) : # read log and remove old logs to get just the latest additions with open ( local_loc , 'r' ) as logfile : log = logfile . read ( ) self . wasb_write ( log , remote_loc , append = true ) if self . delete_local_copy : shutil . rmtree ( os . path . dirname ( local_loc ) ) # mark closed so we don't double write if close is called twice self . closed = true"
364,apache/airflow,airflow/utils/log/wasb_task_handler.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/wasb_task_handler.py#L97-L121,"def _read(self, ti, try_number, metadata=None):
        """"""
        Read logs of given task instance and try_number from Wasb remote storage.
        If failed, read the log from task instance host machine.
        :param ti: task instance object
        :param try_number: task instance try_number to read logs from
        :param metadata: log metadata,
                         can be used for steaming log reading and auto-tailing.
        """"""
        # Explicitly getting log relative path is necessary as the given
        # task instance might be different than task instance passed in
        # in set_context method.
        log_relative_path = self._render_filename(ti, try_number)
        remote_loc = os.path.join(self.remote_base, log_relative_path)

        if self.wasb_log_exists(remote_loc):
            # If Wasb remote file exists, we do not fetch logs from task instance
            # local machine even if there are errors reading remote logs, as
            # returned remote_log will contain error messages.
            remote_log = self.wasb_read(remote_loc, return_error=True)
            log = '*** Reading remote log from {}.\n{}\n'.format(
                remote_loc, remote_log)
            return log, {'end_of_log': True}
        else:
            return super()._read(ti, try_number)","['def', '_read', '(', 'self', ',', 'ti', ',', 'try_number', ',', 'metadata', '=', 'None', ')', ':', '# Explicitly getting log relative path is necessary as the given', '# task instance might be different than task instance passed in', '# in set_context method.', 'log_relative_path', '=', 'self', '.', '_render_filename', '(', 'ti', ',', 'try_number', ')', 'remote_loc', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'remote_base', ',', 'log_relative_path', ')', 'if', 'self', '.', 'wasb_log_exists', '(', 'remote_loc', ')', ':', '# If Wasb remote file exists, we do not fetch logs from task instance', '# local machine even if there are errors reading remote logs, as', '# returned remote_log will contain error messages.', 'remote_log', '=', 'self', '.', 'wasb_read', '(', 'remote_loc', ',', 'return_error', '=', 'True', ')', 'log', '=', ""'*** Reading remote log from {}.\\n{}\\n'"", '.', 'format', '(', 'remote_loc', ',', 'remote_log', ')', 'return', 'log', ',', '{', ""'end_of_log'"", ':', 'True', '}', 'else', ':', 'return', 'super', '(', ')', '.', '_read', '(', 'ti', ',', 'try_number', ')']","Read logs of given task instance and try_number from Wasb remote storage.
        If failed, read the log from task instance host machine.
        :param ti: task instance object
        :param try_number: task instance try_number to read logs from
        :param metadata: log metadata,
                         can be used for steaming log reading and auto-tailing.","['Read', 'logs', 'of', 'given', 'task', 'instance', 'and', 'try_number', 'from', 'Wasb', 'remote', 'storage', '.', 'If', 'failed', 'read', 'the', 'log', 'from', 'task', 'instance', 'host', 'machine', '.', ':', 'param', 'ti', ':', 'task', 'instance', 'object', ':', 'param', 'try_number', ':', 'task', 'instance', 'try_number', 'to', 'read', 'logs', 'from', ':', 'param', 'metadata', ':', 'log', 'metadata', 'can', 'be', 'used', 'for', 'steaming', 'log', 'reading', 'and', 'auto', '-', 'tailing', '.']",python,test,"['read', 'logs', 'of', 'given', 'task', 'instance', 'and', 'try_number', 'from', 'wasb', 'remote', 'storage', '.', 'if', 'failed', 'read', 'the', 'log', 'from', 'task', 'instance', 'host', 'machine', '.', ':', 'param', 'ti', ':', 'task', 'instance', 'object', ':', 'param', 'try_number', ':', 'task', 'instance', 'try_number', 'to', 'read', 'logs', 'from', ':', 'param', 'metadata', ':', 'log', 'metadata', 'can', 'be', 'used', 'for', 'steaming', 'log', 'reading', 'and', 'auto', '-', 'tailing', '.']",read logs of given task instance and try_number from wasb remote storage . if failed read the log from task instance host machine . : param ti : task instance object : param try_number : task instance try_number to read logs from : param metadata : log metadata can be used for steaming log reading and auto - tailing .,"['def', '_read', '(', 'self', ',', 'ti', ',', 'try_number', ',', 'metadata', '=', 'none', ')', ':', '# explicitly getting log relative path is necessary as the given', '# task instance might be different than task instance passed in', '# in set_context method.', 'log_relative_path', '=', 'self', '.', '_render_filename', '(', 'ti', ',', 'try_number', ')', 'remote_loc', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'remote_base', ',', 'log_relative_path', ')', 'if', 'self', '.', 'wasb_log_exists', '(', 'remote_loc', ')', ':', '# if wasb remote file exists, we do not fetch logs from task instance', '# local machine even if there are errors reading remote logs, as', '# returned remote_log will contain error messages.', 'remote_log', '=', 'self', '.', 'wasb_read', '(', 'remote_loc', ',', 'return_error', '=', 'true', ')', 'log', '=', ""'*** reading remote log from {}.\\n{}\\n'"", '.', 'format', '(', 'remote_loc', ',', 'remote_log', ')', 'return', 'log', ',', '{', ""'end_of_log'"", ':', 'true', '}', 'else', ':', 'return', 'super', '(', ')', '.', '_read', '(', 'ti', ',', 'try_number', ')']","def _read ( self , ti , try_number , metadata = none ) : # explicitly getting log relative path is necessary as the given # task instance might be different than task instance passed in # in set_context method. log_relative_path = self . _render_filename ( ti , try_number ) remote_loc = os . path . join ( self . remote_base , log_relative_path ) if self . wasb_log_exists ( remote_loc ) : # if wasb remote file exists, we do not fetch logs from task instance # local machine even if there are errors reading remote logs, as # returned remote_log will contain error messages. remote_log = self . wasb_read ( remote_loc , return_error = true ) log = '*** reading remote log from {}.\n{}\n' . format ( remote_loc , remote_log ) return log , { 'end_of_log' : true } else : return super ( ) . _read ( ti , try_number )"
365,apache/airflow,airflow/utils/log/wasb_task_handler.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/wasb_task_handler.py#L123-L133,"def wasb_log_exists(self, remote_log_location):
        """"""
        Check if remote_log_location exists in remote storage
        :param remote_log_location: log's location in remote storage
        :return: True if location exists else False
        """"""
        try:
            return self.hook.check_for_blob(self.wasb_container, remote_log_location)
        except Exception:
            pass
        return False","['def', 'wasb_log_exists', '(', 'self', ',', 'remote_log_location', ')', ':', 'try', ':', 'return', 'self', '.', 'hook', '.', 'check_for_blob', '(', 'self', '.', 'wasb_container', ',', 'remote_log_location', ')', 'except', 'Exception', ':', 'pass', 'return', 'False']","Check if remote_log_location exists in remote storage
        :param remote_log_location: log's location in remote storage
        :return: True if location exists else False","['Check', 'if', 'remote_log_location', 'exists', 'in', 'remote', 'storage', ':', 'param', 'remote_log_location', ':', 'log', 's', 'location', 'in', 'remote', 'storage', ':', 'return', ':', 'True', 'if', 'location', 'exists', 'else', 'False']",python,test,"['check', 'if', 'remote_log_location', 'exists', 'in', 'remote', 'storage', ':', 'param', 'remote_log_location', ':', 'log', 's', 'location', 'in', 'remote', 'storage', ':', 'return', ':', 'true', 'if', 'location', 'exists', 'else', 'false']",check if remote_log_location exists in remote storage : param remote_log_location : log s location in remote storage : return : true if location exists else false,"['def', 'wasb_log_exists', '(', 'self', ',', 'remote_log_location', ')', ':', 'try', ':', 'return', 'self', '.', 'hook', '.', 'check_for_blob', '(', 'self', '.', 'wasb_container', ',', 'remote_log_location', ')', 'except', 'exception', ':', 'pass', 'return', 'false']","def wasb_log_exists ( self , remote_log_location ) : try : return self . hook . check_for_blob ( self . wasb_container , remote_log_location ) except exception : pass return false"
366,apache/airflow,airflow/utils/log/wasb_task_handler.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/wasb_task_handler.py#L135-L152,"def wasb_read(self, remote_log_location, return_error=False):
        """"""
        Returns the log found at the remote_log_location. Returns '' if no
        logs are found or there is an error.
        :param remote_log_location: the log's location in remote storage
        :type remote_log_location: str (path)
        :param return_error: if True, returns a string error message if an
            error occurs. Otherwise returns '' when an error occurs.
        :type return_error: bool
        """"""
        try:
            return self.hook.read_file(self.wasb_container, remote_log_location)
        except AzureHttpError:
            msg = 'Could not read logs from {}'.format(remote_log_location)
            self.log.exception(msg)
            # return error if needed
            if return_error:
                return msg","['def', 'wasb_read', '(', 'self', ',', 'remote_log_location', ',', 'return_error', '=', 'False', ')', ':', 'try', ':', 'return', 'self', '.', 'hook', '.', 'read_file', '(', 'self', '.', 'wasb_container', ',', 'remote_log_location', ')', 'except', 'AzureHttpError', ':', 'msg', '=', ""'Could not read logs from {}'"", '.', 'format', '(', 'remote_log_location', ')', 'self', '.', 'log', '.', 'exception', '(', 'msg', ')', '# return error if needed', 'if', 'return_error', ':', 'return', 'msg']","Returns the log found at the remote_log_location. Returns '' if no
        logs are found or there is an error.
        :param remote_log_location: the log's location in remote storage
        :type remote_log_location: str (path)
        :param return_error: if True, returns a string error message if an
            error occurs. Otherwise returns '' when an error occurs.
        :type return_error: bool","['Returns', 'the', 'log', 'found', 'at', 'the', 'remote_log_location', '.', 'Returns', 'if', 'no', 'logs', 'are', 'found', 'or', 'there', 'is', 'an', 'error', '.', ':', 'param', 'remote_log_location', ':', 'the', 'log', 's', 'location', 'in', 'remote', 'storage', ':', 'type', 'remote_log_location', ':', 'str', '(', 'path', ')', ':', 'param', 'return_error', ':', 'if', 'True', 'returns', 'a', 'string', 'error', 'message', 'if', 'an', 'error', 'occurs', '.', 'Otherwise', 'returns', 'when', 'an', 'error', 'occurs', '.', ':', 'type', 'return_error', ':', 'bool']",python,test,"['returns', 'the', 'log', 'found', 'at', 'the', 'remote_log_location', '.', 'returns', 'if', 'no', 'logs', 'are', 'found', 'or', 'there', 'is', 'an', 'error', '.', ':', 'param', 'remote_log_location', ':', 'the', 'log', 's', 'location', 'in', 'remote', 'storage', ':', 'type', 'remote_log_location', ':', 'str', '(', 'path', ')', ':', 'param', 'return_error', ':', 'if', 'true', 'returns', 'a', 'string', 'error', 'message', 'if', 'an', 'error', 'occurs', '.', 'otherwise', 'returns', 'when', 'an', 'error', 'occurs', '.', ':', 'type', 'return_error', ':', 'bool']",returns the log found at the remote_log_location . returns if no logs are found or there is an error . : param remote_log_location : the log s location in remote storage : type remote_log_location : str ( path ) : param return_error : if true returns a string error message if an error occurs . otherwise returns when an error occurs . : type return_error : bool,"['def', 'wasb_read', '(', 'self', ',', 'remote_log_location', ',', 'return_error', '=', 'false', ')', ':', 'try', ':', 'return', 'self', '.', 'hook', '.', 'read_file', '(', 'self', '.', 'wasb_container', ',', 'remote_log_location', ')', 'except', 'azurehttperror', ':', 'msg', '=', ""'could not read logs from {}'"", '.', 'format', '(', 'remote_log_location', ')', 'self', '.', 'log', '.', 'exception', '(', 'msg', ')', '# return error if needed', 'if', 'return_error', ':', 'return', 'msg']","def wasb_read ( self , remote_log_location , return_error = false ) : try : return self . hook . read_file ( self . wasb_container , remote_log_location ) except azurehttperror : msg = 'could not read logs from {}' . format ( remote_log_location ) self . log . exception ( msg ) # return error if needed if return_error : return msg"
367,apache/airflow,airflow/utils/log/wasb_task_handler.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/wasb_task_handler.py#L154-L178,"def wasb_write(self, log, remote_log_location, append=True):
        """"""
        Writes the log to the remote_log_location. Fails silently if no hook
        was created.
        :param log: the log to write to the remote_log_location
        :type log: str
        :param remote_log_location: the log's location in remote storage
        :type remote_log_location: str (path)
        :param append: if False, any existing log file is overwritten. If True,
            the new log is appended to any existing logs.
        :type append: bool
        """"""
        if append and self.wasb_log_exists(remote_log_location):
            old_log = self.wasb_read(remote_log_location)
            log = '\n'.join([old_log, log]) if old_log else log

        try:
            self.hook.load_string(
                log,
                self.wasb_container,
                remote_log_location,
            )
        except AzureHttpError:
            self.log.exception('Could not write logs to %s',
                               remote_log_location)","['def', 'wasb_write', '(', 'self', ',', 'log', ',', 'remote_log_location', ',', 'append', '=', 'True', ')', ':', 'if', 'append', 'and', 'self', '.', 'wasb_log_exists', '(', 'remote_log_location', ')', ':', 'old_log', '=', 'self', '.', 'wasb_read', '(', 'remote_log_location', ')', 'log', '=', ""'\\n'"", '.', 'join', '(', '[', 'old_log', ',', 'log', ']', ')', 'if', 'old_log', 'else', 'log', 'try', ':', 'self', '.', 'hook', '.', 'load_string', '(', 'log', ',', 'self', '.', 'wasb_container', ',', 'remote_log_location', ',', ')', 'except', 'AzureHttpError', ':', 'self', '.', 'log', '.', 'exception', '(', ""'Could not write logs to %s'"", ',', 'remote_log_location', ')']","Writes the log to the remote_log_location. Fails silently if no hook
        was created.
        :param log: the log to write to the remote_log_location
        :type log: str
        :param remote_log_location: the log's location in remote storage
        :type remote_log_location: str (path)
        :param append: if False, any existing log file is overwritten. If True,
            the new log is appended to any existing logs.
        :type append: bool","['Writes', 'the', 'log', 'to', 'the', 'remote_log_location', '.', 'Fails', 'silently', 'if', 'no', 'hook', 'was', 'created', '.', ':', 'param', 'log', ':', 'the', 'log', 'to', 'write', 'to', 'the', 'remote_log_location', ':', 'type', 'log', ':', 'str', ':', 'param', 'remote_log_location', ':', 'the', 'log', 's', 'location', 'in', 'remote', 'storage', ':', 'type', 'remote_log_location', ':', 'str', '(', 'path', ')', ':', 'param', 'append', ':', 'if', 'False', 'any', 'existing', 'log', 'file', 'is', 'overwritten', '.', 'If', 'True', 'the', 'new', 'log', 'is', 'appended', 'to', 'any', 'existing', 'logs', '.', ':', 'type', 'append', ':', 'bool']",python,test,"['writes', 'the', 'log', 'to', 'the', 'remote_log_location', '.', 'fails', 'silently', 'if', 'no', 'hook', 'was', 'created', '.', ':', 'param', 'log', ':', 'the', 'log', 'to', 'write', 'to', 'the', 'remote_log_location', ':', 'type', 'log', ':', 'str', ':', 'param', 'remote_log_location', ':', 'the', 'log', 's', 'location', 'in', 'remote', 'storage', ':', 'type', 'remote_log_location', ':', 'str', '(', 'path', ')', ':', 'param', 'append', ':', 'if', 'false', 'any', 'existing', 'log', 'file', 'is', 'overwritten', '.', 'if', 'true', 'the', 'new', 'log', 'is', 'appended', 'to', 'any', 'existing', 'logs', '.', ':', 'type', 'append', ':', 'bool']",writes the log to the remote_log_location . fails silently if no hook was created . : param log : the log to write to the remote_log_location : type log : str : param remote_log_location : the log s location in remote storage : type remote_log_location : str ( path ) : param append : if false any existing log file is overwritten . if true the new log is appended to any existing logs . : type append : bool,"['def', 'wasb_write', '(', 'self', ',', 'log', ',', 'remote_log_location', ',', 'append', '=', 'true', ')', ':', 'if', 'append', 'and', 'self', '.', 'wasb_log_exists', '(', 'remote_log_location', ')', ':', 'old_log', '=', 'self', '.', 'wasb_read', '(', 'remote_log_location', ')', 'log', '=', ""'\\n'"", '.', 'join', '(', '[', 'old_log', ',', 'log', ']', ')', 'if', 'old_log', 'else', 'log', 'try', ':', 'self', '.', 'hook', '.', 'load_string', '(', 'log', ',', 'self', '.', 'wasb_container', ',', 'remote_log_location', ',', ')', 'except', 'azurehttperror', ':', 'self', '.', 'log', '.', 'exception', '(', ""'could not write logs to %s'"", ',', 'remote_log_location', ')']","def wasb_write ( self , log , remote_log_location , append = true ) : if append and self . wasb_log_exists ( remote_log_location ) : old_log = self . wasb_read ( remote_log_location ) log = '\n' . join ( [ old_log , log ] ) if old_log else log try : self . hook . load_string ( log , self . wasb_container , remote_log_location , ) except azurehttperror : self . log . exception ( 'could not write logs to %s' , remote_log_location )"
368,apache/airflow,airflow/contrib/hooks/gcp_compute_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_compute_hook.py#L55-L66,"def get_conn(self):
        """"""
        Retrieves connection to Google Compute Engine.

        :return: Google Compute Engine services object
        :rtype: dict
        """"""
        if not self._conn:
            http_authorized = self._authorize()
            self._conn = build('compute', self.api_version,
                               http=http_authorized, cache_discovery=False)
        return self._conn","['def', 'get_conn', '(', 'self', ')', ':', 'if', 'not', 'self', '.', '_conn', ':', 'http_authorized', '=', 'self', '.', '_authorize', '(', ')', 'self', '.', '_conn', '=', 'build', '(', ""'compute'"", ',', 'self', '.', 'api_version', ',', 'http', '=', 'http_authorized', ',', 'cache_discovery', '=', 'False', ')', 'return', 'self', '.', '_conn']","Retrieves connection to Google Compute Engine.

        :return: Google Compute Engine services object
        :rtype: dict","['Retrieves', 'connection', 'to', 'Google', 'Compute', 'Engine', '.']",python,test,"['retrieves', 'connection', 'to', 'google', 'compute', 'engine', '.']",retrieves connection to google compute engine .,"['def', 'get_conn', '(', 'self', ')', ':', 'if', 'not', 'self', '.', '_conn', ':', 'http_authorized', '=', 'self', '.', '_authorize', '(', ')', 'self', '.', '_conn', '=', 'build', '(', ""'compute'"", ',', 'self', '.', 'api_version', ',', 'http', '=', 'http_authorized', ',', 'cache_discovery', '=', 'false', ')', 'return', 'self', '.', '_conn']","def get_conn ( self ) : if not self . _conn : http_authorized = self . _authorize ( ) self . _conn = build ( 'compute' , self . api_version , http = http_authorized , cache_discovery = false ) return self . _conn"
369,apache/airflow,airflow/contrib/hooks/gcp_compute_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_compute_hook.py#L69-L97,"def start_instance(self, zone, resource_id, project_id=None):
        """"""
        Starts an existing instance defined by project_id, zone and resource_id.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the instance exists
        :type zone: str
        :param resource_id: Name of the Compute Engine instance resource
        :type resource_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().instances().start(
            project=project_id,
            zone=zone,
            instance=resource_id
        ).execute(num_retries=self.num_retries)
        try:
            operation_name = response[""name""]
        except KeyError:
            raise AirflowException(
                ""Wrong response '{}' returned - it should contain ""
                ""'name' field"".format(response))
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name,
                                             zone=zone)","['def', 'start_instance', '(', 'self', ',', 'zone', ',', 'resource_id', ',', 'project_id', '=', 'None', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'instances', '(', ')', '.', 'start', '(', 'project', '=', 'project_id', ',', 'zone', '=', 'zone', ',', 'instance', '=', 'resource_id', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'try', ':', 'operation_name', '=', 'response', '[', '""name""', ']', 'except', 'KeyError', ':', 'raise', 'AirflowException', '(', '""Wrong response \'{}\' returned - it should contain ""', '""\'name\' field""', '.', 'format', '(', 'response', ')', ')', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ',', 'zone', '=', 'zone', ')']","Starts an existing instance defined by project_id, zone and resource_id.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the instance exists
        :type zone: str
        :param resource_id: Name of the Compute Engine instance resource
        :type resource_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None","['Starts', 'an', 'existing', 'instance', 'defined', 'by', 'project_id', 'zone', 'and', 'resource_id', '.', 'Must', 'be', 'called', 'with', 'keyword', 'arguments', 'rather', 'than', 'positional', '.']",python,test,"['starts', 'an', 'existing', 'instance', 'defined', 'by', 'project_id', 'zone', 'and', 'resource_id', '.', 'must', 'be', 'called', 'with', 'keyword', 'arguments', 'rather', 'than', 'positional', '.']",starts an existing instance defined by project_id zone and resource_id . must be called with keyword arguments rather than positional .,"['def', 'start_instance', '(', 'self', ',', 'zone', ',', 'resource_id', ',', 'project_id', '=', 'none', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'instances', '(', ')', '.', 'start', '(', 'project', '=', 'project_id', ',', 'zone', '=', 'zone', ',', 'instance', '=', 'resource_id', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'try', ':', 'operation_name', '=', 'response', '[', '""name""', ']', 'except', 'keyerror', ':', 'raise', 'airflowexception', '(', '""wrong response \'{}\' returned - it should contain ""', '""\'name\' field""', '.', 'format', '(', 'response', ')', ')', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ',', 'zone', '=', 'zone', ')']","def start_instance ( self , zone , resource_id , project_id = none ) : response = self . get_conn ( ) . instances ( ) . start ( project = project_id , zone = zone , instance = resource_id ) . execute ( num_retries = self . num_retries ) try : operation_name = response [ ""name"" ] except keyerror : raise airflowexception ( ""wrong response '{}' returned - it should contain "" ""'name' field"" . format ( response ) ) self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name , zone = zone )"
370,apache/airflow,airflow/contrib/hooks/gcp_compute_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_compute_hook.py#L131-L159,"def set_machine_type(self, zone, resource_id, body, project_id=None):
        """"""
        Sets machine type of an instance defined by project_id, zone and resource_id.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the instance exists.
        :type zone: str
        :param resource_id: Name of the Compute Engine instance resource
        :type resource_id: str
        :param body: Body required by the Compute Engine setMachineType API,
            as described in
            https://cloud.google.com/compute/docs/reference/rest/v1/instances/setMachineType
        :type body: dict
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self._execute_set_machine_type(zone, resource_id, body, project_id)
        try:
            operation_name = response[""name""]
        except KeyError:
            raise AirflowException(
                ""Wrong response '{}' returned - it should contain ""
                ""'name' field"".format(response))
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name,
                                             zone=zone)","['def', 'set_machine_type', '(', 'self', ',', 'zone', ',', 'resource_id', ',', 'body', ',', 'project_id', '=', 'None', ')', ':', 'response', '=', 'self', '.', '_execute_set_machine_type', '(', 'zone', ',', 'resource_id', ',', 'body', ',', 'project_id', ')', 'try', ':', 'operation_name', '=', 'response', '[', '""name""', ']', 'except', 'KeyError', ':', 'raise', 'AirflowException', '(', '""Wrong response \'{}\' returned - it should contain ""', '""\'name\' field""', '.', 'format', '(', 'response', ')', ')', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ',', 'zone', '=', 'zone', ')']","Sets machine type of an instance defined by project_id, zone and resource_id.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the instance exists.
        :type zone: str
        :param resource_id: Name of the Compute Engine instance resource
        :type resource_id: str
        :param body: Body required by the Compute Engine setMachineType API,
            as described in
            https://cloud.google.com/compute/docs/reference/rest/v1/instances/setMachineType
        :type body: dict
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None","['Sets', 'machine', 'type', 'of', 'an', 'instance', 'defined', 'by', 'project_id', 'zone', 'and', 'resource_id', '.', 'Must', 'be', 'called', 'with', 'keyword', 'arguments', 'rather', 'than', 'positional', '.']",python,test,"['sets', 'machine', 'type', 'of', 'an', 'instance', 'defined', 'by', 'project_id', 'zone', 'and', 'resource_id', '.', 'must', 'be', 'called', 'with', 'keyword', 'arguments', 'rather', 'than', 'positional', '.']",sets machine type of an instance defined by project_id zone and resource_id . must be called with keyword arguments rather than positional .,"['def', 'set_machine_type', '(', 'self', ',', 'zone', ',', 'resource_id', ',', 'body', ',', 'project_id', '=', 'none', ')', ':', 'response', '=', 'self', '.', '_execute_set_machine_type', '(', 'zone', ',', 'resource_id', ',', 'body', ',', 'project_id', ')', 'try', ':', 'operation_name', '=', 'response', '[', '""name""', ']', 'except', 'keyerror', ':', 'raise', 'airflowexception', '(', '""wrong response \'{}\' returned - it should contain ""', '""\'name\' field""', '.', 'format', '(', 'response', ')', ')', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ',', 'zone', '=', 'zone', ')']","def set_machine_type ( self , zone , resource_id , body , project_id = none ) : response = self . _execute_set_machine_type ( zone , resource_id , body , project_id ) try : operation_name = response [ ""name"" ] except keyerror : raise airflowexception ( ""wrong response '{}' returned - it should contain "" ""'name' field"" . format ( response ) ) self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name , zone = zone )"
371,apache/airflow,airflow/contrib/hooks/gcp_compute_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_compute_hook.py#L167-L186,"def get_instance_template(self, resource_id, project_id=None):
        """"""
        Retrieves instance template by project_id and resource_id.
        Must be called with keyword arguments rather than positional.

        :param resource_id: Name of the instance template
        :type resource_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: Instance template representation as object according to
            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates
        :rtype: dict
        """"""
        response = self.get_conn().instanceTemplates().get(
            project=project_id,
            instanceTemplate=resource_id
        ).execute(num_retries=self.num_retries)
        return response","['def', 'get_instance_template', '(', 'self', ',', 'resource_id', ',', 'project_id', '=', 'None', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'instanceTemplates', '(', ')', '.', 'get', '(', 'project', '=', 'project_id', ',', 'instanceTemplate', '=', 'resource_id', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'return', 'response']","Retrieves instance template by project_id and resource_id.
        Must be called with keyword arguments rather than positional.

        :param resource_id: Name of the instance template
        :type resource_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: Instance template representation as object according to
            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates
        :rtype: dict","['Retrieves', 'instance', 'template', 'by', 'project_id', 'and', 'resource_id', '.', 'Must', 'be', 'called', 'with', 'keyword', 'arguments', 'rather', 'than', 'positional', '.']",python,test,"['retrieves', 'instance', 'template', 'by', 'project_id', 'and', 'resource_id', '.', 'must', 'be', 'called', 'with', 'keyword', 'arguments', 'rather', 'than', 'positional', '.']",retrieves instance template by project_id and resource_id . must be called with keyword arguments rather than positional .,"['def', 'get_instance_template', '(', 'self', ',', 'resource_id', ',', 'project_id', '=', 'none', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'instancetemplates', '(', ')', '.', 'get', '(', 'project', '=', 'project_id', ',', 'instancetemplate', '=', 'resource_id', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'return', 'response']","def get_instance_template ( self , resource_id , project_id = none ) : response = self . get_conn ( ) . instancetemplates ( ) . get ( project = project_id , instancetemplate = resource_id ) . execute ( num_retries = self . num_retries ) return response"
372,apache/airflow,airflow/contrib/hooks/gcp_compute_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_compute_hook.py#L189-L220,"def insert_instance_template(self, body, request_id=None, project_id=None):
        """"""
        Inserts instance template using body specified
        Must be called with keyword arguments rather than positional.

        :param body: Instance template representation as object according to
            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates
        :type body: dict
        :param request_id: Optional, unique request_id that you might add to achieve
            full idempotence (for example when client call times out repeating the request
            with the same request id will not create a new instance template again)
            It should be in UUID format as defined in RFC 4122
        :type request_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().instanceTemplates().insert(
            project=project_id,
            body=body,
            requestId=request_id
        ).execute(num_retries=self.num_retries)
        try:
            operation_name = response[""name""]
        except KeyError:
            raise AirflowException(
                ""Wrong response '{}' returned - it should contain ""
                ""'name' field"".format(response))
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name)","['def', 'insert_instance_template', '(', 'self', ',', 'body', ',', 'request_id', '=', 'None', ',', 'project_id', '=', 'None', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'instanceTemplates', '(', ')', '.', 'insert', '(', 'project', '=', 'project_id', ',', 'body', '=', 'body', ',', 'requestId', '=', 'request_id', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'try', ':', 'operation_name', '=', 'response', '[', '""name""', ']', 'except', 'KeyError', ':', 'raise', 'AirflowException', '(', '""Wrong response \'{}\' returned - it should contain ""', '""\'name\' field""', '.', 'format', '(', 'response', ')', ')', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ')']","Inserts instance template using body specified
        Must be called with keyword arguments rather than positional.

        :param body: Instance template representation as object according to
            https://cloud.google.com/compute/docs/reference/rest/v1/instanceTemplates
        :type body: dict
        :param request_id: Optional, unique request_id that you might add to achieve
            full idempotence (for example when client call times out repeating the request
            with the same request id will not create a new instance template again)
            It should be in UUID format as defined in RFC 4122
        :type request_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None","['Inserts', 'instance', 'template', 'using', 'body', 'specified', 'Must', 'be', 'called', 'with', 'keyword', 'arguments', 'rather', 'than', 'positional', '.']",python,test,"['inserts', 'instance', 'template', 'using', 'body', 'specified', 'must', 'be', 'called', 'with', 'keyword', 'arguments', 'rather', 'than', 'positional', '.']",inserts instance template using body specified must be called with keyword arguments rather than positional .,"['def', 'insert_instance_template', '(', 'self', ',', 'body', ',', 'request_id', '=', 'none', ',', 'project_id', '=', 'none', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'instancetemplates', '(', ')', '.', 'insert', '(', 'project', '=', 'project_id', ',', 'body', '=', 'body', ',', 'requestid', '=', 'request_id', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'try', ':', 'operation_name', '=', 'response', '[', '""name""', ']', 'except', 'keyerror', ':', 'raise', 'airflowexception', '(', '""wrong response \'{}\' returned - it should contain ""', '""\'name\' field""', '.', 'format', '(', 'response', ')', ')', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ')']","def insert_instance_template ( self , body , request_id = none , project_id = none ) : response = self . get_conn ( ) . instancetemplates ( ) . insert ( project = project_id , body = body , requestid = request_id ) . execute ( num_retries = self . num_retries ) try : operation_name = response [ ""name"" ] except keyerror : raise airflowexception ( ""wrong response '{}' returned - it should contain "" ""'name' field"" . format ( response ) ) self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name )"
373,apache/airflow,airflow/contrib/hooks/gcp_compute_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_compute_hook.py#L223-L245,"def get_instance_group_manager(self, zone, resource_id, project_id=None):
        """"""
        Retrieves Instance Group Manager by project_id, zone and resource_id.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the Instance Group Manager exists
        :type zone: str
        :param resource_id: Name of the Instance Group Manager
        :type resource_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: Instance group manager representation as object according to
            https://cloud.google.com/compute/docs/reference/rest/beta/instanceGroupManagers
        :rtype: dict
        """"""
        response = self.get_conn().instanceGroupManagers().get(
            project=project_id,
            zone=zone,
            instanceGroupManager=resource_id
        ).execute(num_retries=self.num_retries)
        return response","['def', 'get_instance_group_manager', '(', 'self', ',', 'zone', ',', 'resource_id', ',', 'project_id', '=', 'None', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'instanceGroupManagers', '(', ')', '.', 'get', '(', 'project', '=', 'project_id', ',', 'zone', '=', 'zone', ',', 'instanceGroupManager', '=', 'resource_id', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'return', 'response']","Retrieves Instance Group Manager by project_id, zone and resource_id.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the Instance Group Manager exists
        :type zone: str
        :param resource_id: Name of the Instance Group Manager
        :type resource_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: Instance group manager representation as object according to
            https://cloud.google.com/compute/docs/reference/rest/beta/instanceGroupManagers
        :rtype: dict","['Retrieves', 'Instance', 'Group', 'Manager', 'by', 'project_id', 'zone', 'and', 'resource_id', '.', 'Must', 'be', 'called', 'with', 'keyword', 'arguments', 'rather', 'than', 'positional', '.']",python,test,"['retrieves', 'instance', 'group', 'manager', 'by', 'project_id', 'zone', 'and', 'resource_id', '.', 'must', 'be', 'called', 'with', 'keyword', 'arguments', 'rather', 'than', 'positional', '.']",retrieves instance group manager by project_id zone and resource_id . must be called with keyword arguments rather than positional .,"['def', 'get_instance_group_manager', '(', 'self', ',', 'zone', ',', 'resource_id', ',', 'project_id', '=', 'none', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'instancegroupmanagers', '(', ')', '.', 'get', '(', 'project', '=', 'project_id', ',', 'zone', '=', 'zone', ',', 'instancegroupmanager', '=', 'resource_id', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'return', 'response']","def get_instance_group_manager ( self , zone , resource_id , project_id = none ) : response = self . get_conn ( ) . instancegroupmanagers ( ) . get ( project = project_id , zone = zone , instancegroupmanager = resource_id ) . execute ( num_retries = self . num_retries ) return response"
374,apache/airflow,airflow/contrib/hooks/gcp_compute_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_compute_hook.py#L248-L288,"def patch_instance_group_manager(self, zone, resource_id,
                                     body, request_id=None, project_id=None):
        """"""
        Patches Instance Group Manager with the specified body.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the Instance Group Manager exists
        :type zone: str
        :param resource_id: Name of the Instance Group Manager
        :type resource_id: str
        :param body: Instance Group Manager representation as json-merge-patch object
            according to
            https://cloud.google.com/compute/docs/reference/rest/beta/instanceTemplates/patch
        :type body: dict
        :param request_id: Optional, unique request_id that you might add to achieve
            full idempotence (for example when client call times out repeating the request
            with the same request id will not create a new instance template again).
            It should be in UUID format as defined in RFC 4122
        :type request_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None
        """"""
        response = self.get_conn().instanceGroupManagers().patch(
            project=project_id,
            zone=zone,
            instanceGroupManager=resource_id,
            body=body,
            requestId=request_id
        ).execute(num_retries=self.num_retries)
        try:
            operation_name = response[""name""]
        except KeyError:
            raise AirflowException(
                ""Wrong response '{}' returned - it should contain ""
                ""'name' field"".format(response))
        self._wait_for_operation_to_complete(project_id=project_id,
                                             operation_name=operation_name,
                                             zone=zone)","['def', 'patch_instance_group_manager', '(', 'self', ',', 'zone', ',', 'resource_id', ',', 'body', ',', 'request_id', '=', 'None', ',', 'project_id', '=', 'None', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'instanceGroupManagers', '(', ')', '.', 'patch', '(', 'project', '=', 'project_id', ',', 'zone', '=', 'zone', ',', 'instanceGroupManager', '=', 'resource_id', ',', 'body', '=', 'body', ',', 'requestId', '=', 'request_id', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'try', ':', 'operation_name', '=', 'response', '[', '""name""', ']', 'except', 'KeyError', ':', 'raise', 'AirflowException', '(', '""Wrong response \'{}\' returned - it should contain ""', '""\'name\' field""', '.', 'format', '(', 'response', ')', ')', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ',', 'zone', '=', 'zone', ')']","Patches Instance Group Manager with the specified body.
        Must be called with keyword arguments rather than positional.

        :param zone: Google Cloud Platform zone where the Instance Group Manager exists
        :type zone: str
        :param resource_id: Name of the Instance Group Manager
        :type resource_id: str
        :param body: Instance Group Manager representation as json-merge-patch object
            according to
            https://cloud.google.com/compute/docs/reference/rest/beta/instanceTemplates/patch
        :type body: dict
        :param request_id: Optional, unique request_id that you might add to achieve
            full idempotence (for example when client call times out repeating the request
            with the same request id will not create a new instance template again).
            It should be in UUID format as defined in RFC 4122
        :type request_id: str
        :param project_id: Optional, Google Cloud Platform project ID where the
            Compute Engine Instance exists. If set to None or missing,
            the default project_id from the GCP connection is used.
        :type project_id: str
        :return: None","['Patches', 'Instance', 'Group', 'Manager', 'with', 'the', 'specified', 'body', '.', 'Must', 'be', 'called', 'with', 'keyword', 'arguments', 'rather', 'than', 'positional', '.']",python,test,"['patches', 'instance', 'group', 'manager', 'with', 'the', 'specified', 'body', '.', 'must', 'be', 'called', 'with', 'keyword', 'arguments', 'rather', 'than', 'positional', '.']",patches instance group manager with the specified body . must be called with keyword arguments rather than positional .,"['def', 'patch_instance_group_manager', '(', 'self', ',', 'zone', ',', 'resource_id', ',', 'body', ',', 'request_id', '=', 'none', ',', 'project_id', '=', 'none', ')', ':', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'instancegroupmanagers', '(', ')', '.', 'patch', '(', 'project', '=', 'project_id', ',', 'zone', '=', 'zone', ',', 'instancegroupmanager', '=', 'resource_id', ',', 'body', '=', 'body', ',', 'requestid', '=', 'request_id', ')', '.', 'execute', '(', 'num_retries', '=', 'self', '.', 'num_retries', ')', 'try', ':', 'operation_name', '=', 'response', '[', '""name""', ']', 'except', 'keyerror', ':', 'raise', 'airflowexception', '(', '""wrong response \'{}\' returned - it should contain ""', '""\'name\' field""', '.', 'format', '(', 'response', ')', ')', 'self', '.', '_wait_for_operation_to_complete', '(', 'project_id', '=', 'project_id', ',', 'operation_name', '=', 'operation_name', ',', 'zone', '=', 'zone', ')']","def patch_instance_group_manager ( self , zone , resource_id , body , request_id = none , project_id = none ) : response = self . get_conn ( ) . instancegroupmanagers ( ) . patch ( project = project_id , zone = zone , instancegroupmanager = resource_id , body = body , requestid = request_id ) . execute ( num_retries = self . num_retries ) try : operation_name = response [ ""name"" ] except keyerror : raise airflowexception ( ""wrong response '{}' returned - it should contain "" ""'name' field"" . format ( response ) ) self . _wait_for_operation_to_complete ( project_id = project_id , operation_name = operation_name , zone = zone )"
375,apache/airflow,airflow/contrib/hooks/gcp_compute_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_compute_hook.py#L290-L320,"def _wait_for_operation_to_complete(self, project_id, operation_name, zone=None):
        """"""
        Waits for the named operation to complete - checks status of the async call.

        :param operation_name: name of the operation
        :type operation_name: str
        :param zone: optional region of the request (might be None for global operations)
        :type zone: str
        :return: None
        """"""
        service = self.get_conn()
        while True:
            if zone is None:
                # noinspection PyTypeChecker
                operation_response = self._check_global_operation_status(
                    service, operation_name, project_id)
            else:
                # noinspection PyTypeChecker
                operation_response = self._check_zone_operation_status(
                    service, operation_name, project_id, zone, self.num_retries)
            if operation_response.get(""status"") == GceOperationStatus.DONE:
                error = operation_response.get(""error"")
                if error:
                    code = operation_response.get(""httpErrorStatusCode"")
                    msg = operation_response.get(""httpErrorMessage"")
                    # Extracting the errors list as string and trimming square braces
                    error_msg = str(error.get(""errors""))[1:-1]
                    raise AirflowException(""{} {}: "".format(code, msg) + error_msg)
                # No meaningful info to return from the response in case of success
                return
            time.sleep(TIME_TO_SLEEP_IN_SECONDS)","['def', '_wait_for_operation_to_complete', '(', 'self', ',', 'project_id', ',', 'operation_name', ',', 'zone', '=', 'None', ')', ':', 'service', '=', 'self', '.', 'get_conn', '(', ')', 'while', 'True', ':', 'if', 'zone', 'is', 'None', ':', '# noinspection PyTypeChecker', 'operation_response', '=', 'self', '.', '_check_global_operation_status', '(', 'service', ',', 'operation_name', ',', 'project_id', ')', 'else', ':', '# noinspection PyTypeChecker', 'operation_response', '=', 'self', '.', '_check_zone_operation_status', '(', 'service', ',', 'operation_name', ',', 'project_id', ',', 'zone', ',', 'self', '.', 'num_retries', ')', 'if', 'operation_response', '.', 'get', '(', '""status""', ')', '==', 'GceOperationStatus', '.', 'DONE', ':', 'error', '=', 'operation_response', '.', 'get', '(', '""error""', ')', 'if', 'error', ':', 'code', '=', 'operation_response', '.', 'get', '(', '""httpErrorStatusCode""', ')', 'msg', '=', 'operation_response', '.', 'get', '(', '""httpErrorMessage""', ')', '# Extracting the errors list as string and trimming square braces', 'error_msg', '=', 'str', '(', 'error', '.', 'get', '(', '""errors""', ')', ')', '[', '1', ':', '-', '1', ']', 'raise', 'AirflowException', '(', '""{} {}: ""', '.', 'format', '(', 'code', ',', 'msg', ')', '+', 'error_msg', ')', '# No meaningful info to return from the response in case of success', 'return', 'time', '.', 'sleep', '(', 'TIME_TO_SLEEP_IN_SECONDS', ')']","Waits for the named operation to complete - checks status of the async call.

        :param operation_name: name of the operation
        :type operation_name: str
        :param zone: optional region of the request (might be None for global operations)
        :type zone: str
        :return: None","['Waits', 'for', 'the', 'named', 'operation', 'to', 'complete', '-', 'checks', 'status', 'of', 'the', 'async', 'call', '.']",python,test,"['waits', 'for', 'the', 'named', 'operation', 'to', 'complete', '-', 'checks', 'status', 'of', 'the', 'async', 'call', '.']",waits for the named operation to complete - checks status of the async call .,"['def', '_wait_for_operation_to_complete', '(', 'self', ',', 'project_id', ',', 'operation_name', ',', 'zone', '=', 'none', ')', ':', 'service', '=', 'self', '.', 'get_conn', '(', ')', 'while', 'true', ':', 'if', 'zone', 'is', 'none', ':', '# noinspection pytypechecker', 'operation_response', '=', 'self', '.', '_check_global_operation_status', '(', 'service', ',', 'operation_name', ',', 'project_id', ')', 'else', ':', '# noinspection pytypechecker', 'operation_response', '=', 'self', '.', '_check_zone_operation_status', '(', 'service', ',', 'operation_name', ',', 'project_id', ',', 'zone', ',', 'self', '.', 'num_retries', ')', 'if', 'operation_response', '.', 'get', '(', '""status""', ')', '==', 'gceoperationstatus', '.', 'done', ':', 'error', '=', 'operation_response', '.', 'get', '(', '""error""', ')', 'if', 'error', ':', 'code', '=', 'operation_response', '.', 'get', '(', '""httperrorstatuscode""', ')', 'msg', '=', 'operation_response', '.', 'get', '(', '""httperrormessage""', ')', '# extracting the errors list as string and trimming square braces', 'error_msg', '=', 'str', '(', 'error', '.', 'get', '(', '""errors""', ')', ')', '[', '1', ':', '-', '1', ']', 'raise', 'airflowexception', '(', '""{} {}: ""', '.', 'format', '(', 'code', ',', 'msg', ')', '+', 'error_msg', ')', '# no meaningful info to return from the response in case of success', 'return', 'time', '.', 'sleep', '(', 'time_to_sleep_in_seconds', ')']","def _wait_for_operation_to_complete ( self , project_id , operation_name , zone = none ) : service = self . get_conn ( ) while true : if zone is none : # noinspection pytypechecker operation_response = self . _check_global_operation_status ( service , operation_name , project_id ) else : # noinspection pytypechecker operation_response = self . _check_zone_operation_status ( service , operation_name , project_id , zone , self . num_retries ) if operation_response . get ( ""status"" ) == gceoperationstatus . done : error = operation_response . get ( ""error"" ) if error : code = operation_response . get ( ""httperrorstatuscode"" ) msg = operation_response . get ( ""httperrormessage"" ) # extracting the errors list as string and trimming square braces error_msg = str ( error . get ( ""errors"" ) ) [ 1 : - 1 ] raise airflowexception ( ""{} {}: "" . format ( code , msg ) + error_msg ) # no meaningful info to return from the response in case of success return time . sleep ( time_to_sleep_in_seconds )"
376,apache/airflow,airflow/hooks/S3_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L48-L60,"def check_for_bucket(self, bucket_name):
        """"""
        Check if bucket_name exists.

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        """"""
        try:
            self.get_conn().head_bucket(Bucket=bucket_name)
            return True
        except ClientError as e:
            self.log.info(e.response[""Error""][""Message""])
            return False","['def', 'check_for_bucket', '(', 'self', ',', 'bucket_name', ')', ':', 'try', ':', 'self', '.', 'get_conn', '(', ')', '.', 'head_bucket', '(', 'Bucket', '=', 'bucket_name', ')', 'return', 'True', 'except', 'ClientError', 'as', 'e', ':', 'self', '.', 'log', '.', 'info', '(', 'e', '.', 'response', '[', '""Error""', ']', '[', '""Message""', ']', ')', 'return', 'False']","Check if bucket_name exists.

        :param bucket_name: the name of the bucket
        :type bucket_name: str","['Check', 'if', 'bucket_name', 'exists', '.']",python,test,"['check', 'if', 'bucket_name', 'exists', '.']",check if bucket_name exists .,"['def', 'check_for_bucket', '(', 'self', ',', 'bucket_name', ')', ':', 'try', ':', 'self', '.', 'get_conn', '(', ')', '.', 'head_bucket', '(', 'bucket', '=', 'bucket_name', ')', 'return', 'true', 'except', 'clienterror', 'as', 'e', ':', 'self', '.', 'log', '.', 'info', '(', 'e', '.', 'response', '[', '""error""', ']', '[', '""message""', ']', ')', 'return', 'false']","def check_for_bucket ( self , bucket_name ) : try : self . get_conn ( ) . head_bucket ( bucket = bucket_name ) return true except clienterror as e : self . log . info ( e . response [ ""error"" ] [ ""message"" ] ) return false"
377,apache/airflow,airflow/hooks/S3_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L72-L90,"def create_bucket(self, bucket_name, region_name=None):
        """"""
        Creates an Amazon S3 bucket.

        :param bucket_name: The name of the bucket
        :type bucket_name: str
        :param region_name: The name of the aws region in which to create the bucket.
        :type region_name: str
        """"""
        s3_conn = self.get_conn()
        if not region_name:
            region_name = s3_conn.meta.region_name
        if region_name == 'us-east-1':
            self.get_conn().create_bucket(Bucket=bucket_name)
        else:
            self.get_conn().create_bucket(Bucket=bucket_name,
                                          CreateBucketConfiguration={
                                              'LocationConstraint': region_name
                                          })","['def', 'create_bucket', '(', 'self', ',', 'bucket_name', ',', 'region_name', '=', 'None', ')', ':', 's3_conn', '=', 'self', '.', 'get_conn', '(', ')', 'if', 'not', 'region_name', ':', 'region_name', '=', 's3_conn', '.', 'meta', '.', 'region_name', 'if', 'region_name', '==', ""'us-east-1'"", ':', 'self', '.', 'get_conn', '(', ')', '.', 'create_bucket', '(', 'Bucket', '=', 'bucket_name', ')', 'else', ':', 'self', '.', 'get_conn', '(', ')', '.', 'create_bucket', '(', 'Bucket', '=', 'bucket_name', ',', 'CreateBucketConfiguration', '=', '{', ""'LocationConstraint'"", ':', 'region_name', '}', ')']","Creates an Amazon S3 bucket.

        :param bucket_name: The name of the bucket
        :type bucket_name: str
        :param region_name: The name of the aws region in which to create the bucket.
        :type region_name: str","['Creates', 'an', 'Amazon', 'S3', 'bucket', '.']",python,test,"['creates', 'an', 'amazon', 's3', 'bucket', '.']",creates an amazon s3 bucket .,"['def', 'create_bucket', '(', 'self', ',', 'bucket_name', ',', 'region_name', '=', 'none', ')', ':', 's3_conn', '=', 'self', '.', 'get_conn', '(', ')', 'if', 'not', 'region_name', ':', 'region_name', '=', 's3_conn', '.', 'meta', '.', 'region_name', 'if', 'region_name', '==', ""'us-east-1'"", ':', 'self', '.', 'get_conn', '(', ')', '.', 'create_bucket', '(', 'bucket', '=', 'bucket_name', ')', 'else', ':', 'self', '.', 'get_conn', '(', ')', '.', 'create_bucket', '(', 'bucket', '=', 'bucket_name', ',', 'createbucketconfiguration', '=', '{', ""'locationconstraint'"", ':', 'region_name', '}', ')']","def create_bucket ( self , bucket_name , region_name = none ) : s3_conn = self . get_conn ( ) if not region_name : region_name = s3_conn . meta . region_name if region_name == 'us-east-1' : self . get_conn ( ) . create_bucket ( bucket = bucket_name ) else : self . get_conn ( ) . create_bucket ( bucket = bucket_name , createbucketconfiguration = { 'locationconstraint' : region_name } )"
378,apache/airflow,airflow/hooks/S3_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L92-L107,"def check_for_prefix(self, bucket_name, prefix, delimiter):
        """"""
        Checks that a prefix exists in a bucket

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param prefix: a key prefix
        :type prefix: str
        :param delimiter: the delimiter marks key hierarchy.
        :type delimiter: str
        """"""
        prefix = prefix + delimiter if prefix[-1] != delimiter else prefix
        prefix_split = re.split(r'(\w+[{d}])$'.format(d=delimiter), prefix, 1)
        previous_level = prefix_split[0]
        plist = self.list_prefixes(bucket_name, previous_level, delimiter)
        return False if plist is None else prefix in plist","['def', 'check_for_prefix', '(', 'self', ',', 'bucket_name', ',', 'prefix', ',', 'delimiter', ')', ':', 'prefix', '=', 'prefix', '+', 'delimiter', 'if', 'prefix', '[', '-', '1', ']', '!=', 'delimiter', 'else', 'prefix', 'prefix_split', '=', 're', '.', 'split', '(', ""r'(\\w+[{d}])$'"", '.', 'format', '(', 'd', '=', 'delimiter', ')', ',', 'prefix', ',', '1', ')', 'previous_level', '=', 'prefix_split', '[', '0', ']', 'plist', '=', 'self', '.', 'list_prefixes', '(', 'bucket_name', ',', 'previous_level', ',', 'delimiter', ')', 'return', 'False', 'if', 'plist', 'is', 'None', 'else', 'prefix', 'in', 'plist']","Checks that a prefix exists in a bucket

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param prefix: a key prefix
        :type prefix: str
        :param delimiter: the delimiter marks key hierarchy.
        :type delimiter: str","['Checks', 'that', 'a', 'prefix', 'exists', 'in', 'a', 'bucket']",python,test,"['checks', 'that', 'a', 'prefix', 'exists', 'in', 'a', 'bucket']",checks that a prefix exists in a bucket,"['def', 'check_for_prefix', '(', 'self', ',', 'bucket_name', ',', 'prefix', ',', 'delimiter', ')', ':', 'prefix', '=', 'prefix', '+', 'delimiter', 'if', 'prefix', '[', '-', '1', ']', '!=', 'delimiter', 'else', 'prefix', 'prefix_split', '=', 're', '.', 'split', '(', ""r'(\\w+[{d}])$'"", '.', 'format', '(', 'd', '=', 'delimiter', ')', ',', 'prefix', ',', '1', ')', 'previous_level', '=', 'prefix_split', '[', '0', ']', 'plist', '=', 'self', '.', 'list_prefixes', '(', 'bucket_name', ',', 'previous_level', ',', 'delimiter', ')', 'return', 'false', 'if', 'plist', 'is', 'none', 'else', 'prefix', 'in', 'plist']","def check_for_prefix ( self , bucket_name , prefix , delimiter ) : prefix = prefix + delimiter if prefix [ - 1 ] != delimiter else prefix prefix_split = re . split ( r'(\w+[{d}])$' . format ( d = delimiter ) , prefix , 1 ) previous_level = prefix_split [ 0 ] plist = self . list_prefixes ( bucket_name , previous_level , delimiter ) return false if plist is none else prefix in plist"
379,apache/airflow,airflow/hooks/S3_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L109-L145,"def list_prefixes(self, bucket_name, prefix='', delimiter='',
                      page_size=None, max_items=None):
        """"""
        Lists prefixes in a bucket under prefix

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param prefix: a key prefix
        :type prefix: str
        :param delimiter: the delimiter marks key hierarchy.
        :type delimiter: str
        :param page_size: pagination size
        :type page_size: int
        :param max_items: maximum items to return
        :type max_items: int
        """"""
        config = {
            'PageSize': page_size,
            'MaxItems': max_items,
        }

        paginator = self.get_conn().get_paginator('list_objects_v2')
        response = paginator.paginate(Bucket=bucket_name,
                                      Prefix=prefix,
                                      Delimiter=delimiter,
                                      PaginationConfig=config)

        has_results = False
        prefixes = []
        for page in response:
            if 'CommonPrefixes' in page:
                has_results = True
                for p in page['CommonPrefixes']:
                    prefixes.append(p['Prefix'])

        if has_results:
            return prefixes","['def', 'list_prefixes', '(', 'self', ',', 'bucket_name', ',', 'prefix', '=', ""''"", ',', 'delimiter', '=', ""''"", ',', 'page_size', '=', 'None', ',', 'max_items', '=', 'None', ')', ':', 'config', '=', '{', ""'PageSize'"", ':', 'page_size', ',', ""'MaxItems'"", ':', 'max_items', ',', '}', 'paginator', '=', 'self', '.', 'get_conn', '(', ')', '.', 'get_paginator', '(', ""'list_objects_v2'"", ')', 'response', '=', 'paginator', '.', 'paginate', '(', 'Bucket', '=', 'bucket_name', ',', 'Prefix', '=', 'prefix', ',', 'Delimiter', '=', 'delimiter', ',', 'PaginationConfig', '=', 'config', ')', 'has_results', '=', 'False', 'prefixes', '=', '[', ']', 'for', 'page', 'in', 'response', ':', 'if', ""'CommonPrefixes'"", 'in', 'page', ':', 'has_results', '=', 'True', 'for', 'p', 'in', 'page', '[', ""'CommonPrefixes'"", ']', ':', 'prefixes', '.', 'append', '(', 'p', '[', ""'Prefix'"", ']', ')', 'if', 'has_results', ':', 'return', 'prefixes']","Lists prefixes in a bucket under prefix

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param prefix: a key prefix
        :type prefix: str
        :param delimiter: the delimiter marks key hierarchy.
        :type delimiter: str
        :param page_size: pagination size
        :type page_size: int
        :param max_items: maximum items to return
        :type max_items: int","['Lists', 'prefixes', 'in', 'a', 'bucket', 'under', 'prefix']",python,test,"['lists', 'prefixes', 'in', 'a', 'bucket', 'under', 'prefix']",lists prefixes in a bucket under prefix,"['def', 'list_prefixes', '(', 'self', ',', 'bucket_name', ',', 'prefix', '=', ""''"", ',', 'delimiter', '=', ""''"", ',', 'page_size', '=', 'none', ',', 'max_items', '=', 'none', ')', ':', 'config', '=', '{', ""'pagesize'"", ':', 'page_size', ',', ""'maxitems'"", ':', 'max_items', ',', '}', 'paginator', '=', 'self', '.', 'get_conn', '(', ')', '.', 'get_paginator', '(', ""'list_objects_v2'"", ')', 'response', '=', 'paginator', '.', 'paginate', '(', 'bucket', '=', 'bucket_name', ',', 'prefix', '=', 'prefix', ',', 'delimiter', '=', 'delimiter', ',', 'paginationconfig', '=', 'config', ')', 'has_results', '=', 'false', 'prefixes', '=', '[', ']', 'for', 'page', 'in', 'response', ':', 'if', ""'commonprefixes'"", 'in', 'page', ':', 'has_results', '=', 'true', 'for', 'p', 'in', 'page', '[', ""'commonprefixes'"", ']', ':', 'prefixes', '.', 'append', '(', 'p', '[', ""'prefix'"", ']', ')', 'if', 'has_results', ':', 'return', 'prefixes']","def list_prefixes ( self , bucket_name , prefix = '' , delimiter = '' , page_size = none , max_items = none ) : config = { 'pagesize' : page_size , 'maxitems' : max_items , } paginator = self . get_conn ( ) . get_paginator ( 'list_objects_v2' ) response = paginator . paginate ( bucket = bucket_name , prefix = prefix , delimiter = delimiter , paginationconfig = config ) has_results = false prefixes = [ ] for page in response : if 'commonprefixes' in page : has_results = true for p in page [ 'commonprefixes' ] : prefixes . append ( p [ 'prefix' ] ) if has_results : return prefixes"
380,apache/airflow,airflow/hooks/S3_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L147-L183,"def list_keys(self, bucket_name, prefix='', delimiter='',
                  page_size=None, max_items=None):
        """"""
        Lists keys in a bucket under prefix and not containing delimiter

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param prefix: a key prefix
        :type prefix: str
        :param delimiter: the delimiter marks key hierarchy.
        :type delimiter: str
        :param page_size: pagination size
        :type page_size: int
        :param max_items: maximum items to return
        :type max_items: int
        """"""
        config = {
            'PageSize': page_size,
            'MaxItems': max_items,
        }

        paginator = self.get_conn().get_paginator('list_objects_v2')
        response = paginator.paginate(Bucket=bucket_name,
                                      Prefix=prefix,
                                      Delimiter=delimiter,
                                      PaginationConfig=config)

        has_results = False
        keys = []
        for page in response:
            if 'Contents' in page:
                has_results = True
                for k in page['Contents']:
                    keys.append(k['Key'])

        if has_results:
            return keys","['def', 'list_keys', '(', 'self', ',', 'bucket_name', ',', 'prefix', '=', ""''"", ',', 'delimiter', '=', ""''"", ',', 'page_size', '=', 'None', ',', 'max_items', '=', 'None', ')', ':', 'config', '=', '{', ""'PageSize'"", ':', 'page_size', ',', ""'MaxItems'"", ':', 'max_items', ',', '}', 'paginator', '=', 'self', '.', 'get_conn', '(', ')', '.', 'get_paginator', '(', ""'list_objects_v2'"", ')', 'response', '=', 'paginator', '.', 'paginate', '(', 'Bucket', '=', 'bucket_name', ',', 'Prefix', '=', 'prefix', ',', 'Delimiter', '=', 'delimiter', ',', 'PaginationConfig', '=', 'config', ')', 'has_results', '=', 'False', 'keys', '=', '[', ']', 'for', 'page', 'in', 'response', ':', 'if', ""'Contents'"", 'in', 'page', ':', 'has_results', '=', 'True', 'for', 'k', 'in', 'page', '[', ""'Contents'"", ']', ':', 'keys', '.', 'append', '(', 'k', '[', ""'Key'"", ']', ')', 'if', 'has_results', ':', 'return', 'keys']","Lists keys in a bucket under prefix and not containing delimiter

        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param prefix: a key prefix
        :type prefix: str
        :param delimiter: the delimiter marks key hierarchy.
        :type delimiter: str
        :param page_size: pagination size
        :type page_size: int
        :param max_items: maximum items to return
        :type max_items: int","['Lists', 'keys', 'in', 'a', 'bucket', 'under', 'prefix', 'and', 'not', 'containing', 'delimiter']",python,test,"['lists', 'keys', 'in', 'a', 'bucket', 'under', 'prefix', 'and', 'not', 'containing', 'delimiter']",lists keys in a bucket under prefix and not containing delimiter,"['def', 'list_keys', '(', 'self', ',', 'bucket_name', ',', 'prefix', '=', ""''"", ',', 'delimiter', '=', ""''"", ',', 'page_size', '=', 'none', ',', 'max_items', '=', 'none', ')', ':', 'config', '=', '{', ""'pagesize'"", ':', 'page_size', ',', ""'maxitems'"", ':', 'max_items', ',', '}', 'paginator', '=', 'self', '.', 'get_conn', '(', ')', '.', 'get_paginator', '(', ""'list_objects_v2'"", ')', 'response', '=', 'paginator', '.', 'paginate', '(', 'bucket', '=', 'bucket_name', ',', 'prefix', '=', 'prefix', ',', 'delimiter', '=', 'delimiter', ',', 'paginationconfig', '=', 'config', ')', 'has_results', '=', 'false', 'keys', '=', '[', ']', 'for', 'page', 'in', 'response', ':', 'if', ""'contents'"", 'in', 'page', ':', 'has_results', '=', 'true', 'for', 'k', 'in', 'page', '[', ""'contents'"", ']', ':', 'keys', '.', 'append', '(', 'k', '[', ""'key'"", ']', ')', 'if', 'has_results', ':', 'return', 'keys']","def list_keys ( self , bucket_name , prefix = '' , delimiter = '' , page_size = none , max_items = none ) : config = { 'pagesize' : page_size , 'maxitems' : max_items , } paginator = self . get_conn ( ) . get_paginator ( 'list_objects_v2' ) response = paginator . paginate ( bucket = bucket_name , prefix = prefix , delimiter = delimiter , paginationconfig = config ) has_results = false keys = [ ] for page in response : if 'contents' in page : has_results = true for k in page [ 'contents' ] : keys . append ( k [ 'key' ] ) if has_results : return keys"
381,apache/airflow,airflow/hooks/S3_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L185-L202,"def check_for_key(self, key, bucket_name=None):
        """"""
        Checks if a key exists in a bucket

        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which the file is stored
        :type bucket_name: str
        """"""
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)

        try:
            self.get_conn().head_object(Bucket=bucket_name, Key=key)
            return True
        except ClientError as e:
            self.log.info(e.response[""Error""][""Message""])
            return False","['def', 'check_for_key', '(', 'self', ',', 'key', ',', 'bucket_name', '=', 'None', ')', ':', 'if', 'not', 'bucket_name', ':', '(', 'bucket_name', ',', 'key', ')', '=', 'self', '.', 'parse_s3_url', '(', 'key', ')', 'try', ':', 'self', '.', 'get_conn', '(', ')', '.', 'head_object', '(', 'Bucket', '=', 'bucket_name', ',', 'Key', '=', 'key', ')', 'return', 'True', 'except', 'ClientError', 'as', 'e', ':', 'self', '.', 'log', '.', 'info', '(', 'e', '.', 'response', '[', '""Error""', ']', '[', '""Message""', ']', ')', 'return', 'False']","Checks if a key exists in a bucket

        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which the file is stored
        :type bucket_name: str","['Checks', 'if', 'a', 'key', 'exists', 'in', 'a', 'bucket']",python,test,"['checks', 'if', 'a', 'key', 'exists', 'in', 'a', 'bucket']",checks if a key exists in a bucket,"['def', 'check_for_key', '(', 'self', ',', 'key', ',', 'bucket_name', '=', 'none', ')', ':', 'if', 'not', 'bucket_name', ':', '(', 'bucket_name', ',', 'key', ')', '=', 'self', '.', 'parse_s3_url', '(', 'key', ')', 'try', ':', 'self', '.', 'get_conn', '(', ')', '.', 'head_object', '(', 'bucket', '=', 'bucket_name', ',', 'key', '=', 'key', ')', 'return', 'true', 'except', 'clienterror', 'as', 'e', ':', 'self', '.', 'log', '.', 'info', '(', 'e', '.', 'response', '[', '""error""', ']', '[', '""message""', ']', ')', 'return', 'false']","def check_for_key ( self , key , bucket_name = none ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) try : self . get_conn ( ) . head_object ( bucket = bucket_name , key = key ) return true except clienterror as e : self . log . info ( e . response [ ""error"" ] [ ""message"" ] ) return false"
382,apache/airflow,airflow/hooks/S3_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L204-L218,"def get_key(self, key, bucket_name=None):
        """"""
        Returns a boto3.s3.Object

        :param key: the path to the key
        :type key: str
        :param bucket_name: the name of the bucket
        :type bucket_name: str
        """"""
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)

        obj = self.get_resource_type('s3').Object(bucket_name, key)
        obj.load()
        return obj","['def', 'get_key', '(', 'self', ',', 'key', ',', 'bucket_name', '=', 'None', ')', ':', 'if', 'not', 'bucket_name', ':', '(', 'bucket_name', ',', 'key', ')', '=', 'self', '.', 'parse_s3_url', '(', 'key', ')', 'obj', '=', 'self', '.', 'get_resource_type', '(', ""'s3'"", ')', '.', 'Object', '(', 'bucket_name', ',', 'key', ')', 'obj', '.', 'load', '(', ')', 'return', 'obj']","Returns a boto3.s3.Object

        :param key: the path to the key
        :type key: str
        :param bucket_name: the name of the bucket
        :type bucket_name: str","['Returns', 'a', 'boto3', '.', 's3', '.', 'Object']",python,test,"['returns', 'a', 'boto3', '.', 's3', '.', 'object']",returns a boto3 . s3 . object,"['def', 'get_key', '(', 'self', ',', 'key', ',', 'bucket_name', '=', 'none', ')', ':', 'if', 'not', 'bucket_name', ':', '(', 'bucket_name', ',', 'key', ')', '=', 'self', '.', 'parse_s3_url', '(', 'key', ')', 'obj', '=', 'self', '.', 'get_resource_type', '(', ""'s3'"", ')', '.', 'object', '(', 'bucket_name', ',', 'key', ')', 'obj', '.', 'load', '(', ')', 'return', 'obj']","def get_key ( self , key , bucket_name = none ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) obj = self . get_resource_type ( 's3' ) . object ( bucket_name , key ) obj . load ( ) return obj"
383,apache/airflow,airflow/hooks/S3_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L220-L231,"def read_key(self, key, bucket_name=None):
        """"""
        Reads a key from S3

        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which the file is stored
        :type bucket_name: str
        """"""

        obj = self.get_key(key, bucket_name)
        return obj.get()['Body'].read().decode('utf-8')","['def', 'read_key', '(', 'self', ',', 'key', ',', 'bucket_name', '=', 'None', ')', ':', 'obj', '=', 'self', '.', 'get_key', '(', 'key', ',', 'bucket_name', ')', 'return', 'obj', '.', 'get', '(', ')', '[', ""'Body'"", ']', '.', 'read', '(', ')', '.', 'decode', '(', ""'utf-8'"", ')']","Reads a key from S3

        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which the file is stored
        :type bucket_name: str","['Reads', 'a', 'key', 'from', 'S3']",python,test,"['reads', 'a', 'key', 'from', 's3']",reads a key from s3,"['def', 'read_key', '(', 'self', ',', 'key', ',', 'bucket_name', '=', 'none', ')', ':', 'obj', '=', 'self', '.', 'get_key', '(', 'key', ',', 'bucket_name', ')', 'return', 'obj', '.', 'get', '(', ')', '[', ""'body'"", ']', '.', 'read', '(', ')', '.', 'decode', '(', ""'utf-8'"", ')']","def read_key ( self , key , bucket_name = none ) : obj = self . get_key ( key , bucket_name ) return obj . get ( ) [ 'body' ] . read ( ) . decode ( 'utf-8' )"
384,apache/airflow,airflow/hooks/S3_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L233-L277,"def select_key(self, key, bucket_name=None,
                   expression='SELECT * FROM S3Object',
                   expression_type='SQL',
                   input_serialization=None,
                   output_serialization=None):
        """"""
        Reads a key with S3 Select.

        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which the file is stored
        :type bucket_name: str
        :param expression: S3 Select expression
        :type expression: str
        :param expression_type: S3 Select expression type
        :type expression_type: str
        :param input_serialization: S3 Select input data serialization format
        :type input_serialization: dict
        :param output_serialization: S3 Select output data serialization format
        :type output_serialization: dict
        :return: retrieved subset of original data by S3 Select
        :rtype: str

        .. seealso::
            For more details about S3 Select parameters:
            http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.select_object_content
        """"""
        if input_serialization is None:
            input_serialization = {'CSV': {}}
        if output_serialization is None:
            output_serialization = {'CSV': {}}
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)

        response = self.get_conn().select_object_content(
            Bucket=bucket_name,
            Key=key,
            Expression=expression,
            ExpressionType=expression_type,
            InputSerialization=input_serialization,
            OutputSerialization=output_serialization)

        return ''.join(event['Records']['Payload'].decode('utf-8')
                       for event in response['Payload']
                       if 'Records' in event)","['def', 'select_key', '(', 'self', ',', 'key', ',', 'bucket_name', '=', 'None', ',', 'expression', '=', ""'SELECT * FROM S3Object'"", ',', 'expression_type', '=', ""'SQL'"", ',', 'input_serialization', '=', 'None', ',', 'output_serialization', '=', 'None', ')', ':', 'if', 'input_serialization', 'is', 'None', ':', 'input_serialization', '=', '{', ""'CSV'"", ':', '{', '}', '}', 'if', 'output_serialization', 'is', 'None', ':', 'output_serialization', '=', '{', ""'CSV'"", ':', '{', '}', '}', 'if', 'not', 'bucket_name', ':', '(', 'bucket_name', ',', 'key', ')', '=', 'self', '.', 'parse_s3_url', '(', 'key', ')', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'select_object_content', '(', 'Bucket', '=', 'bucket_name', ',', 'Key', '=', 'key', ',', 'Expression', '=', 'expression', ',', 'ExpressionType', '=', 'expression_type', ',', 'InputSerialization', '=', 'input_serialization', ',', 'OutputSerialization', '=', 'output_serialization', ')', 'return', ""''"", '.', 'join', '(', 'event', '[', ""'Records'"", ']', '[', ""'Payload'"", ']', '.', 'decode', '(', ""'utf-8'"", ')', 'for', 'event', 'in', 'response', '[', ""'Payload'"", ']', 'if', ""'Records'"", 'in', 'event', ')']","Reads a key with S3 Select.

        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which the file is stored
        :type bucket_name: str
        :param expression: S3 Select expression
        :type expression: str
        :param expression_type: S3 Select expression type
        :type expression_type: str
        :param input_serialization: S3 Select input data serialization format
        :type input_serialization: dict
        :param output_serialization: S3 Select output data serialization format
        :type output_serialization: dict
        :return: retrieved subset of original data by S3 Select
        :rtype: str

        .. seealso::
            For more details about S3 Select parameters:
            http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.select_object_content","['Reads', 'a', 'key', 'with', 'S3', 'Select', '.']",python,test,"['reads', 'a', 'key', 'with', 's3', 'select', '.']",reads a key with s3 select .,"['def', 'select_key', '(', 'self', ',', 'key', ',', 'bucket_name', '=', 'none', ',', 'expression', '=', ""'select * from s3object'"", ',', 'expression_type', '=', ""'sql'"", ',', 'input_serialization', '=', 'none', ',', 'output_serialization', '=', 'none', ')', ':', 'if', 'input_serialization', 'is', 'none', ':', 'input_serialization', '=', '{', ""'csv'"", ':', '{', '}', '}', 'if', 'output_serialization', 'is', 'none', ':', 'output_serialization', '=', '{', ""'csv'"", ':', '{', '}', '}', 'if', 'not', 'bucket_name', ':', '(', 'bucket_name', ',', 'key', ')', '=', 'self', '.', 'parse_s3_url', '(', 'key', ')', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'select_object_content', '(', 'bucket', '=', 'bucket_name', ',', 'key', '=', 'key', ',', 'expression', '=', 'expression', ',', 'expressiontype', '=', 'expression_type', ',', 'inputserialization', '=', 'input_serialization', ',', 'outputserialization', '=', 'output_serialization', ')', 'return', ""''"", '.', 'join', '(', 'event', '[', ""'records'"", ']', '[', ""'payload'"", ']', '.', 'decode', '(', ""'utf-8'"", ')', 'for', 'event', 'in', 'response', '[', ""'payload'"", ']', 'if', ""'records'"", 'in', 'event', ')']","def select_key ( self , key , bucket_name = none , expression = 'select * from s3object' , expression_type = 'sql' , input_serialization = none , output_serialization = none ) : if input_serialization is none : input_serialization = { 'csv' : { } } if output_serialization is none : output_serialization = { 'csv' : { } } if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) response = self . get_conn ( ) . select_object_content ( bucket = bucket_name , key = key , expression = expression , expressiontype = expression_type , inputserialization = input_serialization , outputserialization = output_serialization ) return '' . join ( event [ 'records' ] [ 'payload' ] . decode ( 'utf-8' ) for event in response [ 'payload' ] if 'records' in event )"
385,apache/airflow,airflow/hooks/S3_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L279-L293,"def check_for_wildcard_key(self,
                               wildcard_key, bucket_name=None, delimiter=''):
        """"""
        Checks that a key matching a wildcard expression exists in a bucket

        :param wildcard_key: the path to the key
        :type wildcard_key: str
        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param delimiter: the delimiter marks key hierarchy
        :type delimiter: str
        """"""
        return self.get_wildcard_key(wildcard_key=wildcard_key,
                                     bucket_name=bucket_name,
                                     delimiter=delimiter) is not None","['def', 'check_for_wildcard_key', '(', 'self', ',', 'wildcard_key', ',', 'bucket_name', '=', 'None', ',', 'delimiter', '=', ""''"", ')', ':', 'return', 'self', '.', 'get_wildcard_key', '(', 'wildcard_key', '=', 'wildcard_key', ',', 'bucket_name', '=', 'bucket_name', ',', 'delimiter', '=', 'delimiter', ')', 'is', 'not', 'None']","Checks that a key matching a wildcard expression exists in a bucket

        :param wildcard_key: the path to the key
        :type wildcard_key: str
        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param delimiter: the delimiter marks key hierarchy
        :type delimiter: str","['Checks', 'that', 'a', 'key', 'matching', 'a', 'wildcard', 'expression', 'exists', 'in', 'a', 'bucket']",python,test,"['checks', 'that', 'a', 'key', 'matching', 'a', 'wildcard', 'expression', 'exists', 'in', 'a', 'bucket']",checks that a key matching a wildcard expression exists in a bucket,"['def', 'check_for_wildcard_key', '(', 'self', ',', 'wildcard_key', ',', 'bucket_name', '=', 'none', ',', 'delimiter', '=', ""''"", ')', ':', 'return', 'self', '.', 'get_wildcard_key', '(', 'wildcard_key', '=', 'wildcard_key', ',', 'bucket_name', '=', 'bucket_name', ',', 'delimiter', '=', 'delimiter', ')', 'is', 'not', 'none']","def check_for_wildcard_key ( self , wildcard_key , bucket_name = none , delimiter = '' ) : return self . get_wildcard_key ( wildcard_key = wildcard_key , bucket_name = bucket_name , delimiter = delimiter ) is not none"
386,apache/airflow,airflow/hooks/S3_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L295-L314,"def get_wildcard_key(self, wildcard_key, bucket_name=None, delimiter=''):
        """"""
        Returns a boto3.s3.Object object matching the wildcard expression

        :param wildcard_key: the path to the key
        :type wildcard_key: str
        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param delimiter: the delimiter marks key hierarchy
        :type delimiter: str
        """"""
        if not bucket_name:
            (bucket_name, wildcard_key) = self.parse_s3_url(wildcard_key)

        prefix = re.split(r'[*]', wildcard_key, 1)[0]
        klist = self.list_keys(bucket_name, prefix=prefix, delimiter=delimiter)
        if klist:
            key_matches = [k for k in klist if fnmatch.fnmatch(k, wildcard_key)]
            if key_matches:
                return self.get_key(key_matches[0], bucket_name)","['def', 'get_wildcard_key', '(', 'self', ',', 'wildcard_key', ',', 'bucket_name', '=', 'None', ',', 'delimiter', '=', ""''"", ')', ':', 'if', 'not', 'bucket_name', ':', '(', 'bucket_name', ',', 'wildcard_key', ')', '=', 'self', '.', 'parse_s3_url', '(', 'wildcard_key', ')', 'prefix', '=', 're', '.', 'split', '(', ""r'[*]'"", ',', 'wildcard_key', ',', '1', ')', '[', '0', ']', 'klist', '=', 'self', '.', 'list_keys', '(', 'bucket_name', ',', 'prefix', '=', 'prefix', ',', 'delimiter', '=', 'delimiter', ')', 'if', 'klist', ':', 'key_matches', '=', '[', 'k', 'for', 'k', 'in', 'klist', 'if', 'fnmatch', '.', 'fnmatch', '(', 'k', ',', 'wildcard_key', ')', ']', 'if', 'key_matches', ':', 'return', 'self', '.', 'get_key', '(', 'key_matches', '[', '0', ']', ',', 'bucket_name', ')']","Returns a boto3.s3.Object object matching the wildcard expression

        :param wildcard_key: the path to the key
        :type wildcard_key: str
        :param bucket_name: the name of the bucket
        :type bucket_name: str
        :param delimiter: the delimiter marks key hierarchy
        :type delimiter: str","['Returns', 'a', 'boto3', '.', 's3', '.', 'Object', 'object', 'matching', 'the', 'wildcard', 'expression']",python,test,"['returns', 'a', 'boto3', '.', 's3', '.', 'object', 'object', 'matching', 'the', 'wildcard', 'expression']",returns a boto3 . s3 . object object matching the wildcard expression,"['def', 'get_wildcard_key', '(', 'self', ',', 'wildcard_key', ',', 'bucket_name', '=', 'none', ',', 'delimiter', '=', ""''"", ')', ':', 'if', 'not', 'bucket_name', ':', '(', 'bucket_name', ',', 'wildcard_key', ')', '=', 'self', '.', 'parse_s3_url', '(', 'wildcard_key', ')', 'prefix', '=', 're', '.', 'split', '(', ""r'[*]'"", ',', 'wildcard_key', ',', '1', ')', '[', '0', ']', 'klist', '=', 'self', '.', 'list_keys', '(', 'bucket_name', ',', 'prefix', '=', 'prefix', ',', 'delimiter', '=', 'delimiter', ')', 'if', 'klist', ':', 'key_matches', '=', '[', 'k', 'for', 'k', 'in', 'klist', 'if', 'fnmatch', '.', 'fnmatch', '(', 'k', ',', 'wildcard_key', ')', ']', 'if', 'key_matches', ':', 'return', 'self', '.', 'get_key', '(', 'key_matches', '[', '0', ']', ',', 'bucket_name', ')']","def get_wildcard_key ( self , wildcard_key , bucket_name = none , delimiter = '' ) : if not bucket_name : ( bucket_name , wildcard_key ) = self . parse_s3_url ( wildcard_key ) prefix = re . split ( r'[*]' , wildcard_key , 1 ) [ 0 ] klist = self . list_keys ( bucket_name , prefix = prefix , delimiter = delimiter ) if klist : key_matches = [ k for k in klist if fnmatch . fnmatch ( k , wildcard_key ) ] if key_matches : return self . get_key ( key_matches [ 0 ] , bucket_name )"
387,apache/airflow,airflow/hooks/S3_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L316-L350,"def load_file(self,
                  filename,
                  key,
                  bucket_name=None,
                  replace=False,
                  encrypt=False):
        """"""
        Loads a local file to S3

        :param filename: name of the file to load.
        :type filename: str
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag to decide whether or not to overwrite the key
            if it already exists. If replace is False and the key exists, an
            error will be raised.
        :type replace: bool
        :param encrypt: If True, the file will be encrypted on the server-side
            by S3 and will be stored in an encrypted form while at rest in S3.
        :type encrypt: bool
        """"""
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)

        if not replace and self.check_for_key(key, bucket_name):
            raise ValueError(""The key {key} already exists."".format(key=key))

        extra_args = {}
        if encrypt:
            extra_args['ServerSideEncryption'] = ""AES256""

        client = self.get_conn()
        client.upload_file(filename, bucket_name, key, ExtraArgs=extra_args)","['def', 'load_file', '(', 'self', ',', 'filename', ',', 'key', ',', 'bucket_name', '=', 'None', ',', 'replace', '=', 'False', ',', 'encrypt', '=', 'False', ')', ':', 'if', 'not', 'bucket_name', ':', '(', 'bucket_name', ',', 'key', ')', '=', 'self', '.', 'parse_s3_url', '(', 'key', ')', 'if', 'not', 'replace', 'and', 'self', '.', 'check_for_key', '(', 'key', ',', 'bucket_name', ')', ':', 'raise', 'ValueError', '(', '""The key {key} already exists.""', '.', 'format', '(', 'key', '=', 'key', ')', ')', 'extra_args', '=', '{', '}', 'if', 'encrypt', ':', 'extra_args', '[', ""'ServerSideEncryption'"", ']', '=', '""AES256""', 'client', '=', 'self', '.', 'get_conn', '(', ')', 'client', '.', 'upload_file', '(', 'filename', ',', 'bucket_name', ',', 'key', ',', 'ExtraArgs', '=', 'extra_args', ')']","Loads a local file to S3

        :param filename: name of the file to load.
        :type filename: str
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag to decide whether or not to overwrite the key
            if it already exists. If replace is False and the key exists, an
            error will be raised.
        :type replace: bool
        :param encrypt: If True, the file will be encrypted on the server-side
            by S3 and will be stored in an encrypted form while at rest in S3.
        :type encrypt: bool","['Loads', 'a', 'local', 'file', 'to', 'S3']",python,test,"['loads', 'a', 'local', 'file', 'to', 's3']",loads a local file to s3,"['def', 'load_file', '(', 'self', ',', 'filename', ',', 'key', ',', 'bucket_name', '=', 'none', ',', 'replace', '=', 'false', ',', 'encrypt', '=', 'false', ')', ':', 'if', 'not', 'bucket_name', ':', '(', 'bucket_name', ',', 'key', ')', '=', 'self', '.', 'parse_s3_url', '(', 'key', ')', 'if', 'not', 'replace', 'and', 'self', '.', 'check_for_key', '(', 'key', ',', 'bucket_name', ')', ':', 'raise', 'valueerror', '(', '""the key {key} already exists.""', '.', 'format', '(', 'key', '=', 'key', ')', ')', 'extra_args', '=', '{', '}', 'if', 'encrypt', ':', 'extra_args', '[', ""'serversideencryption'"", ']', '=', '""aes256""', 'client', '=', 'self', '.', 'get_conn', '(', ')', 'client', '.', 'upload_file', '(', 'filename', ',', 'bucket_name', ',', 'key', ',', 'extraargs', '=', 'extra_args', ')']","def load_file ( self , filename , key , bucket_name = none , replace = false , encrypt = false ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) if not replace and self . check_for_key ( key , bucket_name ) : raise valueerror ( ""the key {key} already exists."" . format ( key = key ) ) extra_args = { } if encrypt : extra_args [ 'serversideencryption' ] = ""aes256"" client = self . get_conn ( ) client . upload_file ( filename , bucket_name , key , extraargs = extra_args )"
388,apache/airflow,airflow/hooks/S3_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L352-L382,"def load_string(self,
                    string_data,
                    key,
                    bucket_name=None,
                    replace=False,
                    encrypt=False,
                    encoding='utf-8'):
        """"""
        Loads a string to S3

        This is provided as a convenience to drop a string in S3. It uses the
        boto infrastructure to ship a file to s3.

        :param string_data: str to set as content for the key.
        :type string_data: str
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag to decide whether or not to overwrite the key
            if it already exists
        :type replace: bool
        :param encrypt: If True, the file will be encrypted on the server-side
            by S3 and will be stored in an encrypted form while at rest in S3.
        :type encrypt: bool
        """"""
        self.load_bytes(string_data.encode(encoding),
                        key=key,
                        bucket_name=bucket_name,
                        replace=replace,
                        encrypt=encrypt)","['def', 'load_string', '(', 'self', ',', 'string_data', ',', 'key', ',', 'bucket_name', '=', 'None', ',', 'replace', '=', 'False', ',', 'encrypt', '=', 'False', ',', 'encoding', '=', ""'utf-8'"", ')', ':', 'self', '.', 'load_bytes', '(', 'string_data', '.', 'encode', '(', 'encoding', ')', ',', 'key', '=', 'key', ',', 'bucket_name', '=', 'bucket_name', ',', 'replace', '=', 'replace', ',', 'encrypt', '=', 'encrypt', ')']","Loads a string to S3

        This is provided as a convenience to drop a string in S3. It uses the
        boto infrastructure to ship a file to s3.

        :param string_data: str to set as content for the key.
        :type string_data: str
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag to decide whether or not to overwrite the key
            if it already exists
        :type replace: bool
        :param encrypt: If True, the file will be encrypted on the server-side
            by S3 and will be stored in an encrypted form while at rest in S3.
        :type encrypt: bool","['Loads', 'a', 'string', 'to', 'S3']",python,test,"['loads', 'a', 'string', 'to', 's3']",loads a string to s3,"['def', 'load_string', '(', 'self', ',', 'string_data', ',', 'key', ',', 'bucket_name', '=', 'none', ',', 'replace', '=', 'false', ',', 'encrypt', '=', 'false', ',', 'encoding', '=', ""'utf-8'"", ')', ':', 'self', '.', 'load_bytes', '(', 'string_data', '.', 'encode', '(', 'encoding', ')', ',', 'key', '=', 'key', ',', 'bucket_name', '=', 'bucket_name', ',', 'replace', '=', 'replace', ',', 'encrypt', '=', 'encrypt', ')']","def load_string ( self , string_data , key , bucket_name = none , replace = false , encrypt = false , encoding = 'utf-8' ) : self . load_bytes ( string_data . encode ( encoding ) , key = key , bucket_name = bucket_name , replace = replace , encrypt = encrypt )"
389,apache/airflow,airflow/hooks/S3_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L384-L422,"def load_bytes(self,
                   bytes_data,
                   key,
                   bucket_name=None,
                   replace=False,
                   encrypt=False):
        """"""
        Loads bytes to S3

        This is provided as a convenience to drop a string in S3. It uses the
        boto infrastructure to ship a file to s3.

        :param bytes_data: bytes to set as content for the key.
        :type bytes_data: bytes
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag to decide whether or not to overwrite the key
            if it already exists
        :type replace: bool
        :param encrypt: If True, the file will be encrypted on the server-side
            by S3 and will be stored in an encrypted form while at rest in S3.
        :type encrypt: bool
        """"""
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)

        if not replace and self.check_for_key(key, bucket_name):
            raise ValueError(""The key {key} already exists."".format(key=key))

        extra_args = {}
        if encrypt:
            extra_args['ServerSideEncryption'] = ""AES256""

        filelike_buffer = BytesIO(bytes_data)

        client = self.get_conn()
        client.upload_fileobj(filelike_buffer, bucket_name, key, ExtraArgs=extra_args)","['def', 'load_bytes', '(', 'self', ',', 'bytes_data', ',', 'key', ',', 'bucket_name', '=', 'None', ',', 'replace', '=', 'False', ',', 'encrypt', '=', 'False', ')', ':', 'if', 'not', 'bucket_name', ':', '(', 'bucket_name', ',', 'key', ')', '=', 'self', '.', 'parse_s3_url', '(', 'key', ')', 'if', 'not', 'replace', 'and', 'self', '.', 'check_for_key', '(', 'key', ',', 'bucket_name', ')', ':', 'raise', 'ValueError', '(', '""The key {key} already exists.""', '.', 'format', '(', 'key', '=', 'key', ')', ')', 'extra_args', '=', '{', '}', 'if', 'encrypt', ':', 'extra_args', '[', ""'ServerSideEncryption'"", ']', '=', '""AES256""', 'filelike_buffer', '=', 'BytesIO', '(', 'bytes_data', ')', 'client', '=', 'self', '.', 'get_conn', '(', ')', 'client', '.', 'upload_fileobj', '(', 'filelike_buffer', ',', 'bucket_name', ',', 'key', ',', 'ExtraArgs', '=', 'extra_args', ')']","Loads bytes to S3

        This is provided as a convenience to drop a string in S3. It uses the
        boto infrastructure to ship a file to s3.

        :param bytes_data: bytes to set as content for the key.
        :type bytes_data: bytes
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag to decide whether or not to overwrite the key
            if it already exists
        :type replace: bool
        :param encrypt: If True, the file will be encrypted on the server-side
            by S3 and will be stored in an encrypted form while at rest in S3.
        :type encrypt: bool","['Loads', 'bytes', 'to', 'S3']",python,test,"['loads', 'bytes', 'to', 's3']",loads bytes to s3,"['def', 'load_bytes', '(', 'self', ',', 'bytes_data', ',', 'key', ',', 'bucket_name', '=', 'none', ',', 'replace', '=', 'false', ',', 'encrypt', '=', 'false', ')', ':', 'if', 'not', 'bucket_name', ':', '(', 'bucket_name', ',', 'key', ')', '=', 'self', '.', 'parse_s3_url', '(', 'key', ')', 'if', 'not', 'replace', 'and', 'self', '.', 'check_for_key', '(', 'key', ',', 'bucket_name', ')', ':', 'raise', 'valueerror', '(', '""the key {key} already exists.""', '.', 'format', '(', 'key', '=', 'key', ')', ')', 'extra_args', '=', '{', '}', 'if', 'encrypt', ':', 'extra_args', '[', ""'serversideencryption'"", ']', '=', '""aes256""', 'filelike_buffer', '=', 'bytesio', '(', 'bytes_data', ')', 'client', '=', 'self', '.', 'get_conn', '(', ')', 'client', '.', 'upload_fileobj', '(', 'filelike_buffer', ',', 'bucket_name', ',', 'key', ',', 'extraargs', '=', 'extra_args', ')']","def load_bytes ( self , bytes_data , key , bucket_name = none , replace = false , encrypt = false ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) if not replace and self . check_for_key ( key , bucket_name ) : raise valueerror ( ""the key {key} already exists."" . format ( key = key ) ) extra_args = { } if encrypt : extra_args [ 'serversideencryption' ] = ""aes256"" filelike_buffer = bytesio ( bytes_data ) client = self . get_conn ( ) client . upload_fileobj ( filelike_buffer , bucket_name , key , extraargs = extra_args )"
390,apache/airflow,airflow/hooks/S3_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L424-L457,"def load_file_obj(self,
                      file_obj,
                      key,
                      bucket_name=None,
                      replace=False,
                      encrypt=False):
        """"""
        Loads a file object to S3

        :param file_obj: The file-like object to set as the content for the S3 key.
        :type file_obj: file-like object
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag that indicates whether to overwrite the key
            if it already exists.
        :type replace: bool
        :param encrypt: If True, S3 encrypts the file on the server,
            and the file is stored in encrypted form at rest in S3.
        :type encrypt: bool
        """"""
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)

        if not replace and self.check_for_key(key, bucket_name):
            raise ValueError(""The key {key} already exists."".format(key=key))

        extra_args = {}
        if encrypt:
            extra_args['ServerSideEncryption'] = ""AES256""

        client = self.get_conn()
        client.upload_fileobj(file_obj, bucket_name, key, ExtraArgs=extra_args)","['def', 'load_file_obj', '(', 'self', ',', 'file_obj', ',', 'key', ',', 'bucket_name', '=', 'None', ',', 'replace', '=', 'False', ',', 'encrypt', '=', 'False', ')', ':', 'if', 'not', 'bucket_name', ':', '(', 'bucket_name', ',', 'key', ')', '=', 'self', '.', 'parse_s3_url', '(', 'key', ')', 'if', 'not', 'replace', 'and', 'self', '.', 'check_for_key', '(', 'key', ',', 'bucket_name', ')', ':', 'raise', 'ValueError', '(', '""The key {key} already exists.""', '.', 'format', '(', 'key', '=', 'key', ')', ')', 'extra_args', '=', '{', '}', 'if', 'encrypt', ':', 'extra_args', '[', ""'ServerSideEncryption'"", ']', '=', '""AES256""', 'client', '=', 'self', '.', 'get_conn', '(', ')', 'client', '.', 'upload_fileobj', '(', 'file_obj', ',', 'bucket_name', ',', 'key', ',', 'ExtraArgs', '=', 'extra_args', ')']","Loads a file object to S3

        :param file_obj: The file-like object to set as the content for the S3 key.
        :type file_obj: file-like object
        :param key: S3 key that will point to the file
        :type key: str
        :param bucket_name: Name of the bucket in which to store the file
        :type bucket_name: str
        :param replace: A flag that indicates whether to overwrite the key
            if it already exists.
        :type replace: bool
        :param encrypt: If True, S3 encrypts the file on the server,
            and the file is stored in encrypted form at rest in S3.
        :type encrypt: bool","['Loads', 'a', 'file', 'object', 'to', 'S3']",python,test,"['loads', 'a', 'file', 'object', 'to', 's3']",loads a file object to s3,"['def', 'load_file_obj', '(', 'self', ',', 'file_obj', ',', 'key', ',', 'bucket_name', '=', 'none', ',', 'replace', '=', 'false', ',', 'encrypt', '=', 'false', ')', ':', 'if', 'not', 'bucket_name', ':', '(', 'bucket_name', ',', 'key', ')', '=', 'self', '.', 'parse_s3_url', '(', 'key', ')', 'if', 'not', 'replace', 'and', 'self', '.', 'check_for_key', '(', 'key', ',', 'bucket_name', ')', ':', 'raise', 'valueerror', '(', '""the key {key} already exists.""', '.', 'format', '(', 'key', '=', 'key', ')', ')', 'extra_args', '=', '{', '}', 'if', 'encrypt', ':', 'extra_args', '[', ""'serversideencryption'"", ']', '=', '""aes256""', 'client', '=', 'self', '.', 'get_conn', '(', ')', 'client', '.', 'upload_fileobj', '(', 'file_obj', ',', 'bucket_name', ',', 'key', ',', 'extraargs', '=', 'extra_args', ')']","def load_file_obj ( self , file_obj , key , bucket_name = none , replace = false , encrypt = false ) : if not bucket_name : ( bucket_name , key ) = self . parse_s3_url ( key ) if not replace and self . check_for_key ( key , bucket_name ) : raise valueerror ( ""the key {key} already exists."" . format ( key = key ) ) extra_args = { } if encrypt : extra_args [ 'serversideencryption' ] = ""aes256"" client = self . get_conn ( ) client . upload_fileobj ( file_obj , bucket_name , key , extraargs = extra_args )"
391,apache/airflow,airflow/hooks/S3_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L459-L518,"def copy_object(self,
                    source_bucket_key,
                    dest_bucket_key,
                    source_bucket_name=None,
                    dest_bucket_name=None,
                    source_version_id=None):
        """"""
        Creates a copy of an object that is already stored in S3.

        Note: the S3 connection used here needs to have access to both
        source and destination bucket/key.

        :param source_bucket_key: The key of the source object.

            It can be either full s3:// style url or relative path from root level.

            When it's specified as a full s3:// url, please omit source_bucket_name.
        :type source_bucket_key: str
        :param dest_bucket_key: The key of the object to copy to.

            The convention to specify `dest_bucket_key` is the same
            as `source_bucket_key`.
        :type dest_bucket_key: str
        :param source_bucket_name: Name of the S3 bucket where the source object is in.

            It should be omitted when `source_bucket_key` is provided as a full s3:// url.
        :type source_bucket_name: str
        :param dest_bucket_name: Name of the S3 bucket to where the object is copied.

            It should be omitted when `dest_bucket_key` is provided as a full s3:// url.
        :type dest_bucket_name: str
        :param source_version_id: Version ID of the source object (OPTIONAL)
        :type source_version_id: str
        """"""

        if dest_bucket_name is None:
            dest_bucket_name, dest_bucket_key = self.parse_s3_url(dest_bucket_key)
        else:
            parsed_url = urlparse(dest_bucket_key)
            if parsed_url.scheme != '' or parsed_url.netloc != '':
                raise AirflowException('If dest_bucket_name is provided, ' +
                                       'dest_bucket_key should be relative path ' +
                                       'from root level, rather than a full s3:// url')

        if source_bucket_name is None:
            source_bucket_name, source_bucket_key = self.parse_s3_url(source_bucket_key)
        else:
            parsed_url = urlparse(source_bucket_key)
            if parsed_url.scheme != '' or parsed_url.netloc != '':
                raise AirflowException('If source_bucket_name is provided, ' +
                                       'source_bucket_key should be relative path ' +
                                       'from root level, rather than a full s3:// url')

        CopySource = {'Bucket': source_bucket_name,
                      'Key': source_bucket_key,
                      'VersionId': source_version_id}
        response = self.get_conn().copy_object(Bucket=dest_bucket_name,
                                               Key=dest_bucket_key,
                                               CopySource=CopySource)
        return response","['def', 'copy_object', '(', 'self', ',', 'source_bucket_key', ',', 'dest_bucket_key', ',', 'source_bucket_name', '=', 'None', ',', 'dest_bucket_name', '=', 'None', ',', 'source_version_id', '=', 'None', ')', ':', 'if', 'dest_bucket_name', 'is', 'None', ':', 'dest_bucket_name', ',', 'dest_bucket_key', '=', 'self', '.', 'parse_s3_url', '(', 'dest_bucket_key', ')', 'else', ':', 'parsed_url', '=', 'urlparse', '(', 'dest_bucket_key', ')', 'if', 'parsed_url', '.', 'scheme', '!=', ""''"", 'or', 'parsed_url', '.', 'netloc', '!=', ""''"", ':', 'raise', 'AirflowException', '(', ""'If dest_bucket_name is provided, '"", '+', ""'dest_bucket_key should be relative path '"", '+', ""'from root level, rather than a full s3:// url'"", ')', 'if', 'source_bucket_name', 'is', 'None', ':', 'source_bucket_name', ',', 'source_bucket_key', '=', 'self', '.', 'parse_s3_url', '(', 'source_bucket_key', ')', 'else', ':', 'parsed_url', '=', 'urlparse', '(', 'source_bucket_key', ')', 'if', 'parsed_url', '.', 'scheme', '!=', ""''"", 'or', 'parsed_url', '.', 'netloc', '!=', ""''"", ':', 'raise', 'AirflowException', '(', ""'If source_bucket_name is provided, '"", '+', ""'source_bucket_key should be relative path '"", '+', ""'from root level, rather than a full s3:// url'"", ')', 'CopySource', '=', '{', ""'Bucket'"", ':', 'source_bucket_name', ',', ""'Key'"", ':', 'source_bucket_key', ',', ""'VersionId'"", ':', 'source_version_id', '}', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'copy_object', '(', 'Bucket', '=', 'dest_bucket_name', ',', 'Key', '=', 'dest_bucket_key', ',', 'CopySource', '=', 'CopySource', ')', 'return', 'response']","Creates a copy of an object that is already stored in S3.

        Note: the S3 connection used here needs to have access to both
        source and destination bucket/key.

        :param source_bucket_key: The key of the source object.

            It can be either full s3:// style url or relative path from root level.

            When it's specified as a full s3:// url, please omit source_bucket_name.
        :type source_bucket_key: str
        :param dest_bucket_key: The key of the object to copy to.

            The convention to specify `dest_bucket_key` is the same
            as `source_bucket_key`.
        :type dest_bucket_key: str
        :param source_bucket_name: Name of the S3 bucket where the source object is in.

            It should be omitted when `source_bucket_key` is provided as a full s3:// url.
        :type source_bucket_name: str
        :param dest_bucket_name: Name of the S3 bucket to where the object is copied.

            It should be omitted when `dest_bucket_key` is provided as a full s3:// url.
        :type dest_bucket_name: str
        :param source_version_id: Version ID of the source object (OPTIONAL)
        :type source_version_id: str","['Creates', 'a', 'copy', 'of', 'an', 'object', 'that', 'is', 'already', 'stored', 'in', 'S3', '.']",python,test,"['creates', 'a', 'copy', 'of', 'an', 'object', 'that', 'is', 'already', 'stored', 'in', 's3', '.']",creates a copy of an object that is already stored in s3 .,"['def', 'copy_object', '(', 'self', ',', 'source_bucket_key', ',', 'dest_bucket_key', ',', 'source_bucket_name', '=', 'none', ',', 'dest_bucket_name', '=', 'none', ',', 'source_version_id', '=', 'none', ')', ':', 'if', 'dest_bucket_name', 'is', 'none', ':', 'dest_bucket_name', ',', 'dest_bucket_key', '=', 'self', '.', 'parse_s3_url', '(', 'dest_bucket_key', ')', 'else', ':', 'parsed_url', '=', 'urlparse', '(', 'dest_bucket_key', ')', 'if', 'parsed_url', '.', 'scheme', '!=', ""''"", 'or', 'parsed_url', '.', 'netloc', '!=', ""''"", ':', 'raise', 'airflowexception', '(', ""'if dest_bucket_name is provided, '"", '+', ""'dest_bucket_key should be relative path '"", '+', ""'from root level, rather than a full s3:// url'"", ')', 'if', 'source_bucket_name', 'is', 'none', ':', 'source_bucket_name', ',', 'source_bucket_key', '=', 'self', '.', 'parse_s3_url', '(', 'source_bucket_key', ')', 'else', ':', 'parsed_url', '=', 'urlparse', '(', 'source_bucket_key', ')', 'if', 'parsed_url', '.', 'scheme', '!=', ""''"", 'or', 'parsed_url', '.', 'netloc', '!=', ""''"", ':', 'raise', 'airflowexception', '(', ""'if source_bucket_name is provided, '"", '+', ""'source_bucket_key should be relative path '"", '+', ""'from root level, rather than a full s3:// url'"", ')', 'copysource', '=', '{', ""'bucket'"", ':', 'source_bucket_name', ',', ""'key'"", ':', 'source_bucket_key', ',', ""'versionid'"", ':', 'source_version_id', '}', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'copy_object', '(', 'bucket', '=', 'dest_bucket_name', ',', 'key', '=', 'dest_bucket_key', ',', 'copysource', '=', 'copysource', ')', 'return', 'response']","def copy_object ( self , source_bucket_key , dest_bucket_key , source_bucket_name = none , dest_bucket_name = none , source_version_id = none ) : if dest_bucket_name is none : dest_bucket_name , dest_bucket_key = self . parse_s3_url ( dest_bucket_key ) else : parsed_url = urlparse ( dest_bucket_key ) if parsed_url . scheme != '' or parsed_url . netloc != '' : raise airflowexception ( 'if dest_bucket_name is provided, ' + 'dest_bucket_key should be relative path ' + 'from root level, rather than a full s3:// url' ) if source_bucket_name is none : source_bucket_name , source_bucket_key = self . parse_s3_url ( source_bucket_key ) else : parsed_url = urlparse ( source_bucket_key ) if parsed_url . scheme != '' or parsed_url . netloc != '' : raise airflowexception ( 'if source_bucket_name is provided, ' + 'source_bucket_key should be relative path ' + 'from root level, rather than a full s3:// url' ) copysource = { 'bucket' : source_bucket_name , 'key' : source_bucket_key , 'versionid' : source_version_id } response = self . get_conn ( ) . copy_object ( bucket = dest_bucket_name , key = dest_bucket_key , copysource = copysource ) return response"
392,apache/airflow,airflow/hooks/S3_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/hooks/S3_hook.py#L520-L543,"def delete_objects(self,
                       bucket,
                       keys):
        """"""
        :param bucket: Name of the bucket in which you are going to delete object(s)
        :type bucket: str
        :param keys: The key(s) to delete from S3 bucket.

            When ``keys`` is a string, it's supposed to be the key name of
            the single object to delete.

            When ``keys`` is a list, it's supposed to be the list of the
            keys to delete.
        :type keys: str or list
        """"""
        if isinstance(keys, list):
            keys = keys
        else:
            keys = [keys]

        delete_dict = {""Objects"": [{""Key"": k} for k in keys]}
        response = self.get_conn().delete_objects(Bucket=bucket,
                                                  Delete=delete_dict)
        return response","['def', 'delete_objects', '(', 'self', ',', 'bucket', ',', 'keys', ')', ':', 'if', 'isinstance', '(', 'keys', ',', 'list', ')', ':', 'keys', '=', 'keys', 'else', ':', 'keys', '=', '[', 'keys', ']', 'delete_dict', '=', '{', '""Objects""', ':', '[', '{', '""Key""', ':', 'k', '}', 'for', 'k', 'in', 'keys', ']', '}', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'delete_objects', '(', 'Bucket', '=', 'bucket', ',', 'Delete', '=', 'delete_dict', ')', 'return', 'response']",":param bucket: Name of the bucket in which you are going to delete object(s)
        :type bucket: str
        :param keys: The key(s) to delete from S3 bucket.

            When ``keys`` is a string, it's supposed to be the key name of
            the single object to delete.

            When ``keys`` is a list, it's supposed to be the list of the
            keys to delete.
        :type keys: str or list","[':', 'param', 'bucket', ':', 'Name', 'of', 'the', 'bucket', 'in', 'which', 'you', 'are', 'going', 'to', 'delete', 'object', '(', 's', ')', ':', 'type', 'bucket', ':', 'str', ':', 'param', 'keys', ':', 'The', 'key', '(', 's', ')', 'to', 'delete', 'from', 'S3', 'bucket', '.']",python,test,"[':', 'param', 'bucket', ':', 'name', 'of', 'the', 'bucket', 'in', 'which', 'you', 'are', 'going', 'to', 'delete', 'object', '(', 's', ')', ':', 'type', 'bucket', ':', 'str', ':', 'param', 'keys', ':', 'the', 'key', '(', 's', ')', 'to', 'delete', 'from', 's3', 'bucket', '.']",: param bucket : name of the bucket in which you are going to delete object ( s ) : type bucket : str : param keys : the key ( s ) to delete from s3 bucket .,"['def', 'delete_objects', '(', 'self', ',', 'bucket', ',', 'keys', ')', ':', 'if', 'isinstance', '(', 'keys', ',', 'list', ')', ':', 'keys', '=', 'keys', 'else', ':', 'keys', '=', '[', 'keys', ']', 'delete_dict', '=', '{', '""objects""', ':', '[', '{', '""key""', ':', 'k', '}', 'for', 'k', 'in', 'keys', ']', '}', 'response', '=', 'self', '.', 'get_conn', '(', ')', '.', 'delete_objects', '(', 'bucket', '=', 'bucket', ',', 'delete', '=', 'delete_dict', ')', 'return', 'response']","def delete_objects ( self , bucket , keys ) : if isinstance ( keys , list ) : keys = keys else : keys = [ keys ] delete_dict = { ""objects"" : [ { ""key"" : k } for k in keys ] } response = self . get_conn ( ) . delete_objects ( bucket = bucket , delete = delete_dict ) return response"
393,apache/airflow,airflow/contrib/operators/cassandra_to_gcs.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/cassandra_to_gcs.py#L147-L154,"def _query_cassandra(self):
        """"""
        Queries cassandra and returns a cursor to the results.
        """"""
        self.hook = CassandraHook(cassandra_conn_id=self.cassandra_conn_id)
        session = self.hook.get_conn()
        cursor = session.execute(self.cql)
        return cursor","['def', '_query_cassandra', '(', 'self', ')', ':', 'self', '.', 'hook', '=', 'CassandraHook', '(', 'cassandra_conn_id', '=', 'self', '.', 'cassandra_conn_id', ')', 'session', '=', 'self', '.', 'hook', '.', 'get_conn', '(', ')', 'cursor', '=', 'session', '.', 'execute', '(', 'self', '.', 'cql', ')', 'return', 'cursor']",Queries cassandra and returns a cursor to the results.,"['Queries', 'cassandra', 'and', 'returns', 'a', 'cursor', 'to', 'the', 'results', '.']",python,test,"['queries', 'cassandra', 'and', 'returns', 'a', 'cursor', 'to', 'the', 'results', '.']",queries cassandra and returns a cursor to the results .,"['def', '_query_cassandra', '(', 'self', ')', ':', 'self', '.', 'hook', '=', 'cassandrahook', '(', 'cassandra_conn_id', '=', 'self', '.', 'cassandra_conn_id', ')', 'session', '=', 'self', '.', 'hook', '.', 'get_conn', '(', ')', 'cursor', '=', 'session', '.', 'execute', '(', 'self', '.', 'cql', ')', 'return', 'cursor']",def _query_cassandra ( self ) : self . hook = cassandrahook ( cassandra_conn_id = self . cassandra_conn_id ) session = self . hook . get_conn ( ) cursor = session . execute ( self . cql ) return cursor
394,apache/airflow,airflow/contrib/operators/cassandra_to_gcs.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/cassandra_to_gcs.py#L156-L180,"def _write_local_data_files(self, cursor):
        """"""
        Takes a cursor, and writes results to a local file.

        :return: A dictionary where keys are filenames to be used as object
            names in GCS, and values are file handles to local files that
            contain the data for the GCS objects.
        """"""
        file_no = 0
        tmp_file_handle = NamedTemporaryFile(delete=True)
        tmp_file_handles = {self.filename.format(file_no): tmp_file_handle}
        for row in cursor:
            row_dict = self.generate_data_dict(row._fields, row)
            s = json.dumps(row_dict).encode('utf-8')
            tmp_file_handle.write(s)

            # Append newline to make dumps BigQuery compatible.
            tmp_file_handle.write(b'\n')

            if tmp_file_handle.tell() >= self.approx_max_file_size_bytes:
                file_no += 1
                tmp_file_handle = NamedTemporaryFile(delete=True)
                tmp_file_handles[self.filename.format(file_no)] = tmp_file_handle

        return tmp_file_handles","['def', '_write_local_data_files', '(', 'self', ',', 'cursor', ')', ':', 'file_no', '=', '0', 'tmp_file_handle', '=', 'NamedTemporaryFile', '(', 'delete', '=', 'True', ')', 'tmp_file_handles', '=', '{', 'self', '.', 'filename', '.', 'format', '(', 'file_no', ')', ':', 'tmp_file_handle', '}', 'for', 'row', 'in', 'cursor', ':', 'row_dict', '=', 'self', '.', 'generate_data_dict', '(', 'row', '.', '_fields', ',', 'row', ')', 's', '=', 'json', '.', 'dumps', '(', 'row_dict', ')', '.', 'encode', '(', ""'utf-8'"", ')', 'tmp_file_handle', '.', 'write', '(', 's', ')', '# Append newline to make dumps BigQuery compatible.', 'tmp_file_handle', '.', 'write', '(', ""b'\\n'"", ')', 'if', 'tmp_file_handle', '.', 'tell', '(', ')', '>=', 'self', '.', 'approx_max_file_size_bytes', ':', 'file_no', '+=', '1', 'tmp_file_handle', '=', 'NamedTemporaryFile', '(', 'delete', '=', 'True', ')', 'tmp_file_handles', '[', 'self', '.', 'filename', '.', 'format', '(', 'file_no', ')', ']', '=', 'tmp_file_handle', 'return', 'tmp_file_handles']","Takes a cursor, and writes results to a local file.

        :return: A dictionary where keys are filenames to be used as object
            names in GCS, and values are file handles to local files that
            contain the data for the GCS objects.","['Takes', 'a', 'cursor', 'and', 'writes', 'results', 'to', 'a', 'local', 'file', '.']",python,test,"['takes', 'a', 'cursor', 'and', 'writes', 'results', 'to', 'a', 'local', 'file', '.']",takes a cursor and writes results to a local file .,"['def', '_write_local_data_files', '(', 'self', ',', 'cursor', ')', ':', 'file_no', '=', '0', 'tmp_file_handle', '=', 'namedtemporaryfile', '(', 'delete', '=', 'true', ')', 'tmp_file_handles', '=', '{', 'self', '.', 'filename', '.', 'format', '(', 'file_no', ')', ':', 'tmp_file_handle', '}', 'for', 'row', 'in', 'cursor', ':', 'row_dict', '=', 'self', '.', 'generate_data_dict', '(', 'row', '.', '_fields', ',', 'row', ')', 's', '=', 'json', '.', 'dumps', '(', 'row_dict', ')', '.', 'encode', '(', ""'utf-8'"", ')', 'tmp_file_handle', '.', 'write', '(', 's', ')', '# append newline to make dumps bigquery compatible.', 'tmp_file_handle', '.', 'write', '(', ""b'\\n'"", ')', 'if', 'tmp_file_handle', '.', 'tell', '(', ')', '>=', 'self', '.', 'approx_max_file_size_bytes', ':', 'file_no', '+=', '1', 'tmp_file_handle', '=', 'namedtemporaryfile', '(', 'delete', '=', 'true', ')', 'tmp_file_handles', '[', 'self', '.', 'filename', '.', 'format', '(', 'file_no', ')', ']', '=', 'tmp_file_handle', 'return', 'tmp_file_handles']","def _write_local_data_files ( self , cursor ) : file_no = 0 tmp_file_handle = namedtemporaryfile ( delete = true ) tmp_file_handles = { self . filename . format ( file_no ) : tmp_file_handle } for row in cursor : row_dict = self . generate_data_dict ( row . _fields , row ) s = json . dumps ( row_dict ) . encode ( 'utf-8' ) tmp_file_handle . write ( s ) # append newline to make dumps bigquery compatible. tmp_file_handle . write ( b'\n' ) if tmp_file_handle . tell ( ) >= self . approx_max_file_size_bytes : file_no += 1 tmp_file_handle = namedtemporaryfile ( delete = true ) tmp_file_handles [ self . filename . format ( file_no ) ] = tmp_file_handle return tmp_file_handles"
395,apache/airflow,airflow/contrib/operators/cassandra_to_gcs.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/cassandra_to_gcs.py#L182-L199,"def _write_local_schema_file(self, cursor):
        """"""
        Takes a cursor, and writes the BigQuery schema for the results to a
        local file system.

        :return: A dictionary where key is a filename to be used as an object
            name in GCS, and values are file handles to local files that
            contains the BigQuery schema fields in .json format.
        """"""
        schema = []
        tmp_schema_file_handle = NamedTemporaryFile(delete=True)

        for name, type in zip(cursor.column_names, cursor.column_types):
            schema.append(self.generate_schema_dict(name, type))
        json_serialized_schema = json.dumps(schema).encode('utf-8')

        tmp_schema_file_handle.write(json_serialized_schema)
        return {self.schema_filename: tmp_schema_file_handle}","['def', '_write_local_schema_file', '(', 'self', ',', 'cursor', ')', ':', 'schema', '=', '[', ']', 'tmp_schema_file_handle', '=', 'NamedTemporaryFile', '(', 'delete', '=', 'True', ')', 'for', 'name', ',', 'type', 'in', 'zip', '(', 'cursor', '.', 'column_names', ',', 'cursor', '.', 'column_types', ')', ':', 'schema', '.', 'append', '(', 'self', '.', 'generate_schema_dict', '(', 'name', ',', 'type', ')', ')', 'json_serialized_schema', '=', 'json', '.', 'dumps', '(', 'schema', ')', '.', 'encode', '(', ""'utf-8'"", ')', 'tmp_schema_file_handle', '.', 'write', '(', 'json_serialized_schema', ')', 'return', '{', 'self', '.', 'schema_filename', ':', 'tmp_schema_file_handle', '}']","Takes a cursor, and writes the BigQuery schema for the results to a
        local file system.

        :return: A dictionary where key is a filename to be used as an object
            name in GCS, and values are file handles to local files that
            contains the BigQuery schema fields in .json format.","['Takes', 'a', 'cursor', 'and', 'writes', 'the', 'BigQuery', 'schema', 'for', 'the', 'results', 'to', 'a', 'local', 'file', 'system', '.']",python,test,"['takes', 'a', 'cursor', 'and', 'writes', 'the', 'bigquery', 'schema', 'for', 'the', 'results', 'to', 'a', 'local', 'file', 'system', '.']",takes a cursor and writes the bigquery schema for the results to a local file system .,"['def', '_write_local_schema_file', '(', 'self', ',', 'cursor', ')', ':', 'schema', '=', '[', ']', 'tmp_schema_file_handle', '=', 'namedtemporaryfile', '(', 'delete', '=', 'true', ')', 'for', 'name', ',', 'type', 'in', 'zip', '(', 'cursor', '.', 'column_names', ',', 'cursor', '.', 'column_types', ')', ':', 'schema', '.', 'append', '(', 'self', '.', 'generate_schema_dict', '(', 'name', ',', 'type', ')', ')', 'json_serialized_schema', '=', 'json', '.', 'dumps', '(', 'schema', ')', '.', 'encode', '(', ""'utf-8'"", ')', 'tmp_schema_file_handle', '.', 'write', '(', 'json_serialized_schema', ')', 'return', '{', 'self', '.', 'schema_filename', ':', 'tmp_schema_file_handle', '}']","def _write_local_schema_file ( self , cursor ) : schema = [ ] tmp_schema_file_handle = namedtemporaryfile ( delete = true ) for name , type in zip ( cursor . column_names , cursor . column_types ) : schema . append ( self . generate_schema_dict ( name , type ) ) json_serialized_schema = json . dumps ( schema ) . encode ( 'utf-8' ) tmp_schema_file_handle . write ( json_serialized_schema ) return { self . schema_filename : tmp_schema_file_handle }"
396,apache/airflow,airflow/contrib/operators/cassandra_to_gcs.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/cassandra_to_gcs.py#L247-L255,"def convert_user_type(cls, name, value):
        """"""
        Converts a user type to RECORD that contains n fields, where n is the
        number of attributes. Each element in the user type class will be converted to its
        corresponding data type in BQ.
        """"""
        names = value._fields
        values = [cls.convert_value(name, getattr(value, name)) for name in names]
        return cls.generate_data_dict(names, values)","['def', 'convert_user_type', '(', 'cls', ',', 'name', ',', 'value', ')', ':', 'names', '=', 'value', '.', '_fields', 'values', '=', '[', 'cls', '.', 'convert_value', '(', 'name', ',', 'getattr', '(', 'value', ',', 'name', ')', ')', 'for', 'name', 'in', 'names', ']', 'return', 'cls', '.', 'generate_data_dict', '(', 'names', ',', 'values', ')']","Converts a user type to RECORD that contains n fields, where n is the
        number of attributes. Each element in the user type class will be converted to its
        corresponding data type in BQ.","['Converts', 'a', 'user', 'type', 'to', 'RECORD', 'that', 'contains', 'n', 'fields', 'where', 'n', 'is', 'the', 'number', 'of', 'attributes', '.', 'Each', 'element', 'in', 'the', 'user', 'type', 'class', 'will', 'be', 'converted', 'to', 'its', 'corresponding', 'data', 'type', 'in', 'BQ', '.']",python,test,"['converts', 'a', 'user', 'type', 'to', 'record', 'that', 'contains', 'n', 'fields', 'where', 'n', 'is', 'the', 'number', 'of', 'attributes', '.', 'each', 'element', 'in', 'the', 'user', 'type', 'class', 'will', 'be', 'converted', 'to', 'its', 'corresponding', 'data', 'type', 'in', 'bq', '.']",converts a user type to record that contains n fields where n is the number of attributes . each element in the user type class will be converted to its corresponding data type in bq .,"['def', 'convert_user_type', '(', 'cls', ',', 'name', ',', 'value', ')', ':', 'names', '=', 'value', '.', '_fields', 'values', '=', '[', 'cls', '.', 'convert_value', '(', 'name', ',', 'getattr', '(', 'value', ',', 'name', ')', ')', 'for', 'name', 'in', 'names', ']', 'return', 'cls', '.', 'generate_data_dict', '(', 'names', ',', 'values', ')']","def convert_user_type ( cls , name , value ) : names = value . _fields values = [ cls . convert_value ( name , getattr ( value , name ) ) for name in names ] return cls . generate_data_dict ( names , values )"
397,apache/airflow,airflow/contrib/operators/cassandra_to_gcs.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/cassandra_to_gcs.py#L258-L266,"def convert_tuple_type(cls, name, value):
        """"""
        Converts a tuple to RECORD that contains n fields, each will be converted
        to its corresponding data type in bq and will be named 'field_<index>', where
        index is determined by the order of the tuple elements defined in cassandra.
        """"""
        names = ['field_' + str(i) for i in range(len(value))]
        values = [cls.convert_value(name, value) for name, value in zip(names, value)]
        return cls.generate_data_dict(names, values)","['def', 'convert_tuple_type', '(', 'cls', ',', 'name', ',', 'value', ')', ':', 'names', '=', '[', ""'field_'"", '+', 'str', '(', 'i', ')', 'for', 'i', 'in', 'range', '(', 'len', '(', 'value', ')', ')', ']', 'values', '=', '[', 'cls', '.', 'convert_value', '(', 'name', ',', 'value', ')', 'for', 'name', ',', 'value', 'in', 'zip', '(', 'names', ',', 'value', ')', ']', 'return', 'cls', '.', 'generate_data_dict', '(', 'names', ',', 'values', ')']","Converts a tuple to RECORD that contains n fields, each will be converted
        to its corresponding data type in bq and will be named 'field_<index>', where
        index is determined by the order of the tuple elements defined in cassandra.","['Converts', 'a', 'tuple', 'to', 'RECORD', 'that', 'contains', 'n', 'fields', 'each', 'will', 'be', 'converted', 'to', 'its', 'corresponding', 'data', 'type', 'in', 'bq', 'and', 'will', 'be', 'named', 'field_<index', '>', 'where', 'index', 'is', 'determined', 'by', 'the', 'order', 'of', 'the', 'tuple', 'elements', 'defined', 'in', 'cassandra', '.']",python,test,"['converts', 'a', 'tuple', 'to', 'record', 'that', 'contains', 'n', 'fields', 'each', 'will', 'be', 'converted', 'to', 'its', 'corresponding', 'data', 'type', 'in', 'bq', 'and', 'will', 'be', 'named', 'field_<index', '>', 'where', 'index', 'is', 'determined', 'by', 'the', 'order', 'of', 'the', 'tuple', 'elements', 'defined', 'in', 'cassandra', '.']",converts a tuple to record that contains n fields each will be converted to its corresponding data type in bq and will be named field_<index > where index is determined by the order of the tuple elements defined in cassandra .,"['def', 'convert_tuple_type', '(', 'cls', ',', 'name', ',', 'value', ')', ':', 'names', '=', '[', ""'field_'"", '+', 'str', '(', 'i', ')', 'for', 'i', 'in', 'range', '(', 'len', '(', 'value', ')', ')', ']', 'values', '=', '[', 'cls', '.', 'convert_value', '(', 'name', ',', 'value', ')', 'for', 'name', ',', 'value', 'in', 'zip', '(', 'names', ',', 'value', ')', ']', 'return', 'cls', '.', 'generate_data_dict', '(', 'names', ',', 'values', ')']","def convert_tuple_type ( cls , name , value ) : names = [ 'field_' + str ( i ) for i in range ( len ( value ) ) ] values = [ cls . convert_value ( name , value ) for name , value in zip ( names , value ) ] return cls . generate_data_dict ( names , values )"
398,apache/airflow,airflow/contrib/operators/cassandra_to_gcs.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/cassandra_to_gcs.py#L269-L280,"def convert_map_type(cls, name, value):
        """"""
        Converts a map to a repeated RECORD that contains two fields: 'key' and 'value',
        each will be converted to its corresponding data type in BQ.
        """"""
        converted_map = []
        for k, v in zip(value.keys(), value.values()):
            converted_map.append({
                'key': cls.convert_value('key', k),
                'value': cls.convert_value('value', v)
            })
        return converted_map","['def', 'convert_map_type', '(', 'cls', ',', 'name', ',', 'value', ')', ':', 'converted_map', '=', '[', ']', 'for', 'k', ',', 'v', 'in', 'zip', '(', 'value', '.', 'keys', '(', ')', ',', 'value', '.', 'values', '(', ')', ')', ':', 'converted_map', '.', 'append', '(', '{', ""'key'"", ':', 'cls', '.', 'convert_value', '(', ""'key'"", ',', 'k', ')', ',', ""'value'"", ':', 'cls', '.', 'convert_value', '(', ""'value'"", ',', 'v', ')', '}', ')', 'return', 'converted_map']","Converts a map to a repeated RECORD that contains two fields: 'key' and 'value',
        each will be converted to its corresponding data type in BQ.","['Converts', 'a', 'map', 'to', 'a', 'repeated', 'RECORD', 'that', 'contains', 'two', 'fields', ':', 'key', 'and', 'value', 'each', 'will', 'be', 'converted', 'to', 'its', 'corresponding', 'data', 'type', 'in', 'BQ', '.']",python,test,"['converts', 'a', 'map', 'to', 'a', 'repeated', 'record', 'that', 'contains', 'two', 'fields', ':', 'key', 'and', 'value', 'each', 'will', 'be', 'converted', 'to', 'its', 'corresponding', 'data', 'type', 'in', 'bq', '.']",converts a map to a repeated record that contains two fields : key and value each will be converted to its corresponding data type in bq .,"['def', 'convert_map_type', '(', 'cls', ',', 'name', ',', 'value', ')', ':', 'converted_map', '=', '[', ']', 'for', 'k', ',', 'v', 'in', 'zip', '(', 'value', '.', 'keys', '(', ')', ',', 'value', '.', 'values', '(', ')', ')', ':', 'converted_map', '.', 'append', '(', '{', ""'key'"", ':', 'cls', '.', 'convert_value', '(', ""'key'"", ',', 'k', ')', ',', ""'value'"", ':', 'cls', '.', 'convert_value', '(', ""'value'"", ',', 'v', ')', '}', ')', 'return', 'converted_map']","def convert_map_type ( cls , name , value ) : converted_map = [ ] for k , v in zip ( value . keys ( ) , value . values ( ) ) : converted_map . append ( { 'key' : cls . convert_value ( 'key' , k ) , 'value' : cls . convert_value ( 'value' , v ) } ) return converted_map"
399,apache/airflow,airflow/contrib/utils/sendgrid.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/utils/sendgrid.py#L33-L102,"def send_email(to, subject, html_content, files=None, dryrun=False, cc=None,
               bcc=None, mime_subtype='mixed', sandbox_mode=False, **kwargs):
    """"""
    Send an email with html content using sendgrid.

    To use this plugin:
    0. include sendgrid subpackage as part of your Airflow installation, e.g.,
    pip install 'apache-airflow[sendgrid]'
    1. update [email] backend in airflow.cfg, i.e.,
    [email]
    email_backend = airflow.contrib.utils.sendgrid.send_email
    2. configure Sendgrid specific environment variables at all Airflow instances:
    SENDGRID_MAIL_FROM={your-mail-from}
    SENDGRID_API_KEY={your-sendgrid-api-key}.
    """"""
    if files is None:
        files = []

    mail = Mail()
    from_email = kwargs.get('from_email') or os.environ.get('SENDGRID_MAIL_FROM')
    from_name = kwargs.get('from_name') or os.environ.get('SENDGRID_MAIL_SENDER')
    mail.from_email = Email(from_email, from_name)
    mail.subject = subject
    mail.mail_settings = MailSettings()

    if sandbox_mode:
        mail.mail_settings.sandbox_mode = SandBoxMode(enable=True)

    # Add the recipient list of to emails.
    personalization = Personalization()
    to = get_email_address_list(to)
    for to_address in to:
        personalization.add_to(Email(to_address))
    if cc:
        cc = get_email_address_list(cc)
        for cc_address in cc:
            personalization.add_cc(Email(cc_address))
    if bcc:
        bcc = get_email_address_list(bcc)
        for bcc_address in bcc:
            personalization.add_bcc(Email(bcc_address))

    # Add custom_args to personalization if present
    pers_custom_args = kwargs.get('personalization_custom_args', None)
    if isinstance(pers_custom_args, dict):
        for key in pers_custom_args.keys():
            personalization.add_custom_arg(CustomArg(key, pers_custom_args[key]))

    mail.add_personalization(personalization)
    mail.add_content(Content('text/html', html_content))

    categories = kwargs.get('categories', [])
    for cat in categories:
        mail.add_category(Category(cat))

    # Add email attachment.
    for fname in files:
        basename = os.path.basename(fname)

        attachment = Attachment()
        attachment.type = mimetypes.guess_type(basename)[0]
        attachment.filename = basename
        attachment.disposition = ""attachment""
        attachment.content_id = '<{0}>'.format(basename)

        with open(fname, ""rb"") as f:
            attachment.content = base64.b64encode(f.read()).decode('utf-8')

        mail.add_attachment(attachment)
    _post_sendgrid_mail(mail.get())","['def', 'send_email', '(', 'to', ',', 'subject', ',', 'html_content', ',', 'files', '=', 'None', ',', 'dryrun', '=', 'False', ',', 'cc', '=', 'None', ',', 'bcc', '=', 'None', ',', 'mime_subtype', '=', ""'mixed'"", ',', 'sandbox_mode', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', 'if', 'files', 'is', 'None', ':', 'files', '=', '[', ']', 'mail', '=', 'Mail', '(', ')', 'from_email', '=', 'kwargs', '.', 'get', '(', ""'from_email'"", ')', 'or', 'os', '.', 'environ', '.', 'get', '(', ""'SENDGRID_MAIL_FROM'"", ')', 'from_name', '=', 'kwargs', '.', 'get', '(', ""'from_name'"", ')', 'or', 'os', '.', 'environ', '.', 'get', '(', ""'SENDGRID_MAIL_SENDER'"", ')', 'mail', '.', 'from_email', '=', 'Email', '(', 'from_email', ',', 'from_name', ')', 'mail', '.', 'subject', '=', 'subject', 'mail', '.', 'mail_settings', '=', 'MailSettings', '(', ')', 'if', 'sandbox_mode', ':', 'mail', '.', 'mail_settings', '.', 'sandbox_mode', '=', 'SandBoxMode', '(', 'enable', '=', 'True', ')', '# Add the recipient list of to emails.', 'personalization', '=', 'Personalization', '(', ')', 'to', '=', 'get_email_address_list', '(', 'to', ')', 'for', 'to_address', 'in', 'to', ':', 'personalization', '.', 'add_to', '(', 'Email', '(', 'to_address', ')', ')', 'if', 'cc', ':', 'cc', '=', 'get_email_address_list', '(', 'cc', ')', 'for', 'cc_address', 'in', 'cc', ':', 'personalization', '.', 'add_cc', '(', 'Email', '(', 'cc_address', ')', ')', 'if', 'bcc', ':', 'bcc', '=', 'get_email_address_list', '(', 'bcc', ')', 'for', 'bcc_address', 'in', 'bcc', ':', 'personalization', '.', 'add_bcc', '(', 'Email', '(', 'bcc_address', ')', ')', '# Add custom_args to personalization if present', 'pers_custom_args', '=', 'kwargs', '.', 'get', '(', ""'personalization_custom_args'"", ',', 'None', ')', 'if', 'isinstance', '(', 'pers_custom_args', ',', 'dict', ')', ':', 'for', 'key', 'in', 'pers_custom_args', '.', 'keys', '(', ')', ':', 'personalization', '.', 'add_custom_arg', '(', 'CustomArg', '(', 'key', ',', 'pers_custom_args', '[', 'key', ']', ')', ')', 'mail', '.', 'add_personalization', '(', 'personalization', ')', 'mail', '.', 'add_content', '(', 'Content', '(', ""'text/html'"", ',', 'html_content', ')', ')', 'categories', '=', 'kwargs', '.', 'get', '(', ""'categories'"", ',', '[', ']', ')', 'for', 'cat', 'in', 'categories', ':', 'mail', '.', 'add_category', '(', 'Category', '(', 'cat', ')', ')', '# Add email attachment.', 'for', 'fname', 'in', 'files', ':', 'basename', '=', 'os', '.', 'path', '.', 'basename', '(', 'fname', ')', 'attachment', '=', 'Attachment', '(', ')', 'attachment', '.', 'type', '=', 'mimetypes', '.', 'guess_type', '(', 'basename', ')', '[', '0', ']', 'attachment', '.', 'filename', '=', 'basename', 'attachment', '.', 'disposition', '=', '""attachment""', 'attachment', '.', 'content_id', '=', ""'<{0}>'"", '.', 'format', '(', 'basename', ')', 'with', 'open', '(', 'fname', ',', '""rb""', ')', 'as', 'f', ':', 'attachment', '.', 'content', '=', 'base64', '.', 'b64encode', '(', 'f', '.', 'read', '(', ')', ')', '.', 'decode', '(', ""'utf-8'"", ')', 'mail', '.', 'add_attachment', '(', 'attachment', ')', '_post_sendgrid_mail', '(', 'mail', '.', 'get', '(', ')', ')']","Send an email with html content using sendgrid.

    To use this plugin:
    0. include sendgrid subpackage as part of your Airflow installation, e.g.,
    pip install 'apache-airflow[sendgrid]'
    1. update [email] backend in airflow.cfg, i.e.,
    [email]
    email_backend = airflow.contrib.utils.sendgrid.send_email
    2. configure Sendgrid specific environment variables at all Airflow instances:
    SENDGRID_MAIL_FROM={your-mail-from}
    SENDGRID_API_KEY={your-sendgrid-api-key}.","['Send', 'an', 'email', 'with', 'html', 'content', 'using', 'sendgrid', '.']",python,test,"['send', 'an', 'email', 'with', 'html', 'content', 'using', 'sendgrid', '.']",send an email with html content using sendgrid .,"['def', 'send_email', '(', 'to', ',', 'subject', ',', 'html_content', ',', 'files', '=', 'none', ',', 'dryrun', '=', 'false', ',', 'cc', '=', 'none', ',', 'bcc', '=', 'none', ',', 'mime_subtype', '=', ""'mixed'"", ',', 'sandbox_mode', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', 'if', 'files', 'is', 'none', ':', 'files', '=', '[', ']', 'mail', '=', 'mail', '(', ')', 'from_email', '=', 'kwargs', '.', 'get', '(', ""'from_email'"", ')', 'or', 'os', '.', 'environ', '.', 'get', '(', ""'sendgrid_mail_from'"", ')', 'from_name', '=', 'kwargs', '.', 'get', '(', ""'from_name'"", ')', 'or', 'os', '.', 'environ', '.', 'get', '(', ""'sendgrid_mail_sender'"", ')', 'mail', '.', 'from_email', '=', 'email', '(', 'from_email', ',', 'from_name', ')', 'mail', '.', 'subject', '=', 'subject', 'mail', '.', 'mail_settings', '=', 'mailsettings', '(', ')', 'if', 'sandbox_mode', ':', 'mail', '.', 'mail_settings', '.', 'sandbox_mode', '=', 'sandboxmode', '(', 'enable', '=', 'true', ')', '# add the recipient list of to emails.', 'personalization', '=', 'personalization', '(', ')', 'to', '=', 'get_email_address_list', '(', 'to', ')', 'for', 'to_address', 'in', 'to', ':', 'personalization', '.', 'add_to', '(', 'email', '(', 'to_address', ')', ')', 'if', 'cc', ':', 'cc', '=', 'get_email_address_list', '(', 'cc', ')', 'for', 'cc_address', 'in', 'cc', ':', 'personalization', '.', 'add_cc', '(', 'email', '(', 'cc_address', ')', ')', 'if', 'bcc', ':', 'bcc', '=', 'get_email_address_list', '(', 'bcc', ')', 'for', 'bcc_address', 'in', 'bcc', ':', 'personalization', '.', 'add_bcc', '(', 'email', '(', 'bcc_address', ')', ')', '# add custom_args to personalization if present', 'pers_custom_args', '=', 'kwargs', '.', 'get', '(', ""'personalization_custom_args'"", ',', 'none', ')', 'if', 'isinstance', '(', 'pers_custom_args', ',', 'dict', ')', ':', 'for', 'key', 'in', 'pers_custom_args', '.', 'keys', '(', ')', ':', 'personalization', '.', 'add_custom_arg', '(', 'customarg', '(', 'key', ',', 'pers_custom_args', '[', 'key', ']', ')', ')', 'mail', '.', 'add_personalization', '(', 'personalization', ')', 'mail', '.', 'add_content', '(', 'content', '(', ""'text/html'"", ',', 'html_content', ')', ')', 'categories', '=', 'kwargs', '.', 'get', '(', ""'categories'"", ',', '[', ']', ')', 'for', 'cat', 'in', 'categories', ':', 'mail', '.', 'add_category', '(', 'category', '(', 'cat', ')', ')', '# add email attachment.', 'for', 'fname', 'in', 'files', ':', 'basename', '=', 'os', '.', 'path', '.', 'basename', '(', 'fname', ')', 'attachment', '=', 'attachment', '(', ')', 'attachment', '.', 'type', '=', 'mimetypes', '.', 'guess_type', '(', 'basename', ')', '[', '0', ']', 'attachment', '.', 'filename', '=', 'basename', 'attachment', '.', 'disposition', '=', '""attachment""', 'attachment', '.', 'content_id', '=', ""'<{0}>'"", '.', 'format', '(', 'basename', ')', 'with', 'open', '(', 'fname', ',', '""rb""', ')', 'as', 'f', ':', 'attachment', '.', 'content', '=', 'base64', '.', 'b64encode', '(', 'f', '.', 'read', '(', ')', ')', '.', 'decode', '(', ""'utf-8'"", ')', 'mail', '.', 'add_attachment', '(', 'attachment', ')', '_post_sendgrid_mail', '(', 'mail', '.', 'get', '(', ')', ')']","def send_email ( to , subject , html_content , files = none , dryrun = false , cc = none , bcc = none , mime_subtype = 'mixed' , sandbox_mode = false , * * kwargs ) : if files is none : files = [ ] mail = mail ( ) from_email = kwargs . get ( 'from_email' ) or os . environ . get ( 'sendgrid_mail_from' ) from_name = kwargs . get ( 'from_name' ) or os . environ . get ( 'sendgrid_mail_sender' ) mail . from_email = email ( from_email , from_name ) mail . subject = subject mail . mail_settings = mailsettings ( ) if sandbox_mode : mail . mail_settings . sandbox_mode = sandboxmode ( enable = true ) # add the recipient list of to emails. personalization = personalization ( ) to = get_email_address_list ( to ) for to_address in to : personalization . add_to ( email ( to_address ) ) if cc : cc = get_email_address_list ( cc ) for cc_address in cc : personalization . add_cc ( email ( cc_address ) ) if bcc : bcc = get_email_address_list ( bcc ) for bcc_address in bcc : personalization . add_bcc ( email ( bcc_address ) ) # add custom_args to personalization if present pers_custom_args = kwargs . get ( 'personalization_custom_args' , none ) if isinstance ( pers_custom_args , dict ) : for key in pers_custom_args . keys ( ) : personalization . add_custom_arg ( customarg ( key , pers_custom_args [ key ] ) ) mail . add_personalization ( personalization ) mail . add_content ( content ( 'text/html' , html_content ) ) categories = kwargs . get ( 'categories' , [ ] ) for cat in categories : mail . add_category ( category ( cat ) ) # add email attachment. for fname in files : basename = os . path . basename ( fname ) attachment = attachment ( ) attachment . type = mimetypes . guess_type ( basename ) [ 0 ] attachment . filename = basename attachment . disposition = ""attachment"" attachment . content_id = '<{0}>' . format ( basename ) with open ( fname , ""rb"" ) as f : attachment . content = base64 . b64encode ( f . read ( ) ) . decode ( 'utf-8' ) mail . add_attachment ( attachment ) _post_sendgrid_mail ( mail . get ( ) )"
400,apache/airflow,airflow/contrib/hooks/gcp_speech_to_text_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_speech_to_text_hook.py#L42-L51,"def get_conn(self):
        """"""
        Retrieves connection to Cloud Speech.

        :return: Google Cloud Speech client object.
        :rtype: google.cloud.speech_v1.SpeechClient
        """"""
        if not self._client:
            self._client = SpeechClient(credentials=self._get_credentials())
        return self._client","['def', 'get_conn', '(', 'self', ')', ':', 'if', 'not', 'self', '.', '_client', ':', 'self', '.', '_client', '=', 'SpeechClient', '(', 'credentials', '=', 'self', '.', '_get_credentials', '(', ')', ')', 'return', 'self', '.', '_client']","Retrieves connection to Cloud Speech.

        :return: Google Cloud Speech client object.
        :rtype: google.cloud.speech_v1.SpeechClient","['Retrieves', 'connection', 'to', 'Cloud', 'Speech', '.']",python,test,"['retrieves', 'connection', 'to', 'cloud', 'speech', '.']",retrieves connection to cloud speech .,"['def', 'get_conn', '(', 'self', ')', ':', 'if', 'not', 'self', '.', '_client', ':', 'self', '.', '_client', '=', 'speechclient', '(', 'credentials', '=', 'self', '.', '_get_credentials', '(', ')', ')', 'return', 'self', '.', '_client']",def get_conn ( self ) : if not self . _client : self . _client = speechclient ( credentials = self . _get_credentials ( ) ) return self . _client
401,apache/airflow,airflow/contrib/hooks/gcp_speech_to_text_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcp_speech_to_text_hook.py#L53-L73,"def recognize_speech(self, config, audio, retry=None, timeout=None):
        """"""
        Recognizes audio input

        :param config: information to the recognizer that specifies how to process the request.
            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionConfig
        :type config: dict or google.cloud.speech_v1.types.RecognitionConfig
        :param audio: audio data to be recognized
            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionAudio
        :type audio: dict or google.cloud.speech_v1.types.RecognitionAudio
        :param retry: (Optional) A retry object used to retry requests. If None is specified,
            requests will not be retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.
            Note that if retry is specified, the timeout applies to each individual attempt.
        :type timeout: float
        """"""
        client = self.get_conn()
        response = client.recognize(config=config, audio=audio, retry=retry, timeout=timeout)
        self.log.info(""Recognised speech: %s"" % response)
        return response","['def', 'recognize_speech', '(', 'self', ',', 'config', ',', 'audio', ',', 'retry', '=', 'None', ',', 'timeout', '=', 'None', ')', ':', 'client', '=', 'self', '.', 'get_conn', '(', ')', 'response', '=', 'client', '.', 'recognize', '(', 'config', '=', 'config', ',', 'audio', '=', 'audio', ',', 'retry', '=', 'retry', ',', 'timeout', '=', 'timeout', ')', 'self', '.', 'log', '.', 'info', '(', '""Recognised speech: %s""', '%', 'response', ')', 'return', 'response']","Recognizes audio input

        :param config: information to the recognizer that specifies how to process the request.
            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionConfig
        :type config: dict or google.cloud.speech_v1.types.RecognitionConfig
        :param audio: audio data to be recognized
            https://googleapis.github.io/google-cloud-python/latest/speech/gapic/v1/types.html#google.cloud.speech_v1.types.RecognitionAudio
        :type audio: dict or google.cloud.speech_v1.types.RecognitionAudio
        :param retry: (Optional) A retry object used to retry requests. If None is specified,
            requests will not be retried.
        :type retry: google.api_core.retry.Retry
        :param timeout: (Optional) The amount of time, in seconds, to wait for the request to complete.
            Note that if retry is specified, the timeout applies to each individual attempt.
        :type timeout: float","['Recognizes', 'audio', 'input']",python,test,"['recognizes', 'audio', 'input']",recognizes audio input,"['def', 'recognize_speech', '(', 'self', ',', 'config', ',', 'audio', ',', 'retry', '=', 'none', ',', 'timeout', '=', 'none', ')', ':', 'client', '=', 'self', '.', 'get_conn', '(', ')', 'response', '=', 'client', '.', 'recognize', '(', 'config', '=', 'config', ',', 'audio', '=', 'audio', ',', 'retry', '=', 'retry', ',', 'timeout', '=', 'timeout', ')', 'self', '.', 'log', '.', 'info', '(', '""recognised speech: %s""', '%', 'response', ')', 'return', 'response']","def recognize_speech ( self , config , audio , retry = none , timeout = none ) : client = self . get_conn ( ) response = client . recognize ( config = config , audio = audio , retry = retry , timeout = timeout ) self . log . info ( ""recognised speech: %s"" % response ) return response"
402,apache/airflow,airflow/contrib/operators/spark_sql_operator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/spark_sql_operator.py#L91-L108,"def execute(self, context):
        """"""
        Call the SparkSqlHook to run the provided sql query
        """"""
        self._hook = SparkSqlHook(sql=self._sql,
                                  conf=self._conf,
                                  conn_id=self._conn_id,
                                  total_executor_cores=self._total_executor_cores,
                                  executor_cores=self._executor_cores,
                                  executor_memory=self._executor_memory,
                                  keytab=self._keytab,
                                  principal=self._principal,
                                  name=self._name,
                                  num_executors=self._num_executors,
                                  master=self._master,
                                  yarn_queue=self._yarn_queue
                                  )
        self._hook.run_query()","['def', 'execute', '(', 'self', ',', 'context', ')', ':', 'self', '.', '_hook', '=', 'SparkSqlHook', '(', 'sql', '=', 'self', '.', '_sql', ',', 'conf', '=', 'self', '.', '_conf', ',', 'conn_id', '=', 'self', '.', '_conn_id', ',', 'total_executor_cores', '=', 'self', '.', '_total_executor_cores', ',', 'executor_cores', '=', 'self', '.', '_executor_cores', ',', 'executor_memory', '=', 'self', '.', '_executor_memory', ',', 'keytab', '=', 'self', '.', '_keytab', ',', 'principal', '=', 'self', '.', '_principal', ',', 'name', '=', 'self', '.', '_name', ',', 'num_executors', '=', 'self', '.', '_num_executors', ',', 'master', '=', 'self', '.', '_master', ',', 'yarn_queue', '=', 'self', '.', '_yarn_queue', ')', 'self', '.', '_hook', '.', 'run_query', '(', ')']",Call the SparkSqlHook to run the provided sql query,"['Call', 'the', 'SparkSqlHook', 'to', 'run', 'the', 'provided', 'sql', 'query']",python,test,"['call', 'the', 'sparksqlhook', 'to', 'run', 'the', 'provided', 'sql', 'query']",call the sparksqlhook to run the provided sql query,"['def', 'execute', '(', 'self', ',', 'context', ')', ':', 'self', '.', '_hook', '=', 'sparksqlhook', '(', 'sql', '=', 'self', '.', '_sql', ',', 'conf', '=', 'self', '.', '_conf', ',', 'conn_id', '=', 'self', '.', '_conn_id', ',', 'total_executor_cores', '=', 'self', '.', '_total_executor_cores', ',', 'executor_cores', '=', 'self', '.', '_executor_cores', ',', 'executor_memory', '=', 'self', '.', '_executor_memory', ',', 'keytab', '=', 'self', '.', '_keytab', ',', 'principal', '=', 'self', '.', '_principal', ',', 'name', '=', 'self', '.', '_name', ',', 'num_executors', '=', 'self', '.', '_num_executors', ',', 'master', '=', 'self', '.', '_master', ',', 'yarn_queue', '=', 'self', '.', '_yarn_queue', ')', 'self', '.', '_hook', '.', 'run_query', '(', ')']","def execute ( self , context ) : self . _hook = sparksqlhook ( sql = self . _sql , conf = self . _conf , conn_id = self . _conn_id , total_executor_cores = self . _total_executor_cores , executor_cores = self . _executor_cores , executor_memory = self . _executor_memory , keytab = self . _keytab , principal = self . _principal , name = self . _name , num_executors = self . _num_executors , master = self . _master , yarn_queue = self . _yarn_queue ) self . _hook . run_query ( )"
403,apache/airflow,airflow/utils/log/file_task_handler.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/file_task_handler.py#L49-L57,"def set_context(self, ti):
        """"""
        Provide task_instance context to airflow task handler.
        :param ti: task instance object
        """"""
        local_loc = self._init_file(ti)
        self.handler = logging.FileHandler(local_loc)
        self.handler.setFormatter(self.formatter)
        self.handler.setLevel(self.level)","['def', 'set_context', '(', 'self', ',', 'ti', ')', ':', 'local_loc', '=', 'self', '.', '_init_file', '(', 'ti', ')', 'self', '.', 'handler', '=', 'logging', '.', 'FileHandler', '(', 'local_loc', ')', 'self', '.', 'handler', '.', 'setFormatter', '(', 'self', '.', 'formatter', ')', 'self', '.', 'handler', '.', 'setLevel', '(', 'self', '.', 'level', ')']","Provide task_instance context to airflow task handler.
        :param ti: task instance object","['Provide', 'task_instance', 'context', 'to', 'airflow', 'task', 'handler', '.', ':', 'param', 'ti', ':', 'task', 'instance', 'object']",python,test,"['provide', 'task_instance', 'context', 'to', 'airflow', 'task', 'handler', '.', ':', 'param', 'ti', ':', 'task', 'instance', 'object']",provide task_instance context to airflow task handler . : param ti : task instance object,"['def', 'set_context', '(', 'self', ',', 'ti', ')', ':', 'local_loc', '=', 'self', '.', '_init_file', '(', 'ti', ')', 'self', '.', 'handler', '=', 'logging', '.', 'filehandler', '(', 'local_loc', ')', 'self', '.', 'handler', '.', 'setformatter', '(', 'self', '.', 'formatter', ')', 'self', '.', 'handler', '.', 'setlevel', '(', 'self', '.', 'level', ')']","def set_context ( self , ti ) : local_loc = self . _init_file ( ti ) self . handler = logging . filehandler ( local_loc ) self . handler . setformatter ( self . formatter ) self . handler . setlevel ( self . level )"
404,apache/airflow,airflow/utils/log/file_task_handler.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/file_task_handler.py#L82-L133,"def _read(self, ti, try_number, metadata=None):
        """"""
        Template method that contains custom logic of reading
        logs given the try_number.
        :param ti: task instance record
        :param try_number: current try_number to read log from
        :param metadata: log metadata,
                         can be used for steaming log reading and auto-tailing.
        :return: log message as a string and metadata.
        """"""
        # Task instance here might be different from task instance when
        # initializing the handler. Thus explicitly getting log location
        # is needed to get correct log path.
        log_relative_path = self._render_filename(ti, try_number)
        location = os.path.join(self.local_base, log_relative_path)

        log = """"

        if os.path.exists(location):
            try:
                with open(location) as f:
                    log += ""*** Reading local file: {}\n"".format(location)
                    log += """".join(f.readlines())
            except Exception as e:
                log = ""*** Failed to load local log file: {}\n"".format(location)
                log += ""*** {}\n"".format(str(e))
        else:
            url = os.path.join(
                ""http://{ti.hostname}:{worker_log_server_port}/log"", log_relative_path
            ).format(
                ti=ti,
                worker_log_server_port=conf.get('celery', 'WORKER_LOG_SERVER_PORT')
            )
            log += ""*** Log file does not exist: {}\n"".format(location)
            log += ""*** Fetching from: {}\n"".format(url)
            try:
                timeout = None  # No timeout
                try:
                    timeout = conf.getint('webserver', 'log_fetch_timeout_sec')
                except (AirflowConfigException, ValueError):
                    pass

                response = requests.get(url, timeout=timeout)

                # Check if the resource was properly fetched
                response.raise_for_status()

                log += '\n' + response.text
            except Exception as e:
                log += ""*** Failed to fetch log file from worker. {}\n"".format(str(e))

        return log, {'end_of_log': True}","['def', '_read', '(', 'self', ',', 'ti', ',', 'try_number', ',', 'metadata', '=', 'None', ')', ':', '# Task instance here might be different from task instance when', '# initializing the handler. Thus explicitly getting log location', '# is needed to get correct log path.', 'log_relative_path', '=', 'self', '.', '_render_filename', '(', 'ti', ',', 'try_number', ')', 'location', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'local_base', ',', 'log_relative_path', ')', 'log', '=', '""""', 'if', 'os', '.', 'path', '.', 'exists', '(', 'location', ')', ':', 'try', ':', 'with', 'open', '(', 'location', ')', 'as', 'f', ':', 'log', '+=', '""*** Reading local file: {}\\n""', '.', 'format', '(', 'location', ')', 'log', '+=', '""""', '.', 'join', '(', 'f', '.', 'readlines', '(', ')', ')', 'except', 'Exception', 'as', 'e', ':', 'log', '=', '""*** Failed to load local log file: {}\\n""', '.', 'format', '(', 'location', ')', 'log', '+=', '""*** {}\\n""', '.', 'format', '(', 'str', '(', 'e', ')', ')', 'else', ':', 'url', '=', 'os', '.', 'path', '.', 'join', '(', '""http://{ti.hostname}:{worker_log_server_port}/log""', ',', 'log_relative_path', ')', '.', 'format', '(', 'ti', '=', 'ti', ',', 'worker_log_server_port', '=', 'conf', '.', 'get', '(', ""'celery'"", ',', ""'WORKER_LOG_SERVER_PORT'"", ')', ')', 'log', '+=', '""*** Log file does not exist: {}\\n""', '.', 'format', '(', 'location', ')', 'log', '+=', '""*** Fetching from: {}\\n""', '.', 'format', '(', 'url', ')', 'try', ':', 'timeout', '=', 'None', '# No timeout', 'try', ':', 'timeout', '=', 'conf', '.', 'getint', '(', ""'webserver'"", ',', ""'log_fetch_timeout_sec'"", ')', 'except', '(', 'AirflowConfigException', ',', 'ValueError', ')', ':', 'pass', 'response', '=', 'requests', '.', 'get', '(', 'url', ',', 'timeout', '=', 'timeout', ')', '# Check if the resource was properly fetched', 'response', '.', 'raise_for_status', '(', ')', 'log', '+=', ""'\\n'"", '+', 'response', '.', 'text', 'except', 'Exception', 'as', 'e', ':', 'log', '+=', '""*** Failed to fetch log file from worker. {}\\n""', '.', 'format', '(', 'str', '(', 'e', ')', ')', 'return', 'log', ',', '{', ""'end_of_log'"", ':', 'True', '}']","Template method that contains custom logic of reading
        logs given the try_number.
        :param ti: task instance record
        :param try_number: current try_number to read log from
        :param metadata: log metadata,
                         can be used for steaming log reading and auto-tailing.
        :return: log message as a string and metadata.","['Template', 'method', 'that', 'contains', 'custom', 'logic', 'of', 'reading', 'logs', 'given', 'the', 'try_number', '.', ':', 'param', 'ti', ':', 'task', 'instance', 'record', ':', 'param', 'try_number', ':', 'current', 'try_number', 'to', 'read', 'log', 'from', ':', 'param', 'metadata', ':', 'log', 'metadata', 'can', 'be', 'used', 'for', 'steaming', 'log', 'reading', 'and', 'auto', '-', 'tailing', '.', ':', 'return', ':', 'log', 'message', 'as', 'a', 'string', 'and', 'metadata', '.']",python,test,"['template', 'method', 'that', 'contains', 'custom', 'logic', 'of', 'reading', 'logs', 'given', 'the', 'try_number', '.', ':', 'param', 'ti', ':', 'task', 'instance', 'record', ':', 'param', 'try_number', ':', 'current', 'try_number', 'to', 'read', 'log', 'from', ':', 'param', 'metadata', ':', 'log', 'metadata', 'can', 'be', 'used', 'for', 'steaming', 'log', 'reading', 'and', 'auto', '-', 'tailing', '.', ':', 'return', ':', 'log', 'message', 'as', 'a', 'string', 'and', 'metadata', '.']",template method that contains custom logic of reading logs given the try_number . : param ti : task instance record : param try_number : current try_number to read log from : param metadata : log metadata can be used for steaming log reading and auto - tailing . : return : log message as a string and metadata .,"['def', '_read', '(', 'self', ',', 'ti', ',', 'try_number', ',', 'metadata', '=', 'none', ')', ':', '# task instance here might be different from task instance when', '# initializing the handler. thus explicitly getting log location', '# is needed to get correct log path.', 'log_relative_path', '=', 'self', '.', '_render_filename', '(', 'ti', ',', 'try_number', ')', 'location', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'local_base', ',', 'log_relative_path', ')', 'log', '=', '""""', 'if', 'os', '.', 'path', '.', 'exists', '(', 'location', ')', ':', 'try', ':', 'with', 'open', '(', 'location', ')', 'as', 'f', ':', 'log', '+=', '""*** reading local file: {}\\n""', '.', 'format', '(', 'location', ')', 'log', '+=', '""""', '.', 'join', '(', 'f', '.', 'readlines', '(', ')', ')', 'except', 'exception', 'as', 'e', ':', 'log', '=', '""*** failed to load local log file: {}\\n""', '.', 'format', '(', 'location', ')', 'log', '+=', '""*** {}\\n""', '.', 'format', '(', 'str', '(', 'e', ')', ')', 'else', ':', 'url', '=', 'os', '.', 'path', '.', 'join', '(', '""http://{ti.hostname}:{worker_log_server_port}/log""', ',', 'log_relative_path', ')', '.', 'format', '(', 'ti', '=', 'ti', ',', 'worker_log_server_port', '=', 'conf', '.', 'get', '(', ""'celery'"", ',', ""'worker_log_server_port'"", ')', ')', 'log', '+=', '""*** log file does not exist: {}\\n""', '.', 'format', '(', 'location', ')', 'log', '+=', '""*** fetching from: {}\\n""', '.', 'format', '(', 'url', ')', 'try', ':', 'timeout', '=', 'none', '# no timeout', 'try', ':', 'timeout', '=', 'conf', '.', 'getint', '(', ""'webserver'"", ',', ""'log_fetch_timeout_sec'"", ')', 'except', '(', 'airflowconfigexception', ',', 'valueerror', ')', ':', 'pass', 'response', '=', 'requests', '.', 'get', '(', 'url', ',', 'timeout', '=', 'timeout', ')', '# check if the resource was properly fetched', 'response', '.', 'raise_for_status', '(', ')', 'log', '+=', ""'\\n'"", '+', 'response', '.', 'text', 'except', 'exception', 'as', 'e', ':', 'log', '+=', '""*** failed to fetch log file from worker. {}\\n""', '.', 'format', '(', 'str', '(', 'e', ')', ')', 'return', 'log', ',', '{', ""'end_of_log'"", ':', 'true', '}']","def _read ( self , ti , try_number , metadata = none ) : # task instance here might be different from task instance when # initializing the handler. thus explicitly getting log location # is needed to get correct log path. log_relative_path = self . _render_filename ( ti , try_number ) location = os . path . join ( self . local_base , log_relative_path ) log = """" if os . path . exists ( location ) : try : with open ( location ) as f : log += ""*** reading local file: {}\n"" . format ( location ) log += """" . join ( f . readlines ( ) ) except exception as e : log = ""*** failed to load local log file: {}\n"" . format ( location ) log += ""*** {}\n"" . format ( str ( e ) ) else : url = os . path . join ( ""http://{ti.hostname}:{worker_log_server_port}/log"" , log_relative_path ) . format ( ti = ti , worker_log_server_port = conf . get ( 'celery' , 'worker_log_server_port' ) ) log += ""*** log file does not exist: {}\n"" . format ( location ) log += ""*** fetching from: {}\n"" . format ( url ) try : timeout = none # no timeout try : timeout = conf . getint ( 'webserver' , 'log_fetch_timeout_sec' ) except ( airflowconfigexception , valueerror ) : pass response = requests . get ( url , timeout = timeout ) # check if the resource was properly fetched response . raise_for_status ( ) log += '\n' + response . text except exception as e : log += ""*** failed to fetch log file from worker. {}\n"" . format ( str ( e ) ) return log , { 'end_of_log' : true }"
405,apache/airflow,airflow/utils/log/file_task_handler.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/file_task_handler.py#L135-L168,"def read(self, task_instance, try_number=None, metadata=None):
        """"""
        Read logs of given task instance from local machine.
        :param task_instance: task instance object
        :param try_number: task instance try_number to read logs from. If None
                           it returns all logs separated by try_number
        :param metadata: log metadata,
                         can be used for steaming log reading and auto-tailing.
        :return: a list of logs
        """"""
        # Task instance increments its try number when it starts to run.
        # So the log for a particular task try will only show up when
        # try number gets incremented in DB, i.e logs produced the time
        # after cli run and before try_number + 1 in DB will not be displayed.

        if try_number is None:
            next_try = task_instance.next_try_number
            try_numbers = list(range(1, next_try))
        elif try_number < 1:
            logs = [
                'Error fetching the logs. Try number {} is invalid.'.format(try_number),
            ]
            return logs
        else:
            try_numbers = [try_number]

        logs = [''] * len(try_numbers)
        metadatas = [{}] * len(try_numbers)
        for i, try_number in enumerate(try_numbers):
            log, metadata = self._read(task_instance, try_number, metadata)
            logs[i] += log
            metadatas[i] = metadata

        return logs, metadatas","['def', 'read', '(', 'self', ',', 'task_instance', ',', 'try_number', '=', 'None', ',', 'metadata', '=', 'None', ')', ':', '# Task instance increments its try number when it starts to run.', '# So the log for a particular task try will only show up when', '# try number gets incremented in DB, i.e logs produced the time', '# after cli run and before try_number + 1 in DB will not be displayed.', 'if', 'try_number', 'is', 'None', ':', 'next_try', '=', 'task_instance', '.', 'next_try_number', 'try_numbers', '=', 'list', '(', 'range', '(', '1', ',', 'next_try', ')', ')', 'elif', 'try_number', '<', '1', ':', 'logs', '=', '[', ""'Error fetching the logs. Try number {} is invalid.'"", '.', 'format', '(', 'try_number', ')', ',', ']', 'return', 'logs', 'else', ':', 'try_numbers', '=', '[', 'try_number', ']', 'logs', '=', '[', ""''"", ']', '*', 'len', '(', 'try_numbers', ')', 'metadatas', '=', '[', '{', '}', ']', '*', 'len', '(', 'try_numbers', ')', 'for', 'i', ',', 'try_number', 'in', 'enumerate', '(', 'try_numbers', ')', ':', 'log', ',', 'metadata', '=', 'self', '.', '_read', '(', 'task_instance', ',', 'try_number', ',', 'metadata', ')', 'logs', '[', 'i', ']', '+=', 'log', 'metadatas', '[', 'i', ']', '=', 'metadata', 'return', 'logs', ',', 'metadatas']","Read logs of given task instance from local machine.
        :param task_instance: task instance object
        :param try_number: task instance try_number to read logs from. If None
                           it returns all logs separated by try_number
        :param metadata: log metadata,
                         can be used for steaming log reading and auto-tailing.
        :return: a list of logs","['Read', 'logs', 'of', 'given', 'task', 'instance', 'from', 'local', 'machine', '.', ':', 'param', 'task_instance', ':', 'task', 'instance', 'object', ':', 'param', 'try_number', ':', 'task', 'instance', 'try_number', 'to', 'read', 'logs', 'from', '.', 'If', 'None', 'it', 'returns', 'all', 'logs', 'separated', 'by', 'try_number', ':', 'param', 'metadata', ':', 'log', 'metadata', 'can', 'be', 'used', 'for', 'steaming', 'log', 'reading', 'and', 'auto', '-', 'tailing', '.', ':', 'return', ':', 'a', 'list', 'of', 'logs']",python,test,"['read', 'logs', 'of', 'given', 'task', 'instance', 'from', 'local', 'machine', '.', ':', 'param', 'task_instance', ':', 'task', 'instance', 'object', ':', 'param', 'try_number', ':', 'task', 'instance', 'try_number', 'to', 'read', 'logs', 'from', '.', 'if', 'none', 'it', 'returns', 'all', 'logs', 'separated', 'by', 'try_number', ':', 'param', 'metadata', ':', 'log', 'metadata', 'can', 'be', 'used', 'for', 'steaming', 'log', 'reading', 'and', 'auto', '-', 'tailing', '.', ':', 'return', ':', 'a', 'list', 'of', 'logs']",read logs of given task instance from local machine . : param task_instance : task instance object : param try_number : task instance try_number to read logs from . if none it returns all logs separated by try_number : param metadata : log metadata can be used for steaming log reading and auto - tailing . : return : a list of logs,"['def', 'read', '(', 'self', ',', 'task_instance', ',', 'try_number', '=', 'none', ',', 'metadata', '=', 'none', ')', ':', '# task instance increments its try number when it starts to run.', '# so the log for a particular task try will only show up when', '# try number gets incremented in db, i.e logs produced the time', '# after cli run and before try_number + 1 in db will not be displayed.', 'if', 'try_number', 'is', 'none', ':', 'next_try', '=', 'task_instance', '.', 'next_try_number', 'try_numbers', '=', 'list', '(', 'range', '(', '1', ',', 'next_try', ')', ')', 'elif', 'try_number', '<', '1', ':', 'logs', '=', '[', ""'error fetching the logs. try number {} is invalid.'"", '.', 'format', '(', 'try_number', ')', ',', ']', 'return', 'logs', 'else', ':', 'try_numbers', '=', '[', 'try_number', ']', 'logs', '=', '[', ""''"", ']', '*', 'len', '(', 'try_numbers', ')', 'metadatas', '=', '[', '{', '}', ']', '*', 'len', '(', 'try_numbers', ')', 'for', 'i', ',', 'try_number', 'in', 'enumerate', '(', 'try_numbers', ')', ':', 'log', ',', 'metadata', '=', 'self', '.', '_read', '(', 'task_instance', ',', 'try_number', ',', 'metadata', ')', 'logs', '[', 'i', ']', '+=', 'log', 'metadatas', '[', 'i', ']', '=', 'metadata', 'return', 'logs', ',', 'metadatas']","def read ( self , task_instance , try_number = none , metadata = none ) : # task instance increments its try number when it starts to run. # so the log for a particular task try will only show up when # try number gets incremented in db, i.e logs produced the time # after cli run and before try_number + 1 in db will not be displayed. if try_number is none : next_try = task_instance . next_try_number try_numbers = list ( range ( 1 , next_try ) ) elif try_number < 1 : logs = [ 'error fetching the logs. try number {} is invalid.' . format ( try_number ) , ] return logs else : try_numbers = [ try_number ] logs = [ '' ] * len ( try_numbers ) metadatas = [ { } ] * len ( try_numbers ) for i , try_number in enumerate ( try_numbers ) : log , metadata = self . _read ( task_instance , try_number , metadata ) logs [ i ] += log metadatas [ i ] = metadata return logs , metadatas"
406,apache/airflow,airflow/utils/log/file_task_handler.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/file_task_handler.py#L170-L207,"def _init_file(self, ti):
        """"""
        Create log directory and give it correct permissions.
        :param ti: task instance object
        :return: relative log path of the given task instance
        """"""
        # To handle log writing when tasks are impersonated, the log files need to
        # be writable by the user that runs the Airflow command and the user
        # that is impersonated. This is mainly to handle corner cases with the
        # SubDagOperator. When the SubDagOperator is run, all of the operators
        # run under the impersonated user and create appropriate log files
        # as the impersonated user. However, if the user manually runs tasks
        # of the SubDagOperator through the UI, then the log files are created
        # by the user that runs the Airflow command. For example, the Airflow
        # run command may be run by the `airflow_sudoable` user, but the Airflow
        # tasks may be run by the `airflow` user. If the log files are not
        # writable by both users, then it's possible that re-running a task
        # via the UI (or vice versa) results in a permission error as the task
        # tries to write to a log file created by the other user.
        relative_path = self._render_filename(ti, ti.try_number)
        full_path = os.path.join(self.local_base, relative_path)
        directory = os.path.dirname(full_path)
        # Create the log file and give it group writable permissions
        # TODO(aoen): Make log dirs and logs globally readable for now since the SubDag
        # operator is not compatible with impersonation (e.g. if a Celery executor is used
        # for a SubDag operator and the SubDag operator has a different owner than the
        # parent DAG)
        if not os.path.exists(directory):
            # Create the directory as globally writable using custom mkdirs
            # as os.makedirs doesn't set mode properly.
            mkdirs(directory, 0o777)

        if not os.path.exists(full_path):
            open(full_path, ""a"").close()
            # TODO: Investigate using 444 instead of 666.
            os.chmod(full_path, 0o666)

        return full_path","['def', '_init_file', '(', 'self', ',', 'ti', ')', ':', '# To handle log writing when tasks are impersonated, the log files need to', '# be writable by the user that runs the Airflow command and the user', '# that is impersonated. This is mainly to handle corner cases with the', '# SubDagOperator. When the SubDagOperator is run, all of the operators', '# run under the impersonated user and create appropriate log files', '# as the impersonated user. However, if the user manually runs tasks', '# of the SubDagOperator through the UI, then the log files are created', '# by the user that runs the Airflow command. For example, the Airflow', '# run command may be run by the `airflow_sudoable` user, but the Airflow', '# tasks may be run by the `airflow` user. If the log files are not', ""# writable by both users, then it's possible that re-running a task"", '# via the UI (or vice versa) results in a permission error as the task', '# tries to write to a log file created by the other user.', 'relative_path', '=', 'self', '.', '_render_filename', '(', 'ti', ',', 'ti', '.', 'try_number', ')', 'full_path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'local_base', ',', 'relative_path', ')', 'directory', '=', 'os', '.', 'path', '.', 'dirname', '(', 'full_path', ')', '# Create the log file and give it group writable permissions', '# TODO(aoen): Make log dirs and logs globally readable for now since the SubDag', '# operator is not compatible with impersonation (e.g. if a Celery executor is used', '# for a SubDag operator and the SubDag operator has a different owner than the', '# parent DAG)', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'directory', ')', ':', '# Create the directory as globally writable using custom mkdirs', ""# as os.makedirs doesn't set mode properly."", 'mkdirs', '(', 'directory', ',', '0o777', ')', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'full_path', ')', ':', 'open', '(', 'full_path', ',', '""a""', ')', '.', 'close', '(', ')', '# TODO: Investigate using 444 instead of 666.', 'os', '.', 'chmod', '(', 'full_path', ',', '0o666', ')', 'return', 'full_path']","Create log directory and give it correct permissions.
        :param ti: task instance object
        :return: relative log path of the given task instance","['Create', 'log', 'directory', 'and', 'give', 'it', 'correct', 'permissions', '.', ':', 'param', 'ti', ':', 'task', 'instance', 'object', ':', 'return', ':', 'relative', 'log', 'path', 'of', 'the', 'given', 'task', 'instance']",python,test,"['create', 'log', 'directory', 'and', 'give', 'it', 'correct', 'permissions', '.', ':', 'param', 'ti', ':', 'task', 'instance', 'object', ':', 'return', ':', 'relative', 'log', 'path', 'of', 'the', 'given', 'task', 'instance']",create log directory and give it correct permissions . : param ti : task instance object : return : relative log path of the given task instance,"['def', '_init_file', '(', 'self', ',', 'ti', ')', ':', '# to handle log writing when tasks are impersonated, the log files need to', '# be writable by the user that runs the airflow command and the user', '# that is impersonated. this is mainly to handle corner cases with the', '# subdagoperator. when the subdagoperator is run, all of the operators', '# run under the impersonated user and create appropriate log files', '# as the impersonated user. however, if the user manually runs tasks', '# of the subdagoperator through the ui, then the log files are created', '# by the user that runs the airflow command. for example, the airflow', '# run command may be run by the `airflow_sudoable` user, but the airflow', '# tasks may be run by the `airflow` user. if the log files are not', ""# writable by both users, then it's possible that re-running a task"", '# via the ui (or vice versa) results in a permission error as the task', '# tries to write to a log file created by the other user.', 'relative_path', '=', 'self', '.', '_render_filename', '(', 'ti', ',', 'ti', '.', 'try_number', ')', 'full_path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'local_base', ',', 'relative_path', ')', 'directory', '=', 'os', '.', 'path', '.', 'dirname', '(', 'full_path', ')', '# create the log file and give it group writable permissions', '# todo(aoen): make log dirs and logs globally readable for now since the subdag', '# operator is not compatible with impersonation (e.g. if a celery executor is used', '# for a subdag operator and the subdag operator has a different owner than the', '# parent dag)', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'directory', ')', ':', '# create the directory as globally writable using custom mkdirs', ""# as os.makedirs doesn't set mode properly."", 'mkdirs', '(', 'directory', ',', '0o777', ')', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'full_path', ')', ':', 'open', '(', 'full_path', ',', '""a""', ')', '.', 'close', '(', ')', '# todo: investigate using 444 instead of 666.', 'os', '.', 'chmod', '(', 'full_path', ',', '0o666', ')', 'return', 'full_path']","def _init_file ( self , ti ) : # to handle log writing when tasks are impersonated, the log files need to # be writable by the user that runs the airflow command and the user # that is impersonated. this is mainly to handle corner cases with the # subdagoperator. when the subdagoperator is run, all of the operators # run under the impersonated user and create appropriate log files # as the impersonated user. however, if the user manually runs tasks # of the subdagoperator through the ui, then the log files are created # by the user that runs the airflow command. for example, the airflow # run command may be run by the `airflow_sudoable` user, but the airflow # tasks may be run by the `airflow` user. if the log files are not # writable by both users, then it's possible that re-running a task # via the ui (or vice versa) results in a permission error as the task # tries to write to a log file created by the other user. relative_path = self . _render_filename ( ti , ti . try_number ) full_path = os . path . join ( self . local_base , relative_path ) directory = os . path . dirname ( full_path ) # create the log file and give it group writable permissions # todo(aoen): make log dirs and logs globally readable for now since the subdag # operator is not compatible with impersonation (e.g. if a celery executor is used # for a subdag operator and the subdag operator has a different owner than the # parent dag) if not os . path . exists ( directory ) : # create the directory as globally writable using custom mkdirs # as os.makedirs doesn't set mode properly. mkdirs ( directory , 0o777 ) if not os . path . exists ( full_path ) : open ( full_path , ""a"" ) . close ( ) # todo: investigate using 444 instead of 666. os . chmod ( full_path , 0o666 ) return full_path"
407,apache/airflow,airflow/plugins_manager.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/plugins_manager.py#L79-L98,"def load_entrypoint_plugins(entry_points, airflow_plugins):
    """"""
    Load AirflowPlugin subclasses from the entrypoints
    provided. The entry_point group should be 'airflow.plugins'.

    :param entry_points: A collection of entrypoints to search for plugins
    :type entry_points: Generator[setuptools.EntryPoint, None, None]
    :param airflow_plugins: A collection of existing airflow plugins to
        ensure we don't load duplicates
    :type airflow_plugins: list[type[airflow.plugins_manager.AirflowPlugin]]
    :rtype: list[airflow.plugins_manager.AirflowPlugin]
    """"""
    for entry_point in entry_points:
        log.debug('Importing entry_point plugin %s', entry_point.name)
        plugin_obj = entry_point.load()
        if is_valid_plugin(plugin_obj, airflow_plugins):
            if callable(getattr(plugin_obj, 'on_load', None)):
                plugin_obj.on_load()
                airflow_plugins.append(plugin_obj)
    return airflow_plugins","['def', 'load_entrypoint_plugins', '(', 'entry_points', ',', 'airflow_plugins', ')', ':', 'for', 'entry_point', 'in', 'entry_points', ':', 'log', '.', 'debug', '(', ""'Importing entry_point plugin %s'"", ',', 'entry_point', '.', 'name', ')', 'plugin_obj', '=', 'entry_point', '.', 'load', '(', ')', 'if', 'is_valid_plugin', '(', 'plugin_obj', ',', 'airflow_plugins', ')', ':', 'if', 'callable', '(', 'getattr', '(', 'plugin_obj', ',', ""'on_load'"", ',', 'None', ')', ')', ':', 'plugin_obj', '.', 'on_load', '(', ')', 'airflow_plugins', '.', 'append', '(', 'plugin_obj', ')', 'return', 'airflow_plugins']","Load AirflowPlugin subclasses from the entrypoints
    provided. The entry_point group should be 'airflow.plugins'.

    :param entry_points: A collection of entrypoints to search for plugins
    :type entry_points: Generator[setuptools.EntryPoint, None, None]
    :param airflow_plugins: A collection of existing airflow plugins to
        ensure we don't load duplicates
    :type airflow_plugins: list[type[airflow.plugins_manager.AirflowPlugin]]
    :rtype: list[airflow.plugins_manager.AirflowPlugin]","['Load', 'AirflowPlugin', 'subclasses', 'from', 'the', 'entrypoints', 'provided', '.', 'The', 'entry_point', 'group', 'should', 'be', 'airflow', '.', 'plugins', '.']",python,test,"['load', 'airflowplugin', 'subclasses', 'from', 'the', 'entrypoints', 'provided', '.', 'the', 'entry_point', 'group', 'should', 'be', 'airflow', '.', 'plugins', '.']",load airflowplugin subclasses from the entrypoints provided . the entry_point group should be airflow . plugins .,"['def', 'load_entrypoint_plugins', '(', 'entry_points', ',', 'airflow_plugins', ')', ':', 'for', 'entry_point', 'in', 'entry_points', ':', 'log', '.', 'debug', '(', ""'importing entry_point plugin %s'"", ',', 'entry_point', '.', 'name', ')', 'plugin_obj', '=', 'entry_point', '.', 'load', '(', ')', 'if', 'is_valid_plugin', '(', 'plugin_obj', ',', 'airflow_plugins', ')', ':', 'if', 'callable', '(', 'getattr', '(', 'plugin_obj', ',', ""'on_load'"", ',', 'none', ')', ')', ':', 'plugin_obj', '.', 'on_load', '(', ')', 'airflow_plugins', '.', 'append', '(', 'plugin_obj', ')', 'return', 'airflow_plugins']","def load_entrypoint_plugins ( entry_points , airflow_plugins ) : for entry_point in entry_points : log . debug ( 'importing entry_point plugin %s' , entry_point . name ) plugin_obj = entry_point . load ( ) if is_valid_plugin ( plugin_obj , airflow_plugins ) : if callable ( getattr ( plugin_obj , 'on_load' , none ) ) : plugin_obj . on_load ( ) airflow_plugins . append ( plugin_obj ) return airflow_plugins"
408,apache/airflow,airflow/plugins_manager.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/plugins_manager.py#L101-L118,"def is_valid_plugin(plugin_obj, existing_plugins):
    """"""
    Check whether a potential object is a subclass of
    the AirflowPlugin class.

    :param plugin_obj: potential subclass of AirflowPlugin
    :param existing_plugins: Existing list of AirflowPlugin subclasses
    :return: Whether or not the obj is a valid subclass of
        AirflowPlugin
    """"""
    if (
        inspect.isclass(plugin_obj) and
        issubclass(plugin_obj, AirflowPlugin) and
        (plugin_obj is not AirflowPlugin)
    ):
        plugin_obj.validate()
        return plugin_obj not in existing_plugins
    return False","['def', 'is_valid_plugin', '(', 'plugin_obj', ',', 'existing_plugins', ')', ':', 'if', '(', 'inspect', '.', 'isclass', '(', 'plugin_obj', ')', 'and', 'issubclass', '(', 'plugin_obj', ',', 'AirflowPlugin', ')', 'and', '(', 'plugin_obj', 'is', 'not', 'AirflowPlugin', ')', ')', ':', 'plugin_obj', '.', 'validate', '(', ')', 'return', 'plugin_obj', 'not', 'in', 'existing_plugins', 'return', 'False']","Check whether a potential object is a subclass of
    the AirflowPlugin class.

    :param plugin_obj: potential subclass of AirflowPlugin
    :param existing_plugins: Existing list of AirflowPlugin subclasses
    :return: Whether or not the obj is a valid subclass of
        AirflowPlugin","['Check', 'whether', 'a', 'potential', 'object', 'is', 'a', 'subclass', 'of', 'the', 'AirflowPlugin', 'class', '.']",python,test,"['check', 'whether', 'a', 'potential', 'object', 'is', 'a', 'subclass', 'of', 'the', 'airflowplugin', 'class', '.']",check whether a potential object is a subclass of the airflowplugin class .,"['def', 'is_valid_plugin', '(', 'plugin_obj', ',', 'existing_plugins', ')', ':', 'if', '(', 'inspect', '.', 'isclass', '(', 'plugin_obj', ')', 'and', 'issubclass', '(', 'plugin_obj', ',', 'airflowplugin', ')', 'and', '(', 'plugin_obj', 'is', 'not', 'airflowplugin', ')', ')', ':', 'plugin_obj', '.', 'validate', '(', ')', 'return', 'plugin_obj', 'not', 'in', 'existing_plugins', 'return', 'false']","def is_valid_plugin ( plugin_obj , existing_plugins ) : if ( inspect . isclass ( plugin_obj ) and issubclass ( plugin_obj , airflowplugin ) and ( plugin_obj is not airflowplugin ) ) : plugin_obj . validate ( ) return plugin_obj not in existing_plugins return false"
409,apache/airflow,airflow/models/skipmixin.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/models/skipmixin.py#L29-L66,"def skip(self, dag_run, execution_date, tasks, session=None):
        """"""
        Sets tasks instances to skipped from the same dag run.

        :param dag_run: the DagRun for which to set the tasks to skipped
        :param execution_date: execution_date
        :param tasks: tasks to skip (not task_ids)
        :param session: db session to use
        """"""
        if not tasks:
            return

        task_ids = [d.task_id for d in tasks]
        now = timezone.utcnow()

        if dag_run:
            session.query(TaskInstance).filter(
                TaskInstance.dag_id == dag_run.dag_id,
                TaskInstance.execution_date == dag_run.execution_date,
                TaskInstance.task_id.in_(task_ids)
            ).update({TaskInstance.state: State.SKIPPED,
                      TaskInstance.start_date: now,
                      TaskInstance.end_date: now},
                     synchronize_session=False)
            session.commit()
        else:
            assert execution_date is not None, ""Execution date is None and no dag run""

            self.log.warning(""No DAG RUN present this should not happen"")
            # this is defensive against dag runs that are not complete
            for task in tasks:
                ti = TaskInstance(task, execution_date=execution_date)
                ti.state = State.SKIPPED
                ti.start_date = now
                ti.end_date = now
                session.merge(ti)

            session.commit()","['def', 'skip', '(', 'self', ',', 'dag_run', ',', 'execution_date', ',', 'tasks', ',', 'session', '=', 'None', ')', ':', 'if', 'not', 'tasks', ':', 'return', 'task_ids', '=', '[', 'd', '.', 'task_id', 'for', 'd', 'in', 'tasks', ']', 'now', '=', 'timezone', '.', 'utcnow', '(', ')', 'if', 'dag_run', ':', 'session', '.', 'query', '(', 'TaskInstance', ')', '.', 'filter', '(', 'TaskInstance', '.', 'dag_id', '==', 'dag_run', '.', 'dag_id', ',', 'TaskInstance', '.', 'execution_date', '==', 'dag_run', '.', 'execution_date', ',', 'TaskInstance', '.', 'task_id', '.', 'in_', '(', 'task_ids', ')', ')', '.', 'update', '(', '{', 'TaskInstance', '.', 'state', ':', 'State', '.', 'SKIPPED', ',', 'TaskInstance', '.', 'start_date', ':', 'now', ',', 'TaskInstance', '.', 'end_date', ':', 'now', '}', ',', 'synchronize_session', '=', 'False', ')', 'session', '.', 'commit', '(', ')', 'else', ':', 'assert', 'execution_date', 'is', 'not', 'None', ',', '""Execution date is None and no dag run""', 'self', '.', 'log', '.', 'warning', '(', '""No DAG RUN present this should not happen""', ')', '# this is defensive against dag runs that are not complete', 'for', 'task', 'in', 'tasks', ':', 'ti', '=', 'TaskInstance', '(', 'task', ',', 'execution_date', '=', 'execution_date', ')', 'ti', '.', 'state', '=', 'State', '.', 'SKIPPED', 'ti', '.', 'start_date', '=', 'now', 'ti', '.', 'end_date', '=', 'now', 'session', '.', 'merge', '(', 'ti', ')', 'session', '.', 'commit', '(', ')']","Sets tasks instances to skipped from the same dag run.

        :param dag_run: the DagRun for which to set the tasks to skipped
        :param execution_date: execution_date
        :param tasks: tasks to skip (not task_ids)
        :param session: db session to use","['Sets', 'tasks', 'instances', 'to', 'skipped', 'from', 'the', 'same', 'dag', 'run', '.']",python,test,"['sets', 'tasks', 'instances', 'to', 'skipped', 'from', 'the', 'same', 'dag', 'run', '.']",sets tasks instances to skipped from the same dag run .,"['def', 'skip', '(', 'self', ',', 'dag_run', ',', 'execution_date', ',', 'tasks', ',', 'session', '=', 'none', ')', ':', 'if', 'not', 'tasks', ':', 'return', 'task_ids', '=', '[', 'd', '.', 'task_id', 'for', 'd', 'in', 'tasks', ']', 'now', '=', 'timezone', '.', 'utcnow', '(', ')', 'if', 'dag_run', ':', 'session', '.', 'query', '(', 'taskinstance', ')', '.', 'filter', '(', 'taskinstance', '.', 'dag_id', '==', 'dag_run', '.', 'dag_id', ',', 'taskinstance', '.', 'execution_date', '==', 'dag_run', '.', 'execution_date', ',', 'taskinstance', '.', 'task_id', '.', 'in_', '(', 'task_ids', ')', ')', '.', 'update', '(', '{', 'taskinstance', '.', 'state', ':', 'state', '.', 'skipped', ',', 'taskinstance', '.', 'start_date', ':', 'now', ',', 'taskinstance', '.', 'end_date', ':', 'now', '}', ',', 'synchronize_session', '=', 'false', ')', 'session', '.', 'commit', '(', ')', 'else', ':', 'assert', 'execution_date', 'is', 'not', 'none', ',', '""execution date is none and no dag run""', 'self', '.', 'log', '.', 'warning', '(', '""no dag run present this should not happen""', ')', '# this is defensive against dag runs that are not complete', 'for', 'task', 'in', 'tasks', ':', 'ti', '=', 'taskinstance', '(', 'task', ',', 'execution_date', '=', 'execution_date', ')', 'ti', '.', 'state', '=', 'state', '.', 'skipped', 'ti', '.', 'start_date', '=', 'now', 'ti', '.', 'end_date', '=', 'now', 'session', '.', 'merge', '(', 'ti', ')', 'session', '.', 'commit', '(', ')']","def skip ( self , dag_run , execution_date , tasks , session = none ) : if not tasks : return task_ids = [ d . task_id for d in tasks ] now = timezone . utcnow ( ) if dag_run : session . query ( taskinstance ) . filter ( taskinstance . dag_id == dag_run . dag_id , taskinstance . execution_date == dag_run . execution_date , taskinstance . task_id . in_ ( task_ids ) ) . update ( { taskinstance . state : state . skipped , taskinstance . start_date : now , taskinstance . end_date : now } , synchronize_session = false ) session . commit ( ) else : assert execution_date is not none , ""execution date is none and no dag run"" self . log . warning ( ""no dag run present this should not happen"" ) # this is defensive against dag runs that are not complete for task in tasks : ti = taskinstance ( task , execution_date = execution_date ) ti . state = state . skipped ti . start_date = now ti . end_date = now session . merge ( ti ) session . commit ( )"
410,apache/airflow,airflow/contrib/hooks/azure_data_lake_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_data_lake_hook.py#L41-L53,"def get_conn(self):
        """"""Return a AzureDLFileSystem object.""""""
        conn = self.get_connection(self.conn_id)
        service_options = conn.extra_dejson
        self.account_name = service_options.get('account_name')

        adlCreds = lib.auth(tenant_id=service_options.get('tenant'),
                            client_secret=conn.password,
                            client_id=conn.login)
        adlsFileSystemClient = core.AzureDLFileSystem(adlCreds,
                                                      store_name=self.account_name)
        adlsFileSystemClient.connect()
        return adlsFileSystemClient","['def', 'get_conn', '(', 'self', ')', ':', 'conn', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'conn_id', ')', 'service_options', '=', 'conn', '.', 'extra_dejson', 'self', '.', 'account_name', '=', 'service_options', '.', 'get', '(', ""'account_name'"", ')', 'adlCreds', '=', 'lib', '.', 'auth', '(', 'tenant_id', '=', 'service_options', '.', 'get', '(', ""'tenant'"", ')', ',', 'client_secret', '=', 'conn', '.', 'password', ',', 'client_id', '=', 'conn', '.', 'login', ')', 'adlsFileSystemClient', '=', 'core', '.', 'AzureDLFileSystem', '(', 'adlCreds', ',', 'store_name', '=', 'self', '.', 'account_name', ')', 'adlsFileSystemClient', '.', 'connect', '(', ')', 'return', 'adlsFileSystemClient']",Return a AzureDLFileSystem object.,"['Return', 'a', 'AzureDLFileSystem', 'object', '.']",python,test,"['return', 'a', 'azuredlfilesystem', 'object', '.']",return a azuredlfilesystem object .,"['def', 'get_conn', '(', 'self', ')', ':', 'conn', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'conn_id', ')', 'service_options', '=', 'conn', '.', 'extra_dejson', 'self', '.', 'account_name', '=', 'service_options', '.', 'get', '(', ""'account_name'"", ')', 'adlcreds', '=', 'lib', '.', 'auth', '(', 'tenant_id', '=', 'service_options', '.', 'get', '(', ""'tenant'"", ')', ',', 'client_secret', '=', 'conn', '.', 'password', ',', 'client_id', '=', 'conn', '.', 'login', ')', 'adlsfilesystemclient', '=', 'core', '.', 'azuredlfilesystem', '(', 'adlcreds', ',', 'store_name', '=', 'self', '.', 'account_name', ')', 'adlsfilesystemclient', '.', 'connect', '(', ')', 'return', 'adlsfilesystemclient']","def get_conn ( self ) : conn = self . get_connection ( self . conn_id ) service_options = conn . extra_dejson self . account_name = service_options . get ( 'account_name' ) adlcreds = lib . auth ( tenant_id = service_options . get ( 'tenant' ) , client_secret = conn . password , client_id = conn . login ) adlsfilesystemclient = core . azuredlfilesystem ( adlcreds , store_name = self . account_name ) adlsfilesystemclient . connect ( ) return adlsfilesystemclient"
411,apache/airflow,airflow/contrib/hooks/azure_data_lake_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_data_lake_hook.py#L55-L68,"def check_for_file(self, file_path):
        """"""
        Check if a file exists on Azure Data Lake.

        :param file_path: Path and name of the file.
        :type file_path: str
        :return: True if the file exists, False otherwise.
        :rtype: bool
        """"""
        try:
            files = self.connection.glob(file_path, details=False, invalidate_cache=True)
            return len(files) == 1
        except FileNotFoundError:
            return False","['def', 'check_for_file', '(', 'self', ',', 'file_path', ')', ':', 'try', ':', 'files', '=', 'self', '.', 'connection', '.', 'glob', '(', 'file_path', ',', 'details', '=', 'False', ',', 'invalidate_cache', '=', 'True', ')', 'return', 'len', '(', 'files', ')', '==', '1', 'except', 'FileNotFoundError', ':', 'return', 'False']","Check if a file exists on Azure Data Lake.

        :param file_path: Path and name of the file.
        :type file_path: str
        :return: True if the file exists, False otherwise.
        :rtype: bool","['Check', 'if', 'a', 'file', 'exists', 'on', 'Azure', 'Data', 'Lake', '.']",python,test,"['check', 'if', 'a', 'file', 'exists', 'on', 'azure', 'data', 'lake', '.']",check if a file exists on azure data lake .,"['def', 'check_for_file', '(', 'self', ',', 'file_path', ')', ':', 'try', ':', 'files', '=', 'self', '.', 'connection', '.', 'glob', '(', 'file_path', ',', 'details', '=', 'false', ',', 'invalidate_cache', '=', 'true', ')', 'return', 'len', '(', 'files', ')', '==', '1', 'except', 'filenotfounderror', ':', 'return', 'false']","def check_for_file ( self , file_path ) : try : files = self . connection . glob ( file_path , details = false , invalidate_cache = true ) return len ( files ) == 1 except filenotfounderror : return false"
412,apache/airflow,airflow/contrib/hooks/azure_data_lake_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_data_lake_hook.py#L70-L104,"def upload_file(self, local_path, remote_path, nthreads=64, overwrite=True,
                    buffersize=4194304, blocksize=4194304):
        """"""
        Upload a file to Azure Data Lake.

        :param local_path: local path. Can be single file, directory (in which case,
            upload recursively) or glob pattern. Recursive glob patterns using `**`
            are not supported.
        :type local_path: str
        :param remote_path: Remote path to upload to; if multiple files, this is the
            directory root to write within.
        :type remote_path: str
        :param nthreads: Number of threads to use. If None, uses the number of cores.
        :type nthreads: int
        :param overwrite: Whether to forcibly overwrite existing files/directories.
            If False and remote path is a directory, will quit regardless if any files
            would be overwritten or not. If True, only matching filenames are actually
            overwritten.
        :type overwrite: bool
        :param buffersize: int [2**22]
            Number of bytes for internal buffer. This block cannot be bigger than
            a chunk and cannot be smaller than a block.
        :type buffersize: int
        :param blocksize: int [2**22]
            Number of bytes for a block. Within each chunk, we write a smaller
            block for each API call. This block cannot be bigger than a chunk.
        :type blocksize: int
        """"""
        multithread.ADLUploader(self.connection,
                                lpath=local_path,
                                rpath=remote_path,
                                nthreads=nthreads,
                                overwrite=overwrite,
                                buffersize=buffersize,
                                blocksize=blocksize)","['def', 'upload_file', '(', 'self', ',', 'local_path', ',', 'remote_path', ',', 'nthreads', '=', '64', ',', 'overwrite', '=', 'True', ',', 'buffersize', '=', '4194304', ',', 'blocksize', '=', '4194304', ')', ':', 'multithread', '.', 'ADLUploader', '(', 'self', '.', 'connection', ',', 'lpath', '=', 'local_path', ',', 'rpath', '=', 'remote_path', ',', 'nthreads', '=', 'nthreads', ',', 'overwrite', '=', 'overwrite', ',', 'buffersize', '=', 'buffersize', ',', 'blocksize', '=', 'blocksize', ')']","Upload a file to Azure Data Lake.

        :param local_path: local path. Can be single file, directory (in which case,
            upload recursively) or glob pattern. Recursive glob patterns using `**`
            are not supported.
        :type local_path: str
        :param remote_path: Remote path to upload to; if multiple files, this is the
            directory root to write within.
        :type remote_path: str
        :param nthreads: Number of threads to use. If None, uses the number of cores.
        :type nthreads: int
        :param overwrite: Whether to forcibly overwrite existing files/directories.
            If False and remote path is a directory, will quit regardless if any files
            would be overwritten or not. If True, only matching filenames are actually
            overwritten.
        :type overwrite: bool
        :param buffersize: int [2**22]
            Number of bytes for internal buffer. This block cannot be bigger than
            a chunk and cannot be smaller than a block.
        :type buffersize: int
        :param blocksize: int [2**22]
            Number of bytes for a block. Within each chunk, we write a smaller
            block for each API call. This block cannot be bigger than a chunk.
        :type blocksize: int","['Upload', 'a', 'file', 'to', 'Azure', 'Data', 'Lake', '.']",python,test,"['upload', 'a', 'file', 'to', 'azure', 'data', 'lake', '.']",upload a file to azure data lake .,"['def', 'upload_file', '(', 'self', ',', 'local_path', ',', 'remote_path', ',', 'nthreads', '=', '64', ',', 'overwrite', '=', 'true', ',', 'buffersize', '=', '4194304', ',', 'blocksize', '=', '4194304', ')', ':', 'multithread', '.', 'adluploader', '(', 'self', '.', 'connection', ',', 'lpath', '=', 'local_path', ',', 'rpath', '=', 'remote_path', ',', 'nthreads', '=', 'nthreads', ',', 'overwrite', '=', 'overwrite', ',', 'buffersize', '=', 'buffersize', ',', 'blocksize', '=', 'blocksize', ')']","def upload_file ( self , local_path , remote_path , nthreads = 64 , overwrite = true , buffersize = 4194304 , blocksize = 4194304 ) : multithread . adluploader ( self . connection , lpath = local_path , rpath = remote_path , nthreads = nthreads , overwrite = overwrite , buffersize = buffersize , blocksize = blocksize )"
413,apache/airflow,airflow/contrib/hooks/azure_data_lake_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_data_lake_hook.py#L106-L141,"def download_file(self, local_path, remote_path, nthreads=64, overwrite=True,
                      buffersize=4194304, blocksize=4194304):
        """"""
        Download a file from Azure Blob Storage.

        :param local_path: local path. If downloading a single file, will write to this
            specific file, unless it is an existing directory, in which case a file is
            created within it. If downloading multiple files, this is the root
            directory to write within. Will create directories as required.
        :type local_path: str
        :param remote_path: remote path/globstring to use to find remote files.
            Recursive glob patterns using `**` are not supported.
        :type remote_path: str
        :param nthreads: Number of threads to use. If None, uses the number of cores.
        :type nthreads: int
        :param overwrite: Whether to forcibly overwrite existing files/directories.
            If False and remote path is a directory, will quit regardless if any files
            would be overwritten or not. If True, only matching filenames are actually
            overwritten.
        :type overwrite: bool
        :param buffersize: int [2**22]
            Number of bytes for internal buffer. This block cannot be bigger than
            a chunk and cannot be smaller than a block.
        :type buffersize: int
        :param blocksize: int [2**22]
            Number of bytes for a block. Within each chunk, we write a smaller
            block for each API call. This block cannot be bigger than a chunk.
        :type blocksize: int
        """"""
        multithread.ADLDownloader(self.connection,
                                  lpath=local_path,
                                  rpath=remote_path,
                                  nthreads=nthreads,
                                  overwrite=overwrite,
                                  buffersize=buffersize,
                                  blocksize=blocksize)","['def', 'download_file', '(', 'self', ',', 'local_path', ',', 'remote_path', ',', 'nthreads', '=', '64', ',', 'overwrite', '=', 'True', ',', 'buffersize', '=', '4194304', ',', 'blocksize', '=', '4194304', ')', ':', 'multithread', '.', 'ADLDownloader', '(', 'self', '.', 'connection', ',', 'lpath', '=', 'local_path', ',', 'rpath', '=', 'remote_path', ',', 'nthreads', '=', 'nthreads', ',', 'overwrite', '=', 'overwrite', ',', 'buffersize', '=', 'buffersize', ',', 'blocksize', '=', 'blocksize', ')']","Download a file from Azure Blob Storage.

        :param local_path: local path. If downloading a single file, will write to this
            specific file, unless it is an existing directory, in which case a file is
            created within it. If downloading multiple files, this is the root
            directory to write within. Will create directories as required.
        :type local_path: str
        :param remote_path: remote path/globstring to use to find remote files.
            Recursive glob patterns using `**` are not supported.
        :type remote_path: str
        :param nthreads: Number of threads to use. If None, uses the number of cores.
        :type nthreads: int
        :param overwrite: Whether to forcibly overwrite existing files/directories.
            If False and remote path is a directory, will quit regardless if any files
            would be overwritten or not. If True, only matching filenames are actually
            overwritten.
        :type overwrite: bool
        :param buffersize: int [2**22]
            Number of bytes for internal buffer. This block cannot be bigger than
            a chunk and cannot be smaller than a block.
        :type buffersize: int
        :param blocksize: int [2**22]
            Number of bytes for a block. Within each chunk, we write a smaller
            block for each API call. This block cannot be bigger than a chunk.
        :type blocksize: int","['Download', 'a', 'file', 'from', 'Azure', 'Blob', 'Storage', '.']",python,test,"['download', 'a', 'file', 'from', 'azure', 'blob', 'storage', '.']",download a file from azure blob storage .,"['def', 'download_file', '(', 'self', ',', 'local_path', ',', 'remote_path', ',', 'nthreads', '=', '64', ',', 'overwrite', '=', 'true', ',', 'buffersize', '=', '4194304', ',', 'blocksize', '=', '4194304', ')', ':', 'multithread', '.', 'adldownloader', '(', 'self', '.', 'connection', ',', 'lpath', '=', 'local_path', ',', 'rpath', '=', 'remote_path', ',', 'nthreads', '=', 'nthreads', ',', 'overwrite', '=', 'overwrite', ',', 'buffersize', '=', 'buffersize', ',', 'blocksize', '=', 'blocksize', ')']","def download_file ( self , local_path , remote_path , nthreads = 64 , overwrite = true , buffersize = 4194304 , blocksize = 4194304 ) : multithread . adldownloader ( self . connection , lpath = local_path , rpath = remote_path , nthreads = nthreads , overwrite = overwrite , buffersize = buffersize , blocksize = blocksize )"
414,apache/airflow,airflow/contrib/hooks/azure_data_lake_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_data_lake_hook.py#L143-L153,"def list(self, path):
        """"""
        List files in Azure Data Lake Storage

        :param path: full path/globstring to use to list files in ADLS
        :type path: str
        """"""
        if ""*"" in path:
            return self.connection.glob(path)
        else:
            return self.connection.walk(path)","['def', 'list', '(', 'self', ',', 'path', ')', ':', 'if', '""*""', 'in', 'path', ':', 'return', 'self', '.', 'connection', '.', 'glob', '(', 'path', ')', 'else', ':', 'return', 'self', '.', 'connection', '.', 'walk', '(', 'path', ')']","List files in Azure Data Lake Storage

        :param path: full path/globstring to use to list files in ADLS
        :type path: str","['List', 'files', 'in', 'Azure', 'Data', 'Lake', 'Storage']",python,test,"['list', 'files', 'in', 'azure', 'data', 'lake', 'storage']",list files in azure data lake storage,"['def', 'list', '(', 'self', ',', 'path', ')', ':', 'if', '""*""', 'in', 'path', ':', 'return', 'self', '.', 'connection', '.', 'glob', '(', 'path', ')', 'else', ':', 'return', 'self', '.', 'connection', '.', 'walk', '(', 'path', ')']","def list ( self , path ) : if ""*"" in path : return self . connection . glob ( path ) else : return self . connection . walk ( path )"
415,apache/airflow,airflow/contrib/operators/aws_athena_operator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/aws_athena_operator.py#L70-L91,"def execute(self, context):
        """"""
        Run Presto Query on Athena
        """"""
        self.hook = self.get_hook()
        self.hook.get_conn()

        self.query_execution_context['Database'] = self.database
        self.result_configuration['OutputLocation'] = self.output_location
        self.query_execution_id = self.hook.run_query(self.query, self.query_execution_context,
                                                      self.result_configuration, self.client_request_token)
        query_status = self.hook.poll_query_status(self.query_execution_id, self.max_tries)

        if query_status in AWSAthenaHook.FAILURE_STATES:
            raise Exception(
                'Final state of Athena job is {}, query_execution_id is {}.'
                .format(query_status, self.query_execution_id))
        elif not query_status or query_status in AWSAthenaHook.INTERMEDIATE_STATES:
            raise Exception(
                'Final state of Athena job is {}. '
                'Max tries of poll status exceeded, query_execution_id is {}.'
                .format(query_status, self.query_execution_id))","['def', 'execute', '(', 'self', ',', 'context', ')', ':', 'self', '.', 'hook', '=', 'self', '.', 'get_hook', '(', ')', 'self', '.', 'hook', '.', 'get_conn', '(', ')', 'self', '.', 'query_execution_context', '[', ""'Database'"", ']', '=', 'self', '.', 'database', 'self', '.', 'result_configuration', '[', ""'OutputLocation'"", ']', '=', 'self', '.', 'output_location', 'self', '.', 'query_execution_id', '=', 'self', '.', 'hook', '.', 'run_query', '(', 'self', '.', 'query', ',', 'self', '.', 'query_execution_context', ',', 'self', '.', 'result_configuration', ',', 'self', '.', 'client_request_token', ')', 'query_status', '=', 'self', '.', 'hook', '.', 'poll_query_status', '(', 'self', '.', 'query_execution_id', ',', 'self', '.', 'max_tries', ')', 'if', 'query_status', 'in', 'AWSAthenaHook', '.', 'FAILURE_STATES', ':', 'raise', 'Exception', '(', ""'Final state of Athena job is {}, query_execution_id is {}.'"", '.', 'format', '(', 'query_status', ',', 'self', '.', 'query_execution_id', ')', ')', 'elif', 'not', 'query_status', 'or', 'query_status', 'in', 'AWSAthenaHook', '.', 'INTERMEDIATE_STATES', ':', 'raise', 'Exception', '(', ""'Final state of Athena job is {}. '"", ""'Max tries of poll status exceeded, query_execution_id is {}.'"", '.', 'format', '(', 'query_status', ',', 'self', '.', 'query_execution_id', ')', ')']",Run Presto Query on Athena,"['Run', 'Presto', 'Query', 'on', 'Athena']",python,test,"['run', 'presto', 'query', 'on', 'athena']",run presto query on athena,"['def', 'execute', '(', 'self', ',', 'context', ')', ':', 'self', '.', 'hook', '=', 'self', '.', 'get_hook', '(', ')', 'self', '.', 'hook', '.', 'get_conn', '(', ')', 'self', '.', 'query_execution_context', '[', ""'database'"", ']', '=', 'self', '.', 'database', 'self', '.', 'result_configuration', '[', ""'outputlocation'"", ']', '=', 'self', '.', 'output_location', 'self', '.', 'query_execution_id', '=', 'self', '.', 'hook', '.', 'run_query', '(', 'self', '.', 'query', ',', 'self', '.', 'query_execution_context', ',', 'self', '.', 'result_configuration', ',', 'self', '.', 'client_request_token', ')', 'query_status', '=', 'self', '.', 'hook', '.', 'poll_query_status', '(', 'self', '.', 'query_execution_id', ',', 'self', '.', 'max_tries', ')', 'if', 'query_status', 'in', 'awsathenahook', '.', 'failure_states', ':', 'raise', 'exception', '(', ""'final state of athena job is {}, query_execution_id is {}.'"", '.', 'format', '(', 'query_status', ',', 'self', '.', 'query_execution_id', ')', ')', 'elif', 'not', 'query_status', 'or', 'query_status', 'in', 'awsathenahook', '.', 'intermediate_states', ':', 'raise', 'exception', '(', ""'final state of athena job is {}. '"", ""'max tries of poll status exceeded, query_execution_id is {}.'"", '.', 'format', '(', 'query_status', ',', 'self', '.', 'query_execution_id', ')', ')']","def execute ( self , context ) : self . hook = self . get_hook ( ) self . hook . get_conn ( ) self . query_execution_context [ 'database' ] = self . database self . result_configuration [ 'outputlocation' ] = self . output_location self . query_execution_id = self . hook . run_query ( self . query , self . query_execution_context , self . result_configuration , self . client_request_token ) query_status = self . hook . poll_query_status ( self . query_execution_id , self . max_tries ) if query_status in awsathenahook . failure_states : raise exception ( 'final state of athena job is {}, query_execution_id is {}.' . format ( query_status , self . query_execution_id ) ) elif not query_status or query_status in awsathenahook . intermediate_states : raise exception ( 'final state of athena job is {}. ' 'max tries of poll status exceeded, query_execution_id is {}.' . format ( query_status , self . query_execution_id ) )"
416,apache/airflow,airflow/contrib/operators/aws_athena_operator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/aws_athena_operator.py#L93-L115,"def on_kill(self):
        """"""
        Cancel the submitted athena query
        """"""
        if self.query_execution_id:
            self.log.info('⚰️⚰️⚰️ Received a kill Signal. Time to Die')
            self.log.info(
                'Stopping Query with executionId - %s', self.query_execution_id
            )
            response = self.hook.stop_query(self.query_execution_id)
            http_status_code = None
            try:
                http_status_code = response['ResponseMetadata']['HTTPStatusCode']
            except Exception as ex:
                self.log.error('Exception while cancelling query', ex)
            finally:
                if http_status_code is None or http_status_code != 200:
                    self.log.error('Unable to request query cancel on athena. Exiting')
                else:
                    self.log.info(
                        'Polling Athena for query with id %s to reach final state', self.query_execution_id
                    )
                    self.hook.poll_query_status(self.query_execution_id)","['def', 'on_kill', '(', 'self', ')', ':', 'if', 'self', '.', 'query_execution_id', ':', 'self', '.', 'log', '.', 'info', '(', ""'⚰️⚰️⚰️ Received a kill Signal. Time to Die')"", '', 'self', '.', 'log', '.', 'info', '(', ""'Stopping Query with executionId - %s'"", ',', 'self', '.', 'query_execution_id', ')', 'response', '=', 'self', '.', 'hook', '.', 'stop_query', '(', 'self', '.', 'query_execution_id', ')', 'http_status_code', '=', 'None', 'try', ':', 'http_status_code', '=', 'response', '[', ""'ResponseMetadata'"", ']', '[', ""'HTTPStatusCode'"", ']', 'except', 'Exception', 'as', 'ex', ':', 'self', '.', 'log', '.', 'error', '(', ""'Exception while cancelling query'"", ',', 'ex', ')', 'finally', ':', 'if', 'http_status_code', 'is', 'None', 'or', 'http_status_code', '!=', '200', ':', 'self', '.', 'log', '.', 'error', '(', ""'Unable to request query cancel on athena. Exiting'"", ')', 'else', ':', 'self', '.', 'log', '.', 'info', '(', ""'Polling Athena for query with id %s to reach final state'"", ',', 'self', '.', 'query_execution_id', ')', 'self', '.', 'hook', '.', 'poll_query_status', '(', 'self', '.', 'query_execution_id', ')']",Cancel the submitted athena query,"['Cancel', 'the', 'submitted', 'athena', 'query']",python,test,"['cancel', 'the', 'submitted', 'athena', 'query']",cancel the submitted athena query,"['def', 'on_kill', '(', 'self', ')', ':', 'if', 'self', '.', 'query_execution_id', ':', 'self', '.', 'log', '.', 'info', '(', ""'⚰️⚰️⚰️ received a kill signal. time to die')"", '', 'self', '.', 'log', '.', 'info', '(', ""'stopping query with executionid - %s'"", ',', 'self', '.', 'query_execution_id', ')', 'response', '=', 'self', '.', 'hook', '.', 'stop_query', '(', 'self', '.', 'query_execution_id', ')', 'http_status_code', '=', 'none', 'try', ':', 'http_status_code', '=', 'response', '[', ""'responsemetadata'"", ']', '[', ""'httpstatuscode'"", ']', 'except', 'exception', 'as', 'ex', ':', 'self', '.', 'log', '.', 'error', '(', ""'exception while cancelling query'"", ',', 'ex', ')', 'finally', ':', 'if', 'http_status_code', 'is', 'none', 'or', 'http_status_code', '!=', '200', ':', 'self', '.', 'log', '.', 'error', '(', ""'unable to request query cancel on athena. exiting'"", ')', 'else', ':', 'self', '.', 'log', '.', 'info', '(', ""'polling athena for query with id %s to reach final state'"", ',', 'self', '.', 'query_execution_id', ')', 'self', '.', 'hook', '.', 'poll_query_status', '(', 'self', '.', 'query_execution_id', ')']","def on_kill ( self ) : if self . query_execution_id : self . log . info ( '⚰️⚰️⚰️ received a kill signal. time to die')  self . log . info ( 'stopping query with executionid - %s' , self . query_execution_id ) response = self . hook . stop_query ( self . query_execution_id ) http_status_code = none try : http_status_code = response [ 'responsemetadata' ] [ 'httpstatuscode' ] except exception as ex : self . log . error ( 'exception while cancelling query' , ex ) finally : if http_status_code is none or http_status_code != 200 : self . log . error ( 'unable to request query cancel on athena. exiting' ) else : self . log . info ( 'polling athena for query with id %s to reach final state' , self . query_execution_id ) self . hook . poll_query_status ( self . query_execution_id )"
417,apache/airflow,airflow/utils/compression.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/compression.py#L26-L43,"def uncompress_file(input_file_name, file_extension, dest_dir):
    """"""
    Uncompress gz and bz2 files
    """"""
    if file_extension.lower() not in ('.gz', '.bz2'):
        raise NotImplementedError(""Received {} format. Only gz and bz2 ""
                                  ""files can currently be uncompressed.""
                                  .format(file_extension))
    if file_extension.lower() == '.gz':
        fmodule = gzip.GzipFile
    elif file_extension.lower() == '.bz2':
        fmodule = bz2.BZ2File
    with fmodule(input_file_name, mode='rb') as f_compressed,\
        NamedTemporaryFile(dir=dest_dir,
                           mode='wb',
                           delete=False) as f_uncompressed:
        shutil.copyfileobj(f_compressed, f_uncompressed)
    return f_uncompressed.name","['def', 'uncompress_file', '(', 'input_file_name', ',', 'file_extension', ',', 'dest_dir', ')', ':', 'if', 'file_extension', '.', 'lower', '(', ')', 'not', 'in', '(', ""'.gz'"", ',', ""'.bz2'"", ')', ':', 'raise', 'NotImplementedError', '(', '""Received {} format. Only gz and bz2 ""', '""files can currently be uncompressed.""', '.', 'format', '(', 'file_extension', ')', ')', 'if', 'file_extension', '.', 'lower', '(', ')', '==', ""'.gz'"", ':', 'fmodule', '=', 'gzip', '.', 'GzipFile', 'elif', 'file_extension', '.', 'lower', '(', ')', '==', ""'.bz2'"", ':', 'fmodule', '=', 'bz2', '.', 'BZ2File', 'with', 'fmodule', '(', 'input_file_name', ',', 'mode', '=', ""'rb'"", ')', 'as', 'f_compressed', ',', 'NamedTemporaryFile', '(', 'dir', '=', 'dest_dir', ',', 'mode', '=', ""'wb'"", ',', 'delete', '=', 'False', ')', 'as', 'f_uncompressed', ':', 'shutil', '.', 'copyfileobj', '(', 'f_compressed', ',', 'f_uncompressed', ')', 'return', 'f_uncompressed', '.', 'name']",Uncompress gz and bz2 files,"['Uncompress', 'gz', 'and', 'bz2', 'files']",python,test,"['uncompress', 'gz', 'and', 'bz2', 'files']",uncompress gz and bz2 files,"['def', 'uncompress_file', '(', 'input_file_name', ',', 'file_extension', ',', 'dest_dir', ')', ':', 'if', 'file_extension', '.', 'lower', '(', ')', 'not', 'in', '(', ""'.gz'"", ',', ""'.bz2'"", ')', ':', 'raise', 'notimplementederror', '(', '""received {} format. only gz and bz2 ""', '""files can currently be uncompressed.""', '.', 'format', '(', 'file_extension', ')', ')', 'if', 'file_extension', '.', 'lower', '(', ')', '==', ""'.gz'"", ':', 'fmodule', '=', 'gzip', '.', 'gzipfile', 'elif', 'file_extension', '.', 'lower', '(', ')', '==', ""'.bz2'"", ':', 'fmodule', '=', 'bz2', '.', 'bz2file', 'with', 'fmodule', '(', 'input_file_name', ',', 'mode', '=', ""'rb'"", ')', 'as', 'f_compressed', ',', 'namedtemporaryfile', '(', 'dir', '=', 'dest_dir', ',', 'mode', '=', ""'wb'"", ',', 'delete', '=', 'false', ')', 'as', 'f_uncompressed', ':', 'shutil', '.', 'copyfileobj', '(', 'f_compressed', ',', 'f_uncompressed', ')', 'return', 'f_uncompressed', '.', 'name']","def uncompress_file ( input_file_name , file_extension , dest_dir ) : if file_extension . lower ( ) not in ( '.gz' , '.bz2' ) : raise notimplementederror ( ""received {} format. only gz and bz2 "" ""files can currently be uncompressed."" . format ( file_extension ) ) if file_extension . lower ( ) == '.gz' : fmodule = gzip . gzipfile elif file_extension . lower ( ) == '.bz2' : fmodule = bz2 . bz2file with fmodule ( input_file_name , mode = 'rb' ) as f_compressed , namedtemporaryfile ( dir = dest_dir , mode = 'wb' , delete = false ) as f_uncompressed : shutil . copyfileobj ( f_compressed , f_uncompressed ) return f_uncompressed . name"
418,apache/airflow,airflow/contrib/operators/mssql_to_gcs.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mssql_to_gcs.py#L127-L137,"def _query_mssql(self):
        """"""
        Queries MSSQL and returns a cursor of results.

        :return: mssql cursor
        """"""
        mssql = MsSqlHook(mssql_conn_id=self.mssql_conn_id)
        conn = mssql.get_conn()
        cursor = conn.cursor()
        cursor.execute(self.sql)
        return cursor","['def', '_query_mssql', '(', 'self', ')', ':', 'mssql', '=', 'MsSqlHook', '(', 'mssql_conn_id', '=', 'self', '.', 'mssql_conn_id', ')', 'conn', '=', 'mssql', '.', 'get_conn', '(', ')', 'cursor', '=', 'conn', '.', 'cursor', '(', ')', 'cursor', '.', 'execute', '(', 'self', '.', 'sql', ')', 'return', 'cursor']","Queries MSSQL and returns a cursor of results.

        :return: mssql cursor","['Queries', 'MSSQL', 'and', 'returns', 'a', 'cursor', 'of', 'results', '.']",python,test,"['queries', 'mssql', 'and', 'returns', 'a', 'cursor', 'of', 'results', '.']",queries mssql and returns a cursor of results .,"['def', '_query_mssql', '(', 'self', ')', ':', 'mssql', '=', 'mssqlhook', '(', 'mssql_conn_id', '=', 'self', '.', 'mssql_conn_id', ')', 'conn', '=', 'mssql', '.', 'get_conn', '(', ')', 'cursor', '=', 'conn', '.', 'cursor', '(', ')', 'cursor', '.', 'execute', '(', 'self', '.', 'sql', ')', 'return', 'cursor']",def _query_mssql ( self ) : mssql = mssqlhook ( mssql_conn_id = self . mssql_conn_id ) conn = mssql . get_conn ( ) cursor = conn . cursor ( ) cursor . execute ( self . sql ) return cursor
419,apache/airflow,airflow/contrib/operators/mssql_to_gcs.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mssql_to_gcs.py#L139-L170,"def _write_local_data_files(self, cursor):
        """"""
        Takes a cursor, and writes results to a local file.

        :return: A dictionary where keys are filenames to be used as object
            names in GCS, and values are file handles to local files that
            contain the data for the GCS objects.
        """"""
        schema = list(map(lambda schema_tuple: schema_tuple[0].replace(' ', '_'), cursor.description))
        file_no = 0
        tmp_file_handle = NamedTemporaryFile(delete=True)
        tmp_file_handles = {self.filename.format(file_no): tmp_file_handle}

        for row in cursor:
            # Convert if needed
            row = map(self.convert_types, row)
            row_dict = dict(zip(schema, row))

            s = json.dumps(row_dict, sort_keys=True)
            s = s.encode('utf-8')
            tmp_file_handle.write(s)

            # Append newline to make dumps BQ compatible
            tmp_file_handle.write(b'\n')

            # Stop if the file exceeds the file size limit
            if tmp_file_handle.tell() >= self.approx_max_file_size_bytes:
                file_no += 1
                tmp_file_handle = NamedTemporaryFile(delete=True)
                tmp_file_handles[self.filename.format(file_no)] = tmp_file_handle

        return tmp_file_handles","['def', '_write_local_data_files', '(', 'self', ',', 'cursor', ')', ':', 'schema', '=', 'list', '(', 'map', '(', 'lambda', 'schema_tuple', ':', 'schema_tuple', '[', '0', ']', '.', 'replace', '(', ""' '"", ',', ""'_'"", ')', ',', 'cursor', '.', 'description', ')', ')', 'file_no', '=', '0', 'tmp_file_handle', '=', 'NamedTemporaryFile', '(', 'delete', '=', 'True', ')', 'tmp_file_handles', '=', '{', 'self', '.', 'filename', '.', 'format', '(', 'file_no', ')', ':', 'tmp_file_handle', '}', 'for', 'row', 'in', 'cursor', ':', '# Convert if needed', 'row', '=', 'map', '(', 'self', '.', 'convert_types', ',', 'row', ')', 'row_dict', '=', 'dict', '(', 'zip', '(', 'schema', ',', 'row', ')', ')', 's', '=', 'json', '.', 'dumps', '(', 'row_dict', ',', 'sort_keys', '=', 'True', ')', 's', '=', 's', '.', 'encode', '(', ""'utf-8'"", ')', 'tmp_file_handle', '.', 'write', '(', 's', ')', '# Append newline to make dumps BQ compatible', 'tmp_file_handle', '.', 'write', '(', ""b'\\n'"", ')', '# Stop if the file exceeds the file size limit', 'if', 'tmp_file_handle', '.', 'tell', '(', ')', '>=', 'self', '.', 'approx_max_file_size_bytes', ':', 'file_no', '+=', '1', 'tmp_file_handle', '=', 'NamedTemporaryFile', '(', 'delete', '=', 'True', ')', 'tmp_file_handles', '[', 'self', '.', 'filename', '.', 'format', '(', 'file_no', ')', ']', '=', 'tmp_file_handle', 'return', 'tmp_file_handles']","Takes a cursor, and writes results to a local file.

        :return: A dictionary where keys are filenames to be used as object
            names in GCS, and values are file handles to local files that
            contain the data for the GCS objects.","['Takes', 'a', 'cursor', 'and', 'writes', 'results', 'to', 'a', 'local', 'file', '.']",python,test,"['takes', 'a', 'cursor', 'and', 'writes', 'results', 'to', 'a', 'local', 'file', '.']",takes a cursor and writes results to a local file .,"['def', '_write_local_data_files', '(', 'self', ',', 'cursor', ')', ':', 'schema', '=', 'list', '(', 'map', '(', 'lambda', 'schema_tuple', ':', 'schema_tuple', '[', '0', ']', '.', 'replace', '(', ""' '"", ',', ""'_'"", ')', ',', 'cursor', '.', 'description', ')', ')', 'file_no', '=', '0', 'tmp_file_handle', '=', 'namedtemporaryfile', '(', 'delete', '=', 'true', ')', 'tmp_file_handles', '=', '{', 'self', '.', 'filename', '.', 'format', '(', 'file_no', ')', ':', 'tmp_file_handle', '}', 'for', 'row', 'in', 'cursor', ':', '# convert if needed', 'row', '=', 'map', '(', 'self', '.', 'convert_types', ',', 'row', ')', 'row_dict', '=', 'dict', '(', 'zip', '(', 'schema', ',', 'row', ')', ')', 's', '=', 'json', '.', 'dumps', '(', 'row_dict', ',', 'sort_keys', '=', 'true', ')', 's', '=', 's', '.', 'encode', '(', ""'utf-8'"", ')', 'tmp_file_handle', '.', 'write', '(', 's', ')', '# append newline to make dumps bq compatible', 'tmp_file_handle', '.', 'write', '(', ""b'\\n'"", ')', '# stop if the file exceeds the file size limit', 'if', 'tmp_file_handle', '.', 'tell', '(', ')', '>=', 'self', '.', 'approx_max_file_size_bytes', ':', 'file_no', '+=', '1', 'tmp_file_handle', '=', 'namedtemporaryfile', '(', 'delete', '=', 'true', ')', 'tmp_file_handles', '[', 'self', '.', 'filename', '.', 'format', '(', 'file_no', ')', ']', '=', 'tmp_file_handle', 'return', 'tmp_file_handles']","def _write_local_data_files ( self , cursor ) : schema = list ( map ( lambda schema_tuple : schema_tuple [ 0 ] . replace ( ' ' , '_' ) , cursor . description ) ) file_no = 0 tmp_file_handle = namedtemporaryfile ( delete = true ) tmp_file_handles = { self . filename . format ( file_no ) : tmp_file_handle } for row in cursor : # convert if needed row = map ( self . convert_types , row ) row_dict = dict ( zip ( schema , row ) ) s = json . dumps ( row_dict , sort_keys = true ) s = s . encode ( 'utf-8' ) tmp_file_handle . write ( s ) # append newline to make dumps bq compatible tmp_file_handle . write ( b'\n' ) # stop if the file exceeds the file size limit if tmp_file_handle . tell ( ) >= self . approx_max_file_size_bytes : file_no += 1 tmp_file_handle = namedtemporaryfile ( delete = true ) tmp_file_handles [ self . filename . format ( file_no ) ] = tmp_file_handle return tmp_file_handles"
420,apache/airflow,airflow/contrib/operators/mssql_to_gcs.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mssql_to_gcs.py#L201-L211,"def _upload_to_gcs(self, files_to_upload):
        """"""
        Upload all of the file splits (and optionally the schema .json file) to
        Google cloud storage.
        """"""
        hook = GoogleCloudStorageHook(
            google_cloud_storage_conn_id=self.google_cloud_storage_conn_id,
            delegate_to=self.delegate_to)
        for object_name, tmp_file_handle in files_to_upload.items():
            hook.upload(self.bucket, object_name, tmp_file_handle.name, 'application/json',
                        (self.gzip if object_name != self.schema_filename else False))","['def', '_upload_to_gcs', '(', 'self', ',', 'files_to_upload', ')', ':', 'hook', '=', 'GoogleCloudStorageHook', '(', 'google_cloud_storage_conn_id', '=', 'self', '.', 'google_cloud_storage_conn_id', ',', 'delegate_to', '=', 'self', '.', 'delegate_to', ')', 'for', 'object_name', ',', 'tmp_file_handle', 'in', 'files_to_upload', '.', 'items', '(', ')', ':', 'hook', '.', 'upload', '(', 'self', '.', 'bucket', ',', 'object_name', ',', 'tmp_file_handle', '.', 'name', ',', ""'application/json'"", ',', '(', 'self', '.', 'gzip', 'if', 'object_name', '!=', 'self', '.', 'schema_filename', 'else', 'False', ')', ')']","Upload all of the file splits (and optionally the schema .json file) to
        Google cloud storage.","['Upload', 'all', 'of', 'the', 'file', 'splits', '(', 'and', 'optionally', 'the', 'schema', '.', 'json', 'file', ')', 'to', 'Google', 'cloud', 'storage', '.']",python,test,"['upload', 'all', 'of', 'the', 'file', 'splits', '(', 'and', 'optionally', 'the', 'schema', '.', 'json', 'file', ')', 'to', 'google', 'cloud', 'storage', '.']",upload all of the file splits ( and optionally the schema . json file ) to google cloud storage .,"['def', '_upload_to_gcs', '(', 'self', ',', 'files_to_upload', ')', ':', 'hook', '=', 'googlecloudstoragehook', '(', 'google_cloud_storage_conn_id', '=', 'self', '.', 'google_cloud_storage_conn_id', ',', 'delegate_to', '=', 'self', '.', 'delegate_to', ')', 'for', 'object_name', ',', 'tmp_file_handle', 'in', 'files_to_upload', '.', 'items', '(', ')', ':', 'hook', '.', 'upload', '(', 'self', '.', 'bucket', ',', 'object_name', ',', 'tmp_file_handle', '.', 'name', ',', ""'application/json'"", ',', '(', 'self', '.', 'gzip', 'if', 'object_name', '!=', 'self', '.', 'schema_filename', 'else', 'false', ')', ')']","def _upload_to_gcs ( self , files_to_upload ) : hook = googlecloudstoragehook ( google_cloud_storage_conn_id = self . google_cloud_storage_conn_id , delegate_to = self . delegate_to ) for object_name , tmp_file_handle in files_to_upload . items ( ) : hook . upload ( self . bucket , object_name , tmp_file_handle . name , 'application/json' , ( self . gzip if object_name != self . schema_filename else false ) )"
421,apache/airflow,airflow/contrib/operators/mssql_to_gcs.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/mssql_to_gcs.py#L214-L222,"def convert_types(cls, value):
        """"""
        Takes a value from MSSQL, and converts it to a value that's safe for
        JSON/Google Cloud Storage/BigQuery.
        """"""
        if isinstance(value, decimal.Decimal):
            return float(value)
        else:
            return value","['def', 'convert_types', '(', 'cls', ',', 'value', ')', ':', 'if', 'isinstance', '(', 'value', ',', 'decimal', '.', 'Decimal', ')', ':', 'return', 'float', '(', 'value', ')', 'else', ':', 'return', 'value']","Takes a value from MSSQL, and converts it to a value that's safe for
        JSON/Google Cloud Storage/BigQuery.","['Takes', 'a', 'value', 'from', 'MSSQL', 'and', 'converts', 'it', 'to', 'a', 'value', 'that', 's', 'safe', 'for', 'JSON', '/', 'Google', 'Cloud', 'Storage', '/', 'BigQuery', '.']",python,test,"['takes', 'a', 'value', 'from', 'mssql', 'and', 'converts', 'it', 'to', 'a', 'value', 'that', 's', 'safe', 'for', 'json', '/', 'google', 'cloud', 'storage', '/', 'bigquery', '.']",takes a value from mssql and converts it to a value that s safe for json / google cloud storage / bigquery .,"['def', 'convert_types', '(', 'cls', ',', 'value', ')', ':', 'if', 'isinstance', '(', 'value', ',', 'decimal', '.', 'decimal', ')', ':', 'return', 'float', '(', 'value', ')', 'else', ':', 'return', 'value']","def convert_types ( cls , value ) : if isinstance ( value , decimal . decimal ) : return float ( value ) else : return value"
422,apache/airflow,airflow/utils/cli.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/cli.py#L36-L81,"def action_logging(f):
    """"""
    Decorates function to execute function at the same time submitting action_logging
    but in CLI context. It will call action logger callbacks twice,
    one for pre-execution and the other one for post-execution.

    Action logger will be called with below keyword parameters:
        sub_command : name of sub-command
        start_datetime : start datetime instance by utc
        end_datetime : end datetime instance by utc
        full_command : full command line arguments
        user : current user
        log : airflow.models.log.Log ORM instance
        dag_id : dag id (optional)
        task_id : task_id (optional)
        execution_date : execution date (optional)
        error : exception instance if there's an exception

    :param f: function instance
    :return: wrapped function
    """"""
    @functools.wraps(f)
    def wrapper(*args, **kwargs):
        """"""
        An wrapper for cli functions. It assumes to have Namespace instance
        at 1st positional argument
        :param args: Positional argument. It assumes to have Namespace instance
        at 1st positional argument
        :param kwargs: A passthrough keyword argument
        """"""
        assert args
        assert isinstance(args[0], Namespace), \
            ""1st positional argument should be argparse.Namespace instance, "" \
            ""but {}"".format(args[0])
        metrics = _build_metrics(f.__name__, args[0])
        cli_action_loggers.on_pre_execution(**metrics)
        try:
            return f(*args, **kwargs)
        except Exception as e:
            metrics['error'] = e
            raise
        finally:
            metrics['end_datetime'] = datetime.utcnow()
            cli_action_loggers.on_post_execution(**metrics)

    return wrapper","['def', 'action_logging', '(', 'f', ')', ':', '@', 'functools', '.', 'wraps', '(', 'f', ')', 'def', 'wrapper', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', '""""""\n        An wrapper for cli functions. It assumes to have Namespace instance\n        at 1st positional argument\n        :param args: Positional argument. It assumes to have Namespace instance\n        at 1st positional argument\n        :param kwargs: A passthrough keyword argument\n        """"""', 'assert', 'args', 'assert', 'isinstance', '(', 'args', '[', '0', ']', ',', 'Namespace', ')', ',', '""1st positional argument should be argparse.Namespace instance, ""', '""but {}""', '.', 'format', '(', 'args', '[', '0', ']', ')', 'metrics', '=', '_build_metrics', '(', 'f', '.', '__name__', ',', 'args', '[', '0', ']', ')', 'cli_action_loggers', '.', 'on_pre_execution', '(', '*', '*', 'metrics', ')', 'try', ':', 'return', 'f', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', 'except', 'Exception', 'as', 'e', ':', 'metrics', '[', ""'error'"", ']', '=', 'e', 'raise', 'finally', ':', 'metrics', '[', ""'end_datetime'"", ']', '=', 'datetime', '.', 'utcnow', '(', ')', 'cli_action_loggers', '.', 'on_post_execution', '(', '*', '*', 'metrics', ')', 'return', 'wrapper']","Decorates function to execute function at the same time submitting action_logging
    but in CLI context. It will call action logger callbacks twice,
    one for pre-execution and the other one for post-execution.

    Action logger will be called with below keyword parameters:
        sub_command : name of sub-command
        start_datetime : start datetime instance by utc
        end_datetime : end datetime instance by utc
        full_command : full command line arguments
        user : current user
        log : airflow.models.log.Log ORM instance
        dag_id : dag id (optional)
        task_id : task_id (optional)
        execution_date : execution date (optional)
        error : exception instance if there's an exception

    :param f: function instance
    :return: wrapped function","['Decorates', 'function', 'to', 'execute', 'function', 'at', 'the', 'same', 'time', 'submitting', 'action_logging', 'but', 'in', 'CLI', 'context', '.', 'It', 'will', 'call', 'action', 'logger', 'callbacks', 'twice', 'one', 'for', 'pre', '-', 'execution', 'and', 'the', 'other', 'one', 'for', 'post', '-', 'execution', '.']",python,test,"['decorates', 'function', 'to', 'execute', 'function', 'at', 'the', 'same', 'time', 'submitting', 'action_logging', 'but', 'in', 'cli', 'context', '.', 'it', 'will', 'call', 'action', 'logger', 'callbacks', 'twice', 'one', 'for', 'pre', '-', 'execution', 'and', 'the', 'other', 'one', 'for', 'post', '-', 'execution', '.']",decorates function to execute function at the same time submitting action_logging but in cli context . it will call action logger callbacks twice one for pre - execution and the other one for post - execution .,"['def', 'action_logging', '(', 'f', ')', ':', '@', 'functools', '.', 'wraps', '(', 'f', ')', 'def', 'wrapper', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', '""""""\n        an wrapper for cli functions. it assumes to have namespace instance\n        at 1st positional argument\n        :param args: positional argument. it assumes to have namespace instance\n        at 1st positional argument\n        :param kwargs: a passthrough keyword argument\n        """"""', 'assert', 'args', 'assert', 'isinstance', '(', 'args', '[', '0', ']', ',', 'namespace', ')', ',', '""1st positional argument should be argparse.namespace instance, ""', '""but {}""', '.', 'format', '(', 'args', '[', '0', ']', ')', 'metrics', '=', '_build_metrics', '(', 'f', '.', '__name__', ',', 'args', '[', '0', ']', ')', 'cli_action_loggers', '.', 'on_pre_execution', '(', '*', '*', 'metrics', ')', 'try', ':', 'return', 'f', '(', '*', 'args', ',', '*', '*', 'kwargs', ')', 'except', 'exception', 'as', 'e', ':', 'metrics', '[', ""'error'"", ']', '=', 'e', 'raise', 'finally', ':', 'metrics', '[', ""'end_datetime'"", ']', '=', 'datetime', '.', 'utcnow', '(', ')', 'cli_action_loggers', '.', 'on_post_execution', '(', '*', '*', 'metrics', ')', 'return', 'wrapper']","def action_logging ( f ) : @ functools . wraps ( f ) def wrapper ( * args , * * kwargs ) : """"""
        an wrapper for cli functions. it assumes to have namespace instance
        at 1st positional argument
        :param args: positional argument. it assumes to have namespace instance
        at 1st positional argument
        :param kwargs: a passthrough keyword argument
        """""" assert args assert isinstance ( args [ 0 ] , namespace ) , ""1st positional argument should be argparse.namespace instance, "" ""but {}"" . format ( args [ 0 ] ) metrics = _build_metrics ( f . __name__ , args [ 0 ] ) cli_action_loggers . on_pre_execution ( * * metrics ) try : return f ( * args , * * kwargs ) except exception as e : metrics [ 'error' ] = e raise finally : metrics [ 'end_datetime' ] = datetime . utcnow ( ) cli_action_loggers . on_post_execution ( * * metrics ) return wrapper"
423,apache/airflow,airflow/utils/cli.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/cli.py#L84-L116,"def _build_metrics(func_name, namespace):
    """"""
    Builds metrics dict from function args
    It assumes that function arguments is from airflow.bin.cli module's function
    and has Namespace instance where it optionally contains ""dag_id"", ""task_id"",
    and ""execution_date"".

    :param func_name: name of function
    :param namespace: Namespace instance from argparse
    :return: dict with metrics
    """"""

    metrics = {'sub_command': func_name, 'start_datetime': datetime.utcnow(),
               'full_command': '{}'.format(list(sys.argv)), 'user': getpass.getuser()}

    assert isinstance(namespace, Namespace)
    tmp_dic = vars(namespace)
    metrics['dag_id'] = tmp_dic.get('dag_id')
    metrics['task_id'] = tmp_dic.get('task_id')
    metrics['execution_date'] = tmp_dic.get('execution_date')
    metrics['host_name'] = socket.gethostname()

    extra = json.dumps(dict((k, metrics[k]) for k in ('host_name', 'full_command')))
    log = Log(
        event='cli_{}'.format(func_name),
        task_instance=None,
        owner=metrics['user'],
        extra=extra,
        task_id=metrics.get('task_id'),
        dag_id=metrics.get('dag_id'),
        execution_date=metrics.get('execution_date'))
    metrics['log'] = log
    return metrics","['def', '_build_metrics', '(', 'func_name', ',', 'namespace', ')', ':', 'metrics', '=', '{', ""'sub_command'"", ':', 'func_name', ',', ""'start_datetime'"", ':', 'datetime', '.', 'utcnow', '(', ')', ',', ""'full_command'"", ':', ""'{}'"", '.', 'format', '(', 'list', '(', 'sys', '.', 'argv', ')', ')', ',', ""'user'"", ':', 'getpass', '.', 'getuser', '(', ')', '}', 'assert', 'isinstance', '(', 'namespace', ',', 'Namespace', ')', 'tmp_dic', '=', 'vars', '(', 'namespace', ')', 'metrics', '[', ""'dag_id'"", ']', '=', 'tmp_dic', '.', 'get', '(', ""'dag_id'"", ')', 'metrics', '[', ""'task_id'"", ']', '=', 'tmp_dic', '.', 'get', '(', ""'task_id'"", ')', 'metrics', '[', ""'execution_date'"", ']', '=', 'tmp_dic', '.', 'get', '(', ""'execution_date'"", ')', 'metrics', '[', ""'host_name'"", ']', '=', 'socket', '.', 'gethostname', '(', ')', 'extra', '=', 'json', '.', 'dumps', '(', 'dict', '(', '(', 'k', ',', 'metrics', '[', 'k', ']', ')', 'for', 'k', 'in', '(', ""'host_name'"", ',', ""'full_command'"", ')', ')', ')', 'log', '=', 'Log', '(', 'event', '=', ""'cli_{}'"", '.', 'format', '(', 'func_name', ')', ',', 'task_instance', '=', 'None', ',', 'owner', '=', 'metrics', '[', ""'user'"", ']', ',', 'extra', '=', 'extra', ',', 'task_id', '=', 'metrics', '.', 'get', '(', ""'task_id'"", ')', ',', 'dag_id', '=', 'metrics', '.', 'get', '(', ""'dag_id'"", ')', ',', 'execution_date', '=', 'metrics', '.', 'get', '(', ""'execution_date'"", ')', ')', 'metrics', '[', ""'log'"", ']', '=', 'log', 'return', 'metrics']","Builds metrics dict from function args
    It assumes that function arguments is from airflow.bin.cli module's function
    and has Namespace instance where it optionally contains ""dag_id"", ""task_id"",
    and ""execution_date"".

    :param func_name: name of function
    :param namespace: Namespace instance from argparse
    :return: dict with metrics","['Builds', 'metrics', 'dict', 'from', 'function', 'args', 'It', 'assumes', 'that', 'function', 'arguments', 'is', 'from', 'airflow', '.', 'bin', '.', 'cli', 'module', 's', 'function', 'and', 'has', 'Namespace', 'instance', 'where', 'it', 'optionally', 'contains', 'dag_id', 'task_id', 'and', 'execution_date', '.']",python,test,"['builds', 'metrics', 'dict', 'from', 'function', 'args', 'it', 'assumes', 'that', 'function', 'arguments', 'is', 'from', 'airflow', '.', 'bin', '.', 'cli', 'module', 's', 'function', 'and', 'has', 'namespace', 'instance', 'where', 'it', 'optionally', 'contains', 'dag_id', 'task_id', 'and', 'execution_date', '.']",builds metrics dict from function args it assumes that function arguments is from airflow . bin . cli module s function and has namespace instance where it optionally contains dag_id task_id and execution_date .,"['def', '_build_metrics', '(', 'func_name', ',', 'namespace', ')', ':', 'metrics', '=', '{', ""'sub_command'"", ':', 'func_name', ',', ""'start_datetime'"", ':', 'datetime', '.', 'utcnow', '(', ')', ',', ""'full_command'"", ':', ""'{}'"", '.', 'format', '(', 'list', '(', 'sys', '.', 'argv', ')', ')', ',', ""'user'"", ':', 'getpass', '.', 'getuser', '(', ')', '}', 'assert', 'isinstance', '(', 'namespace', ',', 'namespace', ')', 'tmp_dic', '=', 'vars', '(', 'namespace', ')', 'metrics', '[', ""'dag_id'"", ']', '=', 'tmp_dic', '.', 'get', '(', ""'dag_id'"", ')', 'metrics', '[', ""'task_id'"", ']', '=', 'tmp_dic', '.', 'get', '(', ""'task_id'"", ')', 'metrics', '[', ""'execution_date'"", ']', '=', 'tmp_dic', '.', 'get', '(', ""'execution_date'"", ')', 'metrics', '[', ""'host_name'"", ']', '=', 'socket', '.', 'gethostname', '(', ')', 'extra', '=', 'json', '.', 'dumps', '(', 'dict', '(', '(', 'k', ',', 'metrics', '[', 'k', ']', ')', 'for', 'k', 'in', '(', ""'host_name'"", ',', ""'full_command'"", ')', ')', ')', 'log', '=', 'log', '(', 'event', '=', ""'cli_{}'"", '.', 'format', '(', 'func_name', ')', ',', 'task_instance', '=', 'none', ',', 'owner', '=', 'metrics', '[', ""'user'"", ']', ',', 'extra', '=', 'extra', ',', 'task_id', '=', 'metrics', '.', 'get', '(', ""'task_id'"", ')', ',', 'dag_id', '=', 'metrics', '.', 'get', '(', ""'dag_id'"", ')', ',', 'execution_date', '=', 'metrics', '.', 'get', '(', ""'execution_date'"", ')', ')', 'metrics', '[', ""'log'"", ']', '=', 'log', 'return', 'metrics']","def _build_metrics ( func_name , namespace ) : metrics = { 'sub_command' : func_name , 'start_datetime' : datetime . utcnow ( ) , 'full_command' : '{}' . format ( list ( sys . argv ) ) , 'user' : getpass . getuser ( ) } assert isinstance ( namespace , namespace ) tmp_dic = vars ( namespace ) metrics [ 'dag_id' ] = tmp_dic . get ( 'dag_id' ) metrics [ 'task_id' ] = tmp_dic . get ( 'task_id' ) metrics [ 'execution_date' ] = tmp_dic . get ( 'execution_date' ) metrics [ 'host_name' ] = socket . gethostname ( ) extra = json . dumps ( dict ( ( k , metrics [ k ] ) for k in ( 'host_name' , 'full_command' ) ) ) log = log ( event = 'cli_{}' . format ( func_name ) , task_instance = none , owner = metrics [ 'user' ] , extra = extra , task_id = metrics . get ( 'task_id' ) , dag_id = metrics . get ( 'dag_id' ) , execution_date = metrics . get ( 'execution_date' ) ) metrics [ 'log' ] = log return metrics"
424,apache/airflow,airflow/ti_deps/deps/trigger_rule_dep.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/ti_deps/deps/trigger_rule_dep.py#L91-L224,"def _evaluate_trigger_rule(
            self,
            ti,
            successes,
            skipped,
            failed,
            upstream_failed,
            done,
            flag_upstream_failed,
            session):
        """"""
        Yields a dependency status that indicate whether the given task instance's trigger
        rule was met.

        :param ti: the task instance to evaluate the trigger rule of
        :type ti: airflow.models.TaskInstance
        :param successes: Number of successful upstream tasks
        :type successes: bool
        :param skipped: Number of skipped upstream tasks
        :type skipped: bool
        :param failed: Number of failed upstream tasks
        :type failed: bool
        :param upstream_failed: Number of upstream_failed upstream tasks
        :type upstream_failed: bool
        :param done: Number of completed upstream tasks
        :type done: bool
        :param flag_upstream_failed: This is a hack to generate
            the upstream_failed state creation while checking to see
            whether the task instance is runnable. It was the shortest
            path to add the feature
        :type flag_upstream_failed: bool
        :param session: database session
        :type session: sqlalchemy.orm.session.Session
        """"""

        TR = airflow.utils.trigger_rule.TriggerRule

        task = ti.task
        upstream = len(task.upstream_task_ids)
        tr = task.trigger_rule
        upstream_done = done >= upstream
        upstream_tasks_state = {
            ""total"": upstream, ""successes"": successes, ""skipped"": skipped,
            ""failed"": failed, ""upstream_failed"": upstream_failed, ""done"": done
        }
        # TODO(aoen): Ideally each individual trigger rules would be its own class, but
        # this isn't very feasible at the moment since the database queries need to be
        # bundled together for efficiency.
        # handling instant state assignment based on trigger rules
        if flag_upstream_failed:
            if tr == TR.ALL_SUCCESS:
                if upstream_failed or failed:
                    ti.set_state(State.UPSTREAM_FAILED, session)
                elif skipped:
                    ti.set_state(State.SKIPPED, session)
            elif tr == TR.ALL_FAILED:
                if successes or skipped:
                    ti.set_state(State.SKIPPED, session)
            elif tr == TR.ONE_SUCCESS:
                if upstream_done and not successes:
                    ti.set_state(State.SKIPPED, session)
            elif tr == TR.ONE_FAILED:
                if upstream_done and not (failed or upstream_failed):
                    ti.set_state(State.SKIPPED, session)
            elif tr == TR.NONE_FAILED:
                if upstream_failed or failed:
                    ti.set_state(State.UPSTREAM_FAILED, session)
                elif skipped == upstream:
                    ti.set_state(State.SKIPPED, session)
            elif tr == TR.NONE_SKIPPED:
                if skipped:
                    ti.set_state(State.SKIPPED, session)

        if tr == TR.ONE_SUCCESS:
            if successes <= 0:
                yield self._failing_status(
                    reason=""Task's trigger rule '{0}' requires one upstream ""
                    ""task success, but none were found. ""
                    ""upstream_tasks_state={1}, upstream_task_ids={2}""
                    .format(tr, upstream_tasks_state, task.upstream_task_ids))
        elif tr == TR.ONE_FAILED:
            if not failed and not upstream_failed:
                yield self._failing_status(
                    reason=""Task's trigger rule '{0}' requires one upstream ""
                    ""task failure, but none were found. ""
                    ""upstream_tasks_state={1}, upstream_task_ids={2}""
                    .format(tr, upstream_tasks_state, task.upstream_task_ids))
        elif tr == TR.ALL_SUCCESS:
            num_failures = upstream - successes
            if num_failures > 0:
                yield self._failing_status(
                    reason=""Task's trigger rule '{0}' requires all upstream ""
                    ""tasks to have succeeded, but found {1} non-success(es). ""
                    ""upstream_tasks_state={2}, upstream_task_ids={3}""
                    .format(tr, num_failures, upstream_tasks_state,
                            task.upstream_task_ids))
        elif tr == TR.ALL_FAILED:
            num_successes = upstream - failed - upstream_failed
            if num_successes > 0:
                yield self._failing_status(
                    reason=""Task's trigger rule '{0}' requires all upstream ""
                    ""tasks to have failed, but found {1} non-failure(s). ""
                    ""upstream_tasks_state={2}, upstream_task_ids={3}""
                    .format(tr, num_successes, upstream_tasks_state,
                            task.upstream_task_ids))
        elif tr == TR.ALL_DONE:
            if not upstream_done:
                yield self._failing_status(
                    reason=""Task's trigger rule '{0}' requires all upstream ""
                    ""tasks to have completed, but found {1} task(s) that ""
                    ""weren't done. upstream_tasks_state={2}, ""
                    ""upstream_task_ids={3}""
                    .format(tr, upstream_done, upstream_tasks_state,
                            task.upstream_task_ids))
        elif tr == TR.NONE_FAILED:
            num_failures = upstream - successes - skipped
            if num_failures > 0:
                yield self._failing_status(
                    reason=""Task's trigger rule '{0}' requires all upstream ""
                    ""tasks to have succeeded or been skipped, but found {1} non-success(es). ""
                    ""upstream_tasks_state={2}, upstream_task_ids={3}""
                    .format(tr, num_failures, upstream_tasks_state,
                            task.upstream_task_ids))
        elif tr == TR.NONE_SKIPPED:
            if skipped > 0:
                yield self._failing_status(
                    reason=""Task's trigger rule '{0}' requires all upstream ""
                    ""tasks to not have been skipped, but found {1} task(s) skipped. ""
                    ""upstream_tasks_state={2}, upstream_task_ids={3}""
                    .format(tr, skipped, upstream_tasks_state,
                            task.upstream_task_ids))
        else:
            yield self._failing_status(
                reason=""No strategy to evaluate trigger rule '{0}'."".format(tr))","['def', '_evaluate_trigger_rule', '(', 'self', ',', 'ti', ',', 'successes', ',', 'skipped', ',', 'failed', ',', 'upstream_failed', ',', 'done', ',', 'flag_upstream_failed', ',', 'session', ')', ':', 'TR', '=', 'airflow', '.', 'utils', '.', 'trigger_rule', '.', 'TriggerRule', 'task', '=', 'ti', '.', 'task', 'upstream', '=', 'len', '(', 'task', '.', 'upstream_task_ids', ')', 'tr', '=', 'task', '.', 'trigger_rule', 'upstream_done', '=', 'done', '>=', 'upstream', 'upstream_tasks_state', '=', '{', '""total""', ':', 'upstream', ',', '""successes""', ':', 'successes', ',', '""skipped""', ':', 'skipped', ',', '""failed""', ':', 'failed', ',', '""upstream_failed""', ':', 'upstream_failed', ',', '""done""', ':', 'done', '}', '# TODO(aoen): Ideally each individual trigger rules would be its own class, but', ""# this isn't very feasible at the moment since the database queries need to be"", '# bundled together for efficiency.', '# handling instant state assignment based on trigger rules', 'if', 'flag_upstream_failed', ':', 'if', 'tr', '==', 'TR', '.', 'ALL_SUCCESS', ':', 'if', 'upstream_failed', 'or', 'failed', ':', 'ti', '.', 'set_state', '(', 'State', '.', 'UPSTREAM_FAILED', ',', 'session', ')', 'elif', 'skipped', ':', 'ti', '.', 'set_state', '(', 'State', '.', 'SKIPPED', ',', 'session', ')', 'elif', 'tr', '==', 'TR', '.', 'ALL_FAILED', ':', 'if', 'successes', 'or', 'skipped', ':', 'ti', '.', 'set_state', '(', 'State', '.', 'SKIPPED', ',', 'session', ')', 'elif', 'tr', '==', 'TR', '.', 'ONE_SUCCESS', ':', 'if', 'upstream_done', 'and', 'not', 'successes', ':', 'ti', '.', 'set_state', '(', 'State', '.', 'SKIPPED', ',', 'session', ')', 'elif', 'tr', '==', 'TR', '.', 'ONE_FAILED', ':', 'if', 'upstream_done', 'and', 'not', '(', 'failed', 'or', 'upstream_failed', ')', ':', 'ti', '.', 'set_state', '(', 'State', '.', 'SKIPPED', ',', 'session', ')', 'elif', 'tr', '==', 'TR', '.', 'NONE_FAILED', ':', 'if', 'upstream_failed', 'or', 'failed', ':', 'ti', '.', 'set_state', '(', 'State', '.', 'UPSTREAM_FAILED', ',', 'session', ')', 'elif', 'skipped', '==', 'upstream', ':', 'ti', '.', 'set_state', '(', 'State', '.', 'SKIPPED', ',', 'session', ')', 'elif', 'tr', '==', 'TR', '.', 'NONE_SKIPPED', ':', 'if', 'skipped', ':', 'ti', '.', 'set_state', '(', 'State', '.', 'SKIPPED', ',', 'session', ')', 'if', 'tr', '==', 'TR', '.', 'ONE_SUCCESS', ':', 'if', 'successes', '<=', '0', ':', 'yield', 'self', '.', '_failing_status', '(', 'reason', '=', '""Task\'s trigger rule \'{0}\' requires one upstream ""', '""task success, but none were found. ""', '""upstream_tasks_state={1}, upstream_task_ids={2}""', '.', 'format', '(', 'tr', ',', 'upstream_tasks_state', ',', 'task', '.', 'upstream_task_ids', ')', ')', 'elif', 'tr', '==', 'TR', '.', 'ONE_FAILED', ':', 'if', 'not', 'failed', 'and', 'not', 'upstream_failed', ':', 'yield', 'self', '.', '_failing_status', '(', 'reason', '=', '""Task\'s trigger rule \'{0}\' requires one upstream ""', '""task failure, but none were found. ""', '""upstream_tasks_state={1}, upstream_task_ids={2}""', '.', 'format', '(', 'tr', ',', 'upstream_tasks_state', ',', 'task', '.', 'upstream_task_ids', ')', ')', 'elif', 'tr', '==', 'TR', '.', 'ALL_SUCCESS', ':', 'num_failures', '=', 'upstream', '-', 'successes', 'if', 'num_failures', '>', '0', ':', 'yield', 'self', '.', '_failing_status', '(', 'reason', '=', '""Task\'s trigger rule \'{0}\' requires all upstream ""', '""tasks to have succeeded, but found {1} non-success(es). ""', '""upstream_tasks_state={2}, upstream_task_ids={3}""', '.', 'format', '(', 'tr', ',', 'num_failures', ',', 'upstream_tasks_state', ',', 'task', '.', 'upstream_task_ids', ')', ')', 'elif', 'tr', '==', 'TR', '.', 'ALL_FAILED', ':', 'num_successes', '=', 'upstream', '-', 'failed', '-', 'upstream_failed', 'if', 'num_successes', '>', '0', ':', 'yield', 'self', '.', '_failing_status', '(', 'reason', '=', '""Task\'s trigger rule \'{0}\' requires all upstream ""', '""tasks to have failed, but found {1} non-failure(s). ""', '""upstream_tasks_state={2}, upstream_task_ids={3}""', '.', 'format', '(', 'tr', ',', 'num_successes', ',', 'upstream_tasks_state', ',', 'task', '.', 'upstream_task_ids', ')', ')', 'elif', 'tr', '==', 'TR', '.', 'ALL_DONE', ':', 'if', 'not', 'upstream_done', ':', 'yield', 'self', '.', '_failing_status', '(', 'reason', '=', '""Task\'s trigger rule \'{0}\' requires all upstream ""', '""tasks to have completed, but found {1} task(s) that ""', '""weren\'t done. upstream_tasks_state={2}, ""', '""upstream_task_ids={3}""', '.', 'format', '(', 'tr', ',', 'upstream_done', ',', 'upstream_tasks_state', ',', 'task', '.', 'upstream_task_ids', ')', ')', 'elif', 'tr', '==', 'TR', '.', 'NONE_FAILED', ':', 'num_failures', '=', 'upstream', '-', 'successes', '-', 'skipped', 'if', 'num_failures', '>', '0', ':', 'yield', 'self', '.', '_failing_status', '(', 'reason', '=', '""Task\'s trigger rule \'{0}\' requires all upstream ""', '""tasks to have succeeded or been skipped, but found {1} non-success(es). ""', '""upstream_tasks_state={2}, upstream_task_ids={3}""', '.', 'format', '(', 'tr', ',', 'num_failures', ',', 'upstream_tasks_state', ',', 'task', '.', 'upstream_task_ids', ')', ')', 'elif', 'tr', '==', 'TR', '.', 'NONE_SKIPPED', ':', 'if', 'skipped', '>', '0', ':', 'yield', 'self', '.', '_failing_status', '(', 'reason', '=', '""Task\'s trigger rule \'{0}\' requires all upstream ""', '""tasks to not have been skipped, but found {1} task(s) skipped. ""', '""upstream_tasks_state={2}, upstream_task_ids={3}""', '.', 'format', '(', 'tr', ',', 'skipped', ',', 'upstream_tasks_state', ',', 'task', '.', 'upstream_task_ids', ')', ')', 'else', ':', 'yield', 'self', '.', '_failing_status', '(', 'reason', '=', '""No strategy to evaluate trigger rule \'{0}\'.""', '.', 'format', '(', 'tr', ')', ')']","Yields a dependency status that indicate whether the given task instance's trigger
        rule was met.

        :param ti: the task instance to evaluate the trigger rule of
        :type ti: airflow.models.TaskInstance
        :param successes: Number of successful upstream tasks
        :type successes: bool
        :param skipped: Number of skipped upstream tasks
        :type skipped: bool
        :param failed: Number of failed upstream tasks
        :type failed: bool
        :param upstream_failed: Number of upstream_failed upstream tasks
        :type upstream_failed: bool
        :param done: Number of completed upstream tasks
        :type done: bool
        :param flag_upstream_failed: This is a hack to generate
            the upstream_failed state creation while checking to see
            whether the task instance is runnable. It was the shortest
            path to add the feature
        :type flag_upstream_failed: bool
        :param session: database session
        :type session: sqlalchemy.orm.session.Session","['Yields', 'a', 'dependency', 'status', 'that', 'indicate', 'whether', 'the', 'given', 'task', 'instance', 's', 'trigger', 'rule', 'was', 'met', '.']",python,test,"['yields', 'a', 'dependency', 'status', 'that', 'indicate', 'whether', 'the', 'given', 'task', 'instance', 's', 'trigger', 'rule', 'was', 'met', '.']",yields a dependency status that indicate whether the given task instance s trigger rule was met .,"['def', '_evaluate_trigger_rule', '(', 'self', ',', 'ti', ',', 'successes', ',', 'skipped', ',', 'failed', ',', 'upstream_failed', ',', 'done', ',', 'flag_upstream_failed', ',', 'session', ')', ':', 'tr', '=', 'airflow', '.', 'utils', '.', 'trigger_rule', '.', 'triggerrule', 'task', '=', 'ti', '.', 'task', 'upstream', '=', 'len', '(', 'task', '.', 'upstream_task_ids', ')', 'tr', '=', 'task', '.', 'trigger_rule', 'upstream_done', '=', 'done', '>=', 'upstream', 'upstream_tasks_state', '=', '{', '""total""', ':', 'upstream', ',', '""successes""', ':', 'successes', ',', '""skipped""', ':', 'skipped', ',', '""failed""', ':', 'failed', ',', '""upstream_failed""', ':', 'upstream_failed', ',', '""done""', ':', 'done', '}', '# todo(aoen): ideally each individual trigger rules would be its own class, but', ""# this isn't very feasible at the moment since the database queries need to be"", '# bundled together for efficiency.', '# handling instant state assignment based on trigger rules', 'if', 'flag_upstream_failed', ':', 'if', 'tr', '==', 'tr', '.', 'all_success', ':', 'if', 'upstream_failed', 'or', 'failed', ':', 'ti', '.', 'set_state', '(', 'state', '.', 'upstream_failed', ',', 'session', ')', 'elif', 'skipped', ':', 'ti', '.', 'set_state', '(', 'state', '.', 'skipped', ',', 'session', ')', 'elif', 'tr', '==', 'tr', '.', 'all_failed', ':', 'if', 'successes', 'or', 'skipped', ':', 'ti', '.', 'set_state', '(', 'state', '.', 'skipped', ',', 'session', ')', 'elif', 'tr', '==', 'tr', '.', 'one_success', ':', 'if', 'upstream_done', 'and', 'not', 'successes', ':', 'ti', '.', 'set_state', '(', 'state', '.', 'skipped', ',', 'session', ')', 'elif', 'tr', '==', 'tr', '.', 'one_failed', ':', 'if', 'upstream_done', 'and', 'not', '(', 'failed', 'or', 'upstream_failed', ')', ':', 'ti', '.', 'set_state', '(', 'state', '.', 'skipped', ',', 'session', ')', 'elif', 'tr', '==', 'tr', '.', 'none_failed', ':', 'if', 'upstream_failed', 'or', 'failed', ':', 'ti', '.', 'set_state', '(', 'state', '.', 'upstream_failed', ',', 'session', ')', 'elif', 'skipped', '==', 'upstream', ':', 'ti', '.', 'set_state', '(', 'state', '.', 'skipped', ',', 'session', ')', 'elif', 'tr', '==', 'tr', '.', 'none_skipped', ':', 'if', 'skipped', ':', 'ti', '.', 'set_state', '(', 'state', '.', 'skipped', ',', 'session', ')', 'if', 'tr', '==', 'tr', '.', 'one_success', ':', 'if', 'successes', '<=', '0', ':', 'yield', 'self', '.', '_failing_status', '(', 'reason', '=', '""task\'s trigger rule \'{0}\' requires one upstream ""', '""task success, but none were found. ""', '""upstream_tasks_state={1}, upstream_task_ids={2}""', '.', 'format', '(', 'tr', ',', 'upstream_tasks_state', ',', 'task', '.', 'upstream_task_ids', ')', ')', 'elif', 'tr', '==', 'tr', '.', 'one_failed', ':', 'if', 'not', 'failed', 'and', 'not', 'upstream_failed', ':', 'yield', 'self', '.', '_failing_status', '(', 'reason', '=', '""task\'s trigger rule \'{0}\' requires one upstream ""', '""task failure, but none were found. ""', '""upstream_tasks_state={1}, upstream_task_ids={2}""', '.', 'format', '(', 'tr', ',', 'upstream_tasks_state', ',', 'task', '.', 'upstream_task_ids', ')', ')', 'elif', 'tr', '==', 'tr', '.', 'all_success', ':', 'num_failures', '=', 'upstream', '-', 'successes', 'if', 'num_failures', '>', '0', ':', 'yield', 'self', '.', '_failing_status', '(', 'reason', '=', '""task\'s trigger rule \'{0}\' requires all upstream ""', '""tasks to have succeeded, but found {1} non-success(es). ""', '""upstream_tasks_state={2}, upstream_task_ids={3}""', '.', 'format', '(', 'tr', ',', 'num_failures', ',', 'upstream_tasks_state', ',', 'task', '.', 'upstream_task_ids', ')', ')', 'elif', 'tr', '==', 'tr', '.', 'all_failed', ':', 'num_successes', '=', 'upstream', '-', 'failed', '-', 'upstream_failed', 'if', 'num_successes', '>', '0', ':', 'yield', 'self', '.', '_failing_status', '(', 'reason', '=', '""task\'s trigger rule \'{0}\' requires all upstream ""', '""tasks to have failed, but found {1} non-failure(s). ""', '""upstream_tasks_state={2}, upstream_task_ids={3}""', '.', 'format', '(', 'tr', ',', 'num_successes', ',', 'upstream_tasks_state', ',', 'task', '.', 'upstream_task_ids', ')', ')', 'elif', 'tr', '==', 'tr', '.', 'all_done', ':', 'if', 'not', 'upstream_done', ':', 'yield', 'self', '.', '_failing_status', '(', 'reason', '=', '""task\'s trigger rule \'{0}\' requires all upstream ""', '""tasks to have completed, but found {1} task(s) that ""', '""weren\'t done. upstream_tasks_state={2}, ""', '""upstream_task_ids={3}""', '.', 'format', '(', 'tr', ',', 'upstream_done', ',', 'upstream_tasks_state', ',', 'task', '.', 'upstream_task_ids', ')', ')', 'elif', 'tr', '==', 'tr', '.', 'none_failed', ':', 'num_failures', '=', 'upstream', '-', 'successes', '-', 'skipped', 'if', 'num_failures', '>', '0', ':', 'yield', 'self', '.', '_failing_status', '(', 'reason', '=', '""task\'s trigger rule \'{0}\' requires all upstream ""', '""tasks to have succeeded or been skipped, but found {1} non-success(es). ""', '""upstream_tasks_state={2}, upstream_task_ids={3}""', '.', 'format', '(', 'tr', ',', 'num_failures', ',', 'upstream_tasks_state', ',', 'task', '.', 'upstream_task_ids', ')', ')', 'elif', 'tr', '==', 'tr', '.', 'none_skipped', ':', 'if', 'skipped', '>', '0', ':', 'yield', 'self', '.', '_failing_status', '(', 'reason', '=', '""task\'s trigger rule \'{0}\' requires all upstream ""', '""tasks to not have been skipped, but found {1} task(s) skipped. ""', '""upstream_tasks_state={2}, upstream_task_ids={3}""', '.', 'format', '(', 'tr', ',', 'skipped', ',', 'upstream_tasks_state', ',', 'task', '.', 'upstream_task_ids', ')', ')', 'else', ':', 'yield', 'self', '.', '_failing_status', '(', 'reason', '=', '""no strategy to evaluate trigger rule \'{0}\'.""', '.', 'format', '(', 'tr', ')', ')']","def _evaluate_trigger_rule ( self , ti , successes , skipped , failed , upstream_failed , done , flag_upstream_failed , session ) : tr = airflow . utils . trigger_rule . triggerrule task = ti . task upstream = len ( task . upstream_task_ids ) tr = task . trigger_rule upstream_done = done >= upstream upstream_tasks_state = { ""total"" : upstream , ""successes"" : successes , ""skipped"" : skipped , ""failed"" : failed , ""upstream_failed"" : upstream_failed , ""done"" : done } # todo(aoen): ideally each individual trigger rules would be its own class, but # this isn't very feasible at the moment since the database queries need to be # bundled together for efficiency. # handling instant state assignment based on trigger rules if flag_upstream_failed : if tr == tr . all_success : if upstream_failed or failed : ti . set_state ( state . upstream_failed , session ) elif skipped : ti . set_state ( state . skipped , session ) elif tr == tr . all_failed : if successes or skipped : ti . set_state ( state . skipped , session ) elif tr == tr . one_success : if upstream_done and not successes : ti . set_state ( state . skipped , session ) elif tr == tr . one_failed : if upstream_done and not ( failed or upstream_failed ) : ti . set_state ( state . skipped , session ) elif tr == tr . none_failed : if upstream_failed or failed : ti . set_state ( state . upstream_failed , session ) elif skipped == upstream : ti . set_state ( state . skipped , session ) elif tr == tr . none_skipped : if skipped : ti . set_state ( state . skipped , session ) if tr == tr . one_success : if successes <= 0 : yield self . _failing_status ( reason = ""task's trigger rule '{0}' requires one upstream "" ""task success, but none were found. "" ""upstream_tasks_state={1}, upstream_task_ids={2}"" . format ( tr , upstream_tasks_state , task . upstream_task_ids ) ) elif tr == tr . one_failed : if not failed and not upstream_failed : yield self . _failing_status ( reason = ""task's trigger rule '{0}' requires one upstream "" ""task failure, but none were found. "" ""upstream_tasks_state={1}, upstream_task_ids={2}"" . format ( tr , upstream_tasks_state , task . upstream_task_ids ) ) elif tr == tr . all_success : num_failures = upstream - successes if num_failures > 0 : yield self . _failing_status ( reason = ""task's trigger rule '{0}' requires all upstream "" ""tasks to have succeeded, but found {1} non-success(es). "" ""upstream_tasks_state={2}, upstream_task_ids={3}"" . format ( tr , num_failures , upstream_tasks_state , task . upstream_task_ids ) ) elif tr == tr . all_failed : num_successes = upstream - failed - upstream_failed if num_successes > 0 : yield self . _failing_status ( reason = ""task's trigger rule '{0}' requires all upstream "" ""tasks to have failed, but found {1} non-failure(s). "" ""upstream_tasks_state={2}, upstream_task_ids={3}"" . format ( tr , num_successes , upstream_tasks_state , task . upstream_task_ids ) ) elif tr == tr . all_done : if not upstream_done : yield self . _failing_status ( reason = ""task's trigger rule '{0}' requires all upstream "" ""tasks to have completed, but found {1} task(s) that "" ""weren't done. upstream_tasks_state={2}, "" ""upstream_task_ids={3}"" . format ( tr , upstream_done , upstream_tasks_state , task . upstream_task_ids ) ) elif tr == tr . none_failed : num_failures = upstream - successes - skipped if num_failures > 0 : yield self . _failing_status ( reason = ""task's trigger rule '{0}' requires all upstream "" ""tasks to have succeeded or been skipped, but found {1} non-success(es). "" ""upstream_tasks_state={2}, upstream_task_ids={3}"" . format ( tr , num_failures , upstream_tasks_state , task . upstream_task_ids ) ) elif tr == tr . none_skipped : if skipped > 0 : yield self . _failing_status ( reason = ""task's trigger rule '{0}' requires all upstream "" ""tasks to not have been skipped, but found {1} task(s) skipped. "" ""upstream_tasks_state={2}, upstream_task_ids={3}"" . format ( tr , skipped , upstream_tasks_state , task . upstream_task_ids ) ) else : yield self . _failing_status ( reason = ""no strategy to evaluate trigger rule '{0}'."" . format ( tr ) )"
425,apache/airflow,airflow/contrib/task_runner/cgroup_task_runner.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/task_runner/cgroup_task_runner.py#L66-L88,"def _create_cgroup(self, path):
        """"""
        Create the specified cgroup.

        :param path: The path of the cgroup to create.
        E.g. cpu/mygroup/mysubgroup
        :return: the Node associated with the created cgroup.
        :rtype: cgroupspy.nodes.Node
        """"""
        node = trees.Tree().root
        path_split = path.split(os.sep)
        for path_element in path_split:
            name_to_node = {x.name: x for x in node.children}
            if path_element not in name_to_node:
                self.log.debug(""Creating cgroup %s in %s"", path_element, node.path)
                node = node.create_cgroup(path_element)
            else:
                self.log.debug(
                    ""Not creating cgroup %s in %s since it already exists"",
                    path_element, node.path
                )
                node = name_to_node[path_element]
        return node","['def', '_create_cgroup', '(', 'self', ',', 'path', ')', ':', 'node', '=', 'trees', '.', 'Tree', '(', ')', '.', 'root', 'path_split', '=', 'path', '.', 'split', '(', 'os', '.', 'sep', ')', 'for', 'path_element', 'in', 'path_split', ':', 'name_to_node', '=', '{', 'x', '.', 'name', ':', 'x', 'for', 'x', 'in', 'node', '.', 'children', '}', 'if', 'path_element', 'not', 'in', 'name_to_node', ':', 'self', '.', 'log', '.', 'debug', '(', '""Creating cgroup %s in %s""', ',', 'path_element', ',', 'node', '.', 'path', ')', 'node', '=', 'node', '.', 'create_cgroup', '(', 'path_element', ')', 'else', ':', 'self', '.', 'log', '.', 'debug', '(', '""Not creating cgroup %s in %s since it already exists""', ',', 'path_element', ',', 'node', '.', 'path', ')', 'node', '=', 'name_to_node', '[', 'path_element', ']', 'return', 'node']","Create the specified cgroup.

        :param path: The path of the cgroup to create.
        E.g. cpu/mygroup/mysubgroup
        :return: the Node associated with the created cgroup.
        :rtype: cgroupspy.nodes.Node","['Create', 'the', 'specified', 'cgroup', '.']",python,test,"['create', 'the', 'specified', 'cgroup', '.']",create the specified cgroup .,"['def', '_create_cgroup', '(', 'self', ',', 'path', ')', ':', 'node', '=', 'trees', '.', 'tree', '(', ')', '.', 'root', 'path_split', '=', 'path', '.', 'split', '(', 'os', '.', 'sep', ')', 'for', 'path_element', 'in', 'path_split', ':', 'name_to_node', '=', '{', 'x', '.', 'name', ':', 'x', 'for', 'x', 'in', 'node', '.', 'children', '}', 'if', 'path_element', 'not', 'in', 'name_to_node', ':', 'self', '.', 'log', '.', 'debug', '(', '""creating cgroup %s in %s""', ',', 'path_element', ',', 'node', '.', 'path', ')', 'node', '=', 'node', '.', 'create_cgroup', '(', 'path_element', ')', 'else', ':', 'self', '.', 'log', '.', 'debug', '(', '""not creating cgroup %s in %s since it already exists""', ',', 'path_element', ',', 'node', '.', 'path', ')', 'node', '=', 'name_to_node', '[', 'path_element', ']', 'return', 'node']","def _create_cgroup ( self , path ) : node = trees . tree ( ) . root path_split = path . split ( os . sep ) for path_element in path_split : name_to_node = { x . name : x for x in node . children } if path_element not in name_to_node : self . log . debug ( ""creating cgroup %s in %s"" , path_element , node . path ) node = node . create_cgroup ( path_element ) else : self . log . debug ( ""not creating cgroup %s in %s since it already exists"" , path_element , node . path ) node = name_to_node [ path_element ] return node"
426,apache/airflow,airflow/contrib/task_runner/cgroup_task_runner.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/task_runner/cgroup_task_runner.py#L90-L109,"def _delete_cgroup(self, path):
        """"""
        Delete the specified cgroup.

        :param path: The path of the cgroup to delete.
        E.g. cpu/mygroup/mysubgroup
        """"""
        node = trees.Tree().root
        path_split = path.split(""/"")
        for path_element in path_split:
            name_to_node = {x.name: x for x in node.children}
            if path_element not in name_to_node:
                self.log.warning(""Cgroup does not exist: %s"", path)
                return
            else:
                node = name_to_node[path_element]
        # node is now the leaf node
        parent = node.parent
        self.log.debug(""Deleting cgroup %s/%s"", parent, node.name)
        parent.delete_cgroup(node.name)","['def', '_delete_cgroup', '(', 'self', ',', 'path', ')', ':', 'node', '=', 'trees', '.', 'Tree', '(', ')', '.', 'root', 'path_split', '=', 'path', '.', 'split', '(', '""/""', ')', 'for', 'path_element', 'in', 'path_split', ':', 'name_to_node', '=', '{', 'x', '.', 'name', ':', 'x', 'for', 'x', 'in', 'node', '.', 'children', '}', 'if', 'path_element', 'not', 'in', 'name_to_node', ':', 'self', '.', 'log', '.', 'warning', '(', '""Cgroup does not exist: %s""', ',', 'path', ')', 'return', 'else', ':', 'node', '=', 'name_to_node', '[', 'path_element', ']', '# node is now the leaf node', 'parent', '=', 'node', '.', 'parent', 'self', '.', 'log', '.', 'debug', '(', '""Deleting cgroup %s/%s""', ',', 'parent', ',', 'node', '.', 'name', ')', 'parent', '.', 'delete_cgroup', '(', 'node', '.', 'name', ')']","Delete the specified cgroup.

        :param path: The path of the cgroup to delete.
        E.g. cpu/mygroup/mysubgroup","['Delete', 'the', 'specified', 'cgroup', '.']",python,test,"['delete', 'the', 'specified', 'cgroup', '.']",delete the specified cgroup .,"['def', '_delete_cgroup', '(', 'self', ',', 'path', ')', ':', 'node', '=', 'trees', '.', 'tree', '(', ')', '.', 'root', 'path_split', '=', 'path', '.', 'split', '(', '""/""', ')', 'for', 'path_element', 'in', 'path_split', ':', 'name_to_node', '=', '{', 'x', '.', 'name', ':', 'x', 'for', 'x', 'in', 'node', '.', 'children', '}', 'if', 'path_element', 'not', 'in', 'name_to_node', ':', 'self', '.', 'log', '.', 'warning', '(', '""cgroup does not exist: %s""', ',', 'path', ')', 'return', 'else', ':', 'node', '=', 'name_to_node', '[', 'path_element', ']', '# node is now the leaf node', 'parent', '=', 'node', '.', 'parent', 'self', '.', 'log', '.', 'debug', '(', '""deleting cgroup %s/%s""', ',', 'parent', ',', 'node', '.', 'name', ')', 'parent', '.', 'delete_cgroup', '(', 'node', '.', 'name', ')']","def _delete_cgroup ( self , path ) : node = trees . tree ( ) . root path_split = path . split ( ""/"" ) for path_element in path_split : name_to_node = { x . name : x for x in node . children } if path_element not in name_to_node : self . log . warning ( ""cgroup does not exist: %s"" , path ) return else : node = name_to_node [ path_element ] # node is now the leaf node parent = node . parent self . log . debug ( ""deleting cgroup %s/%s"" , parent , node . name ) parent . delete_cgroup ( node . name )"
427,apache/airflow,airflow/contrib/task_runner/cgroup_task_runner.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/task_runner/cgroup_task_runner.py#L197-L210,"def _get_cgroup_names():
        """"""
        :return: a mapping between the subsystem name to the cgroup name
        :rtype: dict[str, str]
        """"""
        with open(""/proc/self/cgroup"") as f:
            lines = f.readlines()
            d = {}
            for line in lines:
                line_split = line.rstrip().split("":"")
                subsystem = line_split[1]
                group_name = line_split[2]
                d[subsystem] = group_name
            return d","['def', '_get_cgroup_names', '(', ')', ':', 'with', 'open', '(', '""/proc/self/cgroup""', ')', 'as', 'f', ':', 'lines', '=', 'f', '.', 'readlines', '(', ')', 'd', '=', '{', '}', 'for', 'line', 'in', 'lines', ':', 'line_split', '=', 'line', '.', 'rstrip', '(', ')', '.', 'split', '(', '"":""', ')', 'subsystem', '=', 'line_split', '[', '1', ']', 'group_name', '=', 'line_split', '[', '2', ']', 'd', '[', 'subsystem', ']', '=', 'group_name', 'return', 'd']",":return: a mapping between the subsystem name to the cgroup name
        :rtype: dict[str, str]","[':', 'return', ':', 'a', 'mapping', 'between', 'the', 'subsystem', 'name', 'to', 'the', 'cgroup', 'name', ':', 'rtype', ':', 'dict', '[', 'str', 'str', ']']",python,test,"[':', 'return', ':', 'a', 'mapping', 'between', 'the', 'subsystem', 'name', 'to', 'the', 'cgroup', 'name', ':', 'rtype', ':', 'dict', '[', 'str', 'str', ']']",: return : a mapping between the subsystem name to the cgroup name : rtype : dict [ str str ],"['def', '_get_cgroup_names', '(', ')', ':', 'with', 'open', '(', '""/proc/self/cgroup""', ')', 'as', 'f', ':', 'lines', '=', 'f', '.', 'readlines', '(', ')', 'd', '=', '{', '}', 'for', 'line', 'in', 'lines', ':', 'line_split', '=', 'line', '.', 'rstrip', '(', ')', '.', 'split', '(', '"":""', ')', 'subsystem', '=', 'line_split', '[', '1', ']', 'group_name', '=', 'line_split', '[', '2', ']', 'd', '[', 'subsystem', ']', '=', 'group_name', 'return', 'd']","def _get_cgroup_names ( ) : with open ( ""/proc/self/cgroup"" ) as f : lines = f . readlines ( ) d = { } for line in lines : line_split = line . rstrip ( ) . split ( "":"" ) subsystem = line_split [ 1 ] group_name = line_split [ 2 ] d [ subsystem ] = group_name return d"
428,apache/airflow,airflow/contrib/hooks/databricks_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/databricks_hook.py#L73-L97,"def _parse_host(host):
        """"""
        The purpose of this function is to be robust to improper connections
        settings provided by users, specifically in the host field.

        For example -- when users supply ``https://xx.cloud.databricks.com`` as the
        host, we must strip out the protocol to get the host.::

            h = DatabricksHook()
            assert h._parse_host('https://xx.cloud.databricks.com') == \
                'xx.cloud.databricks.com'

        In the case where users supply the correct ``xx.cloud.databricks.com`` as the
        host, this function is a no-op.::

            assert h._parse_host('xx.cloud.databricks.com') == 'xx.cloud.databricks.com'

        """"""
        urlparse_host = urlparse.urlparse(host).hostname
        if urlparse_host:
            # In this case, host = https://xx.cloud.databricks.com
            return urlparse_host
        else:
            # In this case, host = xx.cloud.databricks.com
            return host","['def', '_parse_host', '(', 'host', ')', ':', 'urlparse_host', '=', 'urlparse', '.', 'urlparse', '(', 'host', ')', '.', 'hostname', 'if', 'urlparse_host', ':', '# In this case, host = https://xx.cloud.databricks.com', 'return', 'urlparse_host', 'else', ':', '# In this case, host = xx.cloud.databricks.com', 'return', 'host']","The purpose of this function is to be robust to improper connections
        settings provided by users, specifically in the host field.

        For example -- when users supply ``https://xx.cloud.databricks.com`` as the
        host, we must strip out the protocol to get the host.::

            h = DatabricksHook()
            assert h._parse_host('https://xx.cloud.databricks.com') == \
                'xx.cloud.databricks.com'

        In the case where users supply the correct ``xx.cloud.databricks.com`` as the
        host, this function is a no-op.::

            assert h._parse_host('xx.cloud.databricks.com') == 'xx.cloud.databricks.com'","['The', 'purpose', 'of', 'this', 'function', 'is', 'to', 'be', 'robust', 'to', 'improper', 'connections', 'settings', 'provided', 'by', 'users', 'specifically', 'in', 'the', 'host', 'field', '.']",python,test,"['the', 'purpose', 'of', 'this', 'function', 'is', 'to', 'be', 'robust', 'to', 'improper', 'connections', 'settings', 'provided', 'by', 'users', 'specifically', 'in', 'the', 'host', 'field', '.']",the purpose of this function is to be robust to improper connections settings provided by users specifically in the host field .,"['def', '_parse_host', '(', 'host', ')', ':', 'urlparse_host', '=', 'urlparse', '.', 'urlparse', '(', 'host', ')', '.', 'hostname', 'if', 'urlparse_host', ':', '# in this case, host = https://xx.cloud.databricks.com', 'return', 'urlparse_host', 'else', ':', '# in this case, host = xx.cloud.databricks.com', 'return', 'host']","def _parse_host ( host ) : urlparse_host = urlparse . urlparse ( host ) . hostname if urlparse_host : # in this case, host = https://xx.cloud.databricks.com return urlparse_host else : # in this case, host = xx.cloud.databricks.com return host"
429,apache/airflow,airflow/contrib/hooks/databricks_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/databricks_hook.py#L99-L154,"def _do_api_call(self, endpoint_info, json):
        """"""
        Utility function to perform an API call with retries

        :param endpoint_info: Tuple of method and endpoint
        :type endpoint_info: tuple[string, string]
        :param json: Parameters for this API call.
        :type json: dict
        :return: If the api call returns a OK status code,
            this function returns the response in JSON. Otherwise,
            we throw an AirflowException.
        :rtype: dict
        """"""
        method, endpoint = endpoint_info
        url = 'https://{host}/{endpoint}'.format(
            host=self._parse_host(self.databricks_conn.host),
            endpoint=endpoint)
        if 'token' in self.databricks_conn.extra_dejson:
            self.log.info('Using token auth.')
            auth = _TokenAuth(self.databricks_conn.extra_dejson['token'])
        else:
            self.log.info('Using basic auth.')
            auth = (self.databricks_conn.login, self.databricks_conn.password)
        if method == 'GET':
            request_func = requests.get
        elif method == 'POST':
            request_func = requests.post
        else:
            raise AirflowException('Unexpected HTTP Method: ' + method)

        attempt_num = 1
        while True:
            try:
                response = request_func(
                    url,
                    json=json,
                    auth=auth,
                    headers=USER_AGENT_HEADER,
                    timeout=self.timeout_seconds)
                response.raise_for_status()
                return response.json()
            except requests_exceptions.RequestException as e:
                if not _retryable_error(e):
                    # In this case, the user probably made a mistake.
                    # Don't retry.
                    raise AirflowException('Response: {0}, Status Code: {1}'.format(
                        e.response.content, e.response.status_code))

                self._log_request_error(attempt_num, e)

            if attempt_num == self.retry_limit:
                raise AirflowException(('API requests to Databricks failed {} times. ' +
                                        'Giving up.').format(self.retry_limit))

            attempt_num += 1
            sleep(self.retry_delay)","['def', '_do_api_call', '(', 'self', ',', 'endpoint_info', ',', 'json', ')', ':', 'method', ',', 'endpoint', '=', 'endpoint_info', 'url', '=', ""'https://{host}/{endpoint}'"", '.', 'format', '(', 'host', '=', 'self', '.', '_parse_host', '(', 'self', '.', 'databricks_conn', '.', 'host', ')', ',', 'endpoint', '=', 'endpoint', ')', 'if', ""'token'"", 'in', 'self', '.', 'databricks_conn', '.', 'extra_dejson', ':', 'self', '.', 'log', '.', 'info', '(', ""'Using token auth.'"", ')', 'auth', '=', '_TokenAuth', '(', 'self', '.', 'databricks_conn', '.', 'extra_dejson', '[', ""'token'"", ']', ')', 'else', ':', 'self', '.', 'log', '.', 'info', '(', ""'Using basic auth.'"", ')', 'auth', '=', '(', 'self', '.', 'databricks_conn', '.', 'login', ',', 'self', '.', 'databricks_conn', '.', 'password', ')', 'if', 'method', '==', ""'GET'"", ':', 'request_func', '=', 'requests', '.', 'get', 'elif', 'method', '==', ""'POST'"", ':', 'request_func', '=', 'requests', '.', 'post', 'else', ':', 'raise', 'AirflowException', '(', ""'Unexpected HTTP Method: '"", '+', 'method', ')', 'attempt_num', '=', '1', 'while', 'True', ':', 'try', ':', 'response', '=', 'request_func', '(', 'url', ',', 'json', '=', 'json', ',', 'auth', '=', 'auth', ',', 'headers', '=', 'USER_AGENT_HEADER', ',', 'timeout', '=', 'self', '.', 'timeout_seconds', ')', 'response', '.', 'raise_for_status', '(', ')', 'return', 'response', '.', 'json', '(', ')', 'except', 'requests_exceptions', '.', 'RequestException', 'as', 'e', ':', 'if', 'not', '_retryable_error', '(', 'e', ')', ':', '# In this case, the user probably made a mistake.', ""# Don't retry."", 'raise', 'AirflowException', '(', ""'Response: {0}, Status Code: {1}'"", '.', 'format', '(', 'e', '.', 'response', '.', 'content', ',', 'e', '.', 'response', '.', 'status_code', ')', ')', 'self', '.', '_log_request_error', '(', 'attempt_num', ',', 'e', ')', 'if', 'attempt_num', '==', 'self', '.', 'retry_limit', ':', 'raise', 'AirflowException', '(', '(', ""'API requests to Databricks failed {} times. '"", '+', ""'Giving up.'"", ')', '.', 'format', '(', 'self', '.', 'retry_limit', ')', ')', 'attempt_num', '+=', '1', 'sleep', '(', 'self', '.', 'retry_delay', ')']","Utility function to perform an API call with retries

        :param endpoint_info: Tuple of method and endpoint
        :type endpoint_info: tuple[string, string]
        :param json: Parameters for this API call.
        :type json: dict
        :return: If the api call returns a OK status code,
            this function returns the response in JSON. Otherwise,
            we throw an AirflowException.
        :rtype: dict","['Utility', 'function', 'to', 'perform', 'an', 'API', 'call', 'with', 'retries']",python,test,"['utility', 'function', 'to', 'perform', 'an', 'api', 'call', 'with', 'retries']",utility function to perform an api call with retries,"['def', '_do_api_call', '(', 'self', ',', 'endpoint_info', ',', 'json', ')', ':', 'method', ',', 'endpoint', '=', 'endpoint_info', 'url', '=', ""'https://{host}/{endpoint}'"", '.', 'format', '(', 'host', '=', 'self', '.', '_parse_host', '(', 'self', '.', 'databricks_conn', '.', 'host', ')', ',', 'endpoint', '=', 'endpoint', ')', 'if', ""'token'"", 'in', 'self', '.', 'databricks_conn', '.', 'extra_dejson', ':', 'self', '.', 'log', '.', 'info', '(', ""'using token auth.'"", ')', 'auth', '=', '_tokenauth', '(', 'self', '.', 'databricks_conn', '.', 'extra_dejson', '[', ""'token'"", ']', ')', 'else', ':', 'self', '.', 'log', '.', 'info', '(', ""'using basic auth.'"", ')', 'auth', '=', '(', 'self', '.', 'databricks_conn', '.', 'login', ',', 'self', '.', 'databricks_conn', '.', 'password', ')', 'if', 'method', '==', ""'get'"", ':', 'request_func', '=', 'requests', '.', 'get', 'elif', 'method', '==', ""'post'"", ':', 'request_func', '=', 'requests', '.', 'post', 'else', ':', 'raise', 'airflowexception', '(', ""'unexpected http method: '"", '+', 'method', ')', 'attempt_num', '=', '1', 'while', 'true', ':', 'try', ':', 'response', '=', 'request_func', '(', 'url', ',', 'json', '=', 'json', ',', 'auth', '=', 'auth', ',', 'headers', '=', 'user_agent_header', ',', 'timeout', '=', 'self', '.', 'timeout_seconds', ')', 'response', '.', 'raise_for_status', '(', ')', 'return', 'response', '.', 'json', '(', ')', 'except', 'requests_exceptions', '.', 'requestexception', 'as', 'e', ':', 'if', 'not', '_retryable_error', '(', 'e', ')', ':', '# in this case, the user probably made a mistake.', ""# don't retry."", 'raise', 'airflowexception', '(', ""'response: {0}, status code: {1}'"", '.', 'format', '(', 'e', '.', 'response', '.', 'content', ',', 'e', '.', 'response', '.', 'status_code', ')', ')', 'self', '.', '_log_request_error', '(', 'attempt_num', ',', 'e', ')', 'if', 'attempt_num', '==', 'self', '.', 'retry_limit', ':', 'raise', 'airflowexception', '(', '(', ""'api requests to databricks failed {} times. '"", '+', ""'giving up.'"", ')', '.', 'format', '(', 'self', '.', 'retry_limit', ')', ')', 'attempt_num', '+=', '1', 'sleep', '(', 'self', '.', 'retry_delay', ')']","def _do_api_call ( self , endpoint_info , json ) : method , endpoint = endpoint_info url = 'https://{host}/{endpoint}' . format ( host = self . _parse_host ( self . databricks_conn . host ) , endpoint = endpoint ) if 'token' in self . databricks_conn . extra_dejson : self . log . info ( 'using token auth.' ) auth = _tokenauth ( self . databricks_conn . extra_dejson [ 'token' ] ) else : self . log . info ( 'using basic auth.' ) auth = ( self . databricks_conn . login , self . databricks_conn . password ) if method == 'get' : request_func = requests . get elif method == 'post' : request_func = requests . post else : raise airflowexception ( 'unexpected http method: ' + method ) attempt_num = 1 while true : try : response = request_func ( url , json = json , auth = auth , headers = user_agent_header , timeout = self . timeout_seconds ) response . raise_for_status ( ) return response . json ( ) except requests_exceptions . requestexception as e : if not _retryable_error ( e ) : # in this case, the user probably made a mistake. # don't retry. raise airflowexception ( 'response: {0}, status code: {1}' . format ( e . response . content , e . response . status_code ) ) self . _log_request_error ( attempt_num , e ) if attempt_num == self . retry_limit : raise airflowexception ( ( 'api requests to databricks failed {} times. ' + 'giving up.' ) . format ( self . retry_limit ) ) attempt_num += 1 sleep ( self . retry_delay )"
430,apache/airflow,airflow/contrib/hooks/salesforce_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/salesforce_hook.py#L60-L74,"def get_conn(self):
        """"""
        Sign into Salesforce, only if we are not already signed in.
        """"""
        if not self.conn:
            connection = self.get_connection(self.conn_id)
            extras = connection.extra_dejson
            self.conn = Salesforce(
                username=connection.login,
                password=connection.password,
                security_token=extras['security_token'],
                instance_url=connection.host,
                sandbox=extras.get('sandbox', False)
            )
        return self.conn","['def', 'get_conn', '(', 'self', ')', ':', 'if', 'not', 'self', '.', 'conn', ':', 'connection', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'conn_id', ')', 'extras', '=', 'connection', '.', 'extra_dejson', 'self', '.', 'conn', '=', 'Salesforce', '(', 'username', '=', 'connection', '.', 'login', ',', 'password', '=', 'connection', '.', 'password', ',', 'security_token', '=', 'extras', '[', ""'security_token'"", ']', ',', 'instance_url', '=', 'connection', '.', 'host', ',', 'sandbox', '=', 'extras', '.', 'get', '(', ""'sandbox'"", ',', 'False', ')', ')', 'return', 'self', '.', 'conn']","Sign into Salesforce, only if we are not already signed in.","['Sign', 'into', 'Salesforce', 'only', 'if', 'we', 'are', 'not', 'already', 'signed', 'in', '.']",python,test,"['sign', 'into', 'salesforce', 'only', 'if', 'we', 'are', 'not', 'already', 'signed', 'in', '.']",sign into salesforce only if we are not already signed in .,"['def', 'get_conn', '(', 'self', ')', ':', 'if', 'not', 'self', '.', 'conn', ':', 'connection', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'conn_id', ')', 'extras', '=', 'connection', '.', 'extra_dejson', 'self', '.', 'conn', '=', 'salesforce', '(', 'username', '=', 'connection', '.', 'login', ',', 'password', '=', 'connection', '.', 'password', ',', 'security_token', '=', 'extras', '[', ""'security_token'"", ']', ',', 'instance_url', '=', 'connection', '.', 'host', ',', 'sandbox', '=', 'extras', '.', 'get', '(', ""'sandbox'"", ',', 'false', ')', ')', 'return', 'self', '.', 'conn']","def get_conn ( self ) : if not self . conn : connection = self . get_connection ( self . conn_id ) extras = connection . extra_dejson self . conn = salesforce ( username = connection . login , password = connection . password , security_token = extras [ 'security_token' ] , instance_url = connection . host , sandbox = extras . get ( 'sandbox' , false ) ) return self . conn"
431,apache/airflow,airflow/contrib/hooks/salesforce_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/salesforce_hook.py#L76-L93,"def make_query(self, query):
        """"""
        Make a query to Salesforce.

        :param query: The query to make to Salesforce.
        :type query: str
        :return: The query result.
        :rtype: dict
        """"""
        conn = self.get_conn()

        self.log.info(""Querying for all objects"")
        query_results = conn.query_all(query)

        self.log.info(""Received results: Total size: %s; Done: %s"",
                      query_results['totalSize'], query_results['done'])

        return query_results","['def', 'make_query', '(', 'self', ',', 'query', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'self', '.', 'log', '.', 'info', '(', '""Querying for all objects""', ')', 'query_results', '=', 'conn', '.', 'query_all', '(', 'query', ')', 'self', '.', 'log', '.', 'info', '(', '""Received results: Total size: %s; Done: %s""', ',', 'query_results', '[', ""'totalSize'"", ']', ',', 'query_results', '[', ""'done'"", ']', ')', 'return', 'query_results']","Make a query to Salesforce.

        :param query: The query to make to Salesforce.
        :type query: str
        :return: The query result.
        :rtype: dict","['Make', 'a', 'query', 'to', 'Salesforce', '.']",python,test,"['make', 'a', 'query', 'to', 'salesforce', '.']",make a query to salesforce .,"['def', 'make_query', '(', 'self', ',', 'query', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'self', '.', 'log', '.', 'info', '(', '""querying for all objects""', ')', 'query_results', '=', 'conn', '.', 'query_all', '(', 'query', ')', 'self', '.', 'log', '.', 'info', '(', '""received results: total size: %s; done: %s""', ',', 'query_results', '[', ""'totalsize'"", ']', ',', 'query_results', '[', ""'done'"", ']', ')', 'return', 'query_results']","def make_query ( self , query ) : conn = self . get_conn ( ) self . log . info ( ""querying for all objects"" ) query_results = conn . query_all ( query ) self . log . info ( ""received results: total size: %s; done: %s"" , query_results [ 'totalsize' ] , query_results [ 'done' ] ) return query_results"
432,apache/airflow,airflow/contrib/hooks/salesforce_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/salesforce_hook.py#L95-L108,"def describe_object(self, obj):
        """"""
        Get the description of an object from Salesforce.
        This description is the object's schema and
        some extra metadata that Salesforce stores for each object.

        :param obj: The name of the Salesforce object that we are getting a description of.
        :type obj: str
        :return: the description of the Salesforce object.
        :rtype: dict
        """"""
        conn = self.get_conn()

        return conn.__getattr__(obj).describe()","['def', 'describe_object', '(', 'self', ',', 'obj', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'return', 'conn', '.', '__getattr__', '(', 'obj', ')', '.', 'describe', '(', ')']","Get the description of an object from Salesforce.
        This description is the object's schema and
        some extra metadata that Salesforce stores for each object.

        :param obj: The name of the Salesforce object that we are getting a description of.
        :type obj: str
        :return: the description of the Salesforce object.
        :rtype: dict","['Get', 'the', 'description', 'of', 'an', 'object', 'from', 'Salesforce', '.', 'This', 'description', 'is', 'the', 'object', 's', 'schema', 'and', 'some', 'extra', 'metadata', 'that', 'Salesforce', 'stores', 'for', 'each', 'object', '.']",python,test,"['get', 'the', 'description', 'of', 'an', 'object', 'from', 'salesforce', '.', 'this', 'description', 'is', 'the', 'object', 's', 'schema', 'and', 'some', 'extra', 'metadata', 'that', 'salesforce', 'stores', 'for', 'each', 'object', '.']",get the description of an object from salesforce . this description is the object s schema and some extra metadata that salesforce stores for each object .,"['def', 'describe_object', '(', 'self', ',', 'obj', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'return', 'conn', '.', '__getattr__', '(', 'obj', ')', '.', 'describe', '(', ')']","def describe_object ( self , obj ) : conn = self . get_conn ( ) return conn . __getattr__ ( obj ) . describe ( )"
433,apache/airflow,airflow/contrib/hooks/salesforce_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/salesforce_hook.py#L110-L123,"def get_available_fields(self, obj):
        """"""
        Get a list of all available fields for an object.

        :param obj: The name of the Salesforce object that we are getting a description of.
        :type obj: str
        :return: the names of the fields.
        :rtype: list of str
        """"""
        self.get_conn()

        obj_description = self.describe_object(obj)

        return [field['name'] for field in obj_description['fields']]","['def', 'get_available_fields', '(', 'self', ',', 'obj', ')', ':', 'self', '.', 'get_conn', '(', ')', 'obj_description', '=', 'self', '.', 'describe_object', '(', 'obj', ')', 'return', '[', 'field', '[', ""'name'"", ']', 'for', 'field', 'in', 'obj_description', '[', ""'fields'"", ']', ']']","Get a list of all available fields for an object.

        :param obj: The name of the Salesforce object that we are getting a description of.
        :type obj: str
        :return: the names of the fields.
        :rtype: list of str","['Get', 'a', 'list', 'of', 'all', 'available', 'fields', 'for', 'an', 'object', '.']",python,test,"['get', 'a', 'list', 'of', 'all', 'available', 'fields', 'for', 'an', 'object', '.']",get a list of all available fields for an object .,"['def', 'get_available_fields', '(', 'self', ',', 'obj', ')', ':', 'self', '.', 'get_conn', '(', ')', 'obj_description', '=', 'self', '.', 'describe_object', '(', 'obj', ')', 'return', '[', 'field', '[', ""'name'"", ']', 'for', 'field', 'in', 'obj_description', '[', ""'fields'"", ']', ']']","def get_available_fields ( self , obj ) : self . get_conn ( ) obj_description = self . describe_object ( obj ) return [ field [ 'name' ] for field in obj_description [ 'fields' ] ]"
434,apache/airflow,airflow/contrib/hooks/salesforce_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/salesforce_hook.py#L125-L145,"def get_object_from_salesforce(self, obj, fields):
        """"""
        Get all instances of the `object` from Salesforce.
        For each model, only get the fields specified in fields.

        All we really do underneath the hood is run:
            SELECT <fields> FROM <obj>;

        :param obj: The object name to get from Salesforce.
        :type obj: str
        :param fields: The fields to get from the object.
        :type fields: iterable
        :return: all instances of the object from Salesforce.
        :rtype: dict
        """"""
        query = ""SELECT {} FROM {}"".format("","".join(fields), obj)

        self.log.info(""Making query to Salesforce: %s"",
                      query if len(query) < 30 else "" ... "".join([query[:15], query[-15:]]))

        return self.make_query(query)","['def', 'get_object_from_salesforce', '(', 'self', ',', 'obj', ',', 'fields', ')', ':', 'query', '=', '""SELECT {} FROM {}""', '.', 'format', '(', '"",""', '.', 'join', '(', 'fields', ')', ',', 'obj', ')', 'self', '.', 'log', '.', 'info', '(', '""Making query to Salesforce: %s""', ',', 'query', 'if', 'len', '(', 'query', ')', '<', '30', 'else', '"" ... ""', '.', 'join', '(', '[', 'query', '[', ':', '15', ']', ',', 'query', '[', '-', '15', ':', ']', ']', ')', ')', 'return', 'self', '.', 'make_query', '(', 'query', ')']","Get all instances of the `object` from Salesforce.
        For each model, only get the fields specified in fields.

        All we really do underneath the hood is run:
            SELECT <fields> FROM <obj>;

        :param obj: The object name to get from Salesforce.
        :type obj: str
        :param fields: The fields to get from the object.
        :type fields: iterable
        :return: all instances of the object from Salesforce.
        :rtype: dict","['Get', 'all', 'instances', 'of', 'the', 'object', 'from', 'Salesforce', '.', 'For', 'each', 'model', 'only', 'get', 'the', 'fields', 'specified', 'in', 'fields', '.']",python,test,"['get', 'all', 'instances', 'of', 'the', 'object', 'from', 'salesforce', '.', 'for', 'each', 'model', 'only', 'get', 'the', 'fields', 'specified', 'in', 'fields', '.']",get all instances of the object from salesforce . for each model only get the fields specified in fields .,"['def', 'get_object_from_salesforce', '(', 'self', ',', 'obj', ',', 'fields', ')', ':', 'query', '=', '""select {} from {}""', '.', 'format', '(', '"",""', '.', 'join', '(', 'fields', ')', ',', 'obj', ')', 'self', '.', 'log', '.', 'info', '(', '""making query to salesforce: %s""', ',', 'query', 'if', 'len', '(', 'query', ')', '<', '30', 'else', '"" ... ""', '.', 'join', '(', '[', 'query', '[', ':', '15', ']', ',', 'query', '[', '-', '15', ':', ']', ']', ')', ')', 'return', 'self', '.', 'make_query', '(', 'query', ')']","def get_object_from_salesforce ( self , obj , fields ) : query = ""select {} from {}"" . format ( "","" . join ( fields ) , obj ) self . log . info ( ""making query to salesforce: %s"" , query if len ( query ) < 30 else "" ... "" . join ( [ query [ : 15 ] , query [ - 15 : ] ] ) ) return self . make_query ( query )"
435,apache/airflow,airflow/contrib/hooks/salesforce_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/salesforce_hook.py#L148-L184,"def _to_timestamp(cls, column):
        """"""
        Convert a column of a dataframe to UNIX timestamps if applicable

        :param column: A Series object representing a column of a dataframe.
        :type column: pd.Series
        :return: a new series that maintains the same index as the original
        :rtype: pd.Series
        """"""
        # try and convert the column to datetimes
        # the column MUST have a four digit year somewhere in the string
        # there should be a better way to do this,
        # but just letting pandas try and convert every column without a format
        # caused it to convert floats as well
        # For example, a column of integers
        # between 0 and 10 are turned into timestamps
        # if the column cannot be converted,
        # just return the original column untouched
        try:
            column = pd.to_datetime(column)
        except ValueError:
            log = LoggingMixin().log
            log.warning(""Could not convert field to timestamps: %s"", column.name)
            return column

        # now convert the newly created datetimes into timestamps
        # we have to be careful here
        # because NaT cannot be converted to a timestamp
        # so we have to return NaN
        converted = []
        for value in column:
            try:
                converted.append(value.timestamp())
            except (ValueError, AttributeError):
                converted.append(pd.np.NaN)

        return pd.Series(converted, index=column.index)","['def', '_to_timestamp', '(', 'cls', ',', 'column', ')', ':', '# try and convert the column to datetimes', '# the column MUST have a four digit year somewhere in the string', '# there should be a better way to do this,', '# but just letting pandas try and convert every column without a format', '# caused it to convert floats as well', '# For example, a column of integers', '# between 0 and 10 are turned into timestamps', '# if the column cannot be converted,', '# just return the original column untouched', 'try', ':', 'column', '=', 'pd', '.', 'to_datetime', '(', 'column', ')', 'except', 'ValueError', ':', 'log', '=', 'LoggingMixin', '(', ')', '.', 'log', 'log', '.', 'warning', '(', '""Could not convert field to timestamps: %s""', ',', 'column', '.', 'name', ')', 'return', 'column', '# now convert the newly created datetimes into timestamps', '# we have to be careful here', '# because NaT cannot be converted to a timestamp', '# so we have to return NaN', 'converted', '=', '[', ']', 'for', 'value', 'in', 'column', ':', 'try', ':', 'converted', '.', 'append', '(', 'value', '.', 'timestamp', '(', ')', ')', 'except', '(', 'ValueError', ',', 'AttributeError', ')', ':', 'converted', '.', 'append', '(', 'pd', '.', 'np', '.', 'NaN', ')', 'return', 'pd', '.', 'Series', '(', 'converted', ',', 'index', '=', 'column', '.', 'index', ')']","Convert a column of a dataframe to UNIX timestamps if applicable

        :param column: A Series object representing a column of a dataframe.
        :type column: pd.Series
        :return: a new series that maintains the same index as the original
        :rtype: pd.Series","['Convert', 'a', 'column', 'of', 'a', 'dataframe', 'to', 'UNIX', 'timestamps', 'if', 'applicable']",python,test,"['convert', 'a', 'column', 'of', 'a', 'dataframe', 'to', 'unix', 'timestamps', 'if', 'applicable']",convert a column of a dataframe to unix timestamps if applicable,"['def', '_to_timestamp', '(', 'cls', ',', 'column', ')', ':', '# try and convert the column to datetimes', '# the column must have a four digit year somewhere in the string', '# there should be a better way to do this,', '# but just letting pandas try and convert every column without a format', '# caused it to convert floats as well', '# for example, a column of integers', '# between 0 and 10 are turned into timestamps', '# if the column cannot be converted,', '# just return the original column untouched', 'try', ':', 'column', '=', 'pd', '.', 'to_datetime', '(', 'column', ')', 'except', 'valueerror', ':', 'log', '=', 'loggingmixin', '(', ')', '.', 'log', 'log', '.', 'warning', '(', '""could not convert field to timestamps: %s""', ',', 'column', '.', 'name', ')', 'return', 'column', '# now convert the newly created datetimes into timestamps', '# we have to be careful here', '# because nat cannot be converted to a timestamp', '# so we have to return nan', 'converted', '=', '[', ']', 'for', 'value', 'in', 'column', ':', 'try', ':', 'converted', '.', 'append', '(', 'value', '.', 'timestamp', '(', ')', ')', 'except', '(', 'valueerror', ',', 'attributeerror', ')', ':', 'converted', '.', 'append', '(', 'pd', '.', 'np', '.', 'nan', ')', 'return', 'pd', '.', 'series', '(', 'converted', ',', 'index', '=', 'column', '.', 'index', ')']","def _to_timestamp ( cls , column ) : # try and convert the column to datetimes # the column must have a four digit year somewhere in the string # there should be a better way to do this, # but just letting pandas try and convert every column without a format # caused it to convert floats as well # for example, a column of integers # between 0 and 10 are turned into timestamps # if the column cannot be converted, # just return the original column untouched try : column = pd . to_datetime ( column ) except valueerror : log = loggingmixin ( ) . log log . warning ( ""could not convert field to timestamps: %s"" , column . name ) return column # now convert the newly created datetimes into timestamps # we have to be careful here # because nat cannot be converted to a timestamp # so we have to return nan converted = [ ] for value in column : try : converted . append ( value . timestamp ( ) ) except ( valueerror , attributeerror ) : converted . append ( pd . np . nan ) return pd . series ( converted , index = column . index )"
436,apache/airflow,airflow/contrib/hooks/salesforce_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/salesforce_hook.py#L186-L293,"def write_object_to_file(self,
                             query_results,
                             filename,
                             fmt=""csv"",
                             coerce_to_timestamp=False,
                             record_time_added=False):
        """"""
        Write query results to file.

        Acceptable formats are:
            - csv:
                comma-separated-values file. This is the default format.
            - json:
                JSON array. Each element in the array is a different row.
            - ndjson:
                JSON array but each element is new-line delimited instead of comma delimited like in `json`

        This requires a significant amount of cleanup.
        Pandas doesn't handle output to CSV and json in a uniform way.
        This is especially painful for datetime types.
        Pandas wants to write them as strings in CSV, but as millisecond Unix timestamps.

        By default, this function will try and leave all values as they are represented in Salesforce.
        You use the `coerce_to_timestamp` flag to force all datetimes to become Unix timestamps (UTC).
        This is can be greatly beneficial as it will make all of your datetime fields look the same,
        and makes it easier to work with in other database environments

        :param query_results: the results from a SQL query
        :type query_results: list of dict
        :param filename: the name of the file where the data should be dumped to
        :type filename: str
        :param fmt: the format you want the output in. Default:  'csv'
        :type fmt: str
        :param coerce_to_timestamp: True if you want all datetime fields to be converted into Unix timestamps.
            False if you want them to be left in the same format as they were in Salesforce.
            Leaving the value as False will result in datetimes being strings. Default: False
        :type coerce_to_timestamp: bool
        :param record_time_added: True if you want to add a Unix timestamp field
            to the resulting data that marks when the data was fetched from Salesforce. Default: False
        :type record_time_added: bool
        :return: the dataframe that gets written to the file.
        :rtype: pd.Dataframe
        """"""
        fmt = fmt.lower()
        if fmt not in ['csv', 'json', 'ndjson']:
            raise ValueError(""Format value is not recognized: {}"".format(fmt))

        # this line right here will convert all integers to floats
        # if there are any None/np.nan values in the column
        # that's because None/np.nan cannot exist in an integer column
        # we should write all of our timestamps as FLOATS in our final schema
        df = pd.DataFrame.from_records(query_results, exclude=[""attributes""])

        df.columns = [column.lower() for column in df.columns]

        # convert columns with datetime strings to datetimes
        # not all strings will be datetimes, so we ignore any errors that occur
        # we get the object's definition at this point and only consider
        # features that are DATE or DATETIME
        if coerce_to_timestamp and df.shape[0] > 0:
            # get the object name out of the query results
            # it's stored in the ""attributes"" dictionary
            # for each returned record
            object_name = query_results[0]['attributes']['type']

            self.log.info(""Coercing timestamps for: %s"", object_name)

            schema = self.describe_object(object_name)

            # possible columns that can be converted to timestamps
            # are the ones that are either date or datetime types
            # strings are too general and we risk unintentional conversion
            possible_timestamp_cols = [
                field['name'].lower()
                for field in schema['fields']
                if field['type'] in [""date"", ""datetime""] and field['name'].lower() in df.columns
            ]
            df[possible_timestamp_cols] = df[possible_timestamp_cols].apply(self._to_timestamp)

        if record_time_added:
            fetched_time = time.time()
            df[""time_fetched_from_salesforce""] = fetched_time

        # write the CSV or JSON file depending on the option
        # NOTE:
        #   datetimes here are an issue.
        #   There is no good way to manage the difference
        #   for to_json, the options are an epoch or a ISO string
        #   but for to_csv, it will be a string output by datetime
        #   For JSON we decided to output the epoch timestamp in seconds
        #   (as is fairly standard for JavaScript)
        #   And for csv, we do a string
        if fmt == ""csv"":
            # there are also a ton of newline objects that mess up our ability to write to csv
            # we remove these newlines so that the output is a valid CSV format
            self.log.info(""Cleaning data and writing to CSV"")
            possible_strings = df.columns[df.dtypes == ""object""]
            df[possible_strings] = df[possible_strings].apply(
                lambda x: x.str.replace(""\r\n"", """").str.replace(""\n"", """")
            )
            # write the dataframe
            df.to_csv(filename, index=False)
        elif fmt == ""json"":
            df.to_json(filename, ""records"", date_unit=""s"")
        elif fmt == ""ndjson"":
            df.to_json(filename, ""records"", lines=True, date_unit=""s"")

        return df","['def', 'write_object_to_file', '(', 'self', ',', 'query_results', ',', 'filename', ',', 'fmt', '=', '""csv""', ',', 'coerce_to_timestamp', '=', 'False', ',', 'record_time_added', '=', 'False', ')', ':', 'fmt', '=', 'fmt', '.', 'lower', '(', ')', 'if', 'fmt', 'not', 'in', '[', ""'csv'"", ',', ""'json'"", ',', ""'ndjson'"", ']', ':', 'raise', 'ValueError', '(', '""Format value is not recognized: {}""', '.', 'format', '(', 'fmt', ')', ')', '# this line right here will convert all integers to floats', '# if there are any None/np.nan values in the column', ""# that's because None/np.nan cannot exist in an integer column"", '# we should write all of our timestamps as FLOATS in our final schema', 'df', '=', 'pd', '.', 'DataFrame', '.', 'from_records', '(', 'query_results', ',', 'exclude', '=', '[', '""attributes""', ']', ')', 'df', '.', 'columns', '=', '[', 'column', '.', 'lower', '(', ')', 'for', 'column', 'in', 'df', '.', 'columns', ']', '# convert columns with datetime strings to datetimes', '# not all strings will be datetimes, so we ignore any errors that occur', ""# we get the object's definition at this point and only consider"", '# features that are DATE or DATETIME', 'if', 'coerce_to_timestamp', 'and', 'df', '.', 'shape', '[', '0', ']', '>', '0', ':', '# get the object name out of the query results', '# it\'s stored in the ""attributes"" dictionary', '# for each returned record', 'object_name', '=', 'query_results', '[', '0', ']', '[', ""'attributes'"", ']', '[', ""'type'"", ']', 'self', '.', 'log', '.', 'info', '(', '""Coercing timestamps for: %s""', ',', 'object_name', ')', 'schema', '=', 'self', '.', 'describe_object', '(', 'object_name', ')', '# possible columns that can be converted to timestamps', '# are the ones that are either date or datetime types', '# strings are too general and we risk unintentional conversion', 'possible_timestamp_cols', '=', '[', 'field', '[', ""'name'"", ']', '.', 'lower', '(', ')', 'for', 'field', 'in', 'schema', '[', ""'fields'"", ']', 'if', 'field', '[', ""'type'"", ']', 'in', '[', '""date""', ',', '""datetime""', ']', 'and', 'field', '[', ""'name'"", ']', '.', 'lower', '(', ')', 'in', 'df', '.', 'columns', ']', 'df', '[', 'possible_timestamp_cols', ']', '=', 'df', '[', 'possible_timestamp_cols', ']', '.', 'apply', '(', 'self', '.', '_to_timestamp', ')', 'if', 'record_time_added', ':', 'fetched_time', '=', 'time', '.', 'time', '(', ')', 'df', '[', '""time_fetched_from_salesforce""', ']', '=', 'fetched_time', '# write the CSV or JSON file depending on the option', '# NOTE:', '#   datetimes here are an issue.', '#   There is no good way to manage the difference', '#   for to_json, the options are an epoch or a ISO string', '#   but for to_csv, it will be a string output by datetime', '#   For JSON we decided to output the epoch timestamp in seconds', '#   (as is fairly standard for JavaScript)', '#   And for csv, we do a string', 'if', 'fmt', '==', '""csv""', ':', '# there are also a ton of newline objects that mess up our ability to write to csv', '# we remove these newlines so that the output is a valid CSV format', 'self', '.', 'log', '.', 'info', '(', '""Cleaning data and writing to CSV""', ')', 'possible_strings', '=', 'df', '.', 'columns', '[', 'df', '.', 'dtypes', '==', '""object""', ']', 'df', '[', 'possible_strings', ']', '=', 'df', '[', 'possible_strings', ']', '.', 'apply', '(', 'lambda', 'x', ':', 'x', '.', 'str', '.', 'replace', '(', '""\\r\\n""', ',', '""""', ')', '.', 'str', '.', 'replace', '(', '""\\n""', ',', '""""', ')', ')', '# write the dataframe', 'df', '.', 'to_csv', '(', 'filename', ',', 'index', '=', 'False', ')', 'elif', 'fmt', '==', '""json""', ':', 'df', '.', 'to_json', '(', 'filename', ',', '""records""', ',', 'date_unit', '=', '""s""', ')', 'elif', 'fmt', '==', '""ndjson""', ':', 'df', '.', 'to_json', '(', 'filename', ',', '""records""', ',', 'lines', '=', 'True', ',', 'date_unit', '=', '""s""', ')', 'return', 'df']","Write query results to file.

        Acceptable formats are:
            - csv:
                comma-separated-values file. This is the default format.
            - json:
                JSON array. Each element in the array is a different row.
            - ndjson:
                JSON array but each element is new-line delimited instead of comma delimited like in `json`

        This requires a significant amount of cleanup.
        Pandas doesn't handle output to CSV and json in a uniform way.
        This is especially painful for datetime types.
        Pandas wants to write them as strings in CSV, but as millisecond Unix timestamps.

        By default, this function will try and leave all values as they are represented in Salesforce.
        You use the `coerce_to_timestamp` flag to force all datetimes to become Unix timestamps (UTC).
        This is can be greatly beneficial as it will make all of your datetime fields look the same,
        and makes it easier to work with in other database environments

        :param query_results: the results from a SQL query
        :type query_results: list of dict
        :param filename: the name of the file where the data should be dumped to
        :type filename: str
        :param fmt: the format you want the output in. Default:  'csv'
        :type fmt: str
        :param coerce_to_timestamp: True if you want all datetime fields to be converted into Unix timestamps.
            False if you want them to be left in the same format as they were in Salesforce.
            Leaving the value as False will result in datetimes being strings. Default: False
        :type coerce_to_timestamp: bool
        :param record_time_added: True if you want to add a Unix timestamp field
            to the resulting data that marks when the data was fetched from Salesforce. Default: False
        :type record_time_added: bool
        :return: the dataframe that gets written to the file.
        :rtype: pd.Dataframe","['Write', 'query', 'results', 'to', 'file', '.']",python,test,"['write', 'query', 'results', 'to', 'file', '.']",write query results to file .,"['def', 'write_object_to_file', '(', 'self', ',', 'query_results', ',', 'filename', ',', 'fmt', '=', '""csv""', ',', 'coerce_to_timestamp', '=', 'false', ',', 'record_time_added', '=', 'false', ')', ':', 'fmt', '=', 'fmt', '.', 'lower', '(', ')', 'if', 'fmt', 'not', 'in', '[', ""'csv'"", ',', ""'json'"", ',', ""'ndjson'"", ']', ':', 'raise', 'valueerror', '(', '""format value is not recognized: {}""', '.', 'format', '(', 'fmt', ')', ')', '# this line right here will convert all integers to floats', '# if there are any none/np.nan values in the column', ""# that's because none/np.nan cannot exist in an integer column"", '# we should write all of our timestamps as floats in our final schema', 'df', '=', 'pd', '.', 'dataframe', '.', 'from_records', '(', 'query_results', ',', 'exclude', '=', '[', '""attributes""', ']', ')', 'df', '.', 'columns', '=', '[', 'column', '.', 'lower', '(', ')', 'for', 'column', 'in', 'df', '.', 'columns', ']', '# convert columns with datetime strings to datetimes', '# not all strings will be datetimes, so we ignore any errors that occur', ""# we get the object's definition at this point and only consider"", '# features that are date or datetime', 'if', 'coerce_to_timestamp', 'and', 'df', '.', 'shape', '[', '0', ']', '>', '0', ':', '# get the object name out of the query results', '# it\'s stored in the ""attributes"" dictionary', '# for each returned record', 'object_name', '=', 'query_results', '[', '0', ']', '[', ""'attributes'"", ']', '[', ""'type'"", ']', 'self', '.', 'log', '.', 'info', '(', '""coercing timestamps for: %s""', ',', 'object_name', ')', 'schema', '=', 'self', '.', 'describe_object', '(', 'object_name', ')', '# possible columns that can be converted to timestamps', '# are the ones that are either date or datetime types', '# strings are too general and we risk unintentional conversion', 'possible_timestamp_cols', '=', '[', 'field', '[', ""'name'"", ']', '.', 'lower', '(', ')', 'for', 'field', 'in', 'schema', '[', ""'fields'"", ']', 'if', 'field', '[', ""'type'"", ']', 'in', '[', '""date""', ',', '""datetime""', ']', 'and', 'field', '[', ""'name'"", ']', '.', 'lower', '(', ')', 'in', 'df', '.', 'columns', ']', 'df', '[', 'possible_timestamp_cols', ']', '=', 'df', '[', 'possible_timestamp_cols', ']', '.', 'apply', '(', 'self', '.', '_to_timestamp', ')', 'if', 'record_time_added', ':', 'fetched_time', '=', 'time', '.', 'time', '(', ')', 'df', '[', '""time_fetched_from_salesforce""', ']', '=', 'fetched_time', '# write the csv or json file depending on the option', '# note:', '#   datetimes here are an issue.', '#   there is no good way to manage the difference', '#   for to_json, the options are an epoch or a iso string', '#   but for to_csv, it will be a string output by datetime', '#   for json we decided to output the epoch timestamp in seconds', '#   (as is fairly standard for javascript)', '#   and for csv, we do a string', 'if', 'fmt', '==', '""csv""', ':', '# there are also a ton of newline objects that mess up our ability to write to csv', '# we remove these newlines so that the output is a valid csv format', 'self', '.', 'log', '.', 'info', '(', '""cleaning data and writing to csv""', ')', 'possible_strings', '=', 'df', '.', 'columns', '[', 'df', '.', 'dtypes', '==', '""object""', ']', 'df', '[', 'possible_strings', ']', '=', 'df', '[', 'possible_strings', ']', '.', 'apply', '(', 'lambda', 'x', ':', 'x', '.', 'str', '.', 'replace', '(', '""\\r\\n""', ',', '""""', ')', '.', 'str', '.', 'replace', '(', '""\\n""', ',', '""""', ')', ')', '# write the dataframe', 'df', '.', 'to_csv', '(', 'filename', ',', 'index', '=', 'false', ')', 'elif', 'fmt', '==', '""json""', ':', 'df', '.', 'to_json', '(', 'filename', ',', '""records""', ',', 'date_unit', '=', '""s""', ')', 'elif', 'fmt', '==', '""ndjson""', ':', 'df', '.', 'to_json', '(', 'filename', ',', '""records""', ',', 'lines', '=', 'true', ',', 'date_unit', '=', '""s""', ')', 'return', 'df']","def write_object_to_file ( self , query_results , filename , fmt = ""csv"" , coerce_to_timestamp = false , record_time_added = false ) : fmt = fmt . lower ( ) if fmt not in [ 'csv' , 'json' , 'ndjson' ] : raise valueerror ( ""format value is not recognized: {}"" . format ( fmt ) ) # this line right here will convert all integers to floats # if there are any none/np.nan values in the column # that's because none/np.nan cannot exist in an integer column # we should write all of our timestamps as floats in our final schema df = pd . dataframe . from_records ( query_results , exclude = [ ""attributes"" ] ) df . columns = [ column . lower ( ) for column in df . columns ] # convert columns with datetime strings to datetimes # not all strings will be datetimes, so we ignore any errors that occur # we get the object's definition at this point and only consider # features that are date or datetime if coerce_to_timestamp and df . shape [ 0 ] > 0 : # get the object name out of the query results # it's stored in the ""attributes"" dictionary # for each returned record object_name = query_results [ 0 ] [ 'attributes' ] [ 'type' ] self . log . info ( ""coercing timestamps for: %s"" , object_name ) schema = self . describe_object ( object_name ) # possible columns that can be converted to timestamps # are the ones that are either date or datetime types # strings are too general and we risk unintentional conversion possible_timestamp_cols = [ field [ 'name' ] . lower ( ) for field in schema [ 'fields' ] if field [ 'type' ] in [ ""date"" , ""datetime"" ] and field [ 'name' ] . lower ( ) in df . columns ] df [ possible_timestamp_cols ] = df [ possible_timestamp_cols ] . apply ( self . _to_timestamp ) if record_time_added : fetched_time = time . time ( ) df [ ""time_fetched_from_salesforce"" ] = fetched_time # write the csv or json file depending on the option # note: #   datetimes here are an issue. #   there is no good way to manage the difference #   for to_json, the options are an epoch or a iso string #   but for to_csv, it will be a string output by datetime #   for json we decided to output the epoch timestamp in seconds #   (as is fairly standard for javascript) #   and for csv, we do a string if fmt == ""csv"" : # there are also a ton of newline objects that mess up our ability to write to csv # we remove these newlines so that the output is a valid csv format self . log . info ( ""cleaning data and writing to csv"" ) possible_strings = df . columns [ df . dtypes == ""object"" ] df [ possible_strings ] = df [ possible_strings ] . apply ( lambda x : x . str . replace ( ""\r\n"" , """" ) . str . replace ( ""\n"" , """" ) ) # write the dataframe df . to_csv ( filename , index = false ) elif fmt == ""json"" : df . to_json ( filename , ""records"" , date_unit = ""s"" ) elif fmt == ""ndjson"" : df . to_json ( filename , ""records"" , lines = true , date_unit = ""s"" ) return df"
437,apache/airflow,airflow/utils/log/gcs_task_handler.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/gcs_task_handler.py#L95-L121,"def _read(self, ti, try_number, metadata=None):
        """"""
        Read logs of given task instance and try_number from GCS.
        If failed, read the log from task instance host machine.
        :param ti: task instance object
        :param try_number: task instance try_number to read logs from
        :param metadata: log metadata,
                         can be used for steaming log reading and auto-tailing.
        """"""
        # Explicitly getting log relative path is necessary as the given
        # task instance might be different than task instance passed in
        # in set_context method.
        log_relative_path = self._render_filename(ti, try_number)
        remote_loc = os.path.join(self.remote_base, log_relative_path)

        try:
            remote_log = self.gcs_read(remote_loc)
            log = '*** Reading remote log from {}.\n{}\n'.format(
                remote_loc, remote_log)
            return log, {'end_of_log': True}
        except Exception as e:
            log = '*** Unable to read remote log from {}\n*** {}\n\n'.format(
                remote_loc, str(e))
            self.log.error(log)
            local_log, metadata = super()._read(ti, try_number)
            log += local_log
            return log, metadata","['def', '_read', '(', 'self', ',', 'ti', ',', 'try_number', ',', 'metadata', '=', 'None', ')', ':', '# Explicitly getting log relative path is necessary as the given', '# task instance might be different than task instance passed in', '# in set_context method.', 'log_relative_path', '=', 'self', '.', '_render_filename', '(', 'ti', ',', 'try_number', ')', 'remote_loc', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'remote_base', ',', 'log_relative_path', ')', 'try', ':', 'remote_log', '=', 'self', '.', 'gcs_read', '(', 'remote_loc', ')', 'log', '=', ""'*** Reading remote log from {}.\\n{}\\n'"", '.', 'format', '(', 'remote_loc', ',', 'remote_log', ')', 'return', 'log', ',', '{', ""'end_of_log'"", ':', 'True', '}', 'except', 'Exception', 'as', 'e', ':', 'log', '=', ""'*** Unable to read remote log from {}\\n*** {}\\n\\n'"", '.', 'format', '(', 'remote_loc', ',', 'str', '(', 'e', ')', ')', 'self', '.', 'log', '.', 'error', '(', 'log', ')', 'local_log', ',', 'metadata', '=', 'super', '(', ')', '.', '_read', '(', 'ti', ',', 'try_number', ')', 'log', '+=', 'local_log', 'return', 'log', ',', 'metadata']","Read logs of given task instance and try_number from GCS.
        If failed, read the log from task instance host machine.
        :param ti: task instance object
        :param try_number: task instance try_number to read logs from
        :param metadata: log metadata,
                         can be used for steaming log reading and auto-tailing.","['Read', 'logs', 'of', 'given', 'task', 'instance', 'and', 'try_number', 'from', 'GCS', '.', 'If', 'failed', 'read', 'the', 'log', 'from', 'task', 'instance', 'host', 'machine', '.', ':', 'param', 'ti', ':', 'task', 'instance', 'object', ':', 'param', 'try_number', ':', 'task', 'instance', 'try_number', 'to', 'read', 'logs', 'from', ':', 'param', 'metadata', ':', 'log', 'metadata', 'can', 'be', 'used', 'for', 'steaming', 'log', 'reading', 'and', 'auto', '-', 'tailing', '.']",python,test,"['read', 'logs', 'of', 'given', 'task', 'instance', 'and', 'try_number', 'from', 'gcs', '.', 'if', 'failed', 'read', 'the', 'log', 'from', 'task', 'instance', 'host', 'machine', '.', ':', 'param', 'ti', ':', 'task', 'instance', 'object', ':', 'param', 'try_number', ':', 'task', 'instance', 'try_number', 'to', 'read', 'logs', 'from', ':', 'param', 'metadata', ':', 'log', 'metadata', 'can', 'be', 'used', 'for', 'steaming', 'log', 'reading', 'and', 'auto', '-', 'tailing', '.']",read logs of given task instance and try_number from gcs . if failed read the log from task instance host machine . : param ti : task instance object : param try_number : task instance try_number to read logs from : param metadata : log metadata can be used for steaming log reading and auto - tailing .,"['def', '_read', '(', 'self', ',', 'ti', ',', 'try_number', ',', 'metadata', '=', 'none', ')', ':', '# explicitly getting log relative path is necessary as the given', '# task instance might be different than task instance passed in', '# in set_context method.', 'log_relative_path', '=', 'self', '.', '_render_filename', '(', 'ti', ',', 'try_number', ')', 'remote_loc', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'remote_base', ',', 'log_relative_path', ')', 'try', ':', 'remote_log', '=', 'self', '.', 'gcs_read', '(', 'remote_loc', ')', 'log', '=', ""'*** reading remote log from {}.\\n{}\\n'"", '.', 'format', '(', 'remote_loc', ',', 'remote_log', ')', 'return', 'log', ',', '{', ""'end_of_log'"", ':', 'true', '}', 'except', 'exception', 'as', 'e', ':', 'log', '=', ""'*** unable to read remote log from {}\\n*** {}\\n\\n'"", '.', 'format', '(', 'remote_loc', ',', 'str', '(', 'e', ')', ')', 'self', '.', 'log', '.', 'error', '(', 'log', ')', 'local_log', ',', 'metadata', '=', 'super', '(', ')', '.', '_read', '(', 'ti', ',', 'try_number', ')', 'log', '+=', 'local_log', 'return', 'log', ',', 'metadata']","def _read ( self , ti , try_number , metadata = none ) : # explicitly getting log relative path is necessary as the given # task instance might be different than task instance passed in # in set_context method. log_relative_path = self . _render_filename ( ti , try_number ) remote_loc = os . path . join ( self . remote_base , log_relative_path ) try : remote_log = self . gcs_read ( remote_loc ) log = '*** reading remote log from {}.\n{}\n' . format ( remote_loc , remote_log ) return log , { 'end_of_log' : true } except exception as e : log = '*** unable to read remote log from {}\n*** {}\n\n' . format ( remote_loc , str ( e ) ) self . log . error ( log ) local_log , metadata = super ( ) . _read ( ti , try_number ) log += local_log return log , metadata"
438,apache/airflow,airflow/utils/log/gcs_task_handler.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/gcs_task_handler.py#L123-L130,"def gcs_read(self, remote_log_location):
        """"""
        Returns the log found at the remote_log_location.
        :param remote_log_location: the log's location in remote storage
        :type remote_log_location: str (path)
        """"""
        bkt, blob = self.parse_gcs_url(remote_log_location)
        return self.hook.download(bkt, blob).decode('utf-8')","['def', 'gcs_read', '(', 'self', ',', 'remote_log_location', ')', ':', 'bkt', ',', 'blob', '=', 'self', '.', 'parse_gcs_url', '(', 'remote_log_location', ')', 'return', 'self', '.', 'hook', '.', 'download', '(', 'bkt', ',', 'blob', ')', '.', 'decode', '(', ""'utf-8'"", ')']","Returns the log found at the remote_log_location.
        :param remote_log_location: the log's location in remote storage
        :type remote_log_location: str (path)","['Returns', 'the', 'log', 'found', 'at', 'the', 'remote_log_location', '.', ':', 'param', 'remote_log_location', ':', 'the', 'log', 's', 'location', 'in', 'remote', 'storage', ':', 'type', 'remote_log_location', ':', 'str', '(', 'path', ')']",python,test,"['returns', 'the', 'log', 'found', 'at', 'the', 'remote_log_location', '.', ':', 'param', 'remote_log_location', ':', 'the', 'log', 's', 'location', 'in', 'remote', 'storage', ':', 'type', 'remote_log_location', ':', 'str', '(', 'path', ')']",returns the log found at the remote_log_location . : param remote_log_location : the log s location in remote storage : type remote_log_location : str ( path ),"['def', 'gcs_read', '(', 'self', ',', 'remote_log_location', ')', ':', 'bkt', ',', 'blob', '=', 'self', '.', 'parse_gcs_url', '(', 'remote_log_location', ')', 'return', 'self', '.', 'hook', '.', 'download', '(', 'bkt', ',', 'blob', ')', '.', 'decode', '(', ""'utf-8'"", ')']","def gcs_read ( self , remote_log_location ) : bkt , blob = self . parse_gcs_url ( remote_log_location ) return self . hook . download ( bkt , blob ) . decode ( 'utf-8' )"
439,apache/airflow,airflow/utils/log/gcs_task_handler.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/gcs_task_handler.py#L132-L163,"def gcs_write(self, log, remote_log_location, append=True):
        """"""
        Writes the log to the remote_log_location. Fails silently if no hook
        was created.
        :param log: the log to write to the remote_log_location
        :type log: str
        :param remote_log_location: the log's location in remote storage
        :type remote_log_location: str (path)
        :param append: if False, any existing log file is overwritten. If True,
            the new log is appended to any existing logs.
        :type append: bool
        """"""
        if append:
            try:
                old_log = self.gcs_read(remote_log_location)
                log = '\n'.join([old_log, log]) if old_log else log
            except Exception as e:
                if not hasattr(e, 'resp') or e.resp.get('status') != '404':
                    log = '*** Previous log discarded: {}\n\n'.format(str(e)) + log

        try:
            bkt, blob = self.parse_gcs_url(remote_log_location)
            from tempfile import NamedTemporaryFile
            with NamedTemporaryFile(mode='w+') as tmpfile:
                tmpfile.write(log)
                # Force the file to be flushed, since we're doing the
                # upload from within the file context (it hasn't been
                # closed).
                tmpfile.flush()
                self.hook.upload(bkt, blob, tmpfile.name)
        except Exception as e:
            self.log.error('Could not write logs to %s: %s', remote_log_location, e)","['def', 'gcs_write', '(', 'self', ',', 'log', ',', 'remote_log_location', ',', 'append', '=', 'True', ')', ':', 'if', 'append', ':', 'try', ':', 'old_log', '=', 'self', '.', 'gcs_read', '(', 'remote_log_location', ')', 'log', '=', ""'\\n'"", '.', 'join', '(', '[', 'old_log', ',', 'log', ']', ')', 'if', 'old_log', 'else', 'log', 'except', 'Exception', 'as', 'e', ':', 'if', 'not', 'hasattr', '(', 'e', ',', ""'resp'"", ')', 'or', 'e', '.', 'resp', '.', 'get', '(', ""'status'"", ')', '!=', ""'404'"", ':', 'log', '=', ""'*** Previous log discarded: {}\\n\\n'"", '.', 'format', '(', 'str', '(', 'e', ')', ')', '+', 'log', 'try', ':', 'bkt', ',', 'blob', '=', 'self', '.', 'parse_gcs_url', '(', 'remote_log_location', ')', 'from', 'tempfile', 'import', 'NamedTemporaryFile', 'with', 'NamedTemporaryFile', '(', 'mode', '=', ""'w+'"", ')', 'as', 'tmpfile', ':', 'tmpfile', '.', 'write', '(', 'log', ')', ""# Force the file to be flushed, since we're doing the"", ""# upload from within the file context (it hasn't been"", '# closed).', 'tmpfile', '.', 'flush', '(', ')', 'self', '.', 'hook', '.', 'upload', '(', 'bkt', ',', 'blob', ',', 'tmpfile', '.', 'name', ')', 'except', 'Exception', 'as', 'e', ':', 'self', '.', 'log', '.', 'error', '(', ""'Could not write logs to %s: %s'"", ',', 'remote_log_location', ',', 'e', ')']","Writes the log to the remote_log_location. Fails silently if no hook
        was created.
        :param log: the log to write to the remote_log_location
        :type log: str
        :param remote_log_location: the log's location in remote storage
        :type remote_log_location: str (path)
        :param append: if False, any existing log file is overwritten. If True,
            the new log is appended to any existing logs.
        :type append: bool","['Writes', 'the', 'log', 'to', 'the', 'remote_log_location', '.', 'Fails', 'silently', 'if', 'no', 'hook', 'was', 'created', '.', ':', 'param', 'log', ':', 'the', 'log', 'to', 'write', 'to', 'the', 'remote_log_location', ':', 'type', 'log', ':', 'str', ':', 'param', 'remote_log_location', ':', 'the', 'log', 's', 'location', 'in', 'remote', 'storage', ':', 'type', 'remote_log_location', ':', 'str', '(', 'path', ')', ':', 'param', 'append', ':', 'if', 'False', 'any', 'existing', 'log', 'file', 'is', 'overwritten', '.', 'If', 'True', 'the', 'new', 'log', 'is', 'appended', 'to', 'any', 'existing', 'logs', '.', ':', 'type', 'append', ':', 'bool']",python,test,"['writes', 'the', 'log', 'to', 'the', 'remote_log_location', '.', 'fails', 'silently', 'if', 'no', 'hook', 'was', 'created', '.', ':', 'param', 'log', ':', 'the', 'log', 'to', 'write', 'to', 'the', 'remote_log_location', ':', 'type', 'log', ':', 'str', ':', 'param', 'remote_log_location', ':', 'the', 'log', 's', 'location', 'in', 'remote', 'storage', ':', 'type', 'remote_log_location', ':', 'str', '(', 'path', ')', ':', 'param', 'append', ':', 'if', 'false', 'any', 'existing', 'log', 'file', 'is', 'overwritten', '.', 'if', 'true', 'the', 'new', 'log', 'is', 'appended', 'to', 'any', 'existing', 'logs', '.', ':', 'type', 'append', ':', 'bool']",writes the log to the remote_log_location . fails silently if no hook was created . : param log : the log to write to the remote_log_location : type log : str : param remote_log_location : the log s location in remote storage : type remote_log_location : str ( path ) : param append : if false any existing log file is overwritten . if true the new log is appended to any existing logs . : type append : bool,"['def', 'gcs_write', '(', 'self', ',', 'log', ',', 'remote_log_location', ',', 'append', '=', 'true', ')', ':', 'if', 'append', ':', 'try', ':', 'old_log', '=', 'self', '.', 'gcs_read', '(', 'remote_log_location', ')', 'log', '=', ""'\\n'"", '.', 'join', '(', '[', 'old_log', ',', 'log', ']', ')', 'if', 'old_log', 'else', 'log', 'except', 'exception', 'as', 'e', ':', 'if', 'not', 'hasattr', '(', 'e', ',', ""'resp'"", ')', 'or', 'e', '.', 'resp', '.', 'get', '(', ""'status'"", ')', '!=', ""'404'"", ':', 'log', '=', ""'*** previous log discarded: {}\\n\\n'"", '.', 'format', '(', 'str', '(', 'e', ')', ')', '+', 'log', 'try', ':', 'bkt', ',', 'blob', '=', 'self', '.', 'parse_gcs_url', '(', 'remote_log_location', ')', 'from', 'tempfile', 'import', 'namedtemporaryfile', 'with', 'namedtemporaryfile', '(', 'mode', '=', ""'w+'"", ')', 'as', 'tmpfile', ':', 'tmpfile', '.', 'write', '(', 'log', ')', ""# force the file to be flushed, since we're doing the"", ""# upload from within the file context (it hasn't been"", '# closed).', 'tmpfile', '.', 'flush', '(', ')', 'self', '.', 'hook', '.', 'upload', '(', 'bkt', ',', 'blob', ',', 'tmpfile', '.', 'name', ')', 'except', 'exception', 'as', 'e', ':', 'self', '.', 'log', '.', 'error', '(', ""'could not write logs to %s: %s'"", ',', 'remote_log_location', ',', 'e', ')']","def gcs_write ( self , log , remote_log_location , append = true ) : if append : try : old_log = self . gcs_read ( remote_log_location ) log = '\n' . join ( [ old_log , log ] ) if old_log else log except exception as e : if not hasattr ( e , 'resp' ) or e . resp . get ( 'status' ) != '404' : log = '*** previous log discarded: {}\n\n' . format ( str ( e ) ) + log try : bkt , blob = self . parse_gcs_url ( remote_log_location ) from tempfile import namedtemporaryfile with namedtemporaryfile ( mode = 'w+' ) as tmpfile : tmpfile . write ( log ) # force the file to be flushed, since we're doing the # upload from within the file context (it hasn't been # closed). tmpfile . flush ( ) self . hook . upload ( bkt , blob , tmpfile . name ) except exception as e : self . log . error ( 'could not write logs to %s: %s' , remote_log_location , e )"
440,apache/airflow,airflow/utils/log/gcs_task_handler.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/gcs_task_handler.py#L166-L177,"def parse_gcs_url(gsurl):
        """"""
        Given a Google Cloud Storage URL (gs://<bucket>/<blob>), returns a
        tuple containing the corresponding bucket and blob.
        """"""
        parsed_url = urlparse(gsurl)
        if not parsed_url.netloc:
            raise AirflowException('Please provide a bucket name')
        else:
            bucket = parsed_url.netloc
            blob = parsed_url.path.strip('/')
            return bucket, blob","['def', 'parse_gcs_url', '(', 'gsurl', ')', ':', 'parsed_url', '=', 'urlparse', '(', 'gsurl', ')', 'if', 'not', 'parsed_url', '.', 'netloc', ':', 'raise', 'AirflowException', '(', ""'Please provide a bucket name'"", ')', 'else', ':', 'bucket', '=', 'parsed_url', '.', 'netloc', 'blob', '=', 'parsed_url', '.', 'path', '.', 'strip', '(', ""'/'"", ')', 'return', 'bucket', ',', 'blob']","Given a Google Cloud Storage URL (gs://<bucket>/<blob>), returns a
        tuple containing the corresponding bucket and blob.","['Given', 'a', 'Google', 'Cloud', 'Storage', 'URL', '(', 'gs', ':', '//', '<bucket', '>', '/', '<blob', '>', ')', 'returns', 'a', 'tuple', 'containing', 'the', 'corresponding', 'bucket', 'and', 'blob', '.']",python,test,"['given', 'a', 'google', 'cloud', 'storage', 'url', '(', 'gs', ':', '//', '<bucket', '>', '/', '<blob', '>', ')', 'returns', 'a', 'tuple', 'containing', 'the', 'corresponding', 'bucket', 'and', 'blob', '.']",given a google cloud storage url ( gs : // <bucket > / <blob > ) returns a tuple containing the corresponding bucket and blob .,"['def', 'parse_gcs_url', '(', 'gsurl', ')', ':', 'parsed_url', '=', 'urlparse', '(', 'gsurl', ')', 'if', 'not', 'parsed_url', '.', 'netloc', ':', 'raise', 'airflowexception', '(', ""'please provide a bucket name'"", ')', 'else', ':', 'bucket', '=', 'parsed_url', '.', 'netloc', 'blob', '=', 'parsed_url', '.', 'path', '.', 'strip', '(', ""'/'"", ')', 'return', 'bucket', ',', 'blob']","def parse_gcs_url ( gsurl ) : parsed_url = urlparse ( gsurl ) if not parsed_url . netloc : raise airflowexception ( 'please provide a bucket name' ) else : bucket = parsed_url . netloc blob = parsed_url . path . strip ( '/' ) return bucket , blob"
441,apache/airflow,airflow/contrib/hooks/mongo_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/mongo_hook.py#L69-L85,"def get_conn(self):
        """"""
        Fetches PyMongo Client
        """"""
        if self.client is not None:
            return self.client

        # Mongo Connection Options dict that is unpacked when passed to MongoClient
        options = self.extras

        # If we are using SSL disable requiring certs from specific hostname
        if options.get('ssl', False):
            options.update({'ssl_cert_reqs': CERT_NONE})

        self.client = MongoClient(self.uri, **options)

        return self.client","['def', 'get_conn', '(', 'self', ')', ':', 'if', 'self', '.', 'client', 'is', 'not', 'None', ':', 'return', 'self', '.', 'client', '# Mongo Connection Options dict that is unpacked when passed to MongoClient', 'options', '=', 'self', '.', 'extras', '# If we are using SSL disable requiring certs from specific hostname', 'if', 'options', '.', 'get', '(', ""'ssl'"", ',', 'False', ')', ':', 'options', '.', 'update', '(', '{', ""'ssl_cert_reqs'"", ':', 'CERT_NONE', '}', ')', 'self', '.', 'client', '=', 'MongoClient', '(', 'self', '.', 'uri', ',', '*', '*', 'options', ')', 'return', 'self', '.', 'client']",Fetches PyMongo Client,"['Fetches', 'PyMongo', 'Client']",python,test,"['fetches', 'pymongo', 'client']",fetches pymongo client,"['def', 'get_conn', '(', 'self', ')', ':', 'if', 'self', '.', 'client', 'is', 'not', 'none', ':', 'return', 'self', '.', 'client', '# mongo connection options dict that is unpacked when passed to mongoclient', 'options', '=', 'self', '.', 'extras', '# if we are using ssl disable requiring certs from specific hostname', 'if', 'options', '.', 'get', '(', ""'ssl'"", ',', 'false', ')', ':', 'options', '.', 'update', '(', '{', ""'ssl_cert_reqs'"", ':', 'cert_none', '}', ')', 'self', '.', 'client', '=', 'mongoclient', '(', 'self', '.', 'uri', ',', '*', '*', 'options', ')', 'return', 'self', '.', 'client']","def get_conn ( self ) : if self . client is not none : return self . client # mongo connection options dict that is unpacked when passed to mongoclient options = self . extras # if we are using ssl disable requiring certs from specific hostname if options . get ( 'ssl' , false ) : options . update ( { 'ssl_cert_reqs' : cert_none } ) self . client = mongoclient ( self . uri , * * options ) return self . client"
442,apache/airflow,airflow/contrib/hooks/mongo_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/mongo_hook.py#L93-L102,"def get_collection(self, mongo_collection, mongo_db=None):
        """"""
        Fetches a mongo collection object for querying.

        Uses connection schema as DB unless specified.
        """"""
        mongo_db = mongo_db if mongo_db is not None else self.connection.schema
        mongo_conn = self.get_conn()

        return mongo_conn.get_database(mongo_db).get_collection(mongo_collection)","['def', 'get_collection', '(', 'self', ',', 'mongo_collection', ',', 'mongo_db', '=', 'None', ')', ':', 'mongo_db', '=', 'mongo_db', 'if', 'mongo_db', 'is', 'not', 'None', 'else', 'self', '.', 'connection', '.', 'schema', 'mongo_conn', '=', 'self', '.', 'get_conn', '(', ')', 'return', 'mongo_conn', '.', 'get_database', '(', 'mongo_db', ')', '.', 'get_collection', '(', 'mongo_collection', ')']","Fetches a mongo collection object for querying.

        Uses connection schema as DB unless specified.","['Fetches', 'a', 'mongo', 'collection', 'object', 'for', 'querying', '.']",python,test,"['fetches', 'a', 'mongo', 'collection', 'object', 'for', 'querying', '.']",fetches a mongo collection object for querying .,"['def', 'get_collection', '(', 'self', ',', 'mongo_collection', ',', 'mongo_db', '=', 'none', ')', ':', 'mongo_db', '=', 'mongo_db', 'if', 'mongo_db', 'is', 'not', 'none', 'else', 'self', '.', 'connection', '.', 'schema', 'mongo_conn', '=', 'self', '.', 'get_conn', '(', ')', 'return', 'mongo_conn', '.', 'get_database', '(', 'mongo_db', ')', '.', 'get_collection', '(', 'mongo_collection', ')']","def get_collection ( self , mongo_collection , mongo_db = none ) : mongo_db = mongo_db if mongo_db is not none else self . connection . schema mongo_conn = self . get_conn ( ) return mongo_conn . get_database ( mongo_db ) . get_collection ( mongo_collection )"
443,apache/airflow,airflow/contrib/hooks/mongo_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/mongo_hook.py#L104-L112,"def aggregate(self, mongo_collection, aggregate_query, mongo_db=None, **kwargs):
        """"""
        Runs an aggregation pipeline and returns the results
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.aggregate
        https://api.mongodb.com/python/current/examples/aggregation.html
        """"""
        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)

        return collection.aggregate(aggregate_query, **kwargs)","['def', 'aggregate', '(', 'self', ',', 'mongo_collection', ',', 'aggregate_query', ',', 'mongo_db', '=', 'None', ',', '*', '*', 'kwargs', ')', ':', 'collection', '=', 'self', '.', 'get_collection', '(', 'mongo_collection', ',', 'mongo_db', '=', 'mongo_db', ')', 'return', 'collection', '.', 'aggregate', '(', 'aggregate_query', ',', '*', '*', 'kwargs', ')']","Runs an aggregation pipeline and returns the results
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.aggregate
        https://api.mongodb.com/python/current/examples/aggregation.html","['Runs', 'an', 'aggregation', 'pipeline', 'and', 'returns', 'the', 'results', 'https', ':', '//', 'api', '.', 'mongodb', '.', 'com', '/', 'python', '/', 'current', '/', 'api', '/', 'pymongo', '/', 'collection', '.', 'html#pymongo', '.', 'collection', '.', 'Collection', '.', 'aggregate', 'https', ':', '//', 'api', '.', 'mongodb', '.', 'com', '/', 'python', '/', 'current', '/', 'examples', '/', 'aggregation', '.', 'html']",python,test,"['runs', 'an', 'aggregation', 'pipeline', 'and', 'returns', 'the', 'results', 'https', ':', '//', 'api', '.', 'mongodb', '.', 'com', '/', 'python', '/', 'current', '/', 'api', '/', 'pymongo', '/', 'collection', '.', 'html#pymongo', '.', 'collection', '.', 'collection', '.', 'aggregate', 'https', ':', '//', 'api', '.', 'mongodb', '.', 'com', '/', 'python', '/', 'current', '/', 'examples', '/', 'aggregation', '.', 'html']",runs an aggregation pipeline and returns the results https : // api . mongodb . com / python / current / api / pymongo / collection . html#pymongo . collection . collection . aggregate https : // api . mongodb . com / python / current / examples / aggregation . html,"['def', 'aggregate', '(', 'self', ',', 'mongo_collection', ',', 'aggregate_query', ',', 'mongo_db', '=', 'none', ',', '*', '*', 'kwargs', ')', ':', 'collection', '=', 'self', '.', 'get_collection', '(', 'mongo_collection', ',', 'mongo_db', '=', 'mongo_db', ')', 'return', 'collection', '.', 'aggregate', '(', 'aggregate_query', ',', '*', '*', 'kwargs', ')']","def aggregate ( self , mongo_collection , aggregate_query , mongo_db = none , * * kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) return collection . aggregate ( aggregate_query , * * kwargs )"
444,apache/airflow,airflow/contrib/hooks/mongo_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/mongo_hook.py#L114-L124,"def find(self, mongo_collection, query, find_one=False, mongo_db=None, **kwargs):
        """"""
        Runs a mongo find query and returns the results
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.find
        """"""
        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)

        if find_one:
            return collection.find_one(query, **kwargs)
        else:
            return collection.find(query, **kwargs)","['def', 'find', '(', 'self', ',', 'mongo_collection', ',', 'query', ',', 'find_one', '=', 'False', ',', 'mongo_db', '=', 'None', ',', '*', '*', 'kwargs', ')', ':', 'collection', '=', 'self', '.', 'get_collection', '(', 'mongo_collection', ',', 'mongo_db', '=', 'mongo_db', ')', 'if', 'find_one', ':', 'return', 'collection', '.', 'find_one', '(', 'query', ',', '*', '*', 'kwargs', ')', 'else', ':', 'return', 'collection', '.', 'find', '(', 'query', ',', '*', '*', 'kwargs', ')']","Runs a mongo find query and returns the results
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.find","['Runs', 'a', 'mongo', 'find', 'query', 'and', 'returns', 'the', 'results', 'https', ':', '//', 'api', '.', 'mongodb', '.', 'com', '/', 'python', '/', 'current', '/', 'api', '/', 'pymongo', '/', 'collection', '.', 'html#pymongo', '.', 'collection', '.', 'Collection', '.', 'find']",python,test,"['runs', 'a', 'mongo', 'find', 'query', 'and', 'returns', 'the', 'results', 'https', ':', '//', 'api', '.', 'mongodb', '.', 'com', '/', 'python', '/', 'current', '/', 'api', '/', 'pymongo', '/', 'collection', '.', 'html#pymongo', '.', 'collection', '.', 'collection', '.', 'find']",runs a mongo find query and returns the results https : // api . mongodb . com / python / current / api / pymongo / collection . html#pymongo . collection . collection . find,"['def', 'find', '(', 'self', ',', 'mongo_collection', ',', 'query', ',', 'find_one', '=', 'false', ',', 'mongo_db', '=', 'none', ',', '*', '*', 'kwargs', ')', ':', 'collection', '=', 'self', '.', 'get_collection', '(', 'mongo_collection', ',', 'mongo_db', '=', 'mongo_db', ')', 'if', 'find_one', ':', 'return', 'collection', '.', 'find_one', '(', 'query', ',', '*', '*', 'kwargs', ')', 'else', ':', 'return', 'collection', '.', 'find', '(', 'query', ',', '*', '*', 'kwargs', ')']","def find ( self , mongo_collection , query , find_one = false , mongo_db = none , * * kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) if find_one : return collection . find_one ( query , * * kwargs ) else : return collection . find ( query , * * kwargs )"
445,apache/airflow,airflow/contrib/hooks/mongo_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/mongo_hook.py#L126-L133,"def insert_one(self, mongo_collection, doc, mongo_db=None, **kwargs):
        """"""
        Inserts a single document into a mongo collection
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.insert_one
        """"""
        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)

        return collection.insert_one(doc, **kwargs)","['def', 'insert_one', '(', 'self', ',', 'mongo_collection', ',', 'doc', ',', 'mongo_db', '=', 'None', ',', '*', '*', 'kwargs', ')', ':', 'collection', '=', 'self', '.', 'get_collection', '(', 'mongo_collection', ',', 'mongo_db', '=', 'mongo_db', ')', 'return', 'collection', '.', 'insert_one', '(', 'doc', ',', '*', '*', 'kwargs', ')']","Inserts a single document into a mongo collection
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.insert_one","['Inserts', 'a', 'single', 'document', 'into', 'a', 'mongo', 'collection', 'https', ':', '//', 'api', '.', 'mongodb', '.', 'com', '/', 'python', '/', 'current', '/', 'api', '/', 'pymongo', '/', 'collection', '.', 'html#pymongo', '.', 'collection', '.', 'Collection', '.', 'insert_one']",python,test,"['inserts', 'a', 'single', 'document', 'into', 'a', 'mongo', 'collection', 'https', ':', '//', 'api', '.', 'mongodb', '.', 'com', '/', 'python', '/', 'current', '/', 'api', '/', 'pymongo', '/', 'collection', '.', 'html#pymongo', '.', 'collection', '.', 'collection', '.', 'insert_one']",inserts a single document into a mongo collection https : // api . mongodb . com / python / current / api / pymongo / collection . html#pymongo . collection . collection . insert_one,"['def', 'insert_one', '(', 'self', ',', 'mongo_collection', ',', 'doc', ',', 'mongo_db', '=', 'none', ',', '*', '*', 'kwargs', ')', ':', 'collection', '=', 'self', '.', 'get_collection', '(', 'mongo_collection', ',', 'mongo_db', '=', 'mongo_db', ')', 'return', 'collection', '.', 'insert_one', '(', 'doc', ',', '*', '*', 'kwargs', ')']","def insert_one ( self , mongo_collection , doc , mongo_db = none , * * kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) return collection . insert_one ( doc , * * kwargs )"
446,apache/airflow,airflow/contrib/hooks/mongo_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/mongo_hook.py#L135-L142,"def insert_many(self, mongo_collection, docs, mongo_db=None, **kwargs):
        """"""
        Inserts many docs into a mongo collection.
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.insert_many
        """"""
        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)

        return collection.insert_many(docs, **kwargs)","['def', 'insert_many', '(', 'self', ',', 'mongo_collection', ',', 'docs', ',', 'mongo_db', '=', 'None', ',', '*', '*', 'kwargs', ')', ':', 'collection', '=', 'self', '.', 'get_collection', '(', 'mongo_collection', ',', 'mongo_db', '=', 'mongo_db', ')', 'return', 'collection', '.', 'insert_many', '(', 'docs', ',', '*', '*', 'kwargs', ')']","Inserts many docs into a mongo collection.
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.insert_many","['Inserts', 'many', 'docs', 'into', 'a', 'mongo', 'collection', '.', 'https', ':', '//', 'api', '.', 'mongodb', '.', 'com', '/', 'python', '/', 'current', '/', 'api', '/', 'pymongo', '/', 'collection', '.', 'html#pymongo', '.', 'collection', '.', 'Collection', '.', 'insert_many']",python,test,"['inserts', 'many', 'docs', 'into', 'a', 'mongo', 'collection', '.', 'https', ':', '//', 'api', '.', 'mongodb', '.', 'com', '/', 'python', '/', 'current', '/', 'api', '/', 'pymongo', '/', 'collection', '.', 'html#pymongo', '.', 'collection', '.', 'collection', '.', 'insert_many']",inserts many docs into a mongo collection . https : // api . mongodb . com / python / current / api / pymongo / collection . html#pymongo . collection . collection . insert_many,"['def', 'insert_many', '(', 'self', ',', 'mongo_collection', ',', 'docs', ',', 'mongo_db', '=', 'none', ',', '*', '*', 'kwargs', ')', ':', 'collection', '=', 'self', '.', 'get_collection', '(', 'mongo_collection', ',', 'mongo_db', '=', 'mongo_db', ')', 'return', 'collection', '.', 'insert_many', '(', 'docs', ',', '*', '*', 'kwargs', ')']","def insert_many ( self , mongo_collection , docs , mongo_db = none , * * kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) return collection . insert_many ( docs , * * kwargs )"
447,apache/airflow,airflow/contrib/hooks/mongo_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/mongo_hook.py#L144-L163,"def update_one(self, mongo_collection, filter_doc, update_doc,
                   mongo_db=None, **kwargs):
        """"""
        Updates a single document in a mongo collection.
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.update_one

        :param mongo_collection: The name of the collection to update.
        :type mongo_collection: str
        :param filter_doc: A query that matches the documents to update.
        :type filter_doc: dict
        :param update_doc: The modifications to apply.
        :type update_doc: dict
        :param mongo_db: The name of the database to use.
            Can be omitted; then the database from the connection string is used.
        :type mongo_db: str

        """"""
        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)

        return collection.update_one(filter_doc, update_doc, **kwargs)","['def', 'update_one', '(', 'self', ',', 'mongo_collection', ',', 'filter_doc', ',', 'update_doc', ',', 'mongo_db', '=', 'None', ',', '*', '*', 'kwargs', ')', ':', 'collection', '=', 'self', '.', 'get_collection', '(', 'mongo_collection', ',', 'mongo_db', '=', 'mongo_db', ')', 'return', 'collection', '.', 'update_one', '(', 'filter_doc', ',', 'update_doc', ',', '*', '*', 'kwargs', ')']","Updates a single document in a mongo collection.
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.update_one

        :param mongo_collection: The name of the collection to update.
        :type mongo_collection: str
        :param filter_doc: A query that matches the documents to update.
        :type filter_doc: dict
        :param update_doc: The modifications to apply.
        :type update_doc: dict
        :param mongo_db: The name of the database to use.
            Can be omitted; then the database from the connection string is used.
        :type mongo_db: str","['Updates', 'a', 'single', 'document', 'in', 'a', 'mongo', 'collection', '.', 'https', ':', '//', 'api', '.', 'mongodb', '.', 'com', '/', 'python', '/', 'current', '/', 'api', '/', 'pymongo', '/', 'collection', '.', 'html#pymongo', '.', 'collection', '.', 'Collection', '.', 'update_one']",python,test,"['updates', 'a', 'single', 'document', 'in', 'a', 'mongo', 'collection', '.', 'https', ':', '//', 'api', '.', 'mongodb', '.', 'com', '/', 'python', '/', 'current', '/', 'api', '/', 'pymongo', '/', 'collection', '.', 'html#pymongo', '.', 'collection', '.', 'collection', '.', 'update_one']",updates a single document in a mongo collection . https : // api . mongodb . com / python / current / api / pymongo / collection . html#pymongo . collection . collection . update_one,"['def', 'update_one', '(', 'self', ',', 'mongo_collection', ',', 'filter_doc', ',', 'update_doc', ',', 'mongo_db', '=', 'none', ',', '*', '*', 'kwargs', ')', ':', 'collection', '=', 'self', '.', 'get_collection', '(', 'mongo_collection', ',', 'mongo_db', '=', 'mongo_db', ')', 'return', 'collection', '.', 'update_one', '(', 'filter_doc', ',', 'update_doc', ',', '*', '*', 'kwargs', ')']","def update_one ( self , mongo_collection , filter_doc , update_doc , mongo_db = none , * * kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) return collection . update_one ( filter_doc , update_doc , * * kwargs )"
448,apache/airflow,airflow/contrib/hooks/mongo_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/mongo_hook.py#L186-L212,"def replace_one(self, mongo_collection, doc, filter_doc=None,
                    mongo_db=None, **kwargs):
        """"""
        Replaces a single document in a mongo collection.
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.replace_one

        .. note::
            If no ``filter_doc`` is given, it is assumed that the replacement
            document contain the ``_id`` field which is then used as filters.

        :param mongo_collection: The name of the collection to update.
        :type mongo_collection: str
        :param doc: The new document.
        :type doc: dict
        :param filter_doc: A query that matches the documents to replace.
            Can be omitted; then the _id field from doc will be used.
        :type filter_doc: dict
        :param mongo_db: The name of the database to use.
            Can be omitted; then the database from the connection string is used.
        :type mongo_db: str
        """"""
        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)

        if not filter_doc:
            filter_doc = {'_id': doc['_id']}

        return collection.replace_one(filter_doc, doc, **kwargs)","['def', 'replace_one', '(', 'self', ',', 'mongo_collection', ',', 'doc', ',', 'filter_doc', '=', 'None', ',', 'mongo_db', '=', 'None', ',', '*', '*', 'kwargs', ')', ':', 'collection', '=', 'self', '.', 'get_collection', '(', 'mongo_collection', ',', 'mongo_db', '=', 'mongo_db', ')', 'if', 'not', 'filter_doc', ':', 'filter_doc', '=', '{', ""'_id'"", ':', 'doc', '[', ""'_id'"", ']', '}', 'return', 'collection', '.', 'replace_one', '(', 'filter_doc', ',', 'doc', ',', '*', '*', 'kwargs', ')']","Replaces a single document in a mongo collection.
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.replace_one

        .. note::
            If no ``filter_doc`` is given, it is assumed that the replacement
            document contain the ``_id`` field which is then used as filters.

        :param mongo_collection: The name of the collection to update.
        :type mongo_collection: str
        :param doc: The new document.
        :type doc: dict
        :param filter_doc: A query that matches the documents to replace.
            Can be omitted; then the _id field from doc will be used.
        :type filter_doc: dict
        :param mongo_db: The name of the database to use.
            Can be omitted; then the database from the connection string is used.
        :type mongo_db: str","['Replaces', 'a', 'single', 'document', 'in', 'a', 'mongo', 'collection', '.', 'https', ':', '//', 'api', '.', 'mongodb', '.', 'com', '/', 'python', '/', 'current', '/', 'api', '/', 'pymongo', '/', 'collection', '.', 'html#pymongo', '.', 'collection', '.', 'Collection', '.', 'replace_one']",python,test,"['replaces', 'a', 'single', 'document', 'in', 'a', 'mongo', 'collection', '.', 'https', ':', '//', 'api', '.', 'mongodb', '.', 'com', '/', 'python', '/', 'current', '/', 'api', '/', 'pymongo', '/', 'collection', '.', 'html#pymongo', '.', 'collection', '.', 'collection', '.', 'replace_one']",replaces a single document in a mongo collection . https : // api . mongodb . com / python / current / api / pymongo / collection . html#pymongo . collection . collection . replace_one,"['def', 'replace_one', '(', 'self', ',', 'mongo_collection', ',', 'doc', ',', 'filter_doc', '=', 'none', ',', 'mongo_db', '=', 'none', ',', '*', '*', 'kwargs', ')', ':', 'collection', '=', 'self', '.', 'get_collection', '(', 'mongo_collection', ',', 'mongo_db', '=', 'mongo_db', ')', 'if', 'not', 'filter_doc', ':', 'filter_doc', '=', '{', ""'_id'"", ':', 'doc', '[', ""'_id'"", ']', '}', 'return', 'collection', '.', 'replace_one', '(', 'filter_doc', ',', 'doc', ',', '*', '*', 'kwargs', ')']","def replace_one ( self , mongo_collection , doc , filter_doc = none , mongo_db = none , * * kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) if not filter_doc : filter_doc = { '_id' : doc [ '_id' ] } return collection . replace_one ( filter_doc , doc , * * kwargs )"
449,apache/airflow,airflow/contrib/hooks/mongo_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/mongo_hook.py#L214-L261,"def replace_many(self, mongo_collection, docs,
                     filter_docs=None, mongo_db=None, upsert=False, collation=None,
                     **kwargs):
        """"""
        Replaces many documents in a mongo collection.

        Uses bulk_write with multiple ReplaceOne operations
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.bulk_write

        .. note::
            If no ``filter_docs``are given, it is assumed that all
            replacement documents contain the ``_id`` field which are then
            used as filters.

        :param mongo_collection: The name of the collection to update.
        :type mongo_collection: str
        :param docs: The new documents.
        :type docs: list[dict]
        :param filter_docs: A list of queries that match the documents to replace.
            Can be omitted; then the _id fields from docs will be used.
        :type filter_docs: list[dict]
        :param mongo_db: The name of the database to use.
            Can be omitted; then the database from the connection string is used.
        :type mongo_db: str
        :param upsert: If ``True``, perform an insert if no documents
            match the filters for the replace operation.
        :type upsert: bool
        :param collation: An instance of
            :class:`~pymongo.collation.Collation`. This option is only
            supported on MongoDB 3.4 and above.
        :type collation: pymongo.collation.Collation

        """"""
        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)

        if not filter_docs:
            filter_docs = [{'_id': doc['_id']} for doc in docs]

        requests = [
            ReplaceOne(
                filter_docs[i],
                docs[i],
                upsert=upsert,
                collation=collation)
            for i in range(len(docs))
        ]

        return collection.bulk_write(requests, **kwargs)","['def', 'replace_many', '(', 'self', ',', 'mongo_collection', ',', 'docs', ',', 'filter_docs', '=', 'None', ',', 'mongo_db', '=', 'None', ',', 'upsert', '=', 'False', ',', 'collation', '=', 'None', ',', '*', '*', 'kwargs', ')', ':', 'collection', '=', 'self', '.', 'get_collection', '(', 'mongo_collection', ',', 'mongo_db', '=', 'mongo_db', ')', 'if', 'not', 'filter_docs', ':', 'filter_docs', '=', '[', '{', ""'_id'"", ':', 'doc', '[', ""'_id'"", ']', '}', 'for', 'doc', 'in', 'docs', ']', 'requests', '=', '[', 'ReplaceOne', '(', 'filter_docs', '[', 'i', ']', ',', 'docs', '[', 'i', ']', ',', 'upsert', '=', 'upsert', ',', 'collation', '=', 'collation', ')', 'for', 'i', 'in', 'range', '(', 'len', '(', 'docs', ')', ')', ']', 'return', 'collection', '.', 'bulk_write', '(', 'requests', ',', '*', '*', 'kwargs', ')']","Replaces many documents in a mongo collection.

        Uses bulk_write with multiple ReplaceOne operations
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.bulk_write

        .. note::
            If no ``filter_docs``are given, it is assumed that all
            replacement documents contain the ``_id`` field which are then
            used as filters.

        :param mongo_collection: The name of the collection to update.
        :type mongo_collection: str
        :param docs: The new documents.
        :type docs: list[dict]
        :param filter_docs: A list of queries that match the documents to replace.
            Can be omitted; then the _id fields from docs will be used.
        :type filter_docs: list[dict]
        :param mongo_db: The name of the database to use.
            Can be omitted; then the database from the connection string is used.
        :type mongo_db: str
        :param upsert: If ``True``, perform an insert if no documents
            match the filters for the replace operation.
        :type upsert: bool
        :param collation: An instance of
            :class:`~pymongo.collation.Collation`. This option is only
            supported on MongoDB 3.4 and above.
        :type collation: pymongo.collation.Collation","['Replaces', 'many', 'documents', 'in', 'a', 'mongo', 'collection', '.']",python,test,"['replaces', 'many', 'documents', 'in', 'a', 'mongo', 'collection', '.']",replaces many documents in a mongo collection .,"['def', 'replace_many', '(', 'self', ',', 'mongo_collection', ',', 'docs', ',', 'filter_docs', '=', 'none', ',', 'mongo_db', '=', 'none', ',', 'upsert', '=', 'false', ',', 'collation', '=', 'none', ',', '*', '*', 'kwargs', ')', ':', 'collection', '=', 'self', '.', 'get_collection', '(', 'mongo_collection', ',', 'mongo_db', '=', 'mongo_db', ')', 'if', 'not', 'filter_docs', ':', 'filter_docs', '=', '[', '{', ""'_id'"", ':', 'doc', '[', ""'_id'"", ']', '}', 'for', 'doc', 'in', 'docs', ']', 'requests', '=', '[', 'replaceone', '(', 'filter_docs', '[', 'i', ']', ',', 'docs', '[', 'i', ']', ',', 'upsert', '=', 'upsert', ',', 'collation', '=', 'collation', ')', 'for', 'i', 'in', 'range', '(', 'len', '(', 'docs', ')', ')', ']', 'return', 'collection', '.', 'bulk_write', '(', 'requests', ',', '*', '*', 'kwargs', ')']","def replace_many ( self , mongo_collection , docs , filter_docs = none , mongo_db = none , upsert = false , collation = none , * * kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) if not filter_docs : filter_docs = [ { '_id' : doc [ '_id' ] } for doc in docs ] requests = [ replaceone ( filter_docs [ i ] , docs [ i ] , upsert = upsert , collation = collation ) for i in range ( len ( docs ) ) ] return collection . bulk_write ( requests , * * kwargs )"
450,apache/airflow,airflow/contrib/hooks/mongo_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/mongo_hook.py#L263-L279,"def delete_one(self, mongo_collection, filter_doc, mongo_db=None, **kwargs):
        """"""
        Deletes a single document in a mongo collection.
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.delete_one

        :param mongo_collection: The name of the collection to delete from.
        :type mongo_collection: str
        :param filter_doc: A query that matches the document to delete.
        :type filter_doc: dict
        :param mongo_db: The name of the database to use.
            Can be omitted; then the database from the connection string is used.
        :type mongo_db: str

        """"""
        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)

        return collection.delete_one(filter_doc, **kwargs)","['def', 'delete_one', '(', 'self', ',', 'mongo_collection', ',', 'filter_doc', ',', 'mongo_db', '=', 'None', ',', '*', '*', 'kwargs', ')', ':', 'collection', '=', 'self', '.', 'get_collection', '(', 'mongo_collection', ',', 'mongo_db', '=', 'mongo_db', ')', 'return', 'collection', '.', 'delete_one', '(', 'filter_doc', ',', '*', '*', 'kwargs', ')']","Deletes a single document in a mongo collection.
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.delete_one

        :param mongo_collection: The name of the collection to delete from.
        :type mongo_collection: str
        :param filter_doc: A query that matches the document to delete.
        :type filter_doc: dict
        :param mongo_db: The name of the database to use.
            Can be omitted; then the database from the connection string is used.
        :type mongo_db: str","['Deletes', 'a', 'single', 'document', 'in', 'a', 'mongo', 'collection', '.', 'https', ':', '//', 'api', '.', 'mongodb', '.', 'com', '/', 'python', '/', 'current', '/', 'api', '/', 'pymongo', '/', 'collection', '.', 'html#pymongo', '.', 'collection', '.', 'Collection', '.', 'delete_one']",python,test,"['deletes', 'a', 'single', 'document', 'in', 'a', 'mongo', 'collection', '.', 'https', ':', '//', 'api', '.', 'mongodb', '.', 'com', '/', 'python', '/', 'current', '/', 'api', '/', 'pymongo', '/', 'collection', '.', 'html#pymongo', '.', 'collection', '.', 'collection', '.', 'delete_one']",deletes a single document in a mongo collection . https : // api . mongodb . com / python / current / api / pymongo / collection . html#pymongo . collection . collection . delete_one,"['def', 'delete_one', '(', 'self', ',', 'mongo_collection', ',', 'filter_doc', ',', 'mongo_db', '=', 'none', ',', '*', '*', 'kwargs', ')', ':', 'collection', '=', 'self', '.', 'get_collection', '(', 'mongo_collection', ',', 'mongo_db', '=', 'mongo_db', ')', 'return', 'collection', '.', 'delete_one', '(', 'filter_doc', ',', '*', '*', 'kwargs', ')']","def delete_one ( self , mongo_collection , filter_doc , mongo_db = none , * * kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) return collection . delete_one ( filter_doc , * * kwargs )"
451,apache/airflow,airflow/contrib/hooks/mongo_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/mongo_hook.py#L281-L297,"def delete_many(self, mongo_collection, filter_doc, mongo_db=None, **kwargs):
        """"""
        Deletes one or more documents in a mongo collection.
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.delete_many

        :param mongo_collection: The name of the collection to delete from.
        :type mongo_collection: str
        :param filter_doc: A query that matches the documents to delete.
        :type filter_doc: dict
        :param mongo_db: The name of the database to use.
            Can be omitted; then the database from the connection string is used.
        :type mongo_db: str

        """"""
        collection = self.get_collection(mongo_collection, mongo_db=mongo_db)

        return collection.delete_many(filter_doc, **kwargs)","['def', 'delete_many', '(', 'self', ',', 'mongo_collection', ',', 'filter_doc', ',', 'mongo_db', '=', 'None', ',', '*', '*', 'kwargs', ')', ':', 'collection', '=', 'self', '.', 'get_collection', '(', 'mongo_collection', ',', 'mongo_db', '=', 'mongo_db', ')', 'return', 'collection', '.', 'delete_many', '(', 'filter_doc', ',', '*', '*', 'kwargs', ')']","Deletes one or more documents in a mongo collection.
        https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.delete_many

        :param mongo_collection: The name of the collection to delete from.
        :type mongo_collection: str
        :param filter_doc: A query that matches the documents to delete.
        :type filter_doc: dict
        :param mongo_db: The name of the database to use.
            Can be omitted; then the database from the connection string is used.
        :type mongo_db: str","['Deletes', 'one', 'or', 'more', 'documents', 'in', 'a', 'mongo', 'collection', '.', 'https', ':', '//', 'api', '.', 'mongodb', '.', 'com', '/', 'python', '/', 'current', '/', 'api', '/', 'pymongo', '/', 'collection', '.', 'html#pymongo', '.', 'collection', '.', 'Collection', '.', 'delete_many']",python,test,"['deletes', 'one', 'or', 'more', 'documents', 'in', 'a', 'mongo', 'collection', '.', 'https', ':', '//', 'api', '.', 'mongodb', '.', 'com', '/', 'python', '/', 'current', '/', 'api', '/', 'pymongo', '/', 'collection', '.', 'html#pymongo', '.', 'collection', '.', 'collection', '.', 'delete_many']",deletes one or more documents in a mongo collection . https : // api . mongodb . com / python / current / api / pymongo / collection . html#pymongo . collection . collection . delete_many,"['def', 'delete_many', '(', 'self', ',', 'mongo_collection', ',', 'filter_doc', ',', 'mongo_db', '=', 'none', ',', '*', '*', 'kwargs', ')', ':', 'collection', '=', 'self', '.', 'get_collection', '(', 'mongo_collection', ',', 'mongo_db', '=', 'mongo_db', ')', 'return', 'collection', '.', 'delete_many', '(', 'filter_doc', ',', '*', '*', 'kwargs', ')']","def delete_many ( self , mongo_collection , filter_doc , mongo_db = none , * * kwargs ) : collection = self . get_collection ( mongo_collection , mongo_db = mongo_db ) return collection . delete_many ( filter_doc , * * kwargs )"
452,apache/airflow,airflow/contrib/hooks/imap_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/imap_hook.py#L49-L66,"def has_mail_attachment(self, name, mail_folder='INBOX', check_regex=False):
        """"""
        Checks the mail folder for mails containing attachments with the given name.

        :param name: The name of the attachment that will be searched for.
        :type name: str
        :param mail_folder: The mail folder where to look at.
        :type mail_folder: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :returns: True if there is an attachment with the given name and False if not.
        :rtype: bool
        """"""
        mail_attachments = self._retrieve_mails_attachments_by_name(name,
                                                                    mail_folder,
                                                                    check_regex,
                                                                    latest_only=True)
        return len(mail_attachments) > 0","['def', 'has_mail_attachment', '(', 'self', ',', 'name', ',', 'mail_folder', '=', ""'INBOX'"", ',', 'check_regex', '=', 'False', ')', ':', 'mail_attachments', '=', 'self', '.', '_retrieve_mails_attachments_by_name', '(', 'name', ',', 'mail_folder', ',', 'check_regex', ',', 'latest_only', '=', 'True', ')', 'return', 'len', '(', 'mail_attachments', ')', '>', '0']","Checks the mail folder for mails containing attachments with the given name.

        :param name: The name of the attachment that will be searched for.
        :type name: str
        :param mail_folder: The mail folder where to look at.
        :type mail_folder: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :returns: True if there is an attachment with the given name and False if not.
        :rtype: bool","['Checks', 'the', 'mail', 'folder', 'for', 'mails', 'containing', 'attachments', 'with', 'the', 'given', 'name', '.']",python,test,"['checks', 'the', 'mail', 'folder', 'for', 'mails', 'containing', 'attachments', 'with', 'the', 'given', 'name', '.']",checks the mail folder for mails containing attachments with the given name .,"['def', 'has_mail_attachment', '(', 'self', ',', 'name', ',', 'mail_folder', '=', ""'inbox'"", ',', 'check_regex', '=', 'false', ')', ':', 'mail_attachments', '=', 'self', '.', '_retrieve_mails_attachments_by_name', '(', 'name', ',', 'mail_folder', ',', 'check_regex', ',', 'latest_only', '=', 'true', ')', 'return', 'len', '(', 'mail_attachments', ')', '>', '0']","def has_mail_attachment ( self , name , mail_folder = 'inbox' , check_regex = false ) : mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only = true ) return len ( mail_attachments ) > 0"
453,apache/airflow,airflow/contrib/hooks/imap_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/imap_hook.py#L68-L102,"def retrieve_mail_attachments(self,
                                  name,
                                  mail_folder='INBOX',
                                  check_regex=False,
                                  latest_only=False,
                                  not_found_mode='raise'):
        """"""
        Retrieves mail's attachments in the mail folder by its name.

        :param name: The name of the attachment that will be downloaded.
        :type name: str
        :param mail_folder: The mail folder where to look at.
        :type mail_folder: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :param latest_only: If set to True it will only retrieve
                            the first matched attachment.
        :type latest_only: bool
        :param not_found_mode: Specify what should happen if no attachment has been found.
                               Supported values are 'raise', 'warn' and 'ignore'.
                               If it is set to 'raise' it will raise an exception,
                               if set to 'warn' it will only print a warning and
                               if set to 'ignore' it won't notify you at all.
        :type not_found_mode: str
        :returns: a list of tuple each containing the attachment filename and its payload.
        :rtype: a list of tuple
        """"""
        mail_attachments = self._retrieve_mails_attachments_by_name(name,
                                                                    mail_folder,
                                                                    check_regex,
                                                                    latest_only)
        if not mail_attachments:
            self._handle_not_found_mode(not_found_mode)

        return mail_attachments","['def', 'retrieve_mail_attachments', '(', 'self', ',', 'name', ',', 'mail_folder', '=', ""'INBOX'"", ',', 'check_regex', '=', 'False', ',', 'latest_only', '=', 'False', ',', 'not_found_mode', '=', ""'raise'"", ')', ':', 'mail_attachments', '=', 'self', '.', '_retrieve_mails_attachments_by_name', '(', 'name', ',', 'mail_folder', ',', 'check_regex', ',', 'latest_only', ')', 'if', 'not', 'mail_attachments', ':', 'self', '.', '_handle_not_found_mode', '(', 'not_found_mode', ')', 'return', 'mail_attachments']","Retrieves mail's attachments in the mail folder by its name.

        :param name: The name of the attachment that will be downloaded.
        :type name: str
        :param mail_folder: The mail folder where to look at.
        :type mail_folder: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :param latest_only: If set to True it will only retrieve
                            the first matched attachment.
        :type latest_only: bool
        :param not_found_mode: Specify what should happen if no attachment has been found.
                               Supported values are 'raise', 'warn' and 'ignore'.
                               If it is set to 'raise' it will raise an exception,
                               if set to 'warn' it will only print a warning and
                               if set to 'ignore' it won't notify you at all.
        :type not_found_mode: str
        :returns: a list of tuple each containing the attachment filename and its payload.
        :rtype: a list of tuple","['Retrieves', 'mail', 's', 'attachments', 'in', 'the', 'mail', 'folder', 'by', 'its', 'name', '.']",python,test,"['retrieves', 'mail', 's', 'attachments', 'in', 'the', 'mail', 'folder', 'by', 'its', 'name', '.']",retrieves mail s attachments in the mail folder by its name .,"['def', 'retrieve_mail_attachments', '(', 'self', ',', 'name', ',', 'mail_folder', '=', ""'inbox'"", ',', 'check_regex', '=', 'false', ',', 'latest_only', '=', 'false', ',', 'not_found_mode', '=', ""'raise'"", ')', ':', 'mail_attachments', '=', 'self', '.', '_retrieve_mails_attachments_by_name', '(', 'name', ',', 'mail_folder', ',', 'check_regex', ',', 'latest_only', ')', 'if', 'not', 'mail_attachments', ':', 'self', '.', '_handle_not_found_mode', '(', 'not_found_mode', ')', 'return', 'mail_attachments']","def retrieve_mail_attachments ( self , name , mail_folder = 'inbox' , check_regex = false , latest_only = false , not_found_mode = 'raise' ) : mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only ) if not mail_attachments : self . _handle_not_found_mode ( not_found_mode ) return mail_attachments"
454,apache/airflow,airflow/contrib/hooks/imap_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/imap_hook.py#L104-L141,"def download_mail_attachments(self,
                                  name,
                                  local_output_directory,
                                  mail_folder='INBOX',
                                  check_regex=False,
                                  latest_only=False,
                                  not_found_mode='raise'):
        """"""
        Downloads mail's attachments in the mail folder by its name to the local directory.

        :param name: The name of the attachment that will be downloaded.
        :type name: str
        :param local_output_directory: The output directory on the local machine
                                       where the files will be downloaded to.
        :type local_output_directory: str
        :param mail_folder: The mail folder where to look at.
        :type mail_folder: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :param latest_only: If set to True it will only download
                            the first matched attachment.
        :type latest_only: bool
        :param not_found_mode: Specify what should happen if no attachment has been found.
                               Supported values are 'raise', 'warn' and 'ignore'.
                               If it is set to 'raise' it will raise an exception,
                               if set to 'warn' it will only print a warning and
                               if set to 'ignore' it won't notify you at all.
        :type not_found_mode: str
        """"""
        mail_attachments = self._retrieve_mails_attachments_by_name(name,
                                                                    mail_folder,
                                                                    check_regex,
                                                                    latest_only)

        if not mail_attachments:
            self._handle_not_found_mode(not_found_mode)

        self._create_files(mail_attachments, local_output_directory)","['def', 'download_mail_attachments', '(', 'self', ',', 'name', ',', 'local_output_directory', ',', 'mail_folder', '=', ""'INBOX'"", ',', 'check_regex', '=', 'False', ',', 'latest_only', '=', 'False', ',', 'not_found_mode', '=', ""'raise'"", ')', ':', 'mail_attachments', '=', 'self', '.', '_retrieve_mails_attachments_by_name', '(', 'name', ',', 'mail_folder', ',', 'check_regex', ',', 'latest_only', ')', 'if', 'not', 'mail_attachments', ':', 'self', '.', '_handle_not_found_mode', '(', 'not_found_mode', ')', 'self', '.', '_create_files', '(', 'mail_attachments', ',', 'local_output_directory', ')']","Downloads mail's attachments in the mail folder by its name to the local directory.

        :param name: The name of the attachment that will be downloaded.
        :type name: str
        :param local_output_directory: The output directory on the local machine
                                       where the files will be downloaded to.
        :type local_output_directory: str
        :param mail_folder: The mail folder where to look at.
        :type mail_folder: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :param latest_only: If set to True it will only download
                            the first matched attachment.
        :type latest_only: bool
        :param not_found_mode: Specify what should happen if no attachment has been found.
                               Supported values are 'raise', 'warn' and 'ignore'.
                               If it is set to 'raise' it will raise an exception,
                               if set to 'warn' it will only print a warning and
                               if set to 'ignore' it won't notify you at all.
        :type not_found_mode: str","['Downloads', 'mail', 's', 'attachments', 'in', 'the', 'mail', 'folder', 'by', 'its', 'name', 'to', 'the', 'local', 'directory', '.']",python,test,"['downloads', 'mail', 's', 'attachments', 'in', 'the', 'mail', 'folder', 'by', 'its', 'name', 'to', 'the', 'local', 'directory', '.']",downloads mail s attachments in the mail folder by its name to the local directory .,"['def', 'download_mail_attachments', '(', 'self', ',', 'name', ',', 'local_output_directory', ',', 'mail_folder', '=', ""'inbox'"", ',', 'check_regex', '=', 'false', ',', 'latest_only', '=', 'false', ',', 'not_found_mode', '=', ""'raise'"", ')', ':', 'mail_attachments', '=', 'self', '.', '_retrieve_mails_attachments_by_name', '(', 'name', ',', 'mail_folder', ',', 'check_regex', ',', 'latest_only', ')', 'if', 'not', 'mail_attachments', ':', 'self', '.', '_handle_not_found_mode', '(', 'not_found_mode', ')', 'self', '.', '_create_files', '(', 'mail_attachments', ',', 'local_output_directory', ')']","def download_mail_attachments ( self , name , local_output_directory , mail_folder = 'inbox' , check_regex = false , latest_only = false , not_found_mode = 'raise' ) : mail_attachments = self . _retrieve_mails_attachments_by_name ( name , mail_folder , check_regex , latest_only ) if not mail_attachments : self . _handle_not_found_mode ( not_found_mode ) self . _create_files ( mail_attachments , local_output_directory )"
455,apache/airflow,airflow/contrib/hooks/imap_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/imap_hook.py#L236-L264,"def get_attachments_by_name(self, name, check_regex, find_first=False):
        """"""
        Gets all attachments by name for the mail.

        :param name: The name of the attachment to look for.
        :type name: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :param find_first: If set to True it will only find the first match and then quit.
        :type find_first: bool
        :returns: a list of tuples each containing name and payload
                  where the attachments name matches the given name.
        :rtype: list of tuple
        """"""
        attachments = []

        for part in self.mail.walk():
            mail_part = MailPart(part)
            if mail_part.is_attachment():
                found_attachment = mail_part.has_matching_name(name) if check_regex \
                    else mail_part.has_equal_name(name)
                if found_attachment:
                    file_name, file_payload = mail_part.get_file()
                    self.log.info('Found attachment: {}'.format(file_name))
                    attachments.append((file_name, file_payload))
                    if find_first:
                        break

        return attachments","['def', 'get_attachments_by_name', '(', 'self', ',', 'name', ',', 'check_regex', ',', 'find_first', '=', 'False', ')', ':', 'attachments', '=', '[', ']', 'for', 'part', 'in', 'self', '.', 'mail', '.', 'walk', '(', ')', ':', 'mail_part', '=', 'MailPart', '(', 'part', ')', 'if', 'mail_part', '.', 'is_attachment', '(', ')', ':', 'found_attachment', '=', 'mail_part', '.', 'has_matching_name', '(', 'name', ')', 'if', 'check_regex', 'else', 'mail_part', '.', 'has_equal_name', '(', 'name', ')', 'if', 'found_attachment', ':', 'file_name', ',', 'file_payload', '=', 'mail_part', '.', 'get_file', '(', ')', 'self', '.', 'log', '.', 'info', '(', ""'Found attachment: {}'"", '.', 'format', '(', 'file_name', ')', ')', 'attachments', '.', 'append', '(', '(', 'file_name', ',', 'file_payload', ')', ')', 'if', 'find_first', ':', 'break', 'return', 'attachments']","Gets all attachments by name for the mail.

        :param name: The name of the attachment to look for.
        :type name: str
        :param check_regex: Checks the name for a regular expression.
        :type check_regex: bool
        :param find_first: If set to True it will only find the first match and then quit.
        :type find_first: bool
        :returns: a list of tuples each containing name and payload
                  where the attachments name matches the given name.
        :rtype: list of tuple","['Gets', 'all', 'attachments', 'by', 'name', 'for', 'the', 'mail', '.']",python,test,"['gets', 'all', 'attachments', 'by', 'name', 'for', 'the', 'mail', '.']",gets all attachments by name for the mail .,"['def', 'get_attachments_by_name', '(', 'self', ',', 'name', ',', 'check_regex', ',', 'find_first', '=', 'false', ')', ':', 'attachments', '=', '[', ']', 'for', 'part', 'in', 'self', '.', 'mail', '.', 'walk', '(', ')', ':', 'mail_part', '=', 'mailpart', '(', 'part', ')', 'if', 'mail_part', '.', 'is_attachment', '(', ')', ':', 'found_attachment', '=', 'mail_part', '.', 'has_matching_name', '(', 'name', ')', 'if', 'check_regex', 'else', 'mail_part', '.', 'has_equal_name', '(', 'name', ')', 'if', 'found_attachment', ':', 'file_name', ',', 'file_payload', '=', 'mail_part', '.', 'get_file', '(', ')', 'self', '.', 'log', '.', 'info', '(', ""'found attachment: {}'"", '.', 'format', '(', 'file_name', ')', ')', 'attachments', '.', 'append', '(', '(', 'file_name', ',', 'file_payload', ')', ')', 'if', 'find_first', ':', 'break', 'return', 'attachments']","def get_attachments_by_name ( self , name , check_regex , find_first = false ) : attachments = [ ] for part in self . mail . walk ( ) : mail_part = mailpart ( part ) if mail_part . is_attachment ( ) : found_attachment = mail_part . has_matching_name ( name ) if check_regex else mail_part . has_equal_name ( name ) if found_attachment : file_name , file_payload = mail_part . get_file ( ) self . log . info ( 'found attachment: {}' . format ( file_name ) ) attachments . append ( ( file_name , file_payload ) ) if find_first : break return attachments"
456,apache/airflow,airflow/contrib/hooks/imap_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/imap_hook.py#L309-L316,"def get_file(self):
        """"""
        Gets the file including name and payload.

        :returns: the part's name and payload.
        :rtype: tuple
        """"""
        return self.part.get_filename(), self.part.get_payload(decode=True)","['def', 'get_file', '(', 'self', ')', ':', 'return', 'self', '.', 'part', '.', 'get_filename', '(', ')', ',', 'self', '.', 'part', '.', 'get_payload', '(', 'decode', '=', 'True', ')']","Gets the file including name and payload.

        :returns: the part's name and payload.
        :rtype: tuple","['Gets', 'the', 'file', 'including', 'name', 'and', 'payload', '.']",python,test,"['gets', 'the', 'file', 'including', 'name', 'and', 'payload', '.']",gets the file including name and payload .,"['def', 'get_file', '(', 'self', ')', ':', 'return', 'self', '.', 'part', '.', 'get_filename', '(', ')', ',', 'self', '.', 'part', '.', 'get_payload', '(', 'decode', '=', 'true', ')']","def get_file ( self ) : return self . part . get_filename ( ) , self . part . get_payload ( decode = true )"
457,apache/airflow,airflow/contrib/hooks/aws_firehose_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/aws_firehose_hook.py#L44-L56,"def put_records(self, records):
        """"""
        Write batch records to Kinesis Firehose
        """"""

        firehose_conn = self.get_conn()

        response = firehose_conn.put_record_batch(
            DeliveryStreamName=self.delivery_stream,
            Records=records
        )

        return response","['def', 'put_records', '(', 'self', ',', 'records', ')', ':', 'firehose_conn', '=', 'self', '.', 'get_conn', '(', ')', 'response', '=', 'firehose_conn', '.', 'put_record_batch', '(', 'DeliveryStreamName', '=', 'self', '.', 'delivery_stream', ',', 'Records', '=', 'records', ')', 'return', 'response']",Write batch records to Kinesis Firehose,"['Write', 'batch', 'records', 'to', 'Kinesis', 'Firehose']",python,test,"['write', 'batch', 'records', 'to', 'kinesis', 'firehose']",write batch records to kinesis firehose,"['def', 'put_records', '(', 'self', ',', 'records', ')', ':', 'firehose_conn', '=', 'self', '.', 'get_conn', '(', ')', 'response', '=', 'firehose_conn', '.', 'put_record_batch', '(', 'deliverystreamname', '=', 'self', '.', 'delivery_stream', ',', 'records', '=', 'records', ')', 'return', 'response']","def put_records ( self , records ) : firehose_conn = self . get_conn ( ) response = firehose_conn . put_record_batch ( deliverystreamname = self . delivery_stream , records = records ) return response"
458,apache/airflow,airflow/ti_deps/deps/ready_to_reschedule.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/ti_deps/deps/ready_to_reschedule.py#L34-L69,"def _get_dep_statuses(self, ti, session, dep_context):
        """"""
        Determines whether a task is ready to be rescheduled. Only tasks in
        NONE state with at least one row in task_reschedule table are
        handled by this dependency class, otherwise this dependency is
        considered as passed. This dependency fails if the latest reschedule
        request's reschedule date is still in future.
        """"""
        if dep_context.ignore_in_reschedule_period:
            yield self._passing_status(
                reason=""The context specified that being in a reschedule period was ""
                       ""permitted."")
            return

        if ti.state not in self.RESCHEDULEABLE_STATES:
            yield self._passing_status(
                reason=""The task instance is not in State_UP_FOR_RESCHEDULE or NONE state."")
            return

        task_reschedules = TaskReschedule.find_for_task_instance(task_instance=ti)
        if not task_reschedules:
            yield self._passing_status(
                reason=""There is no reschedule request for this task instance."")
            return

        now = timezone.utcnow()
        next_reschedule_date = task_reschedules[-1].reschedule_date
        if now >= next_reschedule_date:
            yield self._passing_status(
                reason=""Task instance id ready for reschedule."")
            return

        yield self._failing_status(
            reason=""Task is not ready for reschedule yet but will be rescheduled ""
                   ""automatically. Current date is {0} and task will be rescheduled ""
                   ""at {1}."".format(now.isoformat(), next_reschedule_date.isoformat()))","['def', '_get_dep_statuses', '(', 'self', ',', 'ti', ',', 'session', ',', 'dep_context', ')', ':', 'if', 'dep_context', '.', 'ignore_in_reschedule_period', ':', 'yield', 'self', '.', '_passing_status', '(', 'reason', '=', '""The context specified that being in a reschedule period was ""', '""permitted.""', ')', 'return', 'if', 'ti', '.', 'state', 'not', 'in', 'self', '.', 'RESCHEDULEABLE_STATES', ':', 'yield', 'self', '.', '_passing_status', '(', 'reason', '=', '""The task instance is not in State_UP_FOR_RESCHEDULE or NONE state.""', ')', 'return', 'task_reschedules', '=', 'TaskReschedule', '.', 'find_for_task_instance', '(', 'task_instance', '=', 'ti', ')', 'if', 'not', 'task_reschedules', ':', 'yield', 'self', '.', '_passing_status', '(', 'reason', '=', '""There is no reschedule request for this task instance.""', ')', 'return', 'now', '=', 'timezone', '.', 'utcnow', '(', ')', 'next_reschedule_date', '=', 'task_reschedules', '[', '-', '1', ']', '.', 'reschedule_date', 'if', 'now', '>=', 'next_reschedule_date', ':', 'yield', 'self', '.', '_passing_status', '(', 'reason', '=', '""Task instance id ready for reschedule.""', ')', 'return', 'yield', 'self', '.', '_failing_status', '(', 'reason', '=', '""Task is not ready for reschedule yet but will be rescheduled ""', '""automatically. Current date is {0} and task will be rescheduled ""', '""at {1}.""', '.', 'format', '(', 'now', '.', 'isoformat', '(', ')', ',', 'next_reschedule_date', '.', 'isoformat', '(', ')', ')', ')']","Determines whether a task is ready to be rescheduled. Only tasks in
        NONE state with at least one row in task_reschedule table are
        handled by this dependency class, otherwise this dependency is
        considered as passed. This dependency fails if the latest reschedule
        request's reschedule date is still in future.","['Determines', 'whether', 'a', 'task', 'is', 'ready', 'to', 'be', 'rescheduled', '.', 'Only', 'tasks', 'in', 'NONE', 'state', 'with', 'at', 'least', 'one', 'row', 'in', 'task_reschedule', 'table', 'are', 'handled', 'by', 'this', 'dependency', 'class', 'otherwise', 'this', 'dependency', 'is', 'considered', 'as', 'passed', '.', 'This', 'dependency', 'fails', 'if', 'the', 'latest', 'reschedule', 'request', 's', 'reschedule', 'date', 'is', 'still', 'in', 'future', '.']",python,test,"['determines', 'whether', 'a', 'task', 'is', 'ready', 'to', 'be', 'rescheduled', '.', 'only', 'tasks', 'in', 'none', 'state', 'with', 'at', 'least', 'one', 'row', 'in', 'task_reschedule', 'table', 'are', 'handled', 'by', 'this', 'dependency', 'class', 'otherwise', 'this', 'dependency', 'is', 'considered', 'as', 'passed', '.', 'this', 'dependency', 'fails', 'if', 'the', 'latest', 'reschedule', 'request', 's', 'reschedule', 'date', 'is', 'still', 'in', 'future', '.']",determines whether a task is ready to be rescheduled . only tasks in none state with at least one row in task_reschedule table are handled by this dependency class otherwise this dependency is considered as passed . this dependency fails if the latest reschedule request s reschedule date is still in future .,"['def', '_get_dep_statuses', '(', 'self', ',', 'ti', ',', 'session', ',', 'dep_context', ')', ':', 'if', 'dep_context', '.', 'ignore_in_reschedule_period', ':', 'yield', 'self', '.', '_passing_status', '(', 'reason', '=', '""the context specified that being in a reschedule period was ""', '""permitted.""', ')', 'return', 'if', 'ti', '.', 'state', 'not', 'in', 'self', '.', 'rescheduleable_states', ':', 'yield', 'self', '.', '_passing_status', '(', 'reason', '=', '""the task instance is not in state_up_for_reschedule or none state.""', ')', 'return', 'task_reschedules', '=', 'taskreschedule', '.', 'find_for_task_instance', '(', 'task_instance', '=', 'ti', ')', 'if', 'not', 'task_reschedules', ':', 'yield', 'self', '.', '_passing_status', '(', 'reason', '=', '""there is no reschedule request for this task instance.""', ')', 'return', 'now', '=', 'timezone', '.', 'utcnow', '(', ')', 'next_reschedule_date', '=', 'task_reschedules', '[', '-', '1', ']', '.', 'reschedule_date', 'if', 'now', '>=', 'next_reschedule_date', ':', 'yield', 'self', '.', '_passing_status', '(', 'reason', '=', '""task instance id ready for reschedule.""', ')', 'return', 'yield', 'self', '.', '_failing_status', '(', 'reason', '=', '""task is not ready for reschedule yet but will be rescheduled ""', '""automatically. current date is {0} and task will be rescheduled ""', '""at {1}.""', '.', 'format', '(', 'now', '.', 'isoformat', '(', ')', ',', 'next_reschedule_date', '.', 'isoformat', '(', ')', ')', ')']","def _get_dep_statuses ( self , ti , session , dep_context ) : if dep_context . ignore_in_reschedule_period : yield self . _passing_status ( reason = ""the context specified that being in a reschedule period was "" ""permitted."" ) return if ti . state not in self . rescheduleable_states : yield self . _passing_status ( reason = ""the task instance is not in state_up_for_reschedule or none state."" ) return task_reschedules = taskreschedule . find_for_task_instance ( task_instance = ti ) if not task_reschedules : yield self . _passing_status ( reason = ""there is no reschedule request for this task instance."" ) return now = timezone . utcnow ( ) next_reschedule_date = task_reschedules [ - 1 ] . reschedule_date if now >= next_reschedule_date : yield self . _passing_status ( reason = ""task instance id ready for reschedule."" ) return yield self . _failing_status ( reason = ""task is not ready for reschedule yet but will be rescheduled "" ""automatically. current date is {0} and task will be rescheduled "" ""at {1}."" . format ( now . isoformat ( ) , next_reschedule_date . isoformat ( ) ) )"
459,apache/airflow,airflow/utils/email.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/email.py#L36-L50,"def send_email(to, subject, html_content,
               files=None, dryrun=False, cc=None, bcc=None,
               mime_subtype='mixed', mime_charset='utf-8', **kwargs):
    """"""
    Send email using backend specified in EMAIL_BACKEND.
    """"""
    path, attr = configuration.conf.get('email', 'EMAIL_BACKEND').rsplit('.', 1)
    module = importlib.import_module(path)
    backend = getattr(module, attr)
    to = get_email_address_list(to)
    to = "", "".join(to)

    return backend(to, subject, html_content, files=files,
                   dryrun=dryrun, cc=cc, bcc=bcc,
                   mime_subtype=mime_subtype, mime_charset=mime_charset, **kwargs)","['def', 'send_email', '(', 'to', ',', 'subject', ',', 'html_content', ',', 'files', '=', 'None', ',', 'dryrun', '=', 'False', ',', 'cc', '=', 'None', ',', 'bcc', '=', 'None', ',', 'mime_subtype', '=', ""'mixed'"", ',', 'mime_charset', '=', ""'utf-8'"", ',', '*', '*', 'kwargs', ')', ':', 'path', ',', 'attr', '=', 'configuration', '.', 'conf', '.', 'get', '(', ""'email'"", ',', ""'EMAIL_BACKEND'"", ')', '.', 'rsplit', '(', ""'.'"", ',', '1', ')', 'module', '=', 'importlib', '.', 'import_module', '(', 'path', ')', 'backend', '=', 'getattr', '(', 'module', ',', 'attr', ')', 'to', '=', 'get_email_address_list', '(', 'to', ')', 'to', '=', '"", ""', '.', 'join', '(', 'to', ')', 'return', 'backend', '(', 'to', ',', 'subject', ',', 'html_content', ',', 'files', '=', 'files', ',', 'dryrun', '=', 'dryrun', ',', 'cc', '=', 'cc', ',', 'bcc', '=', 'bcc', ',', 'mime_subtype', '=', 'mime_subtype', ',', 'mime_charset', '=', 'mime_charset', ',', '*', '*', 'kwargs', ')']",Send email using backend specified in EMAIL_BACKEND.,"['Send', 'email', 'using', 'backend', 'specified', 'in', 'EMAIL_BACKEND', '.']",python,test,"['send', 'email', 'using', 'backend', 'specified', 'in', 'email_backend', '.']",send email using backend specified in email_backend .,"['def', 'send_email', '(', 'to', ',', 'subject', ',', 'html_content', ',', 'files', '=', 'none', ',', 'dryrun', '=', 'false', ',', 'cc', '=', 'none', ',', 'bcc', '=', 'none', ',', 'mime_subtype', '=', ""'mixed'"", ',', 'mime_charset', '=', ""'utf-8'"", ',', '*', '*', 'kwargs', ')', ':', 'path', ',', 'attr', '=', 'configuration', '.', 'conf', '.', 'get', '(', ""'email'"", ',', ""'email_backend'"", ')', '.', 'rsplit', '(', ""'.'"", ',', '1', ')', 'module', '=', 'importlib', '.', 'import_module', '(', 'path', ')', 'backend', '=', 'getattr', '(', 'module', ',', 'attr', ')', 'to', '=', 'get_email_address_list', '(', 'to', ')', 'to', '=', '"", ""', '.', 'join', '(', 'to', ')', 'return', 'backend', '(', 'to', ',', 'subject', ',', 'html_content', ',', 'files', '=', 'files', ',', 'dryrun', '=', 'dryrun', ',', 'cc', '=', 'cc', ',', 'bcc', '=', 'bcc', ',', 'mime_subtype', '=', 'mime_subtype', ',', 'mime_charset', '=', 'mime_charset', ',', '*', '*', 'kwargs', ')']","def send_email ( to , subject , html_content , files = none , dryrun = false , cc = none , bcc = none , mime_subtype = 'mixed' , mime_charset = 'utf-8' , * * kwargs ) : path , attr = configuration . conf . get ( 'email' , 'email_backend' ) . rsplit ( '.' , 1 ) module = importlib . import_module ( path ) backend = getattr ( module , attr ) to = get_email_address_list ( to ) to = "", "" . join ( to ) return backend ( to , subject , html_content , files = files , dryrun = dryrun , cc = cc , bcc = bcc , mime_subtype = mime_subtype , mime_charset = mime_charset , * * kwargs )"
460,apache/airflow,airflow/utils/email.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/email.py#L53-L96,"def send_email_smtp(to, subject, html_content, files=None,
                    dryrun=False, cc=None, bcc=None,
                    mime_subtype='mixed', mime_charset='utf-8',
                    **kwargs):
    """"""
    Send an email with html content

    >>> send_email('test@example.com', 'foo', '<b>Foo</b> bar', ['/dev/null'], dryrun=True)
    """"""
    smtp_mail_from = configuration.conf.get('smtp', 'SMTP_MAIL_FROM')

    to = get_email_address_list(to)

    msg = MIMEMultipart(mime_subtype)
    msg['Subject'] = subject
    msg['From'] = smtp_mail_from
    msg['To'] = "", "".join(to)
    recipients = to
    if cc:
        cc = get_email_address_list(cc)
        msg['CC'] = "", "".join(cc)
        recipients = recipients + cc

    if bcc:
        # don't add bcc in header
        bcc = get_email_address_list(bcc)
        recipients = recipients + bcc

    msg['Date'] = formatdate(localtime=True)
    mime_text = MIMEText(html_content, 'html', mime_charset)
    msg.attach(mime_text)

    for fname in files or []:
        basename = os.path.basename(fname)
        with open(fname, ""rb"") as f:
            part = MIMEApplication(
                f.read(),
                Name=basename
            )
            part['Content-Disposition'] = 'attachment; filename=""%s""' % basename
            part['Content-ID'] = '<%s>' % basename
            msg.attach(part)

    send_MIME_email(smtp_mail_from, recipients, msg, dryrun)","['def', 'send_email_smtp', '(', 'to', ',', 'subject', ',', 'html_content', ',', 'files', '=', 'None', ',', 'dryrun', '=', 'False', ',', 'cc', '=', 'None', ',', 'bcc', '=', 'None', ',', 'mime_subtype', '=', ""'mixed'"", ',', 'mime_charset', '=', ""'utf-8'"", ',', '*', '*', 'kwargs', ')', ':', 'smtp_mail_from', '=', 'configuration', '.', 'conf', '.', 'get', '(', ""'smtp'"", ',', ""'SMTP_MAIL_FROM'"", ')', 'to', '=', 'get_email_address_list', '(', 'to', ')', 'msg', '=', 'MIMEMultipart', '(', 'mime_subtype', ')', 'msg', '[', ""'Subject'"", ']', '=', 'subject', 'msg', '[', ""'From'"", ']', '=', 'smtp_mail_from', 'msg', '[', ""'To'"", ']', '=', '"", ""', '.', 'join', '(', 'to', ')', 'recipients', '=', 'to', 'if', 'cc', ':', 'cc', '=', 'get_email_address_list', '(', 'cc', ')', 'msg', '[', ""'CC'"", ']', '=', '"", ""', '.', 'join', '(', 'cc', ')', 'recipients', '=', 'recipients', '+', 'cc', 'if', 'bcc', ':', ""# don't add bcc in header"", 'bcc', '=', 'get_email_address_list', '(', 'bcc', ')', 'recipients', '=', 'recipients', '+', 'bcc', 'msg', '[', ""'Date'"", ']', '=', 'formatdate', '(', 'localtime', '=', 'True', ')', 'mime_text', '=', 'MIMEText', '(', 'html_content', ',', ""'html'"", ',', 'mime_charset', ')', 'msg', '.', 'attach', '(', 'mime_text', ')', 'for', 'fname', 'in', 'files', 'or', '[', ']', ':', 'basename', '=', 'os', '.', 'path', '.', 'basename', '(', 'fname', ')', 'with', 'open', '(', 'fname', ',', '""rb""', ')', 'as', 'f', ':', 'part', '=', 'MIMEApplication', '(', 'f', '.', 'read', '(', ')', ',', 'Name', '=', 'basename', ')', 'part', '[', ""'Content-Disposition'"", ']', '=', '\'attachment; filename=""%s""\'', '%', 'basename', 'part', '[', ""'Content-ID'"", ']', '=', ""'<%s>'"", '%', 'basename', 'msg', '.', 'attach', '(', 'part', ')', 'send_MIME_email', '(', 'smtp_mail_from', ',', 'recipients', ',', 'msg', ',', 'dryrun', ')']","Send an email with html content

    >>> send_email('test@example.com', 'foo', '<b>Foo</b> bar', ['/dev/null'], dryrun=True)","['Send', 'an', 'email', 'with', 'html', 'content']",python,test,"['send', 'an', 'email', 'with', 'html', 'content']",send an email with html content,"['def', 'send_email_smtp', '(', 'to', ',', 'subject', ',', 'html_content', ',', 'files', '=', 'none', ',', 'dryrun', '=', 'false', ',', 'cc', '=', 'none', ',', 'bcc', '=', 'none', ',', 'mime_subtype', '=', ""'mixed'"", ',', 'mime_charset', '=', ""'utf-8'"", ',', '*', '*', 'kwargs', ')', ':', 'smtp_mail_from', '=', 'configuration', '.', 'conf', '.', 'get', '(', ""'smtp'"", ',', ""'smtp_mail_from'"", ')', 'to', '=', 'get_email_address_list', '(', 'to', ')', 'msg', '=', 'mimemultipart', '(', 'mime_subtype', ')', 'msg', '[', ""'subject'"", ']', '=', 'subject', 'msg', '[', ""'from'"", ']', '=', 'smtp_mail_from', 'msg', '[', ""'to'"", ']', '=', '"", ""', '.', 'join', '(', 'to', ')', 'recipients', '=', 'to', 'if', 'cc', ':', 'cc', '=', 'get_email_address_list', '(', 'cc', ')', 'msg', '[', ""'cc'"", ']', '=', '"", ""', '.', 'join', '(', 'cc', ')', 'recipients', '=', 'recipients', '+', 'cc', 'if', 'bcc', ':', ""# don't add bcc in header"", 'bcc', '=', 'get_email_address_list', '(', 'bcc', ')', 'recipients', '=', 'recipients', '+', 'bcc', 'msg', '[', ""'date'"", ']', '=', 'formatdate', '(', 'localtime', '=', 'true', ')', 'mime_text', '=', 'mimetext', '(', 'html_content', ',', ""'html'"", ',', 'mime_charset', ')', 'msg', '.', 'attach', '(', 'mime_text', ')', 'for', 'fname', 'in', 'files', 'or', '[', ']', ':', 'basename', '=', 'os', '.', 'path', '.', 'basename', '(', 'fname', ')', 'with', 'open', '(', 'fname', ',', '""rb""', ')', 'as', 'f', ':', 'part', '=', 'mimeapplication', '(', 'f', '.', 'read', '(', ')', ',', 'name', '=', 'basename', ')', 'part', '[', ""'content-disposition'"", ']', '=', '\'attachment; filename=""%s""\'', '%', 'basename', 'part', '[', ""'content-id'"", ']', '=', ""'<%s>'"", '%', 'basename', 'msg', '.', 'attach', '(', 'part', ')', 'send_mime_email', '(', 'smtp_mail_from', ',', 'recipients', ',', 'msg', ',', 'dryrun', ')']","def send_email_smtp ( to , subject , html_content , files = none , dryrun = false , cc = none , bcc = none , mime_subtype = 'mixed' , mime_charset = 'utf-8' , * * kwargs ) : smtp_mail_from = configuration . conf . get ( 'smtp' , 'smtp_mail_from' ) to = get_email_address_list ( to ) msg = mimemultipart ( mime_subtype ) msg [ 'subject' ] = subject msg [ 'from' ] = smtp_mail_from msg [ 'to' ] = "", "" . join ( to ) recipients = to if cc : cc = get_email_address_list ( cc ) msg [ 'cc' ] = "", "" . join ( cc ) recipients = recipients + cc if bcc : # don't add bcc in header bcc = get_email_address_list ( bcc ) recipients = recipients + bcc msg [ 'date' ] = formatdate ( localtime = true ) mime_text = mimetext ( html_content , 'html' , mime_charset ) msg . attach ( mime_text ) for fname in files or [ ] : basename = os . path . basename ( fname ) with open ( fname , ""rb"" ) as f : part = mimeapplication ( f . read ( ) , name = basename ) part [ 'content-disposition' ] = 'attachment; filename=""%s""' % basename part [ 'content-id' ] = '<%s>' % basename msg . attach ( part ) send_mime_email ( smtp_mail_from , recipients , msg , dryrun )"
461,apache/airflow,airflow/utils/sqlalchemy.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/sqlalchemy.py#L156-L170,"def process_result_value(self, value, dialect):
        """"""
        Processes DateTimes from the DB making sure it is always
        returning UTC. Not using timezone.convert_to_utc as that
        converts to configured TIMEZONE while the DB might be
        running with some other setting. We assume UTC datetimes
        in the database.
        """"""
        if value is not None:
            if value.tzinfo is None:
                value = value.replace(tzinfo=utc)
            else:
                value = value.astimezone(utc)

        return value","['def', 'process_result_value', '(', 'self', ',', 'value', ',', 'dialect', ')', ':', 'if', 'value', 'is', 'not', 'None', ':', 'if', 'value', '.', 'tzinfo', 'is', 'None', ':', 'value', '=', 'value', '.', 'replace', '(', 'tzinfo', '=', 'utc', ')', 'else', ':', 'value', '=', 'value', '.', 'astimezone', '(', 'utc', ')', 'return', 'value']","Processes DateTimes from the DB making sure it is always
        returning UTC. Not using timezone.convert_to_utc as that
        converts to configured TIMEZONE while the DB might be
        running with some other setting. We assume UTC datetimes
        in the database.","['Processes', 'DateTimes', 'from', 'the', 'DB', 'making', 'sure', 'it', 'is', 'always', 'returning', 'UTC', '.', 'Not', 'using', 'timezone', '.', 'convert_to_utc', 'as', 'that', 'converts', 'to', 'configured', 'TIMEZONE', 'while', 'the', 'DB', 'might', 'be', 'running', 'with', 'some', 'other', 'setting', '.', 'We', 'assume', 'UTC', 'datetimes', 'in', 'the', 'database', '.']",python,test,"['processes', 'datetimes', 'from', 'the', 'db', 'making', 'sure', 'it', 'is', 'always', 'returning', 'utc', '.', 'not', 'using', 'timezone', '.', 'convert_to_utc', 'as', 'that', 'converts', 'to', 'configured', 'timezone', 'while', 'the', 'db', 'might', 'be', 'running', 'with', 'some', 'other', 'setting', '.', 'we', 'assume', 'utc', 'datetimes', 'in', 'the', 'database', '.']",processes datetimes from the db making sure it is always returning utc . not using timezone . convert_to_utc as that converts to configured timezone while the db might be running with some other setting . we assume utc datetimes in the database .,"['def', 'process_result_value', '(', 'self', ',', 'value', ',', 'dialect', ')', ':', 'if', 'value', 'is', 'not', 'none', ':', 'if', 'value', '.', 'tzinfo', 'is', 'none', ':', 'value', '=', 'value', '.', 'replace', '(', 'tzinfo', '=', 'utc', ')', 'else', ':', 'value', '=', 'value', '.', 'astimezone', '(', 'utc', ')', 'return', 'value']","def process_result_value ( self , value , dialect ) : if value is not none : if value . tzinfo is none : value = value . replace ( tzinfo = utc ) else : value = value . astimezone ( utc ) return value"
462,apache/airflow,airflow/contrib/hooks/wasb_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/wasb_hook.py#L50-L64,"def check_for_blob(self, container_name, blob_name, **kwargs):
        """"""
        Check if a blob exists on Azure Blob Storage.

        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.exists()` takes.
        :type kwargs: object
        :return: True if the blob exists, False otherwise.
        :rtype: bool
        """"""
        return self.connection.exists(container_name, blob_name, **kwargs)","['def', 'check_for_blob', '(', 'self', ',', 'container_name', ',', 'blob_name', ',', '*', '*', 'kwargs', ')', ':', 'return', 'self', '.', 'connection', '.', 'exists', '(', 'container_name', ',', 'blob_name', ',', '*', '*', 'kwargs', ')']","Check if a blob exists on Azure Blob Storage.

        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.exists()` takes.
        :type kwargs: object
        :return: True if the blob exists, False otherwise.
        :rtype: bool","['Check', 'if', 'a', 'blob', 'exists', 'on', 'Azure', 'Blob', 'Storage', '.']",python,test,"['check', 'if', 'a', 'blob', 'exists', 'on', 'azure', 'blob', 'storage', '.']",check if a blob exists on azure blob storage .,"['def', 'check_for_blob', '(', 'self', ',', 'container_name', ',', 'blob_name', ',', '*', '*', 'kwargs', ')', ':', 'return', 'self', '.', 'connection', '.', 'exists', '(', 'container_name', ',', 'blob_name', ',', '*', '*', 'kwargs', ')']","def check_for_blob ( self , container_name , blob_name , * * kwargs ) : return self . connection . exists ( container_name , blob_name , * * kwargs )"
463,apache/airflow,airflow/contrib/hooks/wasb_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/wasb_hook.py#L66-L82,"def check_for_prefix(self, container_name, prefix, **kwargs):
        """"""
        Check if a prefix exists on Azure Blob storage.

        :param container_name: Name of the container.
        :type container_name: str
        :param prefix: Prefix of the blob.
        :type prefix: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.list_blobs()` takes.
        :type kwargs: object
        :return: True if blobs matching the prefix exist, False otherwise.
        :rtype: bool
        """"""
        matches = self.connection.list_blobs(container_name, prefix,
                                             num_results=1, **kwargs)
        return len(list(matches)) > 0","['def', 'check_for_prefix', '(', 'self', ',', 'container_name', ',', 'prefix', ',', '*', '*', 'kwargs', ')', ':', 'matches', '=', 'self', '.', 'connection', '.', 'list_blobs', '(', 'container_name', ',', 'prefix', ',', 'num_results', '=', '1', ',', '*', '*', 'kwargs', ')', 'return', 'len', '(', 'list', '(', 'matches', ')', ')', '>', '0']","Check if a prefix exists on Azure Blob storage.

        :param container_name: Name of the container.
        :type container_name: str
        :param prefix: Prefix of the blob.
        :type prefix: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.list_blobs()` takes.
        :type kwargs: object
        :return: True if blobs matching the prefix exist, False otherwise.
        :rtype: bool","['Check', 'if', 'a', 'prefix', 'exists', 'on', 'Azure', 'Blob', 'storage', '.']",python,test,"['check', 'if', 'a', 'prefix', 'exists', 'on', 'azure', 'blob', 'storage', '.']",check if a prefix exists on azure blob storage .,"['def', 'check_for_prefix', '(', 'self', ',', 'container_name', ',', 'prefix', ',', '*', '*', 'kwargs', ')', ':', 'matches', '=', 'self', '.', 'connection', '.', 'list_blobs', '(', 'container_name', ',', 'prefix', ',', 'num_results', '=', '1', ',', '*', '*', 'kwargs', ')', 'return', 'len', '(', 'list', '(', 'matches', ')', ')', '>', '0']","def check_for_prefix ( self , container_name , prefix , * * kwargs ) : matches = self . connection . list_blobs ( container_name , prefix , num_results = 1 , * * kwargs ) return len ( list ( matches ) ) > 0"
464,apache/airflow,airflow/contrib/hooks/wasb_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/wasb_hook.py#L84-L100,"def load_file(self, file_path, container_name, blob_name, **kwargs):
        """"""
        Upload a file to Azure Blob Storage.

        :param file_path: Path to the file to load.
        :type file_path: str
        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.create_blob_from_path()` takes.
        :type kwargs: object
        """"""
        # Reorder the argument order from airflow.hooks.S3_hook.load_file.
        self.connection.create_blob_from_path(container_name, blob_name,
                                              file_path, **kwargs)","['def', 'load_file', '(', 'self', ',', 'file_path', ',', 'container_name', ',', 'blob_name', ',', '*', '*', 'kwargs', ')', ':', '# Reorder the argument order from airflow.hooks.S3_hook.load_file.', 'self', '.', 'connection', '.', 'create_blob_from_path', '(', 'container_name', ',', 'blob_name', ',', 'file_path', ',', '*', '*', 'kwargs', ')']","Upload a file to Azure Blob Storage.

        :param file_path: Path to the file to load.
        :type file_path: str
        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.create_blob_from_path()` takes.
        :type kwargs: object","['Upload', 'a', 'file', 'to', 'Azure', 'Blob', 'Storage', '.']",python,test,"['upload', 'a', 'file', 'to', 'azure', 'blob', 'storage', '.']",upload a file to azure blob storage .,"['def', 'load_file', '(', 'self', ',', 'file_path', ',', 'container_name', ',', 'blob_name', ',', '*', '*', 'kwargs', ')', ':', '# reorder the argument order from airflow.hooks.s3_hook.load_file.', 'self', '.', 'connection', '.', 'create_blob_from_path', '(', 'container_name', ',', 'blob_name', ',', 'file_path', ',', '*', '*', 'kwargs', ')']","def load_file ( self , file_path , container_name , blob_name , * * kwargs ) : # reorder the argument order from airflow.hooks.s3_hook.load_file. self . connection . create_blob_from_path ( container_name , blob_name , file_path , * * kwargs )"
465,apache/airflow,airflow/contrib/hooks/wasb_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/wasb_hook.py#L102-L118,"def load_string(self, string_data, container_name, blob_name, **kwargs):
        """"""
        Upload a string to Azure Blob Storage.

        :param string_data: String to load.
        :type string_data: str
        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.create_blob_from_text()` takes.
        :type kwargs: object
        """"""
        # Reorder the argument order from airflow.hooks.S3_hook.load_string.
        self.connection.create_blob_from_text(container_name, blob_name,
                                              string_data, **kwargs)","['def', 'load_string', '(', 'self', ',', 'string_data', ',', 'container_name', ',', 'blob_name', ',', '*', '*', 'kwargs', ')', ':', '# Reorder the argument order from airflow.hooks.S3_hook.load_string.', 'self', '.', 'connection', '.', 'create_blob_from_text', '(', 'container_name', ',', 'blob_name', ',', 'string_data', ',', '*', '*', 'kwargs', ')']","Upload a string to Azure Blob Storage.

        :param string_data: String to load.
        :type string_data: str
        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.create_blob_from_text()` takes.
        :type kwargs: object","['Upload', 'a', 'string', 'to', 'Azure', 'Blob', 'Storage', '.']",python,test,"['upload', 'a', 'string', 'to', 'azure', 'blob', 'storage', '.']",upload a string to azure blob storage .,"['def', 'load_string', '(', 'self', ',', 'string_data', ',', 'container_name', ',', 'blob_name', ',', '*', '*', 'kwargs', ')', ':', '# reorder the argument order from airflow.hooks.s3_hook.load_string.', 'self', '.', 'connection', '.', 'create_blob_from_text', '(', 'container_name', ',', 'blob_name', ',', 'string_data', ',', '*', '*', 'kwargs', ')']","def load_string ( self , string_data , container_name , blob_name , * * kwargs ) : # reorder the argument order from airflow.hooks.s3_hook.load_string. self . connection . create_blob_from_text ( container_name , blob_name , string_data , * * kwargs )"
466,apache/airflow,airflow/contrib/hooks/wasb_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/wasb_hook.py#L120-L135,"def get_file(self, file_path, container_name, blob_name, **kwargs):
        """"""
        Download a file from Azure Blob Storage.

        :param file_path: Path to the file to download.
        :type file_path: str
        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.create_blob_from_path()` takes.
        :type kwargs: object
        """"""
        return self.connection.get_blob_to_path(container_name, blob_name,
                                                file_path, **kwargs)","['def', 'get_file', '(', 'self', ',', 'file_path', ',', 'container_name', ',', 'blob_name', ',', '*', '*', 'kwargs', ')', ':', 'return', 'self', '.', 'connection', '.', 'get_blob_to_path', '(', 'container_name', ',', 'blob_name', ',', 'file_path', ',', '*', '*', 'kwargs', ')']","Download a file from Azure Blob Storage.

        :param file_path: Path to the file to download.
        :type file_path: str
        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.create_blob_from_path()` takes.
        :type kwargs: object","['Download', 'a', 'file', 'from', 'Azure', 'Blob', 'Storage', '.']",python,test,"['download', 'a', 'file', 'from', 'azure', 'blob', 'storage', '.']",download a file from azure blob storage .,"['def', 'get_file', '(', 'self', ',', 'file_path', ',', 'container_name', ',', 'blob_name', ',', '*', '*', 'kwargs', ')', ':', 'return', 'self', '.', 'connection', '.', 'get_blob_to_path', '(', 'container_name', ',', 'blob_name', ',', 'file_path', ',', '*', '*', 'kwargs', ')']","def get_file ( self , file_path , container_name , blob_name , * * kwargs ) : return self . connection . get_blob_to_path ( container_name , blob_name , file_path , * * kwargs )"
467,apache/airflow,airflow/contrib/hooks/wasb_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/wasb_hook.py#L137-L151,"def read_file(self, container_name, blob_name, **kwargs):
        """"""
        Read a file from Azure Blob Storage and return as a string.

        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.create_blob_from_path()` takes.
        :type kwargs: object
        """"""
        return self.connection.get_blob_to_text(container_name,
                                                blob_name,
                                                **kwargs).content","['def', 'read_file', '(', 'self', ',', 'container_name', ',', 'blob_name', ',', '*', '*', 'kwargs', ')', ':', 'return', 'self', '.', 'connection', '.', 'get_blob_to_text', '(', 'container_name', ',', 'blob_name', ',', '*', '*', 'kwargs', ')', '.', 'content']","Read a file from Azure Blob Storage and return as a string.

        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.create_blob_from_path()` takes.
        :type kwargs: object","['Read', 'a', 'file', 'from', 'Azure', 'Blob', 'Storage', 'and', 'return', 'as', 'a', 'string', '.']",python,test,"['read', 'a', 'file', 'from', 'azure', 'blob', 'storage', 'and', 'return', 'as', 'a', 'string', '.']",read a file from azure blob storage and return as a string .,"['def', 'read_file', '(', 'self', ',', 'container_name', ',', 'blob_name', ',', '*', '*', 'kwargs', ')', ':', 'return', 'self', '.', 'connection', '.', 'get_blob_to_text', '(', 'container_name', ',', 'blob_name', ',', '*', '*', 'kwargs', ')', '.', 'content']","def read_file ( self , container_name , blob_name , * * kwargs ) : return self . connection . get_blob_to_text ( container_name , blob_name , * * kwargs ) . content"
468,apache/airflow,airflow/contrib/hooks/wasb_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/wasb_hook.py#L153-L191,"def delete_file(self, container_name, blob_name, is_prefix=False,
                    ignore_if_missing=False, **kwargs):
        """"""
        Delete a file from Azure Blob Storage.

        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param is_prefix: If blob_name is a prefix, delete all matching files
        :type is_prefix: bool
        :param ignore_if_missing: if True, then return success even if the
            blob does not exist.
        :type ignore_if_missing: bool
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.create_blob_from_path()` takes.
        :type kwargs: object
        """"""

        if is_prefix:
            blobs_to_delete = [
                blob.name for blob in self.connection.list_blobs(
                    container_name, prefix=blob_name, **kwargs
                )
            ]
        elif self.check_for_blob(container_name, blob_name):
            blobs_to_delete = [blob_name]
        else:
            blobs_to_delete = []

        if not ignore_if_missing and len(blobs_to_delete) == 0:
            raise AirflowException('Blob(s) not found: {}'.format(blob_name))

        for blob_uri in blobs_to_delete:
            self.log.info(""Deleting blob: "" + blob_uri)
            self.connection.delete_blob(container_name,
                                        blob_uri,
                                        delete_snapshots='include',
                                        **kwargs)","['def', 'delete_file', '(', 'self', ',', 'container_name', ',', 'blob_name', ',', 'is_prefix', '=', 'False', ',', 'ignore_if_missing', '=', 'False', ',', '*', '*', 'kwargs', ')', ':', 'if', 'is_prefix', ':', 'blobs_to_delete', '=', '[', 'blob', '.', 'name', 'for', 'blob', 'in', 'self', '.', 'connection', '.', 'list_blobs', '(', 'container_name', ',', 'prefix', '=', 'blob_name', ',', '*', '*', 'kwargs', ')', ']', 'elif', 'self', '.', 'check_for_blob', '(', 'container_name', ',', 'blob_name', ')', ':', 'blobs_to_delete', '=', '[', 'blob_name', ']', 'else', ':', 'blobs_to_delete', '=', '[', ']', 'if', 'not', 'ignore_if_missing', 'and', 'len', '(', 'blobs_to_delete', ')', '==', '0', ':', 'raise', 'AirflowException', '(', ""'Blob(s) not found: {}'"", '.', 'format', '(', 'blob_name', ')', ')', 'for', 'blob_uri', 'in', 'blobs_to_delete', ':', 'self', '.', 'log', '.', 'info', '(', '""Deleting blob: ""', '+', 'blob_uri', ')', 'self', '.', 'connection', '.', 'delete_blob', '(', 'container_name', ',', 'blob_uri', ',', 'delete_snapshots', '=', ""'include'"", ',', '*', '*', 'kwargs', ')']","Delete a file from Azure Blob Storage.

        :param container_name: Name of the container.
        :type container_name: str
        :param blob_name: Name of the blob.
        :type blob_name: str
        :param is_prefix: If blob_name is a prefix, delete all matching files
        :type is_prefix: bool
        :param ignore_if_missing: if True, then return success even if the
            blob does not exist.
        :type ignore_if_missing: bool
        :param kwargs: Optional keyword arguments that
            `BlockBlobService.create_blob_from_path()` takes.
        :type kwargs: object","['Delete', 'a', 'file', 'from', 'Azure', 'Blob', 'Storage', '.']",python,test,"['delete', 'a', 'file', 'from', 'azure', 'blob', 'storage', '.']",delete a file from azure blob storage .,"['def', 'delete_file', '(', 'self', ',', 'container_name', ',', 'blob_name', ',', 'is_prefix', '=', 'false', ',', 'ignore_if_missing', '=', 'false', ',', '*', '*', 'kwargs', ')', ':', 'if', 'is_prefix', ':', 'blobs_to_delete', '=', '[', 'blob', '.', 'name', 'for', 'blob', 'in', 'self', '.', 'connection', '.', 'list_blobs', '(', 'container_name', ',', 'prefix', '=', 'blob_name', ',', '*', '*', 'kwargs', ')', ']', 'elif', 'self', '.', 'check_for_blob', '(', 'container_name', ',', 'blob_name', ')', ':', 'blobs_to_delete', '=', '[', 'blob_name', ']', 'else', ':', 'blobs_to_delete', '=', '[', ']', 'if', 'not', 'ignore_if_missing', 'and', 'len', '(', 'blobs_to_delete', ')', '==', '0', ':', 'raise', 'airflowexception', '(', ""'blob(s) not found: {}'"", '.', 'format', '(', 'blob_name', ')', ')', 'for', 'blob_uri', 'in', 'blobs_to_delete', ':', 'self', '.', 'log', '.', 'info', '(', '""deleting blob: ""', '+', 'blob_uri', ')', 'self', '.', 'connection', '.', 'delete_blob', '(', 'container_name', ',', 'blob_uri', ',', 'delete_snapshots', '=', ""'include'"", ',', '*', '*', 'kwargs', ')']","def delete_file ( self , container_name , blob_name , is_prefix = false , ignore_if_missing = false , * * kwargs ) : if is_prefix : blobs_to_delete = [ blob . name for blob in self . connection . list_blobs ( container_name , prefix = blob_name , * * kwargs ) ] elif self . check_for_blob ( container_name , blob_name ) : blobs_to_delete = [ blob_name ] else : blobs_to_delete = [ ] if not ignore_if_missing and len ( blobs_to_delete ) == 0 : raise airflowexception ( 'blob(s) not found: {}' . format ( blob_name ) ) for blob_uri in blobs_to_delete : self . log . info ( ""deleting blob: "" + blob_uri ) self . connection . delete_blob ( container_name , blob_uri , delete_snapshots = 'include' , * * kwargs )"
469,apache/airflow,airflow/contrib/hooks/ftp_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/ftp_hook.py#L28-L58,"def mlsd(conn, path="""", facts=None):
    """"""
    BACKPORT FROM PYTHON3 FTPLIB.

    List a directory in a standardized format by using MLSD
    command (RFC-3659). If path is omitted the current directory
    is assumed. ""facts"" is a list of strings representing the type
    of information desired (e.g. [""type"", ""size"", ""perm""]).

    Return a generator object yielding a tuple of two elements
    for every file found in path.
    First element is the file name, the second one is a dictionary
    including a variable number of ""facts"" depending on the server
    and whether ""facts"" argument has been provided.
    """"""
    facts = facts or []
    if facts:
        conn.sendcmd(""OPTS MLST "" + "";"".join(facts) + "";"")
    if path:
        cmd = ""MLSD %s"" % path
    else:
        cmd = ""MLSD""
    lines = []
    conn.retrlines(cmd, lines.append)
    for line in lines:
        facts_found, _, name = line.rstrip(ftplib.CRLF).partition(' ')
        entry = {}
        for fact in facts_found[:-1].split("";""):
            key, _, value = fact.partition(""="")
            entry[key.lower()] = value
        yield (name, entry)","['def', 'mlsd', '(', 'conn', ',', 'path', '=', '""""', ',', 'facts', '=', 'None', ')', ':', 'facts', '=', 'facts', 'or', '[', ']', 'if', 'facts', ':', 'conn', '.', 'sendcmd', '(', '""OPTS MLST ""', '+', '"";""', '.', 'join', '(', 'facts', ')', '+', '"";""', ')', 'if', 'path', ':', 'cmd', '=', '""MLSD %s""', '%', 'path', 'else', ':', 'cmd', '=', '""MLSD""', 'lines', '=', '[', ']', 'conn', '.', 'retrlines', '(', 'cmd', ',', 'lines', '.', 'append', ')', 'for', 'line', 'in', 'lines', ':', 'facts_found', ',', '_', ',', 'name', '=', 'line', '.', 'rstrip', '(', 'ftplib', '.', 'CRLF', ')', '.', 'partition', '(', ""' '"", ')', 'entry', '=', '{', '}', 'for', 'fact', 'in', 'facts_found', '[', ':', '-', '1', ']', '.', 'split', '(', '"";""', ')', ':', 'key', ',', '_', ',', 'value', '=', 'fact', '.', 'partition', '(', '""=""', ')', 'entry', '[', 'key', '.', 'lower', '(', ')', ']', '=', 'value', 'yield', '(', 'name', ',', 'entry', ')']","BACKPORT FROM PYTHON3 FTPLIB.

    List a directory in a standardized format by using MLSD
    command (RFC-3659). If path is omitted the current directory
    is assumed. ""facts"" is a list of strings representing the type
    of information desired (e.g. [""type"", ""size"", ""perm""]).

    Return a generator object yielding a tuple of two elements
    for every file found in path.
    First element is the file name, the second one is a dictionary
    including a variable number of ""facts"" depending on the server
    and whether ""facts"" argument has been provided.","['BACKPORT', 'FROM', 'PYTHON3', 'FTPLIB', '.']",python,test,"['backport', 'from', 'python3', 'ftplib', '.']",backport from python3 ftplib .,"['def', 'mlsd', '(', 'conn', ',', 'path', '=', '""""', ',', 'facts', '=', 'none', ')', ':', 'facts', '=', 'facts', 'or', '[', ']', 'if', 'facts', ':', 'conn', '.', 'sendcmd', '(', '""opts mlst ""', '+', '"";""', '.', 'join', '(', 'facts', ')', '+', '"";""', ')', 'if', 'path', ':', 'cmd', '=', '""mlsd %s""', '%', 'path', 'else', ':', 'cmd', '=', '""mlsd""', 'lines', '=', '[', ']', 'conn', '.', 'retrlines', '(', 'cmd', ',', 'lines', '.', 'append', ')', 'for', 'line', 'in', 'lines', ':', 'facts_found', ',', '_', ',', 'name', '=', 'line', '.', 'rstrip', '(', 'ftplib', '.', 'crlf', ')', '.', 'partition', '(', ""' '"", ')', 'entry', '=', '{', '}', 'for', 'fact', 'in', 'facts_found', '[', ':', '-', '1', ']', '.', 'split', '(', '"";""', ')', ':', 'key', ',', '_', ',', 'value', '=', 'fact', '.', 'partition', '(', '""=""', ')', 'entry', '[', 'key', '.', 'lower', '(', ')', ']', '=', 'value', 'yield', '(', 'name', ',', 'entry', ')']","def mlsd ( conn , path = """" , facts = none ) : facts = facts or [ ] if facts : conn . sendcmd ( ""opts mlst "" + "";"" . join ( facts ) + "";"" ) if path : cmd = ""mlsd %s"" % path else : cmd = ""mlsd"" lines = [ ] conn . retrlines ( cmd , lines . append ) for line in lines : facts_found , _ , name = line . rstrip ( ftplib . crlf ) . partition ( ' ' ) entry = { } for fact in facts_found [ : - 1 ] . split ( "";"" ) : key , _ , value = fact . partition ( ""="" ) entry [ key . lower ( ) ] = value yield ( name , entry )"
470,apache/airflow,airflow/contrib/hooks/ftp_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/ftp_hook.py#L81-L91,"def get_conn(self):
        """"""
        Returns a FTP connection object
        """"""
        if self.conn is None:
            params = self.get_connection(self.ftp_conn_id)
            pasv = params.extra_dejson.get(""passive"", True)
            self.conn = ftplib.FTP(params.host, params.login, params.password)
            self.conn.set_pasv(pasv)

        return self.conn","['def', 'get_conn', '(', 'self', ')', ':', 'if', 'self', '.', 'conn', 'is', 'None', ':', 'params', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'ftp_conn_id', ')', 'pasv', '=', 'params', '.', 'extra_dejson', '.', 'get', '(', '""passive""', ',', 'True', ')', 'self', '.', 'conn', '=', 'ftplib', '.', 'FTP', '(', 'params', '.', 'host', ',', 'params', '.', 'login', ',', 'params', '.', 'password', ')', 'self', '.', 'conn', '.', 'set_pasv', '(', 'pasv', ')', 'return', 'self', '.', 'conn']",Returns a FTP connection object,"['Returns', 'a', 'FTP', 'connection', 'object']",python,test,"['returns', 'a', 'ftp', 'connection', 'object']",returns a ftp connection object,"['def', 'get_conn', '(', 'self', ')', ':', 'if', 'self', '.', 'conn', 'is', 'none', ':', 'params', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'ftp_conn_id', ')', 'pasv', '=', 'params', '.', 'extra_dejson', '.', 'get', '(', '""passive""', ',', 'true', ')', 'self', '.', 'conn', '=', 'ftplib', '.', 'ftp', '(', 'params', '.', 'host', ',', 'params', '.', 'login', ',', 'params', '.', 'password', ')', 'self', '.', 'conn', '.', 'set_pasv', '(', 'pasv', ')', 'return', 'self', '.', 'conn']","def get_conn ( self ) : if self . conn is none : params = self . get_connection ( self . ftp_conn_id ) pasv = params . extra_dejson . get ( ""passive"" , true ) self . conn = ftplib . ftp ( params . host , params . login , params . password ) self . conn . set_pasv ( pasv ) return self . conn"
471,apache/airflow,airflow/contrib/hooks/ftp_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/ftp_hook.py#L102-L117,"def describe_directory(self, path):
        """"""
        Returns a dictionary of {filename: {attributes}} for all files
        on the remote system (where the MLSD command is supported).

        :param path: full path to the remote directory
        :type path: str
        """"""
        conn = self.get_conn()
        conn.cwd(path)
        try:
            # only works in Python 3
            files = dict(conn.mlsd())
        except AttributeError:
            files = dict(mlsd(conn))
        return files","['def', 'describe_directory', '(', 'self', ',', 'path', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'conn', '.', 'cwd', '(', 'path', ')', 'try', ':', '# only works in Python 3', 'files', '=', 'dict', '(', 'conn', '.', 'mlsd', '(', ')', ')', 'except', 'AttributeError', ':', 'files', '=', 'dict', '(', 'mlsd', '(', 'conn', ')', ')', 'return', 'files']","Returns a dictionary of {filename: {attributes}} for all files
        on the remote system (where the MLSD command is supported).

        :param path: full path to the remote directory
        :type path: str","['Returns', 'a', 'dictionary', 'of', '{', 'filename', ':', '{', 'attributes', '}}', 'for', 'all', 'files', 'on', 'the', 'remote', 'system', '(', 'where', 'the', 'MLSD', 'command', 'is', 'supported', ')', '.']",python,test,"['returns', 'a', 'dictionary', 'of', '{', 'filename', ':', '{', 'attributes', '}}', 'for', 'all', 'files', 'on', 'the', 'remote', 'system', '(', 'where', 'the', 'mlsd', 'command', 'is', 'supported', ')', '.']",returns a dictionary of { filename : { attributes }} for all files on the remote system ( where the mlsd command is supported ) .,"['def', 'describe_directory', '(', 'self', ',', 'path', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'conn', '.', 'cwd', '(', 'path', ')', 'try', ':', '# only works in python 3', 'files', '=', 'dict', '(', 'conn', '.', 'mlsd', '(', ')', ')', 'except', 'attributeerror', ':', 'files', '=', 'dict', '(', 'mlsd', '(', 'conn', ')', ')', 'return', 'files']","def describe_directory ( self , path ) : conn = self . get_conn ( ) conn . cwd ( path ) try : # only works in python 3 files = dict ( conn . mlsd ( ) ) except attributeerror : files = dict ( mlsd ( conn ) ) return files"
472,apache/airflow,airflow/contrib/hooks/ftp_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/ftp_hook.py#L119-L130,"def list_directory(self, path, nlst=False):
        """"""
        Returns a list of files on the remote system.

        :param path: full path to the remote directory to list
        :type path: str
        """"""
        conn = self.get_conn()
        conn.cwd(path)

        files = conn.nlst()
        return files","['def', 'list_directory', '(', 'self', ',', 'path', ',', 'nlst', '=', 'False', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'conn', '.', 'cwd', '(', 'path', ')', 'files', '=', 'conn', '.', 'nlst', '(', ')', 'return', 'files']","Returns a list of files on the remote system.

        :param path: full path to the remote directory to list
        :type path: str","['Returns', 'a', 'list', 'of', 'files', 'on', 'the', 'remote', 'system', '.']",python,test,"['returns', 'a', 'list', 'of', 'files', 'on', 'the', 'remote', 'system', '.']",returns a list of files on the remote system .,"['def', 'list_directory', '(', 'self', ',', 'path', ',', 'nlst', '=', 'false', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'conn', '.', 'cwd', '(', 'path', ')', 'files', '=', 'conn', '.', 'nlst', '(', ')', 'return', 'files']","def list_directory ( self , path , nlst = false ) : conn = self . get_conn ( ) conn . cwd ( path ) files = conn . nlst ( ) return files"
473,apache/airflow,airflow/contrib/hooks/ftp_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/ftp_hook.py#L152-L223,"def retrieve_file(
            self,
            remote_full_path,
            local_full_path_or_buffer,
            callback=None):
        """"""
        Transfers the remote file to a local location.

        If local_full_path_or_buffer is a string path, the file will be put
        at that location; if it is a file-like buffer, the file will
        be written to the buffer but not closed.

        :param remote_full_path: full path to the remote file
        :type remote_full_path: str
        :param local_full_path_or_buffer: full path to the local file or a
            file-like buffer
        :type local_full_path_or_buffer: str or file-like buffer
        :param callback: callback which is called each time a block of data
            is read. if you do not use a callback, these blocks will be written
            to the file or buffer passed in. if you do pass in a callback, note
            that writing to a file or buffer will need to be handled inside the
            callback.
            [default: output_handle.write()]
        :type callback: callable

        :Example::

            hook = FTPHook(ftp_conn_id='my_conn')

            remote_path = '/path/to/remote/file'
            local_path = '/path/to/local/file'

            # with a custom callback (in this case displaying progress on each read)
            def print_progress(percent_progress):
                self.log.info('Percent Downloaded: %s%%' % percent_progress)

            total_downloaded = 0
            total_file_size = hook.get_size(remote_path)
            output_handle = open(local_path, 'wb')
            def write_to_file_with_progress(data):
                total_downloaded += len(data)
                output_handle.write(data)
                percent_progress = (total_downloaded / total_file_size) * 100
                print_progress(percent_progress)
            hook.retrieve_file(remote_path, None, callback=write_to_file_with_progress)

            # without a custom callback data is written to the local_path
            hook.retrieve_file(remote_path, local_path)
        """"""
        conn = self.get_conn()

        is_path = isinstance(local_full_path_or_buffer, basestring)

        # without a callback, default to writing to a user-provided file or
        # file-like buffer
        if not callback:
            if is_path:
                output_handle = open(local_full_path_or_buffer, 'wb')
            else:
                output_handle = local_full_path_or_buffer
            callback = output_handle.write
        else:
            output_handle = None

        remote_path, remote_file_name = os.path.split(remote_full_path)
        conn.cwd(remote_path)
        self.log.info('Retrieving file from FTP: %s', remote_full_path)
        conn.retrbinary('RETR %s' % remote_file_name, callback)
        self.log.info('Finished retrieving file from FTP: %s', remote_full_path)

        if is_path and output_handle:
            output_handle.close()","['def', 'retrieve_file', '(', 'self', ',', 'remote_full_path', ',', 'local_full_path_or_buffer', ',', 'callback', '=', 'None', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'is_path', '=', 'isinstance', '(', 'local_full_path_or_buffer', ',', 'basestring', ')', '# without a callback, default to writing to a user-provided file or', '# file-like buffer', 'if', 'not', 'callback', ':', 'if', 'is_path', ':', 'output_handle', '=', 'open', '(', 'local_full_path_or_buffer', ',', ""'wb'"", ')', 'else', ':', 'output_handle', '=', 'local_full_path_or_buffer', 'callback', '=', 'output_handle', '.', 'write', 'else', ':', 'output_handle', '=', 'None', 'remote_path', ',', 'remote_file_name', '=', 'os', '.', 'path', '.', 'split', '(', 'remote_full_path', ')', 'conn', '.', 'cwd', '(', 'remote_path', ')', 'self', '.', 'log', '.', 'info', '(', ""'Retrieving file from FTP: %s'"", ',', 'remote_full_path', ')', 'conn', '.', 'retrbinary', '(', ""'RETR %s'"", '%', 'remote_file_name', ',', 'callback', ')', 'self', '.', 'log', '.', 'info', '(', ""'Finished retrieving file from FTP: %s'"", ',', 'remote_full_path', ')', 'if', 'is_path', 'and', 'output_handle', ':', 'output_handle', '.', 'close', '(', ')']","Transfers the remote file to a local location.

        If local_full_path_or_buffer is a string path, the file will be put
        at that location; if it is a file-like buffer, the file will
        be written to the buffer but not closed.

        :param remote_full_path: full path to the remote file
        :type remote_full_path: str
        :param local_full_path_or_buffer: full path to the local file or a
            file-like buffer
        :type local_full_path_or_buffer: str or file-like buffer
        :param callback: callback which is called each time a block of data
            is read. if you do not use a callback, these blocks will be written
            to the file or buffer passed in. if you do pass in a callback, note
            that writing to a file or buffer will need to be handled inside the
            callback.
            [default: output_handle.write()]
        :type callback: callable

        :Example::

            hook = FTPHook(ftp_conn_id='my_conn')

            remote_path = '/path/to/remote/file'
            local_path = '/path/to/local/file'

            # with a custom callback (in this case displaying progress on each read)
            def print_progress(percent_progress):
                self.log.info('Percent Downloaded: %s%%' % percent_progress)

            total_downloaded = 0
            total_file_size = hook.get_size(remote_path)
            output_handle = open(local_path, 'wb')
            def write_to_file_with_progress(data):
                total_downloaded += len(data)
                output_handle.write(data)
                percent_progress = (total_downloaded / total_file_size) * 100
                print_progress(percent_progress)
            hook.retrieve_file(remote_path, None, callback=write_to_file_with_progress)

            # without a custom callback data is written to the local_path
            hook.retrieve_file(remote_path, local_path)","['Transfers', 'the', 'remote', 'file', 'to', 'a', 'local', 'location', '.']",python,test,"['transfers', 'the', 'remote', 'file', 'to', 'a', 'local', 'location', '.']",transfers the remote file to a local location .,"['def', 'retrieve_file', '(', 'self', ',', 'remote_full_path', ',', 'local_full_path_or_buffer', ',', 'callback', '=', 'none', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'is_path', '=', 'isinstance', '(', 'local_full_path_or_buffer', ',', 'basestring', ')', '# without a callback, default to writing to a user-provided file or', '# file-like buffer', 'if', 'not', 'callback', ':', 'if', 'is_path', ':', 'output_handle', '=', 'open', '(', 'local_full_path_or_buffer', ',', ""'wb'"", ')', 'else', ':', 'output_handle', '=', 'local_full_path_or_buffer', 'callback', '=', 'output_handle', '.', 'write', 'else', ':', 'output_handle', '=', 'none', 'remote_path', ',', 'remote_file_name', '=', 'os', '.', 'path', '.', 'split', '(', 'remote_full_path', ')', 'conn', '.', 'cwd', '(', 'remote_path', ')', 'self', '.', 'log', '.', 'info', '(', ""'retrieving file from ftp: %s'"", ',', 'remote_full_path', ')', 'conn', '.', 'retrbinary', '(', ""'retr %s'"", '%', 'remote_file_name', ',', 'callback', ')', 'self', '.', 'log', '.', 'info', '(', ""'finished retrieving file from ftp: %s'"", ',', 'remote_full_path', ')', 'if', 'is_path', 'and', 'output_handle', ':', 'output_handle', '.', 'close', '(', ')']","def retrieve_file ( self , remote_full_path , local_full_path_or_buffer , callback = none ) : conn = self . get_conn ( ) is_path = isinstance ( local_full_path_or_buffer , basestring ) # without a callback, default to writing to a user-provided file or # file-like buffer if not callback : if is_path : output_handle = open ( local_full_path_or_buffer , 'wb' ) else : output_handle = local_full_path_or_buffer callback = output_handle . write else : output_handle = none remote_path , remote_file_name = os . path . split ( remote_full_path ) conn . cwd ( remote_path ) self . log . info ( 'retrieving file from ftp: %s' , remote_full_path ) conn . retrbinary ( 'retr %s' % remote_file_name , callback ) self . log . info ( 'finished retrieving file from ftp: %s' , remote_full_path ) if is_path and output_handle : output_handle . close ( )"
474,apache/airflow,airflow/contrib/hooks/ftp_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/ftp_hook.py#L225-L252,"def store_file(self, remote_full_path, local_full_path_or_buffer):
        """"""
        Transfers a local file to the remote location.

        If local_full_path_or_buffer is a string path, the file will be read
        from that location; if it is a file-like buffer, the file will
        be read from the buffer but not closed.

        :param remote_full_path: full path to the remote file
        :type remote_full_path: str
        :param local_full_path_or_buffer: full path to the local file or a
            file-like buffer
        :type local_full_path_or_buffer: str or file-like buffer
        """"""
        conn = self.get_conn()

        is_path = isinstance(local_full_path_or_buffer, basestring)

        if is_path:
            input_handle = open(local_full_path_or_buffer, 'rb')
        else:
            input_handle = local_full_path_or_buffer
        remote_path, remote_file_name = os.path.split(remote_full_path)
        conn.cwd(remote_path)
        conn.storbinary('STOR %s' % remote_file_name, input_handle)

        if is_path:
            input_handle.close()","['def', 'store_file', '(', 'self', ',', 'remote_full_path', ',', 'local_full_path_or_buffer', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'is_path', '=', 'isinstance', '(', 'local_full_path_or_buffer', ',', 'basestring', ')', 'if', 'is_path', ':', 'input_handle', '=', 'open', '(', 'local_full_path_or_buffer', ',', ""'rb'"", ')', 'else', ':', 'input_handle', '=', 'local_full_path_or_buffer', 'remote_path', ',', 'remote_file_name', '=', 'os', '.', 'path', '.', 'split', '(', 'remote_full_path', ')', 'conn', '.', 'cwd', '(', 'remote_path', ')', 'conn', '.', 'storbinary', '(', ""'STOR %s'"", '%', 'remote_file_name', ',', 'input_handle', ')', 'if', 'is_path', ':', 'input_handle', '.', 'close', '(', ')']","Transfers a local file to the remote location.

        If local_full_path_or_buffer is a string path, the file will be read
        from that location; if it is a file-like buffer, the file will
        be read from the buffer but not closed.

        :param remote_full_path: full path to the remote file
        :type remote_full_path: str
        :param local_full_path_or_buffer: full path to the local file or a
            file-like buffer
        :type local_full_path_or_buffer: str or file-like buffer","['Transfers', 'a', 'local', 'file', 'to', 'the', 'remote', 'location', '.']",python,test,"['transfers', 'a', 'local', 'file', 'to', 'the', 'remote', 'location', '.']",transfers a local file to the remote location .,"['def', 'store_file', '(', 'self', ',', 'remote_full_path', ',', 'local_full_path_or_buffer', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'is_path', '=', 'isinstance', '(', 'local_full_path_or_buffer', ',', 'basestring', ')', 'if', 'is_path', ':', 'input_handle', '=', 'open', '(', 'local_full_path_or_buffer', ',', ""'rb'"", ')', 'else', ':', 'input_handle', '=', 'local_full_path_or_buffer', 'remote_path', ',', 'remote_file_name', '=', 'os', '.', 'path', '.', 'split', '(', 'remote_full_path', ')', 'conn', '.', 'cwd', '(', 'remote_path', ')', 'conn', '.', 'storbinary', '(', ""'stor %s'"", '%', 'remote_file_name', ',', 'input_handle', ')', 'if', 'is_path', ':', 'input_handle', '.', 'close', '(', ')']","def store_file ( self , remote_full_path , local_full_path_or_buffer ) : conn = self . get_conn ( ) is_path = isinstance ( local_full_path_or_buffer , basestring ) if is_path : input_handle = open ( local_full_path_or_buffer , 'rb' ) else : input_handle = local_full_path_or_buffer remote_path , remote_file_name = os . path . split ( remote_full_path ) conn . cwd ( remote_path ) conn . storbinary ( 'stor %s' % remote_file_name , input_handle ) if is_path : input_handle . close ( )"
475,apache/airflow,airflow/contrib/hooks/ftp_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/ftp_hook.py#L264-L272,"def rename(self, from_name, to_name):
        """"""
        Rename a file.

        :param from_name: rename file from name
        :param to_name: rename file to name
        """"""
        conn = self.get_conn()
        return conn.rename(from_name, to_name)","['def', 'rename', '(', 'self', ',', 'from_name', ',', 'to_name', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'return', 'conn', '.', 'rename', '(', 'from_name', ',', 'to_name', ')']","Rename a file.

        :param from_name: rename file from name
        :param to_name: rename file to name","['Rename', 'a', 'file', '.']",python,test,"['rename', 'a', 'file', '.']",rename a file .,"['def', 'rename', '(', 'self', ',', 'from_name', ',', 'to_name', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'return', 'conn', '.', 'rename', '(', 'from_name', ',', 'to_name', ')']","def rename ( self , from_name , to_name ) : conn = self . get_conn ( ) return conn . rename ( from_name , to_name )"
476,apache/airflow,airflow/contrib/hooks/ftp_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/ftp_hook.py#L274-L288,"def get_mod_time(self, path):
        """"""
        Returns a datetime object representing the last time the file was modified

        :param path: remote file path
        :type path: string
        """"""
        conn = self.get_conn()
        ftp_mdtm = conn.sendcmd('MDTM ' + path)
        time_val = ftp_mdtm[4:]
        # time_val optionally has microseconds
        try:
            return datetime.datetime.strptime(time_val, ""%Y%m%d%H%M%S.%f"")
        except ValueError:
            return datetime.datetime.strptime(time_val, '%Y%m%d%H%M%S')","['def', 'get_mod_time', '(', 'self', ',', 'path', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'ftp_mdtm', '=', 'conn', '.', 'sendcmd', '(', ""'MDTM '"", '+', 'path', ')', 'time_val', '=', 'ftp_mdtm', '[', '4', ':', ']', '# time_val optionally has microseconds', 'try', ':', 'return', 'datetime', '.', 'datetime', '.', 'strptime', '(', 'time_val', ',', '""%Y%m%d%H%M%S.%f""', ')', 'except', 'ValueError', ':', 'return', 'datetime', '.', 'datetime', '.', 'strptime', '(', 'time_val', ',', ""'%Y%m%d%H%M%S'"", ')']","Returns a datetime object representing the last time the file was modified

        :param path: remote file path
        :type path: string","['Returns', 'a', 'datetime', 'object', 'representing', 'the', 'last', 'time', 'the', 'file', 'was', 'modified']",python,test,"['returns', 'a', 'datetime', 'object', 'representing', 'the', 'last', 'time', 'the', 'file', 'was', 'modified']",returns a datetime object representing the last time the file was modified,"['def', 'get_mod_time', '(', 'self', ',', 'path', ')', ':', 'conn', '=', 'self', '.', 'get_conn', '(', ')', 'ftp_mdtm', '=', 'conn', '.', 'sendcmd', '(', ""'mdtm '"", '+', 'path', ')', 'time_val', '=', 'ftp_mdtm', '[', '4', ':', ']', '# time_val optionally has microseconds', 'try', ':', 'return', 'datetime', '.', 'datetime', '.', 'strptime', '(', 'time_val', ',', '""%y%m%d%h%m%s.%f""', ')', 'except', 'valueerror', ':', 'return', 'datetime', '.', 'datetime', '.', 'strptime', '(', 'time_val', ',', ""'%y%m%d%h%m%s'"", ')']","def get_mod_time ( self , path ) : conn = self . get_conn ( ) ftp_mdtm = conn . sendcmd ( 'mdtm ' + path ) time_val = ftp_mdtm [ 4 : ] # time_val optionally has microseconds try : return datetime . datetime . strptime ( time_val , ""%y%m%d%h%m%s.%f"" ) except valueerror : return datetime . datetime . strptime ( time_val , '%y%m%d%h%m%s' )"
477,apache/airflow,airflow/api/common/experimental/mark_tasks.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/api/common/experimental/mark_tasks.py#L30-L53,"def _create_dagruns(dag, execution_dates, state, run_id_template):
    """"""
    Infers from the dates which dag runs need to be created and does so.
    :param dag: the dag to create dag runs for
    :param execution_dates: list of execution dates to evaluate
    :param state: the state to set the dag run to
    :param run_id_template:the template for run id to be with the execution date
    :return: newly created and existing dag runs for the execution dates supplied
    """"""
    # find out if we need to create any dag runs
    drs = DagRun.find(dag_id=dag.dag_id, execution_date=execution_dates)
    dates_to_create = list(set(execution_dates) - set([dr.execution_date for dr in drs]))

    for date in dates_to_create:
        dr = dag.create_dagrun(
            run_id=run_id_template.format(date.isoformat()),
            execution_date=date,
            start_date=timezone.utcnow(),
            external_trigger=False,
            state=state,
        )
        drs.append(dr)

    return drs","['def', '_create_dagruns', '(', 'dag', ',', 'execution_dates', ',', 'state', ',', 'run_id_template', ')', ':', '# find out if we need to create any dag runs', 'drs', '=', 'DagRun', '.', 'find', '(', 'dag_id', '=', 'dag', '.', 'dag_id', ',', 'execution_date', '=', 'execution_dates', ')', 'dates_to_create', '=', 'list', '(', 'set', '(', 'execution_dates', ')', '-', 'set', '(', '[', 'dr', '.', 'execution_date', 'for', 'dr', 'in', 'drs', ']', ')', ')', 'for', 'date', 'in', 'dates_to_create', ':', 'dr', '=', 'dag', '.', 'create_dagrun', '(', 'run_id', '=', 'run_id_template', '.', 'format', '(', 'date', '.', 'isoformat', '(', ')', ')', ',', 'execution_date', '=', 'date', ',', 'start_date', '=', 'timezone', '.', 'utcnow', '(', ')', ',', 'external_trigger', '=', 'False', ',', 'state', '=', 'state', ',', ')', 'drs', '.', 'append', '(', 'dr', ')', 'return', 'drs']","Infers from the dates which dag runs need to be created and does so.
    :param dag: the dag to create dag runs for
    :param execution_dates: list of execution dates to evaluate
    :param state: the state to set the dag run to
    :param run_id_template:the template for run id to be with the execution date
    :return: newly created and existing dag runs for the execution dates supplied","['Infers', 'from', 'the', 'dates', 'which', 'dag', 'runs', 'need', 'to', 'be', 'created', 'and', 'does', 'so', '.', ':', 'param', 'dag', ':', 'the', 'dag', 'to', 'create', 'dag', 'runs', 'for', ':', 'param', 'execution_dates', ':', 'list', 'of', 'execution', 'dates', 'to', 'evaluate', ':', 'param', 'state', ':', 'the', 'state', 'to', 'set', 'the', 'dag', 'run', 'to', ':', 'param', 'run_id_template', ':', 'the', 'template', 'for', 'run', 'id', 'to', 'be', 'with', 'the', 'execution', 'date', ':', 'return', ':', 'newly', 'created', 'and', 'existing', 'dag', 'runs', 'for', 'the', 'execution', 'dates', 'supplied']",python,test,"['infers', 'from', 'the', 'dates', 'which', 'dag', 'runs', 'need', 'to', 'be', 'created', 'and', 'does', 'so', '.', ':', 'param', 'dag', ':', 'the', 'dag', 'to', 'create', 'dag', 'runs', 'for', ':', 'param', 'execution_dates', ':', 'list', 'of', 'execution', 'dates', 'to', 'evaluate', ':', 'param', 'state', ':', 'the', 'state', 'to', 'set', 'the', 'dag', 'run', 'to', ':', 'param', 'run_id_template', ':', 'the', 'template', 'for', 'run', 'id', 'to', 'be', 'with', 'the', 'execution', 'date', ':', 'return', ':', 'newly', 'created', 'and', 'existing', 'dag', 'runs', 'for', 'the', 'execution', 'dates', 'supplied']",infers from the dates which dag runs need to be created and does so . : param dag : the dag to create dag runs for : param execution_dates : list of execution dates to evaluate : param state : the state to set the dag run to : param run_id_template : the template for run id to be with the execution date : return : newly created and existing dag runs for the execution dates supplied,"['def', '_create_dagruns', '(', 'dag', ',', 'execution_dates', ',', 'state', ',', 'run_id_template', ')', ':', '# find out if we need to create any dag runs', 'drs', '=', 'dagrun', '.', 'find', '(', 'dag_id', '=', 'dag', '.', 'dag_id', ',', 'execution_date', '=', 'execution_dates', ')', 'dates_to_create', '=', 'list', '(', 'set', '(', 'execution_dates', ')', '-', 'set', '(', '[', 'dr', '.', 'execution_date', 'for', 'dr', 'in', 'drs', ']', ')', ')', 'for', 'date', 'in', 'dates_to_create', ':', 'dr', '=', 'dag', '.', 'create_dagrun', '(', 'run_id', '=', 'run_id_template', '.', 'format', '(', 'date', '.', 'isoformat', '(', ')', ')', ',', 'execution_date', '=', 'date', ',', 'start_date', '=', 'timezone', '.', 'utcnow', '(', ')', ',', 'external_trigger', '=', 'false', ',', 'state', '=', 'state', ',', ')', 'drs', '.', 'append', '(', 'dr', ')', 'return', 'drs']","def _create_dagruns ( dag , execution_dates , state , run_id_template ) : # find out if we need to create any dag runs drs = dagrun . find ( dag_id = dag . dag_id , execution_date = execution_dates ) dates_to_create = list ( set ( execution_dates ) - set ( [ dr . execution_date for dr in drs ] ) ) for date in dates_to_create : dr = dag . create_dagrun ( run_id = run_id_template . format ( date . isoformat ( ) ) , execution_date = date , start_date = timezone . utcnow ( ) , external_trigger = false , state = state , ) drs . append ( dr ) return drs"
478,apache/airflow,airflow/api/common/experimental/mark_tasks.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/api/common/experimental/mark_tasks.py#L57-L184,"def set_state(task, execution_date, upstream=False, downstream=False,
              future=False, past=False, state=State.SUCCESS, commit=False, session=None):
    """"""
    Set the state of a task instance and if needed its relatives. Can set state
    for future tasks (calculated from execution_date) and retroactively
    for past tasks. Will verify integrity of past dag runs in order to create
    tasks that did not exist. It will not create dag runs that are missing
    on the schedule (but it will as for subdag dag runs if needed).
    :param task: the task from which to work. task.task.dag needs to be set
    :param execution_date: the execution date from which to start looking
    :param upstream: Mark all parents (upstream tasks)
    :param downstream: Mark all siblings (downstream tasks) of task_id, including SubDags
    :param future: Mark all future tasks on the interval of the dag up until
        last execution date.
    :param past: Retroactively mark all tasks starting from start_date of the DAG
    :param state: State to which the tasks need to be set
    :param commit: Commit tasks to be altered to the database
    :param session: database session
    :return: list of tasks that have been created and updated
    """"""
    assert timezone.is_localized(execution_date)

    assert task.dag is not None
    dag = task.dag

    latest_execution_date = dag.latest_execution_date
    assert latest_execution_date is not None

    # determine date range of dag runs and tasks to consider
    end_date = latest_execution_date if future else execution_date

    if 'start_date' in dag.default_args:
        start_date = dag.default_args['start_date']
    elif dag.start_date:
        start_date = dag.start_date
    else:
        start_date = execution_date

    start_date = execution_date if not past else start_date

    if dag.schedule_interval == '@once':
        dates = [start_date]
    else:
        dates = dag.date_range(start_date=start_date, end_date=end_date)

    # find relatives (siblings = downstream, parents = upstream) if needed
    task_ids = [task.task_id]
    if downstream:
        relatives = task.get_flat_relatives(upstream=False)
        task_ids += [t.task_id for t in relatives]
    if upstream:
        relatives = task.get_flat_relatives(upstream=True)
        task_ids += [t.task_id for t in relatives]

    # verify the integrity of the dag runs in case a task was added or removed
    # set the confirmed execution dates as they might be different
    # from what was provided
    confirmed_dates = []
    drs = DagRun.find(dag_id=dag.dag_id, execution_date=dates)
    for dr in drs:
        dr.dag = dag
        dr.verify_integrity()
        confirmed_dates.append(dr.execution_date)

    # go through subdagoperators and create dag runs. We will only work
    # within the scope of the subdag. We wont propagate to the parent dag,
    # but we will propagate from parent to subdag.
    dags = [dag]
    sub_dag_ids = []
    while len(dags) > 0:
        current_dag = dags.pop()
        for task_id in task_ids:
            if not current_dag.has_task(task_id):
                continue

            current_task = current_dag.get_task(task_id)
            if isinstance(current_task, SubDagOperator):
                # this works as a kind of integrity check
                # it creates missing dag runs for subdagoperators,
                # maybe this should be moved to dagrun.verify_integrity
                drs = _create_dagruns(current_task.subdag,
                                      execution_dates=confirmed_dates,
                                      state=State.RUNNING,
                                      run_id_template=BackfillJob.ID_FORMAT_PREFIX)

                for dr in drs:
                    dr.dag = current_task.subdag
                    dr.verify_integrity()
                    if commit:
                        dr.state = state
                        session.merge(dr)

                dags.append(current_task.subdag)
                sub_dag_ids.append(current_task.subdag.dag_id)

    # now look for the task instances that are affected
    TI = TaskInstance

    # get all tasks of the main dag that will be affected by a state change
    qry_dag = session.query(TI).filter(
        TI.dag_id == dag.dag_id,
        TI.execution_date.in_(confirmed_dates),
        TI.task_id.in_(task_ids)).filter(
        or_(TI.state.is_(None),
            TI.state != state)
    )

    # get *all* tasks of the sub dags
    if len(sub_dag_ids) > 0:
        qry_sub_dag = session.query(TI).filter(
            TI.dag_id.in_(sub_dag_ids),
            TI.execution_date.in_(confirmed_dates)).filter(
            or_(TI.state.is_(None),
                TI.state != state)
        )

    if commit:
        tis_altered = qry_dag.with_for_update().all()
        if len(sub_dag_ids) > 0:
            tis_altered += qry_sub_dag.with_for_update().all()
        for ti in tis_altered:
            ti.state = state
    else:
        tis_altered = qry_dag.all()
        if len(sub_dag_ids) > 0:
            tis_altered += qry_sub_dag.all()

    return tis_altered","['def', 'set_state', '(', 'task', ',', 'execution_date', ',', 'upstream', '=', 'False', ',', 'downstream', '=', 'False', ',', 'future', '=', 'False', ',', 'past', '=', 'False', ',', 'state', '=', 'State', '.', 'SUCCESS', ',', 'commit', '=', 'False', ',', 'session', '=', 'None', ')', ':', 'assert', 'timezone', '.', 'is_localized', '(', 'execution_date', ')', 'assert', 'task', '.', 'dag', 'is', 'not', 'None', 'dag', '=', 'task', '.', 'dag', 'latest_execution_date', '=', 'dag', '.', 'latest_execution_date', 'assert', 'latest_execution_date', 'is', 'not', 'None', '# determine date range of dag runs and tasks to consider', 'end_date', '=', 'latest_execution_date', 'if', 'future', 'else', 'execution_date', 'if', ""'start_date'"", 'in', 'dag', '.', 'default_args', ':', 'start_date', '=', 'dag', '.', 'default_args', '[', ""'start_date'"", ']', 'elif', 'dag', '.', 'start_date', ':', 'start_date', '=', 'dag', '.', 'start_date', 'else', ':', 'start_date', '=', 'execution_date', 'start_date', '=', 'execution_date', 'if', 'not', 'past', 'else', 'start_date', 'if', 'dag', '.', 'schedule_interval', '==', ""'@once'"", ':', 'dates', '=', '[', 'start_date', ']', 'else', ':', 'dates', '=', 'dag', '.', 'date_range', '(', 'start_date', '=', 'start_date', ',', 'end_date', '=', 'end_date', ')', '# find relatives (siblings = downstream, parents = upstream) if needed', 'task_ids', '=', '[', 'task', '.', 'task_id', ']', 'if', 'downstream', ':', 'relatives', '=', 'task', '.', 'get_flat_relatives', '(', 'upstream', '=', 'False', ')', 'task_ids', '+=', '[', 't', '.', 'task_id', 'for', 't', 'in', 'relatives', ']', 'if', 'upstream', ':', 'relatives', '=', 'task', '.', 'get_flat_relatives', '(', 'upstream', '=', 'True', ')', 'task_ids', '+=', '[', 't', '.', 'task_id', 'for', 't', 'in', 'relatives', ']', '# verify the integrity of the dag runs in case a task was added or removed', '# set the confirmed execution dates as they might be different', '# from what was provided', 'confirmed_dates', '=', '[', ']', 'drs', '=', 'DagRun', '.', 'find', '(', 'dag_id', '=', 'dag', '.', 'dag_id', ',', 'execution_date', '=', 'dates', ')', 'for', 'dr', 'in', 'drs', ':', 'dr', '.', 'dag', '=', 'dag', 'dr', '.', 'verify_integrity', '(', ')', 'confirmed_dates', '.', 'append', '(', 'dr', '.', 'execution_date', ')', '# go through subdagoperators and create dag runs. We will only work', '# within the scope of the subdag. We wont propagate to the parent dag,', '# but we will propagate from parent to subdag.', 'dags', '=', '[', 'dag', ']', 'sub_dag_ids', '=', '[', ']', 'while', 'len', '(', 'dags', ')', '>', '0', ':', 'current_dag', '=', 'dags', '.', 'pop', '(', ')', 'for', 'task_id', 'in', 'task_ids', ':', 'if', 'not', 'current_dag', '.', 'has_task', '(', 'task_id', ')', ':', 'continue', 'current_task', '=', 'current_dag', '.', 'get_task', '(', 'task_id', ')', 'if', 'isinstance', '(', 'current_task', ',', 'SubDagOperator', ')', ':', '# this works as a kind of integrity check', '# it creates missing dag runs for subdagoperators,', '# maybe this should be moved to dagrun.verify_integrity', 'drs', '=', '_create_dagruns', '(', 'current_task', '.', 'subdag', ',', 'execution_dates', '=', 'confirmed_dates', ',', 'state', '=', 'State', '.', 'RUNNING', ',', 'run_id_template', '=', 'BackfillJob', '.', 'ID_FORMAT_PREFIX', ')', 'for', 'dr', 'in', 'drs', ':', 'dr', '.', 'dag', '=', 'current_task', '.', 'subdag', 'dr', '.', 'verify_integrity', '(', ')', 'if', 'commit', ':', 'dr', '.', 'state', '=', 'state', 'session', '.', 'merge', '(', 'dr', ')', 'dags', '.', 'append', '(', 'current_task', '.', 'subdag', ')', 'sub_dag_ids', '.', 'append', '(', 'current_task', '.', 'subdag', '.', 'dag_id', ')', '# now look for the task instances that are affected', 'TI', '=', 'TaskInstance', '# get all tasks of the main dag that will be affected by a state change', 'qry_dag', '=', 'session', '.', 'query', '(', 'TI', ')', '.', 'filter', '(', 'TI', '.', 'dag_id', '==', 'dag', '.', 'dag_id', ',', 'TI', '.', 'execution_date', '.', 'in_', '(', 'confirmed_dates', ')', ',', 'TI', '.', 'task_id', '.', 'in_', '(', 'task_ids', ')', ')', '.', 'filter', '(', 'or_', '(', 'TI', '.', 'state', '.', 'is_', '(', 'None', ')', ',', 'TI', '.', 'state', '!=', 'state', ')', ')', '# get *all* tasks of the sub dags', 'if', 'len', '(', 'sub_dag_ids', ')', '>', '0', ':', 'qry_sub_dag', '=', 'session', '.', 'query', '(', 'TI', ')', '.', 'filter', '(', 'TI', '.', 'dag_id', '.', 'in_', '(', 'sub_dag_ids', ')', ',', 'TI', '.', 'execution_date', '.', 'in_', '(', 'confirmed_dates', ')', ')', '.', 'filter', '(', 'or_', '(', 'TI', '.', 'state', '.', 'is_', '(', 'None', ')', ',', 'TI', '.', 'state', '!=', 'state', ')', ')', 'if', 'commit', ':', 'tis_altered', '=', 'qry_dag', '.', 'with_for_update', '(', ')', '.', 'all', '(', ')', 'if', 'len', '(', 'sub_dag_ids', ')', '>', '0', ':', 'tis_altered', '+=', 'qry_sub_dag', '.', 'with_for_update', '(', ')', '.', 'all', '(', ')', 'for', 'ti', 'in', 'tis_altered', ':', 'ti', '.', 'state', '=', 'state', 'else', ':', 'tis_altered', '=', 'qry_dag', '.', 'all', '(', ')', 'if', 'len', '(', 'sub_dag_ids', ')', '>', '0', ':', 'tis_altered', '+=', 'qry_sub_dag', '.', 'all', '(', ')', 'return', 'tis_altered']","Set the state of a task instance and if needed its relatives. Can set state
    for future tasks (calculated from execution_date) and retroactively
    for past tasks. Will verify integrity of past dag runs in order to create
    tasks that did not exist. It will not create dag runs that are missing
    on the schedule (but it will as for subdag dag runs if needed).
    :param task: the task from which to work. task.task.dag needs to be set
    :param execution_date: the execution date from which to start looking
    :param upstream: Mark all parents (upstream tasks)
    :param downstream: Mark all siblings (downstream tasks) of task_id, including SubDags
    :param future: Mark all future tasks on the interval of the dag up until
        last execution date.
    :param past: Retroactively mark all tasks starting from start_date of the DAG
    :param state: State to which the tasks need to be set
    :param commit: Commit tasks to be altered to the database
    :param session: database session
    :return: list of tasks that have been created and updated","['Set', 'the', 'state', 'of', 'a', 'task', 'instance', 'and', 'if', 'needed', 'its', 'relatives', '.', 'Can', 'set', 'state', 'for', 'future', 'tasks', '(', 'calculated', 'from', 'execution_date', ')', 'and', 'retroactively', 'for', 'past', 'tasks', '.', 'Will', 'verify', 'integrity', 'of', 'past', 'dag', 'runs', 'in', 'order', 'to', 'create', 'tasks', 'that', 'did', 'not', 'exist', '.', 'It', 'will', 'not', 'create', 'dag', 'runs', 'that', 'are', 'missing', 'on', 'the', 'schedule', '(', 'but', 'it', 'will', 'as', 'for', 'subdag', 'dag', 'runs', 'if', 'needed', ')', '.', ':', 'param', 'task', ':', 'the', 'task', 'from', 'which', 'to', 'work', '.', 'task', '.', 'task', '.', 'dag', 'needs', 'to', 'be', 'set', ':', 'param', 'execution_date', ':', 'the', 'execution', 'date', 'from', 'which', 'to', 'start', 'looking', ':', 'param', 'upstream', ':', 'Mark', 'all', 'parents', '(', 'upstream', 'tasks', ')', ':', 'param', 'downstream', ':', 'Mark', 'all', 'siblings', '(', 'downstream', 'tasks', ')', 'of', 'task_id', 'including', 'SubDags', ':', 'param', 'future', ':', 'Mark', 'all', 'future', 'tasks', 'on', 'the', 'interval', 'of', 'the', 'dag', 'up', 'until', 'last', 'execution', 'date', '.', ':', 'param', 'past', ':', 'Retroactively', 'mark', 'all', 'tasks', 'starting', 'from', 'start_date', 'of', 'the', 'DAG', ':', 'param', 'state', ':', 'State', 'to', 'which', 'the', 'tasks', 'need', 'to', 'be', 'set', ':', 'param', 'commit', ':', 'Commit', 'tasks', 'to', 'be', 'altered', 'to', 'the', 'database', ':', 'param', 'session', ':', 'database', 'session', ':', 'return', ':', 'list', 'of', 'tasks', 'that', 'have', 'been', 'created', 'and', 'updated']",python,test,"['set', 'the', 'state', 'of', 'a', 'task', 'instance', 'and', 'if', 'needed', 'its', 'relatives', '.', 'can', 'set', 'state', 'for', 'future', 'tasks', '(', 'calculated', 'from', 'execution_date', ')', 'and', 'retroactively', 'for', 'past', 'tasks', '.', 'will', 'verify', 'integrity', 'of', 'past', 'dag', 'runs', 'in', 'order', 'to', 'create', 'tasks', 'that', 'did', 'not', 'exist', '.', 'it', 'will', 'not', 'create', 'dag', 'runs', 'that', 'are', 'missing', 'on', 'the', 'schedule', '(', 'but', 'it', 'will', 'as', 'for', 'subdag', 'dag', 'runs', 'if', 'needed', ')', '.', ':', 'param', 'task', ':', 'the', 'task', 'from', 'which', 'to', 'work', '.', 'task', '.', 'task', '.', 'dag', 'needs', 'to', 'be', 'set', ':', 'param', 'execution_date', ':', 'the', 'execution', 'date', 'from', 'which', 'to', 'start', 'looking', ':', 'param', 'upstream', ':', 'mark', 'all', 'parents', '(', 'upstream', 'tasks', ')', ':', 'param', 'downstream', ':', 'mark', 'all', 'siblings', '(', 'downstream', 'tasks', ')', 'of', 'task_id', 'including', 'subdags', ':', 'param', 'future', ':', 'mark', 'all', 'future', 'tasks', 'on', 'the', 'interval', 'of', 'the', 'dag', 'up', 'until', 'last', 'execution', 'date', '.', ':', 'param', 'past', ':', 'retroactively', 'mark', 'all', 'tasks', 'starting', 'from', 'start_date', 'of', 'the', 'dag', ':', 'param', 'state', ':', 'state', 'to', 'which', 'the', 'tasks', 'need', 'to', 'be', 'set', ':', 'param', 'commit', ':', 'commit', 'tasks', 'to', 'be', 'altered', 'to', 'the', 'database', ':', 'param', 'session', ':', 'database', 'session', ':', 'return', ':', 'list', 'of', 'tasks', 'that', 'have', 'been', 'created', 'and', 'updated']",set the state of a task instance and if needed its relatives . can set state for future tasks ( calculated from execution_date ) and retroactively for past tasks . will verify integrity of past dag runs in order to create tasks that did not exist . it will not create dag runs that are missing on the schedule ( but it will as for subdag dag runs if needed ) . : param task : the task from which to work . task . task . dag needs to be set : param execution_date : the execution date from which to start looking : param upstream : mark all parents ( upstream tasks ) : param downstream : mark all siblings ( downstream tasks ) of task_id including subdags : param future : mark all future tasks on the interval of the dag up until last execution date . : param past : retroactively mark all tasks starting from start_date of the dag : param state : state to which the tasks need to be set : param commit : commit tasks to be altered to the database : param session : database session : return : list of tasks that have been created and updated,"['def', 'set_state', '(', 'task', ',', 'execution_date', ',', 'upstream', '=', 'false', ',', 'downstream', '=', 'false', ',', 'future', '=', 'false', ',', 'past', '=', 'false', ',', 'state', '=', 'state', '.', 'success', ',', 'commit', '=', 'false', ',', 'session', '=', 'none', ')', ':', 'assert', 'timezone', '.', 'is_localized', '(', 'execution_date', ')', 'assert', 'task', '.', 'dag', 'is', 'not', 'none', 'dag', '=', 'task', '.', 'dag', 'latest_execution_date', '=', 'dag', '.', 'latest_execution_date', 'assert', 'latest_execution_date', 'is', 'not', 'none', '# determine date range of dag runs and tasks to consider', 'end_date', '=', 'latest_execution_date', 'if', 'future', 'else', 'execution_date', 'if', ""'start_date'"", 'in', 'dag', '.', 'default_args', ':', 'start_date', '=', 'dag', '.', 'default_args', '[', ""'start_date'"", ']', 'elif', 'dag', '.', 'start_date', ':', 'start_date', '=', 'dag', '.', 'start_date', 'else', ':', 'start_date', '=', 'execution_date', 'start_date', '=', 'execution_date', 'if', 'not', 'past', 'else', 'start_date', 'if', 'dag', '.', 'schedule_interval', '==', ""'@once'"", ':', 'dates', '=', '[', 'start_date', ']', 'else', ':', 'dates', '=', 'dag', '.', 'date_range', '(', 'start_date', '=', 'start_date', ',', 'end_date', '=', 'end_date', ')', '# find relatives (siblings = downstream, parents = upstream) if needed', 'task_ids', '=', '[', 'task', '.', 'task_id', ']', 'if', 'downstream', ':', 'relatives', '=', 'task', '.', 'get_flat_relatives', '(', 'upstream', '=', 'false', ')', 'task_ids', '+=', '[', 't', '.', 'task_id', 'for', 't', 'in', 'relatives', ']', 'if', 'upstream', ':', 'relatives', '=', 'task', '.', 'get_flat_relatives', '(', 'upstream', '=', 'true', ')', 'task_ids', '+=', '[', 't', '.', 'task_id', 'for', 't', 'in', 'relatives', ']', '# verify the integrity of the dag runs in case a task was added or removed', '# set the confirmed execution dates as they might be different', '# from what was provided', 'confirmed_dates', '=', '[', ']', 'drs', '=', 'dagrun', '.', 'find', '(', 'dag_id', '=', 'dag', '.', 'dag_id', ',', 'execution_date', '=', 'dates', ')', 'for', 'dr', 'in', 'drs', ':', 'dr', '.', 'dag', '=', 'dag', 'dr', '.', 'verify_integrity', '(', ')', 'confirmed_dates', '.', 'append', '(', 'dr', '.', 'execution_date', ')', '# go through subdagoperators and create dag runs. we will only work', '# within the scope of the subdag. we wont propagate to the parent dag,', '# but we will propagate from parent to subdag.', 'dags', '=', '[', 'dag', ']', 'sub_dag_ids', '=', '[', ']', 'while', 'len', '(', 'dags', ')', '>', '0', ':', 'current_dag', '=', 'dags', '.', 'pop', '(', ')', 'for', 'task_id', 'in', 'task_ids', ':', 'if', 'not', 'current_dag', '.', 'has_task', '(', 'task_id', ')', ':', 'continue', 'current_task', '=', 'current_dag', '.', 'get_task', '(', 'task_id', ')', 'if', 'isinstance', '(', 'current_task', ',', 'subdagoperator', ')', ':', '# this works as a kind of integrity check', '# it creates missing dag runs for subdagoperators,', '# maybe this should be moved to dagrun.verify_integrity', 'drs', '=', '_create_dagruns', '(', 'current_task', '.', 'subdag', ',', 'execution_dates', '=', 'confirmed_dates', ',', 'state', '=', 'state', '.', 'running', ',', 'run_id_template', '=', 'backfilljob', '.', 'id_format_prefix', ')', 'for', 'dr', 'in', 'drs', ':', 'dr', '.', 'dag', '=', 'current_task', '.', 'subdag', 'dr', '.', 'verify_integrity', '(', ')', 'if', 'commit', ':', 'dr', '.', 'state', '=', 'state', 'session', '.', 'merge', '(', 'dr', ')', 'dags', '.', 'append', '(', 'current_task', '.', 'subdag', ')', 'sub_dag_ids', '.', 'append', '(', 'current_task', '.', 'subdag', '.', 'dag_id', ')', '# now look for the task instances that are affected', 'ti', '=', 'taskinstance', '# get all tasks of the main dag that will be affected by a state change', 'qry_dag', '=', 'session', '.', 'query', '(', 'ti', ')', '.', 'filter', '(', 'ti', '.', 'dag_id', '==', 'dag', '.', 'dag_id', ',', 'ti', '.', 'execution_date', '.', 'in_', '(', 'confirmed_dates', ')', ',', 'ti', '.', 'task_id', '.', 'in_', '(', 'task_ids', ')', ')', '.', 'filter', '(', 'or_', '(', 'ti', '.', 'state', '.', 'is_', '(', 'none', ')', ',', 'ti', '.', 'state', '!=', 'state', ')', ')', '# get *all* tasks of the sub dags', 'if', 'len', '(', 'sub_dag_ids', ')', '>', '0', ':', 'qry_sub_dag', '=', 'session', '.', 'query', '(', 'ti', ')', '.', 'filter', '(', 'ti', '.', 'dag_id', '.', 'in_', '(', 'sub_dag_ids', ')', ',', 'ti', '.', 'execution_date', '.', 'in_', '(', 'confirmed_dates', ')', ')', '.', 'filter', '(', 'or_', '(', 'ti', '.', 'state', '.', 'is_', '(', 'none', ')', ',', 'ti', '.', 'state', '!=', 'state', ')', ')', 'if', 'commit', ':', 'tis_altered', '=', 'qry_dag', '.', 'with_for_update', '(', ')', '.', 'all', '(', ')', 'if', 'len', '(', 'sub_dag_ids', ')', '>', '0', ':', 'tis_altered', '+=', 'qry_sub_dag', '.', 'with_for_update', '(', ')', '.', 'all', '(', ')', 'for', 'ti', 'in', 'tis_altered', ':', 'ti', '.', 'state', '=', 'state', 'else', ':', 'tis_altered', '=', 'qry_dag', '.', 'all', '(', ')', 'if', 'len', '(', 'sub_dag_ids', ')', '>', '0', ':', 'tis_altered', '+=', 'qry_sub_dag', '.', 'all', '(', ')', 'return', 'tis_altered']","def set_state ( task , execution_date , upstream = false , downstream = false , future = false , past = false , state = state . success , commit = false , session = none ) : assert timezone . is_localized ( execution_date ) assert task . dag is not none dag = task . dag latest_execution_date = dag . latest_execution_date assert latest_execution_date is not none # determine date range of dag runs and tasks to consider end_date = latest_execution_date if future else execution_date if 'start_date' in dag . default_args : start_date = dag . default_args [ 'start_date' ] elif dag . start_date : start_date = dag . start_date else : start_date = execution_date start_date = execution_date if not past else start_date if dag . schedule_interval == '@once' : dates = [ start_date ] else : dates = dag . date_range ( start_date = start_date , end_date = end_date ) # find relatives (siblings = downstream, parents = upstream) if needed task_ids = [ task . task_id ] if downstream : relatives = task . get_flat_relatives ( upstream = false ) task_ids += [ t . task_id for t in relatives ] if upstream : relatives = task . get_flat_relatives ( upstream = true ) task_ids += [ t . task_id for t in relatives ] # verify the integrity of the dag runs in case a task was added or removed # set the confirmed execution dates as they might be different # from what was provided confirmed_dates = [ ] drs = dagrun . find ( dag_id = dag . dag_id , execution_date = dates ) for dr in drs : dr . dag = dag dr . verify_integrity ( ) confirmed_dates . append ( dr . execution_date ) # go through subdagoperators and create dag runs. we will only work # within the scope of the subdag. we wont propagate to the parent dag, # but we will propagate from parent to subdag. dags = [ dag ] sub_dag_ids = [ ] while len ( dags ) > 0 : current_dag = dags . pop ( ) for task_id in task_ids : if not current_dag . has_task ( task_id ) : continue current_task = current_dag . get_task ( task_id ) if isinstance ( current_task , subdagoperator ) : # this works as a kind of integrity check # it creates missing dag runs for subdagoperators, # maybe this should be moved to dagrun.verify_integrity drs = _create_dagruns ( current_task . subdag , execution_dates = confirmed_dates , state = state . running , run_id_template = backfilljob . id_format_prefix ) for dr in drs : dr . dag = current_task . subdag dr . verify_integrity ( ) if commit : dr . state = state session . merge ( dr ) dags . append ( current_task . subdag ) sub_dag_ids . append ( current_task . subdag . dag_id ) # now look for the task instances that are affected ti = taskinstance # get all tasks of the main dag that will be affected by a state change qry_dag = session . query ( ti ) . filter ( ti . dag_id == dag . dag_id , ti . execution_date . in_ ( confirmed_dates ) , ti . task_id . in_ ( task_ids ) ) . filter ( or_ ( ti . state . is_ ( none ) , ti . state != state ) ) # get *all* tasks of the sub dags if len ( sub_dag_ids ) > 0 : qry_sub_dag = session . query ( ti ) . filter ( ti . dag_id . in_ ( sub_dag_ids ) , ti . execution_date . in_ ( confirmed_dates ) ) . filter ( or_ ( ti . state . is_ ( none ) , ti . state != state ) ) if commit : tis_altered = qry_dag . with_for_update ( ) . all ( ) if len ( sub_dag_ids ) > 0 : tis_altered += qry_sub_dag . with_for_update ( ) . all ( ) for ti in tis_altered : ti . state = state else : tis_altered = qry_dag . all ( ) if len ( sub_dag_ids ) > 0 : tis_altered += qry_sub_dag . all ( ) return tis_altered"
479,apache/airflow,airflow/api/common/experimental/mark_tasks.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/api/common/experimental/mark_tasks.py#L188-L207,"def _set_dag_run_state(dag_id, execution_date, state, session=None):
    """"""
    Helper method that set dag run state in the DB.
    :param dag_id: dag_id of target dag run
    :param execution_date: the execution date from which to start looking
    :param state: target state
    :param session: database session
    """"""
    DR = DagRun
    dr = session.query(DR).filter(
        DR.dag_id == dag_id,
        DR.execution_date == execution_date
    ).one()
    dr.state = state
    if state == State.RUNNING:
        dr.start_date = timezone.utcnow()
        dr.end_date = None
    else:
        dr.end_date = timezone.utcnow()
    session.merge(dr)","['def', '_set_dag_run_state', '(', 'dag_id', ',', 'execution_date', ',', 'state', ',', 'session', '=', 'None', ')', ':', 'DR', '=', 'DagRun', 'dr', '=', 'session', '.', 'query', '(', 'DR', ')', '.', 'filter', '(', 'DR', '.', 'dag_id', '==', 'dag_id', ',', 'DR', '.', 'execution_date', '==', 'execution_date', ')', '.', 'one', '(', ')', 'dr', '.', 'state', '=', 'state', 'if', 'state', '==', 'State', '.', 'RUNNING', ':', 'dr', '.', 'start_date', '=', 'timezone', '.', 'utcnow', '(', ')', 'dr', '.', 'end_date', '=', 'None', 'else', ':', 'dr', '.', 'end_date', '=', 'timezone', '.', 'utcnow', '(', ')', 'session', '.', 'merge', '(', 'dr', ')']","Helper method that set dag run state in the DB.
    :param dag_id: dag_id of target dag run
    :param execution_date: the execution date from which to start looking
    :param state: target state
    :param session: database session","['Helper', 'method', 'that', 'set', 'dag', 'run', 'state', 'in', 'the', 'DB', '.', ':', 'param', 'dag_id', ':', 'dag_id', 'of', 'target', 'dag', 'run', ':', 'param', 'execution_date', ':', 'the', 'execution', 'date', 'from', 'which', 'to', 'start', 'looking', ':', 'param', 'state', ':', 'target', 'state', ':', 'param', 'session', ':', 'database', 'session']",python,test,"['helper', 'method', 'that', 'set', 'dag', 'run', 'state', 'in', 'the', 'db', '.', ':', 'param', 'dag_id', ':', 'dag_id', 'of', 'target', 'dag', 'run', ':', 'param', 'execution_date', ':', 'the', 'execution', 'date', 'from', 'which', 'to', 'start', 'looking', ':', 'param', 'state', ':', 'target', 'state', ':', 'param', 'session', ':', 'database', 'session']",helper method that set dag run state in the db . : param dag_id : dag_id of target dag run : param execution_date : the execution date from which to start looking : param state : target state : param session : database session,"['def', '_set_dag_run_state', '(', 'dag_id', ',', 'execution_date', ',', 'state', ',', 'session', '=', 'none', ')', ':', 'dr', '=', 'dagrun', 'dr', '=', 'session', '.', 'query', '(', 'dr', ')', '.', 'filter', '(', 'dr', '.', 'dag_id', '==', 'dag_id', ',', 'dr', '.', 'execution_date', '==', 'execution_date', ')', '.', 'one', '(', ')', 'dr', '.', 'state', '=', 'state', 'if', 'state', '==', 'state', '.', 'running', ':', 'dr', '.', 'start_date', '=', 'timezone', '.', 'utcnow', '(', ')', 'dr', '.', 'end_date', '=', 'none', 'else', ':', 'dr', '.', 'end_date', '=', 'timezone', '.', 'utcnow', '(', ')', 'session', '.', 'merge', '(', 'dr', ')']","def _set_dag_run_state ( dag_id , execution_date , state , session = none ) : dr = dagrun dr = session . query ( dr ) . filter ( dr . dag_id == dag_id , dr . execution_date == execution_date ) . one ( ) dr . state = state if state == state . running : dr . start_date = timezone . utcnow ( ) dr . end_date = none else : dr . end_date = timezone . utcnow ( ) session . merge ( dr )"
480,apache/airflow,airflow/api/common/experimental/mark_tasks.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/api/common/experimental/mark_tasks.py#L211-L239,"def set_dag_run_state_to_success(dag, execution_date, commit=False, session=None):
    """"""
    Set the dag run for a specific execution date and its task instances
    to success.
    :param dag: the DAG of which to alter state
    :param execution_date: the execution date from which to start looking
    :param commit: commit DAG and tasks to be altered to the database
    :param session: database session
    :return: If commit is true, list of tasks that have been updated,
             otherwise list of tasks that will be updated
    :raises: AssertionError if dag or execution_date is invalid
    """"""
    res = []

    if not dag or not execution_date:
        return res

    # Mark the dag run to success.
    if commit:
        _set_dag_run_state(dag.dag_id, execution_date, State.SUCCESS, session)

    # Mark all task instances of the dag run to success.
    for task in dag.tasks:
        task.dag = dag
        new_state = set_state(task=task, execution_date=execution_date,
                              state=State.SUCCESS, commit=commit)
        res.extend(new_state)

    return res","['def', 'set_dag_run_state_to_success', '(', 'dag', ',', 'execution_date', ',', 'commit', '=', 'False', ',', 'session', '=', 'None', ')', ':', 'res', '=', '[', ']', 'if', 'not', 'dag', 'or', 'not', 'execution_date', ':', 'return', 'res', '# Mark the dag run to success.', 'if', 'commit', ':', '_set_dag_run_state', '(', 'dag', '.', 'dag_id', ',', 'execution_date', ',', 'State', '.', 'SUCCESS', ',', 'session', ')', '# Mark all task instances of the dag run to success.', 'for', 'task', 'in', 'dag', '.', 'tasks', ':', 'task', '.', 'dag', '=', 'dag', 'new_state', '=', 'set_state', '(', 'task', '=', 'task', ',', 'execution_date', '=', 'execution_date', ',', 'state', '=', 'State', '.', 'SUCCESS', ',', 'commit', '=', 'commit', ')', 'res', '.', 'extend', '(', 'new_state', ')', 'return', 'res']","Set the dag run for a specific execution date and its task instances
    to success.
    :param dag: the DAG of which to alter state
    :param execution_date: the execution date from which to start looking
    :param commit: commit DAG and tasks to be altered to the database
    :param session: database session
    :return: If commit is true, list of tasks that have been updated,
             otherwise list of tasks that will be updated
    :raises: AssertionError if dag or execution_date is invalid","['Set', 'the', 'dag', 'run', 'for', 'a', 'specific', 'execution', 'date', 'and', 'its', 'task', 'instances', 'to', 'success', '.', ':', 'param', 'dag', ':', 'the', 'DAG', 'of', 'which', 'to', 'alter', 'state', ':', 'param', 'execution_date', ':', 'the', 'execution', 'date', 'from', 'which', 'to', 'start', 'looking', ':', 'param', 'commit', ':', 'commit', 'DAG', 'and', 'tasks', 'to', 'be', 'altered', 'to', 'the', 'database', ':', 'param', 'session', ':', 'database', 'session', ':', 'return', ':', 'If', 'commit', 'is', 'true', 'list', 'of', 'tasks', 'that', 'have', 'been', 'updated', 'otherwise', 'list', 'of', 'tasks', 'that', 'will', 'be', 'updated', ':', 'raises', ':', 'AssertionError', 'if', 'dag', 'or', 'execution_date', 'is', 'invalid']",python,test,"['set', 'the', 'dag', 'run', 'for', 'a', 'specific', 'execution', 'date', 'and', 'its', 'task', 'instances', 'to', 'success', '.', ':', 'param', 'dag', ':', 'the', 'dag', 'of', 'which', 'to', 'alter', 'state', ':', 'param', 'execution_date', ':', 'the', 'execution', 'date', 'from', 'which', 'to', 'start', 'looking', ':', 'param', 'commit', ':', 'commit', 'dag', 'and', 'tasks', 'to', 'be', 'altered', 'to', 'the', 'database', ':', 'param', 'session', ':', 'database', 'session', ':', 'return', ':', 'if', 'commit', 'is', 'true', 'list', 'of', 'tasks', 'that', 'have', 'been', 'updated', 'otherwise', 'list', 'of', 'tasks', 'that', 'will', 'be', 'updated', ':', 'raises', ':', 'assertionerror', 'if', 'dag', 'or', 'execution_date', 'is', 'invalid']",set the dag run for a specific execution date and its task instances to success . : param dag : the dag of which to alter state : param execution_date : the execution date from which to start looking : param commit : commit dag and tasks to be altered to the database : param session : database session : return : if commit is true list of tasks that have been updated otherwise list of tasks that will be updated : raises : assertionerror if dag or execution_date is invalid,"['def', 'set_dag_run_state_to_success', '(', 'dag', ',', 'execution_date', ',', 'commit', '=', 'false', ',', 'session', '=', 'none', ')', ':', 'res', '=', '[', ']', 'if', 'not', 'dag', 'or', 'not', 'execution_date', ':', 'return', 'res', '# mark the dag run to success.', 'if', 'commit', ':', '_set_dag_run_state', '(', 'dag', '.', 'dag_id', ',', 'execution_date', ',', 'state', '.', 'success', ',', 'session', ')', '# mark all task instances of the dag run to success.', 'for', 'task', 'in', 'dag', '.', 'tasks', ':', 'task', '.', 'dag', '=', 'dag', 'new_state', '=', 'set_state', '(', 'task', '=', 'task', ',', 'execution_date', '=', 'execution_date', ',', 'state', '=', 'state', '.', 'success', ',', 'commit', '=', 'commit', ')', 'res', '.', 'extend', '(', 'new_state', ')', 'return', 'res']","def set_dag_run_state_to_success ( dag , execution_date , commit = false , session = none ) : res = [ ] if not dag or not execution_date : return res # mark the dag run to success. if commit : _set_dag_run_state ( dag . dag_id , execution_date , state . success , session ) # mark all task instances of the dag run to success. for task in dag . tasks : task . dag = dag new_state = set_state ( task = task , execution_date = execution_date , state = state . success , commit = commit ) res . extend ( new_state ) return res"
481,apache/airflow,airflow/api/common/experimental/mark_tasks.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/api/common/experimental/mark_tasks.py#L243-L280,"def set_dag_run_state_to_failed(dag, execution_date, commit=False, session=None):
    """"""
    Set the dag run for a specific execution date and its running task instances
    to failed.
    :param dag: the DAG of which to alter state
    :param execution_date: the execution date from which to start looking
    :param commit: commit DAG and tasks to be altered to the database
    :param session: database session
    :return: If commit is true, list of tasks that have been updated,
             otherwise list of tasks that will be updated
    :raises: AssertionError if dag or execution_date is invalid
    """"""
    res = []

    if not dag or not execution_date:
        return res

    # Mark the dag run to failed.
    if commit:
        _set_dag_run_state(dag.dag_id, execution_date, State.FAILED, session)

    # Mark only RUNNING task instances.
    TI = TaskInstance
    task_ids = [task.task_id for task in dag.tasks]
    tis = session.query(TI).filter(
        TI.dag_id == dag.dag_id,
        TI.execution_date == execution_date,
        TI.task_id.in_(task_ids)).filter(TI.state == State.RUNNING)
    task_ids_of_running_tis = [ti.task_id for ti in tis]
    for task in dag.tasks:
        if task.task_id not in task_ids_of_running_tis:
            continue
        task.dag = dag
        new_state = set_state(task=task, execution_date=execution_date,
                              state=State.FAILED, commit=commit)
        res.extend(new_state)

    return res","['def', 'set_dag_run_state_to_failed', '(', 'dag', ',', 'execution_date', ',', 'commit', '=', 'False', ',', 'session', '=', 'None', ')', ':', 'res', '=', '[', ']', 'if', 'not', 'dag', 'or', 'not', 'execution_date', ':', 'return', 'res', '# Mark the dag run to failed.', 'if', 'commit', ':', '_set_dag_run_state', '(', 'dag', '.', 'dag_id', ',', 'execution_date', ',', 'State', '.', 'FAILED', ',', 'session', ')', '# Mark only RUNNING task instances.', 'TI', '=', 'TaskInstance', 'task_ids', '=', '[', 'task', '.', 'task_id', 'for', 'task', 'in', 'dag', '.', 'tasks', ']', 'tis', '=', 'session', '.', 'query', '(', 'TI', ')', '.', 'filter', '(', 'TI', '.', 'dag_id', '==', 'dag', '.', 'dag_id', ',', 'TI', '.', 'execution_date', '==', 'execution_date', ',', 'TI', '.', 'task_id', '.', 'in_', '(', 'task_ids', ')', ')', '.', 'filter', '(', 'TI', '.', 'state', '==', 'State', '.', 'RUNNING', ')', 'task_ids_of_running_tis', '=', '[', 'ti', '.', 'task_id', 'for', 'ti', 'in', 'tis', ']', 'for', 'task', 'in', 'dag', '.', 'tasks', ':', 'if', 'task', '.', 'task_id', 'not', 'in', 'task_ids_of_running_tis', ':', 'continue', 'task', '.', 'dag', '=', 'dag', 'new_state', '=', 'set_state', '(', 'task', '=', 'task', ',', 'execution_date', '=', 'execution_date', ',', 'state', '=', 'State', '.', 'FAILED', ',', 'commit', '=', 'commit', ')', 'res', '.', 'extend', '(', 'new_state', ')', 'return', 'res']","Set the dag run for a specific execution date and its running task instances
    to failed.
    :param dag: the DAG of which to alter state
    :param execution_date: the execution date from which to start looking
    :param commit: commit DAG and tasks to be altered to the database
    :param session: database session
    :return: If commit is true, list of tasks that have been updated,
             otherwise list of tasks that will be updated
    :raises: AssertionError if dag or execution_date is invalid","['Set', 'the', 'dag', 'run', 'for', 'a', 'specific', 'execution', 'date', 'and', 'its', 'running', 'task', 'instances', 'to', 'failed', '.', ':', 'param', 'dag', ':', 'the', 'DAG', 'of', 'which', 'to', 'alter', 'state', ':', 'param', 'execution_date', ':', 'the', 'execution', 'date', 'from', 'which', 'to', 'start', 'looking', ':', 'param', 'commit', ':', 'commit', 'DAG', 'and', 'tasks', 'to', 'be', 'altered', 'to', 'the', 'database', ':', 'param', 'session', ':', 'database', 'session', ':', 'return', ':', 'If', 'commit', 'is', 'true', 'list', 'of', 'tasks', 'that', 'have', 'been', 'updated', 'otherwise', 'list', 'of', 'tasks', 'that', 'will', 'be', 'updated', ':', 'raises', ':', 'AssertionError', 'if', 'dag', 'or', 'execution_date', 'is', 'invalid']",python,test,"['set', 'the', 'dag', 'run', 'for', 'a', 'specific', 'execution', 'date', 'and', 'its', 'running', 'task', 'instances', 'to', 'failed', '.', ':', 'param', 'dag', ':', 'the', 'dag', 'of', 'which', 'to', 'alter', 'state', ':', 'param', 'execution_date', ':', 'the', 'execution', 'date', 'from', 'which', 'to', 'start', 'looking', ':', 'param', 'commit', ':', 'commit', 'dag', 'and', 'tasks', 'to', 'be', 'altered', 'to', 'the', 'database', ':', 'param', 'session', ':', 'database', 'session', ':', 'return', ':', 'if', 'commit', 'is', 'true', 'list', 'of', 'tasks', 'that', 'have', 'been', 'updated', 'otherwise', 'list', 'of', 'tasks', 'that', 'will', 'be', 'updated', ':', 'raises', ':', 'assertionerror', 'if', 'dag', 'or', 'execution_date', 'is', 'invalid']",set the dag run for a specific execution date and its running task instances to failed . : param dag : the dag of which to alter state : param execution_date : the execution date from which to start looking : param commit : commit dag and tasks to be altered to the database : param session : database session : return : if commit is true list of tasks that have been updated otherwise list of tasks that will be updated : raises : assertionerror if dag or execution_date is invalid,"['def', 'set_dag_run_state_to_failed', '(', 'dag', ',', 'execution_date', ',', 'commit', '=', 'false', ',', 'session', '=', 'none', ')', ':', 'res', '=', '[', ']', 'if', 'not', 'dag', 'or', 'not', 'execution_date', ':', 'return', 'res', '# mark the dag run to failed.', 'if', 'commit', ':', '_set_dag_run_state', '(', 'dag', '.', 'dag_id', ',', 'execution_date', ',', 'state', '.', 'failed', ',', 'session', ')', '# mark only running task instances.', 'ti', '=', 'taskinstance', 'task_ids', '=', '[', 'task', '.', 'task_id', 'for', 'task', 'in', 'dag', '.', 'tasks', ']', 'tis', '=', 'session', '.', 'query', '(', 'ti', ')', '.', 'filter', '(', 'ti', '.', 'dag_id', '==', 'dag', '.', 'dag_id', ',', 'ti', '.', 'execution_date', '==', 'execution_date', ',', 'ti', '.', 'task_id', '.', 'in_', '(', 'task_ids', ')', ')', '.', 'filter', '(', 'ti', '.', 'state', '==', 'state', '.', 'running', ')', 'task_ids_of_running_tis', '=', '[', 'ti', '.', 'task_id', 'for', 'ti', 'in', 'tis', ']', 'for', 'task', 'in', 'dag', '.', 'tasks', ':', 'if', 'task', '.', 'task_id', 'not', 'in', 'task_ids_of_running_tis', ':', 'continue', 'task', '.', 'dag', '=', 'dag', 'new_state', '=', 'set_state', '(', 'task', '=', 'task', ',', 'execution_date', '=', 'execution_date', ',', 'state', '=', 'state', '.', 'failed', ',', 'commit', '=', 'commit', ')', 'res', '.', 'extend', '(', 'new_state', ')', 'return', 'res']","def set_dag_run_state_to_failed ( dag , execution_date , commit = false , session = none ) : res = [ ] if not dag or not execution_date : return res # mark the dag run to failed. if commit : _set_dag_run_state ( dag . dag_id , execution_date , state . failed , session ) # mark only running task instances. ti = taskinstance task_ids = [ task . task_id for task in dag . tasks ] tis = session . query ( ti ) . filter ( ti . dag_id == dag . dag_id , ti . execution_date == execution_date , ti . task_id . in_ ( task_ids ) ) . filter ( ti . state == state . running ) task_ids_of_running_tis = [ ti . task_id for ti in tis ] for task in dag . tasks : if task . task_id not in task_ids_of_running_tis : continue task . dag = dag new_state = set_state ( task = task , execution_date = execution_date , state = state . failed , commit = commit ) res . extend ( new_state ) return res"
482,apache/airflow,airflow/api/common/experimental/mark_tasks.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/api/common/experimental/mark_tasks.py#L284-L303,"def set_dag_run_state_to_running(dag, execution_date, commit=False, session=None):
    """"""
    Set the dag run for a specific execution date to running.
    :param dag: the DAG of which to alter state
    :param execution_date: the execution date from which to start looking
    :param commit: commit DAG and tasks to be altered to the database
    :param session: database session
    :return: If commit is true, list of tasks that have been updated,
             otherwise list of tasks that will be updated
    """"""
    res = []
    if not dag or not execution_date:
        return res

    # Mark the dag run to running.
    if commit:
        _set_dag_run_state(dag.dag_id, execution_date, State.RUNNING, session)

    # To keep the return type consistent with the other similar functions.
    return res","['def', 'set_dag_run_state_to_running', '(', 'dag', ',', 'execution_date', ',', 'commit', '=', 'False', ',', 'session', '=', 'None', ')', ':', 'res', '=', '[', ']', 'if', 'not', 'dag', 'or', 'not', 'execution_date', ':', 'return', 'res', '# Mark the dag run to running.', 'if', 'commit', ':', '_set_dag_run_state', '(', 'dag', '.', 'dag_id', ',', 'execution_date', ',', 'State', '.', 'RUNNING', ',', 'session', ')', '# To keep the return type consistent with the other similar functions.', 'return', 'res']","Set the dag run for a specific execution date to running.
    :param dag: the DAG of which to alter state
    :param execution_date: the execution date from which to start looking
    :param commit: commit DAG and tasks to be altered to the database
    :param session: database session
    :return: If commit is true, list of tasks that have been updated,
             otherwise list of tasks that will be updated","['Set', 'the', 'dag', 'run', 'for', 'a', 'specific', 'execution', 'date', 'to', 'running', '.', ':', 'param', 'dag', ':', 'the', 'DAG', 'of', 'which', 'to', 'alter', 'state', ':', 'param', 'execution_date', ':', 'the', 'execution', 'date', 'from', 'which', 'to', 'start', 'looking', ':', 'param', 'commit', ':', 'commit', 'DAG', 'and', 'tasks', 'to', 'be', 'altered', 'to', 'the', 'database', ':', 'param', 'session', ':', 'database', 'session', ':', 'return', ':', 'If', 'commit', 'is', 'true', 'list', 'of', 'tasks', 'that', 'have', 'been', 'updated', 'otherwise', 'list', 'of', 'tasks', 'that', 'will', 'be', 'updated']",python,test,"['set', 'the', 'dag', 'run', 'for', 'a', 'specific', 'execution', 'date', 'to', 'running', '.', ':', 'param', 'dag', ':', 'the', 'dag', 'of', 'which', 'to', 'alter', 'state', ':', 'param', 'execution_date', ':', 'the', 'execution', 'date', 'from', 'which', 'to', 'start', 'looking', ':', 'param', 'commit', ':', 'commit', 'dag', 'and', 'tasks', 'to', 'be', 'altered', 'to', 'the', 'database', ':', 'param', 'session', ':', 'database', 'session', ':', 'return', ':', 'if', 'commit', 'is', 'true', 'list', 'of', 'tasks', 'that', 'have', 'been', 'updated', 'otherwise', 'list', 'of', 'tasks', 'that', 'will', 'be', 'updated']",set the dag run for a specific execution date to running . : param dag : the dag of which to alter state : param execution_date : the execution date from which to start looking : param commit : commit dag and tasks to be altered to the database : param session : database session : return : if commit is true list of tasks that have been updated otherwise list of tasks that will be updated,"['def', 'set_dag_run_state_to_running', '(', 'dag', ',', 'execution_date', ',', 'commit', '=', 'false', ',', 'session', '=', 'none', ')', ':', 'res', '=', '[', ']', 'if', 'not', 'dag', 'or', 'not', 'execution_date', ':', 'return', 'res', '# mark the dag run to running.', 'if', 'commit', ':', '_set_dag_run_state', '(', 'dag', '.', 'dag_id', ',', 'execution_date', ',', 'state', '.', 'running', ',', 'session', ')', '# to keep the return type consistent with the other similar functions.', 'return', 'res']","def set_dag_run_state_to_running ( dag , execution_date , commit = false , session = none ) : res = [ ] if not dag or not execution_date : return res # mark the dag run to running. if commit : _set_dag_run_state ( dag . dag_id , execution_date , state . running , session ) # to keep the return type consistent with the other similar functions. return res"
483,apache/airflow,setup.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/setup.py#L102-L128,"def git_version(version):
    """"""
    Return a version to identify the state of the underlying git repo. The version will
    indicate whether the head of the current git-backed working directory is tied to a
    release tag or not : it will indicate the former with a 'release:{version}' prefix
    and the latter with a 'dev0' prefix. Following the prefix will be a sha of the current
    branch head. Finally, a ""dirty"" suffix is appended to indicate that uncommitted
    changes are present.
    """"""
    repo = None
    try:
        import git
        repo = git.Repo('.git')
    except ImportError:
        logger.warning('gitpython not found: Cannot compute the git version.')
        return ''
    except Exception as e:
        logger.warning('Cannot compute the git version. {}'.format(e))
        return ''
    if repo:
        sha = repo.head.commit.hexsha
        if repo.is_dirty():
            return '.dev0+{sha}.dirty'.format(sha=sha)
        # commit is clean
        return '.release:{version}+{sha}'.format(version=version, sha=sha)
    else:
        return 'no_git_version'","['def', 'git_version', '(', 'version', ')', ':', 'repo', '=', 'None', 'try', ':', 'import', 'git', 'repo', '=', 'git', '.', 'Repo', '(', ""'.git'"", ')', 'except', 'ImportError', ':', 'logger', '.', 'warning', '(', ""'gitpython not found: Cannot compute the git version.'"", ')', 'return', ""''"", 'except', 'Exception', 'as', 'e', ':', 'logger', '.', 'warning', '(', ""'Cannot compute the git version. {}'"", '.', 'format', '(', 'e', ')', ')', 'return', ""''"", 'if', 'repo', ':', 'sha', '=', 'repo', '.', 'head', '.', 'commit', '.', 'hexsha', 'if', 'repo', '.', 'is_dirty', '(', ')', ':', 'return', ""'.dev0+{sha}.dirty'"", '.', 'format', '(', 'sha', '=', 'sha', ')', '# commit is clean', 'return', ""'.release:{version}+{sha}'"", '.', 'format', '(', 'version', '=', 'version', ',', 'sha', '=', 'sha', ')', 'else', ':', 'return', ""'no_git_version'""]","Return a version to identify the state of the underlying git repo. The version will
    indicate whether the head of the current git-backed working directory is tied to a
    release tag or not : it will indicate the former with a 'release:{version}' prefix
    and the latter with a 'dev0' prefix. Following the prefix will be a sha of the current
    branch head. Finally, a ""dirty"" suffix is appended to indicate that uncommitted
    changes are present.","['Return', 'a', 'version', 'to', 'identify', 'the', 'state', 'of', 'the', 'underlying', 'git', 'repo', '.', 'The', 'version', 'will', 'indicate', 'whether', 'the', 'head', 'of', 'the', 'current', 'git', '-', 'backed', 'working', 'directory', 'is', 'tied', 'to', 'a', 'release', 'tag', 'or', 'not', ':', 'it', 'will', 'indicate', 'the', 'former', 'with', 'a', 'release', ':', '{', 'version', '}', 'prefix', 'and', 'the', 'latter', 'with', 'a', 'dev0', 'prefix', '.', 'Following', 'the', 'prefix', 'will', 'be', 'a', 'sha', 'of', 'the', 'current', 'branch', 'head', '.', 'Finally', 'a', 'dirty', 'suffix', 'is', 'appended', 'to', 'indicate', 'that', 'uncommitted', 'changes', 'are', 'present', '.']",python,test,"['return', 'a', 'version', 'to', 'identify', 'the', 'state', 'of', 'the', 'underlying', 'git', 'repo', '.', 'the', 'version', 'will', 'indicate', 'whether', 'the', 'head', 'of', 'the', 'current', 'git', '-', 'backed', 'working', 'directory', 'is', 'tied', 'to', 'a', 'release', 'tag', 'or', 'not', ':', 'it', 'will', 'indicate', 'the', 'former', 'with', 'a', 'release', ':', '{', 'version', '}', 'prefix', 'and', 'the', 'latter', 'with', 'a', 'dev0', 'prefix', '.', 'following', 'the', 'prefix', 'will', 'be', 'a', 'sha', 'of', 'the', 'current', 'branch', 'head', '.', 'finally', 'a', 'dirty', 'suffix', 'is', 'appended', 'to', 'indicate', 'that', 'uncommitted', 'changes', 'are', 'present', '.']",return a version to identify the state of the underlying git repo . the version will indicate whether the head of the current git - backed working directory is tied to a release tag or not : it will indicate the former with a release : { version } prefix and the latter with a dev0 prefix . following the prefix will be a sha of the current branch head . finally a dirty suffix is appended to indicate that uncommitted changes are present .,"['def', 'git_version', '(', 'version', ')', ':', 'repo', '=', 'none', 'try', ':', 'import', 'git', 'repo', '=', 'git', '.', 'repo', '(', ""'.git'"", ')', 'except', 'importerror', ':', 'logger', '.', 'warning', '(', ""'gitpython not found: cannot compute the git version.'"", ')', 'return', ""''"", 'except', 'exception', 'as', 'e', ':', 'logger', '.', 'warning', '(', ""'cannot compute the git version. {}'"", '.', 'format', '(', 'e', ')', ')', 'return', ""''"", 'if', 'repo', ':', 'sha', '=', 'repo', '.', 'head', '.', 'commit', '.', 'hexsha', 'if', 'repo', '.', 'is_dirty', '(', ')', ':', 'return', ""'.dev0+{sha}.dirty'"", '.', 'format', '(', 'sha', '=', 'sha', ')', '# commit is clean', 'return', ""'.release:{version}+{sha}'"", '.', 'format', '(', 'version', '=', 'version', ',', 'sha', '=', 'sha', ')', 'else', ':', 'return', ""'no_git_version'""]","def git_version ( version ) : repo = none try : import git repo = git . repo ( '.git' ) except importerror : logger . warning ( 'gitpython not found: cannot compute the git version.' ) return '' except exception as e : logger . warning ( 'cannot compute the git version. {}' . format ( e ) ) return '' if repo : sha = repo . head . commit . hexsha if repo . is_dirty ( ) : return '.dev0+{sha}.dirty' . format ( sha = sha ) # commit is clean return '.release:{version}+{sha}' . format ( version = version , sha = sha ) else : return 'no_git_version'"
484,apache/airflow,airflow/contrib/operators/discord_webhook_operator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/operators/discord_webhook_operator.py#L85-L98,"def execute(self, context):
        """"""
        Call the DiscordWebhookHook to post message
        """"""
        self.hook = DiscordWebhookHook(
            self.http_conn_id,
            self.webhook_endpoint,
            self.message,
            self.username,
            self.avatar_url,
            self.tts,
            self.proxy
        )
        self.hook.execute()","['def', 'execute', '(', 'self', ',', 'context', ')', ':', 'self', '.', 'hook', '=', 'DiscordWebhookHook', '(', 'self', '.', 'http_conn_id', ',', 'self', '.', 'webhook_endpoint', ',', 'self', '.', 'message', ',', 'self', '.', 'username', ',', 'self', '.', 'avatar_url', ',', 'self', '.', 'tts', ',', 'self', '.', 'proxy', ')', 'self', '.', 'hook', '.', 'execute', '(', ')']",Call the DiscordWebhookHook to post message,"['Call', 'the', 'DiscordWebhookHook', 'to', 'post', 'message']",python,test,"['call', 'the', 'discordwebhookhook', 'to', 'post', 'message']",call the discordwebhookhook to post message,"['def', 'execute', '(', 'self', ',', 'context', ')', ':', 'self', '.', 'hook', '=', 'discordwebhookhook', '(', 'self', '.', 'http_conn_id', ',', 'self', '.', 'webhook_endpoint', ',', 'self', '.', 'message', ',', 'self', '.', 'username', ',', 'self', '.', 'avatar_url', ',', 'self', '.', 'tts', ',', 'self', '.', 'proxy', ')', 'self', '.', 'hook', '.', 'execute', '(', ')']","def execute ( self , context ) : self . hook = discordwebhookhook ( self . http_conn_id , self . webhook_endpoint , self . message , self . username , self . avatar_url , self . tts , self . proxy ) self . hook . execute ( )"
485,apache/airflow,airflow/contrib/utils/gcp_field_validator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/utils/gcp_field_validator.py#L308-L404,"def _validate_field(self, validation_spec, dictionary_to_validate, parent=None,
                        force_optional=False):
        """"""
        Validates if field is OK.

        :param validation_spec: specification of the field
        :type validation_spec: dict
        :param dictionary_to_validate: dictionary where the field should be present
        :type dictionary_to_validate: dict
        :param parent: full path of parent field
        :type parent: str
        :param force_optional: forces the field to be optional
            (all union fields have force_optional set to True)
        :type force_optional: bool
        :return: True if the field is present
        """"""
        field_name = validation_spec['name']
        field_type = validation_spec.get('type')
        optional = validation_spec.get('optional')
        regexp = validation_spec.get('regexp')
        allow_empty = validation_spec.get('allow_empty')
        children_validation_specs = validation_spec.get('fields')
        required_api_version = validation_spec.get('api_version')
        custom_validation = validation_spec.get('custom_validation')

        full_field_path = self._get_field_name_with_parent(field_name=field_name,
                                                           parent=parent)
        if required_api_version and required_api_version != self._api_version:
            self.log.debug(
                ""Skipping validation of the field '%s' for API version '%s' ""
                ""as it is only valid for API version '%s'"",
                field_name, self._api_version, required_api_version)
            return False
        value = dictionary_to_validate.get(field_name)

        if (optional or force_optional) and value is None:
            self.log.debug(""The optional field '%s' is missing. That's perfectly OK."", full_field_path)
            return False

        # Certainly down from here the field is present (value is not None)
        # so we should only return True from now on

        self._sanity_checks(children_validation_specs=children_validation_specs,
                            field_type=field_type,
                            full_field_path=full_field_path,
                            regexp=regexp,
                            allow_empty=allow_empty,
                            custom_validation=custom_validation,
                            value=value)

        if allow_empty is False:
            self._validate_is_empty(full_field_path, value)
        if regexp:
            self._validate_regexp(full_field_path, regexp, value)
        elif field_type == 'dict':
            if not isinstance(value, dict):
                raise GcpFieldValidationException(
                    ""The field '{}' should be of dictionary type according to the ""
                    ""specification '{}' but it is '{}'"".
                    format(full_field_path, validation_spec, value))
            if children_validation_specs is None:
                self.log.debug(
                    ""The dict field '%s' has no nested fields defined in the ""
                    ""specification '%s'. That's perfectly ok - it's content will ""
                    ""not be validated."", full_field_path, validation_spec)
            else:
                self._validate_dict(children_validation_specs, full_field_path, value)
        elif field_type == 'union':
            if not children_validation_specs:
                raise GcpValidationSpecificationException(
                    ""The union field '%s' has no nested fields ""
                    ""defined in specification '%s'. Unions should have at least one ""
                    ""nested field defined."", full_field_path, validation_spec)
            self._validate_union(children_validation_specs, full_field_path,
                                 dictionary_to_validate)
        elif field_type == 'list':
            if not isinstance(value, list):
                raise GcpFieldValidationException(
                    ""The field '{}' should be of list type according to the ""
                    ""specification '{}' but it is '{}'"".
                    format(full_field_path, validation_spec, value))
        elif custom_validation:
            try:
                custom_validation(value)
            except Exception as e:
                raise GcpFieldValidationException(
                    ""Error while validating custom field '{}' specified by '{}': '{}'"".
                    format(full_field_path, validation_spec, e))
        elif field_type is None:
            self.log.debug(""The type of field '%s' is not specified in '%s'. ""
                           ""Not validating its content."", full_field_path, validation_spec)
        else:
            raise GcpValidationSpecificationException(
                ""The field '{}' is of type '{}' in specification '{}'.""
                ""This type is unknown to validation!"".format(
                    full_field_path, field_type, validation_spec))
        return True","['def', '_validate_field', '(', 'self', ',', 'validation_spec', ',', 'dictionary_to_validate', ',', 'parent', '=', 'None', ',', 'force_optional', '=', 'False', ')', ':', 'field_name', '=', 'validation_spec', '[', ""'name'"", ']', 'field_type', '=', 'validation_spec', '.', 'get', '(', ""'type'"", ')', 'optional', '=', 'validation_spec', '.', 'get', '(', ""'optional'"", ')', 'regexp', '=', 'validation_spec', '.', 'get', '(', ""'regexp'"", ')', 'allow_empty', '=', 'validation_spec', '.', 'get', '(', ""'allow_empty'"", ')', 'children_validation_specs', '=', 'validation_spec', '.', 'get', '(', ""'fields'"", ')', 'required_api_version', '=', 'validation_spec', '.', 'get', '(', ""'api_version'"", ')', 'custom_validation', '=', 'validation_spec', '.', 'get', '(', ""'custom_validation'"", ')', 'full_field_path', '=', 'self', '.', '_get_field_name_with_parent', '(', 'field_name', '=', 'field_name', ',', 'parent', '=', 'parent', ')', 'if', 'required_api_version', 'and', 'required_api_version', '!=', 'self', '.', '_api_version', ':', 'self', '.', 'log', '.', 'debug', '(', '""Skipping validation of the field \'%s\' for API version \'%s\' ""', '""as it is only valid for API version \'%s\'""', ',', 'field_name', ',', 'self', '.', '_api_version', ',', 'required_api_version', ')', 'return', 'False', 'value', '=', 'dictionary_to_validate', '.', 'get', '(', 'field_name', ')', 'if', '(', 'optional', 'or', 'force_optional', ')', 'and', 'value', 'is', 'None', ':', 'self', '.', 'log', '.', 'debug', '(', '""The optional field \'%s\' is missing. That\'s perfectly OK.""', ',', 'full_field_path', ')', 'return', 'False', '# Certainly down from here the field is present (value is not None)', '# so we should only return True from now on', 'self', '.', '_sanity_checks', '(', 'children_validation_specs', '=', 'children_validation_specs', ',', 'field_type', '=', 'field_type', ',', 'full_field_path', '=', 'full_field_path', ',', 'regexp', '=', 'regexp', ',', 'allow_empty', '=', 'allow_empty', ',', 'custom_validation', '=', 'custom_validation', ',', 'value', '=', 'value', ')', 'if', 'allow_empty', 'is', 'False', ':', 'self', '.', '_validate_is_empty', '(', 'full_field_path', ',', 'value', ')', 'if', 'regexp', ':', 'self', '.', '_validate_regexp', '(', 'full_field_path', ',', 'regexp', ',', 'value', ')', 'elif', 'field_type', '==', ""'dict'"", ':', 'if', 'not', 'isinstance', '(', 'value', ',', 'dict', ')', ':', 'raise', 'GcpFieldValidationException', '(', '""The field \'{}\' should be of dictionary type according to the ""', '""specification \'{}\' but it is \'{}\'""', '.', 'format', '(', 'full_field_path', ',', 'validation_spec', ',', 'value', ')', ')', 'if', 'children_validation_specs', 'is', 'None', ':', 'self', '.', 'log', '.', 'debug', '(', '""The dict field \'%s\' has no nested fields defined in the ""', '""specification \'%s\'. That\'s perfectly ok - it\'s content will ""', '""not be validated.""', ',', 'full_field_path', ',', 'validation_spec', ')', 'else', ':', 'self', '.', '_validate_dict', '(', 'children_validation_specs', ',', 'full_field_path', ',', 'value', ')', 'elif', 'field_type', '==', ""'union'"", ':', 'if', 'not', 'children_validation_specs', ':', 'raise', 'GcpValidationSpecificationException', '(', '""The union field \'%s\' has no nested fields ""', '""defined in specification \'%s\'. Unions should have at least one ""', '""nested field defined.""', ',', 'full_field_path', ',', 'validation_spec', ')', 'self', '.', '_validate_union', '(', 'children_validation_specs', ',', 'full_field_path', ',', 'dictionary_to_validate', ')', 'elif', 'field_type', '==', ""'list'"", ':', 'if', 'not', 'isinstance', '(', 'value', ',', 'list', ')', ':', 'raise', 'GcpFieldValidationException', '(', '""The field \'{}\' should be of list type according to the ""', '""specification \'{}\' but it is \'{}\'""', '.', 'format', '(', 'full_field_path', ',', 'validation_spec', ',', 'value', ')', ')', 'elif', 'custom_validation', ':', 'try', ':', 'custom_validation', '(', 'value', ')', 'except', 'Exception', 'as', 'e', ':', 'raise', 'GcpFieldValidationException', '(', '""Error while validating custom field \'{}\' specified by \'{}\': \'{}\'""', '.', 'format', '(', 'full_field_path', ',', 'validation_spec', ',', 'e', ')', ')', 'elif', 'field_type', 'is', 'None', ':', 'self', '.', 'log', '.', 'debug', '(', '""The type of field \'%s\' is not specified in \'%s\'. ""', '""Not validating its content.""', ',', 'full_field_path', ',', 'validation_spec', ')', 'else', ':', 'raise', 'GcpValidationSpecificationException', '(', '""The field \'{}\' is of type \'{}\' in specification \'{}\'.""', '""This type is unknown to validation!""', '.', 'format', '(', 'full_field_path', ',', 'field_type', ',', 'validation_spec', ')', ')', 'return', 'True']","Validates if field is OK.

        :param validation_spec: specification of the field
        :type validation_spec: dict
        :param dictionary_to_validate: dictionary where the field should be present
        :type dictionary_to_validate: dict
        :param parent: full path of parent field
        :type parent: str
        :param force_optional: forces the field to be optional
            (all union fields have force_optional set to True)
        :type force_optional: bool
        :return: True if the field is present","['Validates', 'if', 'field', 'is', 'OK', '.']",python,test,"['validates', 'if', 'field', 'is', 'ok', '.']",validates if field is ok .,"['def', '_validate_field', '(', 'self', ',', 'validation_spec', ',', 'dictionary_to_validate', ',', 'parent', '=', 'none', ',', 'force_optional', '=', 'false', ')', ':', 'field_name', '=', 'validation_spec', '[', ""'name'"", ']', 'field_type', '=', 'validation_spec', '.', 'get', '(', ""'type'"", ')', 'optional', '=', 'validation_spec', '.', 'get', '(', ""'optional'"", ')', 'regexp', '=', 'validation_spec', '.', 'get', '(', ""'regexp'"", ')', 'allow_empty', '=', 'validation_spec', '.', 'get', '(', ""'allow_empty'"", ')', 'children_validation_specs', '=', 'validation_spec', '.', 'get', '(', ""'fields'"", ')', 'required_api_version', '=', 'validation_spec', '.', 'get', '(', ""'api_version'"", ')', 'custom_validation', '=', 'validation_spec', '.', 'get', '(', ""'custom_validation'"", ')', 'full_field_path', '=', 'self', '.', '_get_field_name_with_parent', '(', 'field_name', '=', 'field_name', ',', 'parent', '=', 'parent', ')', 'if', 'required_api_version', 'and', 'required_api_version', '!=', 'self', '.', '_api_version', ':', 'self', '.', 'log', '.', 'debug', '(', '""skipping validation of the field \'%s\' for api version \'%s\' ""', '""as it is only valid for api version \'%s\'""', ',', 'field_name', ',', 'self', '.', '_api_version', ',', 'required_api_version', ')', 'return', 'false', 'value', '=', 'dictionary_to_validate', '.', 'get', '(', 'field_name', ')', 'if', '(', 'optional', 'or', 'force_optional', ')', 'and', 'value', 'is', 'none', ':', 'self', '.', 'log', '.', 'debug', '(', '""the optional field \'%s\' is missing. that\'s perfectly ok.""', ',', 'full_field_path', ')', 'return', 'false', '# certainly down from here the field is present (value is not none)', '# so we should only return true from now on', 'self', '.', '_sanity_checks', '(', 'children_validation_specs', '=', 'children_validation_specs', ',', 'field_type', '=', 'field_type', ',', 'full_field_path', '=', 'full_field_path', ',', 'regexp', '=', 'regexp', ',', 'allow_empty', '=', 'allow_empty', ',', 'custom_validation', '=', 'custom_validation', ',', 'value', '=', 'value', ')', 'if', 'allow_empty', 'is', 'false', ':', 'self', '.', '_validate_is_empty', '(', 'full_field_path', ',', 'value', ')', 'if', 'regexp', ':', 'self', '.', '_validate_regexp', '(', 'full_field_path', ',', 'regexp', ',', 'value', ')', 'elif', 'field_type', '==', ""'dict'"", ':', 'if', 'not', 'isinstance', '(', 'value', ',', 'dict', ')', ':', 'raise', 'gcpfieldvalidationexception', '(', '""the field \'{}\' should be of dictionary type according to the ""', '""specification \'{}\' but it is \'{}\'""', '.', 'format', '(', 'full_field_path', ',', 'validation_spec', ',', 'value', ')', ')', 'if', 'children_validation_specs', 'is', 'none', ':', 'self', '.', 'log', '.', 'debug', '(', '""the dict field \'%s\' has no nested fields defined in the ""', '""specification \'%s\'. that\'s perfectly ok - it\'s content will ""', '""not be validated.""', ',', 'full_field_path', ',', 'validation_spec', ')', 'else', ':', 'self', '.', '_validate_dict', '(', 'children_validation_specs', ',', 'full_field_path', ',', 'value', ')', 'elif', 'field_type', '==', ""'union'"", ':', 'if', 'not', 'children_validation_specs', ':', 'raise', 'gcpvalidationspecificationexception', '(', '""the union field \'%s\' has no nested fields ""', '""defined in specification \'%s\'. unions should have at least one ""', '""nested field defined.""', ',', 'full_field_path', ',', 'validation_spec', ')', 'self', '.', '_validate_union', '(', 'children_validation_specs', ',', 'full_field_path', ',', 'dictionary_to_validate', ')', 'elif', 'field_type', '==', ""'list'"", ':', 'if', 'not', 'isinstance', '(', 'value', ',', 'list', ')', ':', 'raise', 'gcpfieldvalidationexception', '(', '""the field \'{}\' should be of list type according to the ""', '""specification \'{}\' but it is \'{}\'""', '.', 'format', '(', 'full_field_path', ',', 'validation_spec', ',', 'value', ')', ')', 'elif', 'custom_validation', ':', 'try', ':', 'custom_validation', '(', 'value', ')', 'except', 'exception', 'as', 'e', ':', 'raise', 'gcpfieldvalidationexception', '(', '""error while validating custom field \'{}\' specified by \'{}\': \'{}\'""', '.', 'format', '(', 'full_field_path', ',', 'validation_spec', ',', 'e', ')', ')', 'elif', 'field_type', 'is', 'none', ':', 'self', '.', 'log', '.', 'debug', '(', '""the type of field \'%s\' is not specified in \'%s\'. ""', '""not validating its content.""', ',', 'full_field_path', ',', 'validation_spec', ')', 'else', ':', 'raise', 'gcpvalidationspecificationexception', '(', '""the field \'{}\' is of type \'{}\' in specification \'{}\'.""', '""this type is unknown to validation!""', '.', 'format', '(', 'full_field_path', ',', 'field_type', ',', 'validation_spec', ')', ')', 'return', 'true']","def _validate_field ( self , validation_spec , dictionary_to_validate , parent = none , force_optional = false ) : field_name = validation_spec [ 'name' ] field_type = validation_spec . get ( 'type' ) optional = validation_spec . get ( 'optional' ) regexp = validation_spec . get ( 'regexp' ) allow_empty = validation_spec . get ( 'allow_empty' ) children_validation_specs = validation_spec . get ( 'fields' ) required_api_version = validation_spec . get ( 'api_version' ) custom_validation = validation_spec . get ( 'custom_validation' ) full_field_path = self . _get_field_name_with_parent ( field_name = field_name , parent = parent ) if required_api_version and required_api_version != self . _api_version : self . log . debug ( ""skipping validation of the field '%s' for api version '%s' "" ""as it is only valid for api version '%s'"" , field_name , self . _api_version , required_api_version ) return false value = dictionary_to_validate . get ( field_name ) if ( optional or force_optional ) and value is none : self . log . debug ( ""the optional field '%s' is missing. that's perfectly ok."" , full_field_path ) return false # certainly down from here the field is present (value is not none) # so we should only return true from now on self . _sanity_checks ( children_validation_specs = children_validation_specs , field_type = field_type , full_field_path = full_field_path , regexp = regexp , allow_empty = allow_empty , custom_validation = custom_validation , value = value ) if allow_empty is false : self . _validate_is_empty ( full_field_path , value ) if regexp : self . _validate_regexp ( full_field_path , regexp , value ) elif field_type == 'dict' : if not isinstance ( value , dict ) : raise gcpfieldvalidationexception ( ""the field '{}' should be of dictionary type according to the "" ""specification '{}' but it is '{}'"" . format ( full_field_path , validation_spec , value ) ) if children_validation_specs is none : self . log . debug ( ""the dict field '%s' has no nested fields defined in the "" ""specification '%s'. that's perfectly ok - it's content will "" ""not be validated."" , full_field_path , validation_spec ) else : self . _validate_dict ( children_validation_specs , full_field_path , value ) elif field_type == 'union' : if not children_validation_specs : raise gcpvalidationspecificationexception ( ""the union field '%s' has no nested fields "" ""defined in specification '%s'. unions should have at least one "" ""nested field defined."" , full_field_path , validation_spec ) self . _validate_union ( children_validation_specs , full_field_path , dictionary_to_validate ) elif field_type == 'list' : if not isinstance ( value , list ) : raise gcpfieldvalidationexception ( ""the field '{}' should be of list type according to the "" ""specification '{}' but it is '{}'"" . format ( full_field_path , validation_spec , value ) ) elif custom_validation : try : custom_validation ( value ) except exception as e : raise gcpfieldvalidationexception ( ""error while validating custom field '{}' specified by '{}': '{}'"" . format ( full_field_path , validation_spec , e ) ) elif field_type is none : self . log . debug ( ""the type of field '%s' is not specified in '%s'. "" ""not validating its content."" , full_field_path , validation_spec ) else : raise gcpvalidationspecificationexception ( ""the field '{}' is of type '{}' in specification '{}'."" ""this type is unknown to validation!"" . format ( full_field_path , field_type , validation_spec ) ) return true"
486,apache/airflow,airflow/contrib/utils/gcp_field_validator.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/utils/gcp_field_validator.py#L406-L444,"def validate(self, body_to_validate):
        """"""
        Validates if the body (dictionary) follows specification that the validator was
        instantiated with. Raises ValidationSpecificationException or
        ValidationFieldException in case of problems with specification or the
        body not conforming to the specification respectively.

        :param body_to_validate: body that must follow the specification
        :type body_to_validate: dict
        :return: None
        """"""
        try:
            for validation_spec in self._validation_specs:
                self._validate_field(validation_spec=validation_spec,
                                     dictionary_to_validate=body_to_validate)
        except GcpFieldValidationException as e:
            raise GcpFieldValidationException(
                ""There was an error when validating: body '{}': '{}'"".
                format(body_to_validate, e))
        all_field_names = [spec['name'] for spec in self._validation_specs
                           if spec.get('type') != 'union' and
                           spec.get('api_version') != self._api_version]
        all_union_fields = [spec for spec in self._validation_specs
                            if spec.get('type') == 'union']
        for union_field in all_union_fields:
            all_field_names.extend(
                [nested_union_spec['name'] for nested_union_spec in union_field['fields']
                 if nested_union_spec.get('type') != 'union' and
                 nested_union_spec.get('api_version') != self._api_version])
        for field_name in body_to_validate.keys():
            if field_name not in all_field_names:
                self.log.warning(
                    ""The field '%s' is in the body, but is not specified in the ""
                    ""validation specification '%s'. ""
                    ""This might be because you are using newer API version and ""
                    ""new field names defined for that version. Then the warning ""
                    ""can be safely ignored, or you might want to upgrade the operator""
                    ""to the version that supports the new API version."",
                    field_name, self._validation_specs)","['def', 'validate', '(', 'self', ',', 'body_to_validate', ')', ':', 'try', ':', 'for', 'validation_spec', 'in', 'self', '.', '_validation_specs', ':', 'self', '.', '_validate_field', '(', 'validation_spec', '=', 'validation_spec', ',', 'dictionary_to_validate', '=', 'body_to_validate', ')', 'except', 'GcpFieldValidationException', 'as', 'e', ':', 'raise', 'GcpFieldValidationException', '(', '""There was an error when validating: body \'{}\': \'{}\'""', '.', 'format', '(', 'body_to_validate', ',', 'e', ')', ')', 'all_field_names', '=', '[', 'spec', '[', ""'name'"", ']', 'for', 'spec', 'in', 'self', '.', '_validation_specs', 'if', 'spec', '.', 'get', '(', ""'type'"", ')', '!=', ""'union'"", 'and', 'spec', '.', 'get', '(', ""'api_version'"", ')', '!=', 'self', '.', '_api_version', ']', 'all_union_fields', '=', '[', 'spec', 'for', 'spec', 'in', 'self', '.', '_validation_specs', 'if', 'spec', '.', 'get', '(', ""'type'"", ')', '==', ""'union'"", ']', 'for', 'union_field', 'in', 'all_union_fields', ':', 'all_field_names', '.', 'extend', '(', '[', 'nested_union_spec', '[', ""'name'"", ']', 'for', 'nested_union_spec', 'in', 'union_field', '[', ""'fields'"", ']', 'if', 'nested_union_spec', '.', 'get', '(', ""'type'"", ')', '!=', ""'union'"", 'and', 'nested_union_spec', '.', 'get', '(', ""'api_version'"", ')', '!=', 'self', '.', '_api_version', ']', ')', 'for', 'field_name', 'in', 'body_to_validate', '.', 'keys', '(', ')', ':', 'if', 'field_name', 'not', 'in', 'all_field_names', ':', 'self', '.', 'log', '.', 'warning', '(', '""The field \'%s\' is in the body, but is not specified in the ""', '""validation specification \'%s\'. ""', '""This might be because you are using newer API version and ""', '""new field names defined for that version. Then the warning ""', '""can be safely ignored, or you might want to upgrade the operator""', '""to the version that supports the new API version.""', ',', 'field_name', ',', 'self', '.', '_validation_specs', ')']","Validates if the body (dictionary) follows specification that the validator was
        instantiated with. Raises ValidationSpecificationException or
        ValidationFieldException in case of problems with specification or the
        body not conforming to the specification respectively.

        :param body_to_validate: body that must follow the specification
        :type body_to_validate: dict
        :return: None","['Validates', 'if', 'the', 'body', '(', 'dictionary', ')', 'follows', 'specification', 'that', 'the', 'validator', 'was', 'instantiated', 'with', '.', 'Raises', 'ValidationSpecificationException', 'or', 'ValidationFieldException', 'in', 'case', 'of', 'problems', 'with', 'specification', 'or', 'the', 'body', 'not', 'conforming', 'to', 'the', 'specification', 'respectively', '.']",python,test,"['validates', 'if', 'the', 'body', '(', 'dictionary', ')', 'follows', 'specification', 'that', 'the', 'validator', 'was', 'instantiated', 'with', '.', 'raises', 'validationspecificationexception', 'or', 'validationfieldexception', 'in', 'case', 'of', 'problems', 'with', 'specification', 'or', 'the', 'body', 'not', 'conforming', 'to', 'the', 'specification', 'respectively', '.']",validates if the body ( dictionary ) follows specification that the validator was instantiated with . raises validationspecificationexception or validationfieldexception in case of problems with specification or the body not conforming to the specification respectively .,"['def', 'validate', '(', 'self', ',', 'body_to_validate', ')', ':', 'try', ':', 'for', 'validation_spec', 'in', 'self', '.', '_validation_specs', ':', 'self', '.', '_validate_field', '(', 'validation_spec', '=', 'validation_spec', ',', 'dictionary_to_validate', '=', 'body_to_validate', ')', 'except', 'gcpfieldvalidationexception', 'as', 'e', ':', 'raise', 'gcpfieldvalidationexception', '(', '""there was an error when validating: body \'{}\': \'{}\'""', '.', 'format', '(', 'body_to_validate', ',', 'e', ')', ')', 'all_field_names', '=', '[', 'spec', '[', ""'name'"", ']', 'for', 'spec', 'in', 'self', '.', '_validation_specs', 'if', 'spec', '.', 'get', '(', ""'type'"", ')', '!=', ""'union'"", 'and', 'spec', '.', 'get', '(', ""'api_version'"", ')', '!=', 'self', '.', '_api_version', ']', 'all_union_fields', '=', '[', 'spec', 'for', 'spec', 'in', 'self', '.', '_validation_specs', 'if', 'spec', '.', 'get', '(', ""'type'"", ')', '==', ""'union'"", ']', 'for', 'union_field', 'in', 'all_union_fields', ':', 'all_field_names', '.', 'extend', '(', '[', 'nested_union_spec', '[', ""'name'"", ']', 'for', 'nested_union_spec', 'in', 'union_field', '[', ""'fields'"", ']', 'if', 'nested_union_spec', '.', 'get', '(', ""'type'"", ')', '!=', ""'union'"", 'and', 'nested_union_spec', '.', 'get', '(', ""'api_version'"", ')', '!=', 'self', '.', '_api_version', ']', ')', 'for', 'field_name', 'in', 'body_to_validate', '.', 'keys', '(', ')', ':', 'if', 'field_name', 'not', 'in', 'all_field_names', ':', 'self', '.', 'log', '.', 'warning', '(', '""the field \'%s\' is in the body, but is not specified in the ""', '""validation specification \'%s\'. ""', '""this might be because you are using newer api version and ""', '""new field names defined for that version. then the warning ""', '""can be safely ignored, or you might want to upgrade the operator""', '""to the version that supports the new api version.""', ',', 'field_name', ',', 'self', '.', '_validation_specs', ')']","def validate ( self , body_to_validate ) : try : for validation_spec in self . _validation_specs : self . _validate_field ( validation_spec = validation_spec , dictionary_to_validate = body_to_validate ) except gcpfieldvalidationexception as e : raise gcpfieldvalidationexception ( ""there was an error when validating: body '{}': '{}'"" . format ( body_to_validate , e ) ) all_field_names = [ spec [ 'name' ] for spec in self . _validation_specs if spec . get ( 'type' ) != 'union' and spec . get ( 'api_version' ) != self . _api_version ] all_union_fields = [ spec for spec in self . _validation_specs if spec . get ( 'type' ) == 'union' ] for union_field in all_union_fields : all_field_names . extend ( [ nested_union_spec [ 'name' ] for nested_union_spec in union_field [ 'fields' ] if nested_union_spec . get ( 'type' ) != 'union' and nested_union_spec . get ( 'api_version' ) != self . _api_version ] ) for field_name in body_to_validate . keys ( ) : if field_name not in all_field_names : self . log . warning ( ""the field '%s' is in the body, but is not specified in the "" ""validation specification '%s'. "" ""this might be because you are using newer api version and "" ""new field names defined for that version. then the warning "" ""can be safely ignored, or you might want to upgrade the operator"" ""to the version that supports the new api version."" , field_name , self . _validation_specs )"
487,apache/airflow,airflow/contrib/hooks/azure_fileshare_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_fileshare_hook.py#L40-L45,"def get_conn(self):
        """"""Return the FileService object.""""""
        conn = self.get_connection(self.conn_id)
        service_options = conn.extra_dejson
        return FileService(account_name=conn.login,
                           account_key=conn.password, **service_options)","['def', 'get_conn', '(', 'self', ')', ':', 'conn', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'conn_id', ')', 'service_options', '=', 'conn', '.', 'extra_dejson', 'return', 'FileService', '(', 'account_name', '=', 'conn', '.', 'login', ',', 'account_key', '=', 'conn', '.', 'password', ',', '*', '*', 'service_options', ')']",Return the FileService object.,"['Return', 'the', 'FileService', 'object', '.']",python,test,"['return', 'the', 'fileservice', 'object', '.']",return the fileservice object .,"['def', 'get_conn', '(', 'self', ')', ':', 'conn', '=', 'self', '.', 'get_connection', '(', 'self', '.', 'conn_id', ')', 'service_options', '=', 'conn', '.', 'extra_dejson', 'return', 'fileservice', '(', 'account_name', '=', 'conn', '.', 'login', ',', 'account_key', '=', 'conn', '.', 'password', ',', '*', '*', 'service_options', ')']","def get_conn ( self ) : conn = self . get_connection ( self . conn_id ) service_options = conn . extra_dejson return fileservice ( account_name = conn . login , account_key = conn . password , * * service_options )"
488,apache/airflow,airflow/contrib/hooks/azure_fileshare_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_fileshare_hook.py#L47-L62,"def check_for_directory(self, share_name, directory_name, **kwargs):
        """"""
        Check if a directory exists on Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.exists()` takes.
        :type kwargs: object
        :return: True if the file exists, False otherwise.
        :rtype: bool
        """"""
        return self.connection.exists(share_name, directory_name,
                                      **kwargs)","['def', 'check_for_directory', '(', 'self', ',', 'share_name', ',', 'directory_name', ',', '*', '*', 'kwargs', ')', ':', 'return', 'self', '.', 'connection', '.', 'exists', '(', 'share_name', ',', 'directory_name', ',', '*', '*', 'kwargs', ')']","Check if a directory exists on Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.exists()` takes.
        :type kwargs: object
        :return: True if the file exists, False otherwise.
        :rtype: bool","['Check', 'if', 'a', 'directory', 'exists', 'on', 'Azure', 'File', 'Share', '.']",python,test,"['check', 'if', 'a', 'directory', 'exists', 'on', 'azure', 'file', 'share', '.']",check if a directory exists on azure file share .,"['def', 'check_for_directory', '(', 'self', ',', 'share_name', ',', 'directory_name', ',', '*', '*', 'kwargs', ')', ':', 'return', 'self', '.', 'connection', '.', 'exists', '(', 'share_name', ',', 'directory_name', ',', '*', '*', 'kwargs', ')']","def check_for_directory ( self , share_name , directory_name , * * kwargs ) : return self . connection . exists ( share_name , directory_name , * * kwargs )"
489,apache/airflow,airflow/contrib/hooks/azure_fileshare_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_fileshare_hook.py#L64-L81,"def check_for_file(self, share_name, directory_name, file_name, **kwargs):
        """"""
        Check if a file exists on Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.exists()` takes.
        :type kwargs: object
        :return: True if the file exists, False otherwise.
        :rtype: bool
        """"""
        return self.connection.exists(share_name, directory_name,
                                      file_name, **kwargs)","['def', 'check_for_file', '(', 'self', ',', 'share_name', ',', 'directory_name', ',', 'file_name', ',', '*', '*', 'kwargs', ')', ':', 'return', 'self', '.', 'connection', '.', 'exists', '(', 'share_name', ',', 'directory_name', ',', 'file_name', ',', '*', '*', 'kwargs', ')']","Check if a file exists on Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.exists()` takes.
        :type kwargs: object
        :return: True if the file exists, False otherwise.
        :rtype: bool","['Check', 'if', 'a', 'file', 'exists', 'on', 'Azure', 'File', 'Share', '.']",python,test,"['check', 'if', 'a', 'file', 'exists', 'on', 'azure', 'file', 'share', '.']",check if a file exists on azure file share .,"['def', 'check_for_file', '(', 'self', ',', 'share_name', ',', 'directory_name', ',', 'file_name', ',', '*', '*', 'kwargs', ')', ':', 'return', 'self', '.', 'connection', '.', 'exists', '(', 'share_name', ',', 'directory_name', ',', 'file_name', ',', '*', '*', 'kwargs', ')']","def check_for_file ( self , share_name , directory_name , file_name , * * kwargs ) : return self . connection . exists ( share_name , directory_name , file_name , * * kwargs )"
490,apache/airflow,airflow/contrib/hooks/azure_fileshare_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_fileshare_hook.py#L83-L99,"def list_directories_and_files(self, share_name, directory_name=None, **kwargs):
        """"""
        Return the list of directories and files stored on a Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.list_directories_and_files()` takes.
        :type kwargs: object
        :return: A list of files and directories
        :rtype: list
        """"""
        return self.connection.list_directories_and_files(share_name,
                                                          directory_name,
                                                          **kwargs)","['def', 'list_directories_and_files', '(', 'self', ',', 'share_name', ',', 'directory_name', '=', 'None', ',', '*', '*', 'kwargs', ')', ':', 'return', 'self', '.', 'connection', '.', 'list_directories_and_files', '(', 'share_name', ',', 'directory_name', ',', '*', '*', 'kwargs', ')']","Return the list of directories and files stored on a Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.list_directories_and_files()` takes.
        :type kwargs: object
        :return: A list of files and directories
        :rtype: list","['Return', 'the', 'list', 'of', 'directories', 'and', 'files', 'stored', 'on', 'a', 'Azure', 'File', 'Share', '.']",python,test,"['return', 'the', 'list', 'of', 'directories', 'and', 'files', 'stored', 'on', 'a', 'azure', 'file', 'share', '.']",return the list of directories and files stored on a azure file share .,"['def', 'list_directories_and_files', '(', 'self', ',', 'share_name', ',', 'directory_name', '=', 'none', ',', '*', '*', 'kwargs', ')', ':', 'return', 'self', '.', 'connection', '.', 'list_directories_and_files', '(', 'share_name', ',', 'directory_name', ',', '*', '*', 'kwargs', ')']","def list_directories_and_files ( self , share_name , directory_name = none , * * kwargs ) : return self . connection . list_directories_and_files ( share_name , directory_name , * * kwargs )"
491,apache/airflow,airflow/contrib/hooks/azure_fileshare_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_fileshare_hook.py#L101-L115,"def create_directory(self, share_name, directory_name, **kwargs):
        """"""
        Create a new directory on a Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.create_directory()` takes.
        :type kwargs: object
        :return: A list of files and directories
        :rtype: list
        """"""
        return self.connection.create_directory(share_name, directory_name, **kwargs)","['def', 'create_directory', '(', 'self', ',', 'share_name', ',', 'directory_name', ',', '*', '*', 'kwargs', ')', ':', 'return', 'self', '.', 'connection', '.', 'create_directory', '(', 'share_name', ',', 'directory_name', ',', '*', '*', 'kwargs', ')']","Create a new directory on a Azure File Share.

        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.create_directory()` takes.
        :type kwargs: object
        :return: A list of files and directories
        :rtype: list","['Create', 'a', 'new', 'directory', 'on', 'a', 'Azure', 'File', 'Share', '.']",python,test,"['create', 'a', 'new', 'directory', 'on', 'a', 'azure', 'file', 'share', '.']",create a new directory on a azure file share .,"['def', 'create_directory', '(', 'self', ',', 'share_name', ',', 'directory_name', ',', '*', '*', 'kwargs', ')', ':', 'return', 'self', '.', 'connection', '.', 'create_directory', '(', 'share_name', ',', 'directory_name', ',', '*', '*', 'kwargs', ')']","def create_directory ( self , share_name , directory_name , * * kwargs ) : return self . connection . create_directory ( share_name , directory_name , * * kwargs )"
492,apache/airflow,airflow/contrib/hooks/azure_fileshare_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_fileshare_hook.py#L117-L134,"def get_file(self, file_path, share_name, directory_name, file_name, **kwargs):
        """"""
        Download a file from Azure File Share.

        :param file_path: Where to store the file.
        :type file_path: str
        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.get_file_to_path()` takes.
        :type kwargs: object
        """"""
        self.connection.get_file_to_path(share_name, directory_name,
                                         file_name, file_path, **kwargs)","['def', 'get_file', '(', 'self', ',', 'file_path', ',', 'share_name', ',', 'directory_name', ',', 'file_name', ',', '*', '*', 'kwargs', ')', ':', 'self', '.', 'connection', '.', 'get_file_to_path', '(', 'share_name', ',', 'directory_name', ',', 'file_name', ',', 'file_path', ',', '*', '*', 'kwargs', ')']","Download a file from Azure File Share.

        :param file_path: Where to store the file.
        :type file_path: str
        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.get_file_to_path()` takes.
        :type kwargs: object","['Download', 'a', 'file', 'from', 'Azure', 'File', 'Share', '.']",python,test,"['download', 'a', 'file', 'from', 'azure', 'file', 'share', '.']",download a file from azure file share .,"['def', 'get_file', '(', 'self', ',', 'file_path', ',', 'share_name', ',', 'directory_name', ',', 'file_name', ',', '*', '*', 'kwargs', ')', ':', 'self', '.', 'connection', '.', 'get_file_to_path', '(', 'share_name', ',', 'directory_name', ',', 'file_name', ',', 'file_path', ',', '*', '*', 'kwargs', ')']","def get_file ( self , file_path , share_name , directory_name , file_name , * * kwargs ) : self . connection . get_file_to_path ( share_name , directory_name , file_name , file_path , * * kwargs )"
493,apache/airflow,airflow/contrib/hooks/azure_fileshare_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_fileshare_hook.py#L136-L153,"def get_file_to_stream(self, stream, share_name, directory_name, file_name, **kwargs):
        """"""
        Download a file from Azure File Share.

        :param stream: A filehandle to store the file to.
        :type stream: file-like object
        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.get_file_to_stream()` takes.
        :type kwargs: object
        """"""
        self.connection.get_file_to_stream(share_name, directory_name,
                                           file_name, stream, **kwargs)","['def', 'get_file_to_stream', '(', 'self', ',', 'stream', ',', 'share_name', ',', 'directory_name', ',', 'file_name', ',', '*', '*', 'kwargs', ')', ':', 'self', '.', 'connection', '.', 'get_file_to_stream', '(', 'share_name', ',', 'directory_name', ',', 'file_name', ',', 'stream', ',', '*', '*', 'kwargs', ')']","Download a file from Azure File Share.

        :param stream: A filehandle to store the file to.
        :type stream: file-like object
        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.get_file_to_stream()` takes.
        :type kwargs: object","['Download', 'a', 'file', 'from', 'Azure', 'File', 'Share', '.']",python,test,"['download', 'a', 'file', 'from', 'azure', 'file', 'share', '.']",download a file from azure file share .,"['def', 'get_file_to_stream', '(', 'self', ',', 'stream', ',', 'share_name', ',', 'directory_name', ',', 'file_name', ',', '*', '*', 'kwargs', ')', ':', 'self', '.', 'connection', '.', 'get_file_to_stream', '(', 'share_name', ',', 'directory_name', ',', 'file_name', ',', 'stream', ',', '*', '*', 'kwargs', ')']","def get_file_to_stream ( self , stream , share_name , directory_name , file_name , * * kwargs ) : self . connection . get_file_to_stream ( share_name , directory_name , file_name , stream , * * kwargs )"
494,apache/airflow,airflow/contrib/hooks/azure_fileshare_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_fileshare_hook.py#L155-L172,"def load_file(self, file_path, share_name, directory_name, file_name, **kwargs):
        """"""
        Upload a file to Azure File Share.

        :param file_path: Path to the file to load.
        :type file_path: str
        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.create_file_from_path()` takes.
        :type kwargs: object
        """"""
        self.connection.create_file_from_path(share_name, directory_name,
                                              file_name, file_path, **kwargs)","['def', 'load_file', '(', 'self', ',', 'file_path', ',', 'share_name', ',', 'directory_name', ',', 'file_name', ',', '*', '*', 'kwargs', ')', ':', 'self', '.', 'connection', '.', 'create_file_from_path', '(', 'share_name', ',', 'directory_name', ',', 'file_name', ',', 'file_path', ',', '*', '*', 'kwargs', ')']","Upload a file to Azure File Share.

        :param file_path: Path to the file to load.
        :type file_path: str
        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.create_file_from_path()` takes.
        :type kwargs: object","['Upload', 'a', 'file', 'to', 'Azure', 'File', 'Share', '.']",python,test,"['upload', 'a', 'file', 'to', 'azure', 'file', 'share', '.']",upload a file to azure file share .,"['def', 'load_file', '(', 'self', ',', 'file_path', ',', 'share_name', ',', 'directory_name', ',', 'file_name', ',', '*', '*', 'kwargs', ')', ':', 'self', '.', 'connection', '.', 'create_file_from_path', '(', 'share_name', ',', 'directory_name', ',', 'file_name', ',', 'file_path', ',', '*', '*', 'kwargs', ')']","def load_file ( self , file_path , share_name , directory_name , file_name , * * kwargs ) : self . connection . create_file_from_path ( share_name , directory_name , file_name , file_path , * * kwargs )"
495,apache/airflow,airflow/contrib/hooks/azure_fileshare_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_fileshare_hook.py#L174-L191,"def load_string(self, string_data, share_name, directory_name, file_name, **kwargs):
        """"""
        Upload a string to Azure File Share.

        :param string_data: String to load.
        :type string_data: str
        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.create_file_from_text()` takes.
        :type kwargs: object
        """"""
        self.connection.create_file_from_text(share_name, directory_name,
                                              file_name, string_data, **kwargs)","['def', 'load_string', '(', 'self', ',', 'string_data', ',', 'share_name', ',', 'directory_name', ',', 'file_name', ',', '*', '*', 'kwargs', ')', ':', 'self', '.', 'connection', '.', 'create_file_from_text', '(', 'share_name', ',', 'directory_name', ',', 'file_name', ',', 'string_data', ',', '*', '*', 'kwargs', ')']","Upload a string to Azure File Share.

        :param string_data: String to load.
        :type string_data: str
        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param kwargs: Optional keyword arguments that
            `FileService.create_file_from_text()` takes.
        :type kwargs: object","['Upload', 'a', 'string', 'to', 'Azure', 'File', 'Share', '.']",python,test,"['upload', 'a', 'string', 'to', 'azure', 'file', 'share', '.']",upload a string to azure file share .,"['def', 'load_string', '(', 'self', ',', 'string_data', ',', 'share_name', ',', 'directory_name', ',', 'file_name', ',', '*', '*', 'kwargs', ')', ':', 'self', '.', 'connection', '.', 'create_file_from_text', '(', 'share_name', ',', 'directory_name', ',', 'file_name', ',', 'string_data', ',', '*', '*', 'kwargs', ')']","def load_string ( self , string_data , share_name , directory_name , file_name , * * kwargs ) : self . connection . create_file_from_text ( share_name , directory_name , file_name , string_data , * * kwargs )"
496,apache/airflow,airflow/contrib/hooks/azure_fileshare_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/azure_fileshare_hook.py#L193-L212,"def load_stream(self, stream, share_name, directory_name, file_name, count, **kwargs):
        """"""
        Upload a stream to Azure File Share.

        :param stream: Opened file/stream to upload as the file content.
        :type stream: file-like
        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param count: Size of the stream in bytes
        :type count: int
        :param kwargs: Optional keyword arguments that
            `FileService.create_file_from_stream()` takes.
        :type kwargs: object
        """"""
        self.connection.create_file_from_stream(share_name, directory_name,
                                                file_name, stream, count, **kwargs)","['def', 'load_stream', '(', 'self', ',', 'stream', ',', 'share_name', ',', 'directory_name', ',', 'file_name', ',', 'count', ',', '*', '*', 'kwargs', ')', ':', 'self', '.', 'connection', '.', 'create_file_from_stream', '(', 'share_name', ',', 'directory_name', ',', 'file_name', ',', 'stream', ',', 'count', ',', '*', '*', 'kwargs', ')']","Upload a stream to Azure File Share.

        :param stream: Opened file/stream to upload as the file content.
        :type stream: file-like
        :param share_name: Name of the share.
        :type share_name: str
        :param directory_name: Name of the directory.
        :type directory_name: str
        :param file_name: Name of the file.
        :type file_name: str
        :param count: Size of the stream in bytes
        :type count: int
        :param kwargs: Optional keyword arguments that
            `FileService.create_file_from_stream()` takes.
        :type kwargs: object","['Upload', 'a', 'stream', 'to', 'Azure', 'File', 'Share', '.']",python,test,"['upload', 'a', 'stream', 'to', 'azure', 'file', 'share', '.']",upload a stream to azure file share .,"['def', 'load_stream', '(', 'self', ',', 'stream', ',', 'share_name', ',', 'directory_name', ',', 'file_name', ',', 'count', ',', '*', '*', 'kwargs', ')', ':', 'self', '.', 'connection', '.', 'create_file_from_stream', '(', 'share_name', ',', 'directory_name', ',', 'file_name', ',', 'stream', ',', 'count', ',', '*', '*', 'kwargs', ')']","def load_stream ( self , stream , share_name , directory_name , file_name , count , * * kwargs ) : self . connection . create_file_from_stream ( share_name , directory_name , file_name , stream , count , * * kwargs )"
497,apache/airflow,airflow/utils/log/file_processor_handler.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/file_processor_handler.py#L60-L72,"def set_context(self, filename):
        """"""
        Provide filename context to airflow task handler.
        :param filename: filename in which the dag is located
        """"""
        local_loc = self._init_file(filename)
        self.handler = logging.FileHandler(local_loc)
        self.handler.setFormatter(self.formatter)
        self.handler.setLevel(self.level)

        if self._cur_date < datetime.today():
            self._symlink_latest_log_directory()
            self._cur_date = datetime.today()","['def', 'set_context', '(', 'self', ',', 'filename', ')', ':', 'local_loc', '=', 'self', '.', '_init_file', '(', 'filename', ')', 'self', '.', 'handler', '=', 'logging', '.', 'FileHandler', '(', 'local_loc', ')', 'self', '.', 'handler', '.', 'setFormatter', '(', 'self', '.', 'formatter', ')', 'self', '.', 'handler', '.', 'setLevel', '(', 'self', '.', 'level', ')', 'if', 'self', '.', '_cur_date', '<', 'datetime', '.', 'today', '(', ')', ':', 'self', '.', '_symlink_latest_log_directory', '(', ')', 'self', '.', '_cur_date', '=', 'datetime', '.', 'today', '(', ')']","Provide filename context to airflow task handler.
        :param filename: filename in which the dag is located","['Provide', 'filename', 'context', 'to', 'airflow', 'task', 'handler', '.', ':', 'param', 'filename', ':', 'filename', 'in', 'which', 'the', 'dag', 'is', 'located']",python,test,"['provide', 'filename', 'context', 'to', 'airflow', 'task', 'handler', '.', ':', 'param', 'filename', ':', 'filename', 'in', 'which', 'the', 'dag', 'is', 'located']",provide filename context to airflow task handler . : param filename : filename in which the dag is located,"['def', 'set_context', '(', 'self', ',', 'filename', ')', ':', 'local_loc', '=', 'self', '.', '_init_file', '(', 'filename', ')', 'self', '.', 'handler', '=', 'logging', '.', 'filehandler', '(', 'local_loc', ')', 'self', '.', 'handler', '.', 'setformatter', '(', 'self', '.', 'formatter', ')', 'self', '.', 'handler', '.', 'setlevel', '(', 'self', '.', 'level', ')', 'if', 'self', '.', '_cur_date', '<', 'datetime', '.', 'today', '(', ')', ':', 'self', '.', '_symlink_latest_log_directory', '(', ')', 'self', '.', '_cur_date', '=', 'datetime', '.', 'today', '(', ')']","def set_context ( self , filename ) : local_loc = self . _init_file ( filename ) self . handler = logging . filehandler ( local_loc ) self . handler . setformatter ( self . formatter ) self . handler . setlevel ( self . level ) if self . _cur_date < datetime . today ( ) : self . _symlink_latest_log_directory ( ) self . _cur_date = datetime . today ( )"
498,apache/airflow,airflow/utils/log/file_processor_handler.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/log/file_processor_handler.py#L129-L149,"def _init_file(self, filename):
        """"""
        Create log file and directory if required.
        :param filename: task instance object
        :return: relative log path of the given task instance
        """"""
        relative_path = self._render_filename(filename)
        full_path = os.path.join(self._get_log_directory(), relative_path)
        directory = os.path.dirname(full_path)

        if not os.path.exists(directory):
            try:
                os.makedirs(directory)
            except OSError:
                if not os.path.isdir(directory):
                    raise

        if not os.path.exists(full_path):
            open(full_path, ""a"").close()

        return full_path","['def', '_init_file', '(', 'self', ',', 'filename', ')', ':', 'relative_path', '=', 'self', '.', '_render_filename', '(', 'filename', ')', 'full_path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', '_get_log_directory', '(', ')', ',', 'relative_path', ')', 'directory', '=', 'os', '.', 'path', '.', 'dirname', '(', 'full_path', ')', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'directory', ')', ':', 'try', ':', 'os', '.', 'makedirs', '(', 'directory', ')', 'except', 'OSError', ':', 'if', 'not', 'os', '.', 'path', '.', 'isdir', '(', 'directory', ')', ':', 'raise', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'full_path', ')', ':', 'open', '(', 'full_path', ',', '""a""', ')', '.', 'close', '(', ')', 'return', 'full_path']","Create log file and directory if required.
        :param filename: task instance object
        :return: relative log path of the given task instance","['Create', 'log', 'file', 'and', 'directory', 'if', 'required', '.', ':', 'param', 'filename', ':', 'task', 'instance', 'object', ':', 'return', ':', 'relative', 'log', 'path', 'of', 'the', 'given', 'task', 'instance']",python,test,"['create', 'log', 'file', 'and', 'directory', 'if', 'required', '.', ':', 'param', 'filename', ':', 'task', 'instance', 'object', ':', 'return', ':', 'relative', 'log', 'path', 'of', 'the', 'given', 'task', 'instance']",create log file and directory if required . : param filename : task instance object : return : relative log path of the given task instance,"['def', '_init_file', '(', 'self', ',', 'filename', ')', ':', 'relative_path', '=', 'self', '.', '_render_filename', '(', 'filename', ')', 'full_path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', '_get_log_directory', '(', ')', ',', 'relative_path', ')', 'directory', '=', 'os', '.', 'path', '.', 'dirname', '(', 'full_path', ')', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'directory', ')', ':', 'try', ':', 'os', '.', 'makedirs', '(', 'directory', ')', 'except', 'oserror', ':', 'if', 'not', 'os', '.', 'path', '.', 'isdir', '(', 'directory', ')', ':', 'raise', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'full_path', ')', ':', 'open', '(', 'full_path', ',', '""a""', ')', '.', 'close', '(', ')', 'return', 'full_path']","def _init_file ( self , filename ) : relative_path = self . _render_filename ( filename ) full_path = os . path . join ( self . _get_log_directory ( ) , relative_path ) directory = os . path . dirname ( full_path ) if not os . path . exists ( directory ) : try : os . makedirs ( directory ) except oserror : if not os . path . isdir ( directory ) : raise if not os . path . exists ( full_path ) : open ( full_path , ""a"" ) . close ( ) return full_path"
499,apache/airflow,airflow/contrib/hooks/gcs_hook.py,https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/contrib/hooks/gcs_hook.py#L553-L566,"def _parse_gcs_url(gsurl):
    """"""
    Given a Google Cloud Storage URL (gs://<bucket>/<blob>), returns a
    tuple containing the corresponding bucket and blob.
    """"""

    parsed_url = urlparse(gsurl)
    if not parsed_url.netloc:
        raise AirflowException('Please provide a bucket name')
    else:
        bucket = parsed_url.netloc
        # Remove leading '/' but NOT trailing one
        blob = parsed_url.path.lstrip('/')
        return bucket, blob","['def', '_parse_gcs_url', '(', 'gsurl', ')', ':', 'parsed_url', '=', 'urlparse', '(', 'gsurl', ')', 'if', 'not', 'parsed_url', '.', 'netloc', ':', 'raise', 'AirflowException', '(', ""'Please provide a bucket name'"", ')', 'else', ':', 'bucket', '=', 'parsed_url', '.', 'netloc', ""# Remove leading '/' but NOT trailing one"", 'blob', '=', 'parsed_url', '.', 'path', '.', 'lstrip', '(', ""'/'"", ')', 'return', 'bucket', ',', 'blob']","Given a Google Cloud Storage URL (gs://<bucket>/<blob>), returns a
    tuple containing the corresponding bucket and blob.","['Given', 'a', 'Google', 'Cloud', 'Storage', 'URL', '(', 'gs', ':', '//', '<bucket', '>', '/', '<blob', '>', ')', 'returns', 'a', 'tuple', 'containing', 'the', 'corresponding', 'bucket', 'and', 'blob', '.']",python,test,"['given', 'a', 'google', 'cloud', 'storage', 'url', '(', 'gs', ':', '//', '<bucket', '>', '/', '<blob', '>', ')', 'returns', 'a', 'tuple', 'containing', 'the', 'corresponding', 'bucket', 'and', 'blob', '.']",given a google cloud storage url ( gs : // <bucket > / <blob > ) returns a tuple containing the corresponding bucket and blob .,"['def', '_parse_gcs_url', '(', 'gsurl', ')', ':', 'parsed_url', '=', 'urlparse', '(', 'gsurl', ')', 'if', 'not', 'parsed_url', '.', 'netloc', ':', 'raise', 'airflowexception', '(', ""'please provide a bucket name'"", ')', 'else', ':', 'bucket', '=', 'parsed_url', '.', 'netloc', ""# remove leading '/' but not trailing one"", 'blob', '=', 'parsed_url', '.', 'path', '.', 'lstrip', '(', ""'/'"", ')', 'return', 'bucket', ',', 'blob']","def _parse_gcs_url ( gsurl ) : parsed_url = urlparse ( gsurl ) if not parsed_url . netloc : raise airflowexception ( 'please provide a bucket name' ) else : bucket = parsed_url . netloc # remove leading '/' but not trailing one blob = parsed_url . path . lstrip ( '/' ) return bucket , blob"
