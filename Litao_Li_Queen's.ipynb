{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Litao Li Queen's.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lxdragoon/Modified-mnist/blob/master/Litao_Li_Queen's.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADQrhZLrBrC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrkDYPSNBtWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://www.dropbox.com/sh/jn8p1pvpgjy3b9b/AABCIQRx7cVR1Vn0Cwqqqxjya/comp551w18-modified-mnist.zip\n",
        "!unzip comp551w18-modified-mnist.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv41OToCGpSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_ = np.loadtxt(\"train_x.csv\", delimiter=\",\")# load from text\n",
        "y_ = np.loadtxt(\"train_y.csv\", delimiter=\",\")\n",
        "x_train = x_.reshape(-1, 64, 64) # reshape\n",
        "y_train = y_.reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rpp7bKmCXhzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normalize to 0-1, optional\n",
        "x_train = x_train/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kadO--2lXsdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make y into one hot\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "onehot = OneHotEncoder(sparse = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAopBq7_DeYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape to 1 dimension to feed into model\n",
        "x_train = x_train.reshape(len(x_train),4096)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj_ROnYZzPs6",
        "colab_type": "code",
        "outputId": "75579e1a-447a-40ec-bb9e-83e047ba48c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# transform labels into one hot encoding\n",
        "y_train = onehot.fit_transform(y_train)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWgyXqlUDCRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split training data into train and test\n",
        "import sklearn.model_selection as sk\n",
        "\n",
        "X_train, X_test, Y_train, Y_test =  sk.train_test_split(x_train,y_train,test_size=0.2, random_state = 42)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2usyXUGvdUG",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L8rLQmKGpT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.01\n",
        "training_epochs = 50\n",
        "batch_size = 100\n",
        "display_step = 10\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None, 4096])\n",
        "y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# model weights                                                   \n",
        "W = tf.Variable(tf.zeros([4096, 10]))\n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "# Construct model\n",
        "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
        "\n",
        "# Minimize error using cross entropy\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
        "# Gradient Descent\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "# Initialize the variables\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7LZZFB-GpX4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4da5bf14-09d4-4220-b39a-86e16f6c5b03"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "\n",
        "    # Training cycle\n",
        "    for epoch in range(training_epochs):\n",
        "        avg_cost = 0.\n",
        "        total_batch = int(len(X_train)/batch_size)\n",
        "        #temp = tf.data.Dataset.from_tensor_slices((X_train,Y_train))\n",
        "        #temp = temp.shuffle(1000).batch(batch_size)\n",
        "        #iterator = temp.make_initializable_iterator()\n",
        "        #next_element = iterator.get_next()\n",
        "        #sess.run(iterator.initializer)\n",
        "        \n",
        "        # Loop over all batches\n",
        "        n=0\n",
        "        for i in range(total_batch):\n",
        "            if n+batch_size >= len(X_train):\n",
        "              n=0\n",
        "            #batch_xs, batch_ys = sess.run(next_element)\n",
        "            batch_xs = X_train[n:n+batch_size]\n",
        "            batch_ys = Y_train[n:n+batch_size]\n",
        "            n += batch_size\n",
        "            \n",
        "            # Fit training using batch data\n",
        "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
        "                                                          y: batch_ys})\n",
        "            \n",
        "            \n",
        "            # Compute average loss\n",
        "            avg_cost += c / total_batch\n",
        "            \n",
        "        # Display logs per epoch step\n",
        "        if (epoch+1) % display_step == 0:\n",
        "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    print(\"Accuracy:\", accuracy.eval({x: X_test, y: Y_test}))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0010 cost= 2.261566154\n",
            "Epoch: 0020 cost= 2.222527747\n",
            "Epoch: 0030 cost= 2.192996626\n",
            "Epoch: 0040 cost= 2.168911277\n",
            "Epoch: 0050 cost= 2.148548626\n",
            "Optimization Finished!\n",
            "Accuracy: 0.1298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd3ZucD-3yUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzMYEJGeKX83",
        "colab_type": "text"
      },
      "source": [
        "##CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKjMTdfqN35R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Parameters\n",
        "learning_rate = 0.01\n",
        "num_steps = 1000\n",
        "batch_size = 128\n",
        "display_step = 20\n",
        "\n",
        "# Network Parameters\n",
        "num_input = 4096\n",
        "num_classes = 10 # total classes (0-9 digits)\n",
        "dropout = 0.75 # Dropout, probability to keep units\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder(tf.float32, [None, num_input])\n",
        "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
        "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AsahuYwT5mq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wrappers\n",
        "def conv2d(x, W, b, strides=1):\n",
        "    # Conv2D wrapper, with bias and relu activation\n",
        "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    x = tf.nn.bias_add(x, b)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    # MaxPool2D wrapper\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
        "                          padding='SAME')\n",
        "\n",
        "\n",
        "# Create model\n",
        "def conv_net(x, weights, biases, dropout):\n",
        "    # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
        "    # Reshape to match picture format [Height x Width x Channel]\n",
        "    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
        "    x = tf.reshape(x, shape=[-1, 64, 64, 1])\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv1 = maxpool2d(conv1, k=2)\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv2 = maxpool2d(conv2, k=2)\n",
        "    \n",
        "    # Convolution Layer\n",
        "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv3 = maxpool2d(conv3, k=2)\n",
        "    \n",
        "        # Convolution Layer\n",
        "    conv4 = conv2d(conv3, weights['wc4'], biases['bc4'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv4 = maxpool2d(conv4, k=2)\n",
        "    \n",
        "        # Convolution Layer\n",
        "    conv5 = conv2d(conv4, weights['wc5'], biases['bc5'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv5 = maxpool2d(conv5, k=2)\n",
        "    \n",
        "        # Convolution Layer\n",
        "    conv6 = conv2d(conv5, weights['wc6'], biases['bc6'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv6 = maxpool2d(conv6, k=2)\n",
        "    \n",
        "        # Convolution Layer\n",
        "    conv7 = conv2d(conv6, weights['wc7'], biases['bc7'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv7 = maxpool2d(conv7, k=2)\n",
        "    \n",
        "        # Convolution Layer\n",
        "    conv8 = conv2d(conv7, weights['wc8'], biases['bc8'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv8 = maxpool2d(conv8, k=2)\n",
        "\n",
        "    # Fully connected layer\n",
        "    # Reshape conv2 output to fit fully connected layer input\n",
        "    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    #fc1 = tf.contrib.layers.flatten(conv2)\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    # Apply Dropout\n",
        "    fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    # Output, class prediction\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFHJYBhPT5or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    # 5x5 conv, 1 input, 32 outputs\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
        "    # 5x5 conv, 32 inputs, 64 outputs\n",
        "    'wc2': tf.Variable(tf.random_normal([3, 3, 32, 64])),\n",
        "    'wc3': tf.Variable(tf.random_normal([3, 3, 64, 128])),\n",
        "    'wc4': tf.Variable(tf.random_normal([3, 3, 128, 128])),\n",
        "    'wc5': tf.Variable(tf.random_normal([3, 3, 128, 128])),\n",
        "    'wc6': tf.Variable(tf.random_normal([3, 3, 128, 128])),\n",
        "    'wc7': tf.Variable(tf.random_normal([3, 3, 128, 128])),\n",
        "    'wc8': tf.Variable(tf.random_normal([3, 3, 128, 128])),\n",
        "    \n",
        "    'wd1': tf.Variable(tf.random_normal([8*8*128, 1024])),\n",
        "    # 1024 inputs, 10 outputs (class prediction)\n",
        "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bc3': tf.Variable(tf.random_normal([128])),\n",
        "    'bc4': tf.Variable(tf.random_normal([128])),\n",
        "    'bc5': tf.Variable(tf.random_normal([128])),\n",
        "    'bc6': tf.Variable(tf.random_normal([128])),\n",
        "    'bc7': tf.Variable(tf.random_normal([128])),\n",
        "    'bc8': tf.Variable(tf.random_normal([128])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
        "}\n",
        "\n",
        "# Construct model\n",
        "logits = conv_net(X, weights, biases, keep_prob)\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzcDYdAFT5q2",
        "colab_type": "code",
        "outputId": "d69fdef1-02fa-4a0a-d2a3-223a449c724a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    n=0\n",
        "    for step in range(1, num_steps+1):\n",
        "        if n+batch_size >= 40000:\n",
        "          n=0\n",
        "        batch_x = X_train[n:n+batch_size]\n",
        "        batch_y = Y_train[n:n+batch_size]\n",
        "        n = n+batch_size\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
        "                                                                 Y: batch_y,\n",
        "                                                                 keep_prob: 1.0})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Calculate accuracy for 500 test data points\n",
        "    print(\"Testing Accuracy:\", \\\n",
        "        sess.run(accuracy, feed_dict={X: X_test[500:1000],\n",
        "                                      Y: Y_test[500:1000],\n",
        "                                      keep_prob: 1.0}))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1, Minibatch Loss= 3190521.5000, Training Accuracy= 0.148\n",
            "Step 20, Minibatch Loss= 88746.1172, Training Accuracy= 0.148\n",
            "Step 40, Minibatch Loss= 23000.4199, Training Accuracy= 0.164\n",
            "Step 60, Minibatch Loss= 14551.3955, Training Accuracy= 0.102\n",
            "Step 80, Minibatch Loss= 8766.7188, Training Accuracy= 0.164\n",
            "Step 100, Minibatch Loss= 5913.6172, Training Accuracy= 0.109\n",
            "Step 120, Minibatch Loss= 3734.3452, Training Accuracy= 0.125\n",
            "Step 140, Minibatch Loss= 2489.9158, Training Accuracy= 0.078\n",
            "Step 160, Minibatch Loss= 1129.3057, Training Accuracy= 0.117\n",
            "Step 180, Minibatch Loss= 509.2296, Training Accuracy= 0.141\n",
            "Step 200, Minibatch Loss= 160.7523, Training Accuracy= 0.109\n",
            "Step 220, Minibatch Loss= 44.1629, Training Accuracy= 0.086\n",
            "Step 240, Minibatch Loss= 15.5151, Training Accuracy= 0.125\n",
            "Step 260, Minibatch Loss= 14.2335, Training Accuracy= 0.078\n",
            "Step 280, Minibatch Loss= 8.9820, Training Accuracy= 0.086\n",
            "Step 300, Minibatch Loss= 15.9836, Training Accuracy= 0.102\n",
            "Step 320, Minibatch Loss= 2.5199, Training Accuracy= 0.094\n",
            "Step 340, Minibatch Loss= 7.1287, Training Accuracy= 0.078\n",
            "Step 360, Minibatch Loss= 4.6227, Training Accuracy= 0.094\n",
            "Step 380, Minibatch Loss= 4.1984, Training Accuracy= 0.094\n",
            "Step 400, Minibatch Loss= 11.9308, Training Accuracy= 0.109\n",
            "Step 420, Minibatch Loss= 2.5384, Training Accuracy= 0.078\n",
            "Step 440, Minibatch Loss= 2.4063, Training Accuracy= 0.125\n",
            "Step 460, Minibatch Loss= 3.0673, Training Accuracy= 0.070\n",
            "Step 480, Minibatch Loss= 2.3683, Training Accuracy= 0.094\n",
            "Step 500, Minibatch Loss= 2.3071, Training Accuracy= 0.172\n",
            "Step 520, Minibatch Loss= 2.3109, Training Accuracy= 0.102\n",
            "Step 540, Minibatch Loss= 2.2784, Training Accuracy= 0.141\n",
            "Step 560, Minibatch Loss= 2.3370, Training Accuracy= 0.117\n",
            "Step 580, Minibatch Loss= 3.1758, Training Accuracy= 0.102\n",
            "Step 600, Minibatch Loss= 2.3082, Training Accuracy= 0.086\n",
            "Step 620, Minibatch Loss= 2.2741, Training Accuracy= 0.156\n",
            "Step 640, Minibatch Loss= 2.3093, Training Accuracy= 0.086\n",
            "Step 660, Minibatch Loss= 2.3037, Training Accuracy= 0.070\n",
            "Step 680, Minibatch Loss= 2.3194, Training Accuracy= 0.102\n",
            "Step 700, Minibatch Loss= 2.3089, Training Accuracy= 0.039\n",
            "Step 720, Minibatch Loss= 2.2995, Training Accuracy= 0.109\n",
            "Step 740, Minibatch Loss= 2.2979, Training Accuracy= 0.148\n",
            "Step 760, Minibatch Loss= 2.3002, Training Accuracy= 0.164\n",
            "Step 780, Minibatch Loss= 2.2908, Training Accuracy= 0.125\n",
            "Step 800, Minibatch Loss= 2.3026, Training Accuracy= 0.086\n",
            "Step 820, Minibatch Loss= 2.2996, Training Accuracy= 0.070\n",
            "Step 840, Minibatch Loss= 2.3266, Training Accuracy= 0.062\n",
            "Step 860, Minibatch Loss= 2.2901, Training Accuracy= 0.125\n",
            "Step 880, Minibatch Loss= 2.3047, Training Accuracy= 0.141\n",
            "Step 900, Minibatch Loss= 2.3088, Training Accuracy= 0.094\n",
            "Step 920, Minibatch Loss= 2.3142, Training Accuracy= 0.078\n",
            "Step 940, Minibatch Loss= 2.2984, Training Accuracy= 0.094\n",
            "Step 960, Minibatch Loss= 2.3093, Training Accuracy= 0.109\n",
            "Step 980, Minibatch Loss= 2.3211, Training Accuracy= 0.078\n",
            "Step 1000, Minibatch Loss= 2.3000, Training Accuracy= 0.117\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lorCt_dX1wkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPYmP5WHsU1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GmCfNf35aYk",
        "colab_type": "text"
      },
      "source": [
        "## RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pod5KMHV7EvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from tensorflow.contrib import rnn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TUch6PW5aIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "# Training Parameters\n",
        "learning_rate = 0.02\n",
        "training_steps = 10000\n",
        "batch_size = 128\n",
        "display_step = 200\n",
        "\n",
        "# Network Parameters\n",
        "num_input = 64 # data input: 64x64\n",
        "timesteps = 64 # timesteps\n",
        "num_hidden = 128 # hidden layer num of features\n",
        "num_classes = 10 # total classes (0-9 digits)\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
        "Y = tf.placeholder(\"float\", [None, num_classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7Y6v18ysU3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define weights\n",
        "weights = {\n",
        "    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n",
        "}\n",
        "biases = {\n",
        "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfsd_a--67fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RNN(x, weights, biases):\n",
        "\n",
        "    # Prepare data shape to match `rnn` function requirements\n",
        "    # Current data input shape: (batch_size, timesteps, n_input)\n",
        "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
        "\n",
        "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
        "    x = tf.unstack(x, timesteps, 1)\n",
        "\n",
        "    # Define a lstm cell with tensorflow\n",
        "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
        "\n",
        "    # Get lstm cell output\n",
        "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
        "\n",
        "    # Linear activation, using rnn inner loop last output\n",
        "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhJ-fVrY67h2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf.reset_default_graph()\n",
        "logits = RNN(X, weights, biases)\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Evaluate model (with test logits, for dropout to be disabled)\n",
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6TDhzXf67kE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "outputId": "14782498-4012-4d8a-e7c5-9fb9d53176d1"
      },
      "source": [
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    n = 0\n",
        "    for step in range(1, training_steps+1):\n",
        "        if n+batch_size >= 40000:\n",
        "          n= 0 \n",
        "        batch_x = X_train[n:n+batch_size]\n",
        "        batch_y = Y_train[n:n+batch_size]\n",
        "        n = n+batch_size\n",
        "        # Reshape data to get 64 seq of 64 elements\n",
        "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
        "                                                                 Y: batch_y})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Calculate accuracy for 500 test data points\n",
        "    test_data = X_test[1000:1500].reshape((-1, timesteps, num_input))\n",
        "    test_label = Y_test[1000:1500]\n",
        "    print(\"Testing Accuracy:\", \\\n",
        "        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1, Minibatch Loss= 4.3304, Training Accuracy= 0.148\n",
            "Step 200, Minibatch Loss= 2.2809, Training Accuracy= 0.125\n",
            "Step 400, Minibatch Loss= 2.2552, Training Accuracy= 0.203\n",
            "Step 600, Minibatch Loss= 2.2758, Training Accuracy= 0.102\n",
            "Step 800, Minibatch Loss= 2.2975, Training Accuracy= 0.109\n",
            "Step 1000, Minibatch Loss= 2.2795, Training Accuracy= 0.141\n",
            "Step 1200, Minibatch Loss= 2.2633, Training Accuracy= 0.102\n",
            "Step 1400, Minibatch Loss= 2.2359, Training Accuracy= 0.180\n",
            "Step 1600, Minibatch Loss= 2.2883, Training Accuracy= 0.188\n",
            "Step 1800, Minibatch Loss= 2.3011, Training Accuracy= 0.164\n",
            "Step 2000, Minibatch Loss= 2.2954, Training Accuracy= 0.109\n",
            "Step 2200, Minibatch Loss= 2.2864, Training Accuracy= 0.172\n",
            "Step 2400, Minibatch Loss= 2.2724, Training Accuracy= 0.156\n",
            "Step 2600, Minibatch Loss= 2.2821, Training Accuracy= 0.172\n",
            "Step 2800, Minibatch Loss= 2.2882, Training Accuracy= 0.133\n",
            "Step 3000, Minibatch Loss= 2.2554, Training Accuracy= 0.148\n",
            "Step 3200, Minibatch Loss= 2.2895, Training Accuracy= 0.133\n",
            "Step 3400, Minibatch Loss= 2.2604, Training Accuracy= 0.141\n",
            "Step 3600, Minibatch Loss= 2.2612, Training Accuracy= 0.141\n",
            "Step 3800, Minibatch Loss= 2.2995, Training Accuracy= 0.078\n",
            "Step 4000, Minibatch Loss= 2.2503, Training Accuracy= 0.125\n",
            "Step 4200, Minibatch Loss= 2.2588, Training Accuracy= 0.133\n",
            "Step 4400, Minibatch Loss= 2.2696, Training Accuracy= 0.180\n",
            "Step 4600, Minibatch Loss= 2.2367, Training Accuracy= 0.148\n",
            "Step 4800, Minibatch Loss= 2.2928, Training Accuracy= 0.148\n",
            "Step 5000, Minibatch Loss= 2.2128, Training Accuracy= 0.227\n",
            "Step 5200, Minibatch Loss= 2.2623, Training Accuracy= 0.203\n",
            "Step 5400, Minibatch Loss= 2.2508, Training Accuracy= 0.156\n",
            "Step 5600, Minibatch Loss= 2.2498, Training Accuracy= 0.148\n",
            "Step 5800, Minibatch Loss= 2.2387, Training Accuracy= 0.203\n",
            "Step 6000, Minibatch Loss= 2.2731, Training Accuracy= 0.117\n",
            "Step 6200, Minibatch Loss= 2.2526, Training Accuracy= 0.172\n",
            "Step 6400, Minibatch Loss= 2.2665, Training Accuracy= 0.141\n",
            "Step 6600, Minibatch Loss= 2.2607, Training Accuracy= 0.156\n",
            "Step 6800, Minibatch Loss= 2.2402, Training Accuracy= 0.164\n",
            "Step 7000, Minibatch Loss= 2.2203, Training Accuracy= 0.164\n",
            "Step 7200, Minibatch Loss= 2.2403, Training Accuracy= 0.141\n",
            "Step 7400, Minibatch Loss= 2.2414, Training Accuracy= 0.195\n",
            "Step 7600, Minibatch Loss= 2.2247, Training Accuracy= 0.211\n",
            "Step 7800, Minibatch Loss= 2.2680, Training Accuracy= 0.172\n",
            "Step 8000, Minibatch Loss= 2.2457, Training Accuracy= 0.148\n",
            "Step 8200, Minibatch Loss= 2.1919, Training Accuracy= 0.180\n",
            "Step 8400, Minibatch Loss= 2.2424, Training Accuracy= 0.195\n",
            "Step 8600, Minibatch Loss= 2.2429, Training Accuracy= 0.156\n",
            "Step 8800, Minibatch Loss= 2.1828, Training Accuracy= 0.188\n",
            "Step 9000, Minibatch Loss= 2.1993, Training Accuracy= 0.242\n",
            "Step 9200, Minibatch Loss= 2.2124, Training Accuracy= 0.188\n",
            "Step 9400, Minibatch Loss= 2.2536, Training Accuracy= 0.172\n",
            "Step 9600, Minibatch Loss= 2.2481, Training Accuracy= 0.227\n",
            "Step 9800, Minibatch Loss= 2.2098, Training Accuracy= 0.211\n",
            "Step 10000, Minibatch Loss= 2.1905, Training Accuracy= 0.258\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF6mXpui67mW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNSB-4dt67o5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-_a5v7g67rE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FnHjuOLF5Tm",
        "colab_type": "text"
      },
      "source": [
        "# Gated CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89WCSBuF67tR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Parameters\n",
        "learning_rate = 0.1\n",
        "num_steps = 500\n",
        "batch_size = 100\n",
        "display_step = 20\n",
        "\n",
        "# Network Parameters\n",
        "num_input = 4096\n",
        "num_classes = 10 #total classes (0-9 digits)\n",
        "dropout = 0.75 # Dropout, probability to keep units\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder(tf.float32, [None, num_input])\n",
        "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
        "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPVOUswnF7dQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv2d(x, W, b, V, c, num, strides=1):\n",
        "    # Conv2D wrapper, with bias and gates\n",
        "    #paddings = tf.constant([[0,0],[0,0],[num,0],[0,0]])\n",
        "    #x = tf.pad(x, paddings)\n",
        "    A = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    B = tf.nn.conv2d(x, V, strides=[1, strides, strides, 1], padding='SAME')\n",
        "    A = tf.nn.bias_add(A, b)\n",
        "    B = tf.nn.bias_add(B, c)\n",
        "    return tf.multiply(A, tf.nn.sigmoid(B))\n",
        "\n",
        "\n",
        "def maxpool2d(x, k=2):\n",
        "    # MaxPool2D wrapper\n",
        "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
        "                          padding='SAME')\n",
        "\n",
        "\n",
        "# Create model\n",
        "def conv_net(x, weights, biases, dropout):\n",
        "    \n",
        "    x = tf.reshape(x, shape=[-1, 64, 64, 1])\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv1 = conv2d(x, weights['wc1'], biases['bc1'], weights['wv1'], biases['bv1'], weights['c1'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv1 = maxpool2d(conv1, k=2)\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'], weights['wv2'], biases['bv2'], weights['c2'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv2 = maxpool2d(conv2, k=2)\n",
        "\n",
        "    # Convolution Layer\n",
        "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'], weights['wv3'], biases['bv3'], weights['c3'])\n",
        "    # Max Pooling (down-sampling)\n",
        "    conv2 = maxpool2d(conv3, k=2)\n",
        "\n",
        "    \n",
        "    \n",
        "    # Fully connected layer\n",
        "    # Reshape conv2 output to fit fully connected layer input\n",
        "    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
        "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
        "    fc1 = tf.nn.relu(fc1)\n",
        "    # Apply Dropout\n",
        "    fc1 = tf.nn.dropout(fc1, dropout)\n",
        "\n",
        "    # Output, class prediction\n",
        "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_teK8kHYF7fX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
        "    'wv1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
        "    \n",
        "    'c1': 5,\n",
        "    \n",
        "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
        "    'wv2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
        "    'c2': 3,\n",
        "    \n",
        "    'wc3': tf.Variable(tf.random_normal([3, 3, 64, 128])),\n",
        "    'wv3': tf.Variable(tf.random_normal([3, 3, 64, 128])),\n",
        "    'c3': 3,\n",
        " \n",
        "    'wd1': tf.Variable(tf.random_normal([16*16*128, 1024])),\n",
        "    # 1024 inputs, 10 outputs (class prediction)\n",
        "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'bc1': tf.Variable(tf.random_normal([32])),\n",
        "    'bv1': tf.Variable(tf.random_normal([32])),\n",
        "    'bc2': tf.Variable(tf.random_normal([64])),\n",
        "    'bv2': tf.Variable(tf.random_normal([64])),\n",
        "    'bc3': tf.Variable(tf.random_normal([128])),\n",
        "    'bv3': tf.Variable(tf.random_normal([128])),\n",
        "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
        "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
        "}\n",
        "\n",
        "# Construct model\n",
        "logits = conv_net(X, weights, biases, keep_prob)\n",
        "prediction = tf.nn.softmax(logits)\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "\n",
        "# Evaluate model\n",
        "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adhjB9fzF7hc",
        "colab_type": "code",
        "outputId": "5ffb2b44-1791-4c73-984b-3e06dc05c885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# Start training\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    n=0\n",
        "    for step in range(1, num_steps+1):\n",
        "        if n+batch_size >= 40000:\n",
        "          n=0\n",
        "        batch_x = X_train[n:n+batch_size]\n",
        "        batch_y = Y_train[n:n+batch_size]\n",
        "        n = n+batch_size\n",
        "        # Run optimization op (backprop)\n",
        "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
        "        if step % display_step == 0 or step == 1:\n",
        "            # Calculate batch loss and accuracy\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
        "                                                                 Y: batch_y,\n",
        "                                                                 keep_prob: 1.0})\n",
        "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
        "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
        "                  \"{:.3f}\".format(acc))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Calculate accuracy for 500 test data points\n",
        "    print(\"Testing Accuracy:\", \\\n",
        "        sess.run(accuracy, feed_dict={X: X_test[2000:2500],\n",
        "                                      Y: Y_test[2000:2500],\n",
        "                                      keep_prob: 1.0}))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 1, Minibatch Loss= 24301048.0000, Training Accuracy= 0.140\n",
            "Step 20, Minibatch Loss= 1239636.0000, Training Accuracy= 0.110\n",
            "Step 40, Minibatch Loss= 1312.2007, Training Accuracy= 0.100\n",
            "Step 60, Minibatch Loss= 2.3287, Training Accuracy= 0.110\n",
            "Step 80, Minibatch Loss= 2.3035, Training Accuracy= 0.080\n",
            "Step 100, Minibatch Loss= 42.6643, Training Accuracy= 0.110\n",
            "Step 120, Minibatch Loss= 2.3127, Training Accuracy= 0.070\n",
            "Step 140, Minibatch Loss= 2.3184, Training Accuracy= 0.090\n",
            "Step 160, Minibatch Loss= 145.9015, Training Accuracy= 0.100\n",
            "Step 180, Minibatch Loss= 2.3093, Training Accuracy= 0.100\n",
            "Step 200, Minibatch Loss= 2.2941, Training Accuracy= 0.120\n",
            "Step 220, Minibatch Loss= 2.2717, Training Accuracy= 0.100\n",
            "Step 240, Minibatch Loss= 2.2957, Training Accuracy= 0.160\n",
            "Step 260, Minibatch Loss= 2.3218, Training Accuracy= 0.100\n",
            "Step 280, Minibatch Loss= 2.2971, Training Accuracy= 0.130\n",
            "Step 300, Minibatch Loss= 2.3041, Training Accuracy= 0.140\n",
            "Step 320, Minibatch Loss= 2.3038, Training Accuracy= 0.090\n",
            "Step 340, Minibatch Loss= 2.3016, Training Accuracy= 0.100\n",
            "Step 360, Minibatch Loss= 2.3111, Training Accuracy= 0.080\n",
            "Step 380, Minibatch Loss= 2.3120, Training Accuracy= 0.070\n",
            "Step 400, Minibatch Loss= 2.3242, Training Accuracy= 0.080\n",
            "Step 420, Minibatch Loss= 2.2939, Training Accuracy= 0.160\n",
            "Step 440, Minibatch Loss= 2.3124, Training Accuracy= 0.120\n",
            "Step 460, Minibatch Loss= 2.3223, Training Accuracy= 0.050\n",
            "Step 480, Minibatch Loss= 2.2902, Training Accuracy= 0.120\n",
            "Step 500, Minibatch Loss= 2.3025, Training Accuracy= 0.070\n",
            "Optimization Finished!\n",
            "Testing Accuracy: 0.106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH_0w4d6oXSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPo21HuWeDw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jwEY5ineDzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSvLcH8NeD1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIGCDP8IeD3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}